# Comparing `tmp/fastdatasets-0.9.7.post0-py3-none-any.whl.zip` & `tmp/fastdatasets-0.9.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,54 @@
-Zip file size: 42841 bytes, number of entries: 37
--rw-rw-rw-  2.0 fat      116 b- defN 23-May-02 04:08 fastdatasets/__init__.py
--rw-rw-rw-  2.0 fat     2316 b- defN 23-May-07 14:24 fastdatasets/setup.py
--rw-rw-rw-  2.0 fat       76 b- defN 23-May-02 04:08 fastdatasets/common/__init__.py
--rw-rw-rw-  2.0 fat    14096 b- defN 23-May-02 04:08 fastdatasets/common/iterable_dataset.py
--rw-rw-rw-  2.0 fat    10767 b- defN 23-May-02 04:08 fastdatasets/common/random_dataset.py
--rw-rw-rw-  2.0 fat     4251 b- defN 23-May-07 14:23 fastdatasets/common/writer.py
--rw-rw-rw-  2.0 fat     1135 b- defN 23-May-02 04:08 fastdatasets/common/writer_object.py
--rw-rw-rw-  2.0 fat      209 b- defN 23-May-02 04:08 fastdatasets/leveldb/__init__.py
--rw-rw-rw-  2.0 fat     6031 b- defN 23-May-02 04:08 fastdatasets/leveldb/dataset.py
--rw-rw-rw-  2.0 fat     4489 b- defN 23-May-02 04:08 fastdatasets/leveldb/writer.py
--rw-rw-rw-  2.0 fat     7148 b- defN 23-May-02 04:08 fastdatasets/leveldb/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     4716 b- defN 23-May-02 04:08 fastdatasets/leveldb/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat      215 b- defN 23-May-02 04:08 fastdatasets/lmdb/__init__.py
--rw-rw-rw-  2.0 fat     6778 b- defN 23-May-02 04:08 fastdatasets/lmdb/dataset.py
--rw-rw-rw-  2.0 fat     4696 b- defN 23-May-02 04:08 fastdatasets/lmdb/writer.py
--rw-rw-rw-  2.0 fat     7765 b- defN 23-May-02 04:08 fastdatasets/lmdb/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     5796 b- defN 23-May-02 04:08 fastdatasets/lmdb/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat      209 b- defN 23-May-02 04:08 fastdatasets/memory/__init__.py
--rw-rw-rw-  2.0 fat     4704 b- defN 23-May-02 04:08 fastdatasets/memory/dataset.py
--rw-rw-rw-  2.0 fat     3741 b- defN 23-May-02 04:08 fastdatasets/memory/writer.py
--rw-rw-rw-  2.0 fat     6474 b- defN 23-May-02 04:08 fastdatasets/memory/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     4304 b- defN 23-May-02 04:08 fastdatasets/memory/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat      209 b- defN 23-May-02 04:08 fastdatasets/record/__init__.py
--rw-rw-rw-  2.0 fat     6007 b- defN 23-May-02 04:08 fastdatasets/record/dataset.py
--rw-rw-rw-  2.0 fat     3800 b- defN 23-May-02 04:08 fastdatasets/record/writer.py
--rw-rw-rw-  2.0 fat     7358 b- defN 23-May-02 04:08 fastdatasets/record/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     6788 b- defN 23-May-02 04:08 fastdatasets/record/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat     5576 b- defN 23-May-02 04:08 fastdatasets/torch_dataset/__init__.py
--rw-rw-rw-  2.0 fat      980 b- defN 23-May-02 04:08 fastdatasets/utils/MEMORY.py
--rw-rw-rw-  2.0 fat       51 b- defN 23-May-02 04:08 fastdatasets/utils/__init__.py
--rw-rw-rw-  2.0 fat    12220 b- defN 23-May-02 04:08 fastdatasets/utils/numpyadapter.py
--rw-rw-rw-  2.0 fat     5457 b- defN 23-May-02 04:08 fastdatasets/utils/parallel.py
--rw-rw-rw-  2.0 fat      346 b- defN 23-May-02 04:08 fastdatasets/utils/py_features.py
--rw-rw-rw-  2.0 fat    12115 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       13 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3357 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/RECORD
-37 files, 164401 bytes uncompressed, 37381 bytes compressed:  77.3%
+Zip file size: 58212 bytes, number of entries: 52
+-rw-rw-rw-  2.0 fat      116 b- defN 23-Jun-30 11:53 fastdatasets/__init__.py
+-rw-rw-rw-  2.0 fat     2310 b- defN 23-Jul-02 13:07 fastdatasets/setup.py
+-rw-rw-rw-  2.0 fat      245 b- defN 23-Jul-02 04:32 fastdatasets/arrow/__init__.py
+-rw-rw-rw-  2.0 fat     8704 b- defN 23-Jul-02 14:56 fastdatasets/arrow/dataset.py
+-rw-rw-rw-  2.0 fat      240 b- defN 23-Jul-02 13:18 fastdatasets/arrow/default.py
+-rw-rw-rw-  2.0 fat     2832 b- defN 23-Jul-02 13:11 fastdatasets/arrow/writer.py
+-rw-rw-rw-  2.0 fat     9679 b- defN 23-Jul-02 16:28 fastdatasets/arrow/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5842 b- defN 23-Jul-02 16:30 fastdatasets/arrow/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat       76 b- defN 23-Jun-30 11:53 fastdatasets/common/__init__.py
+-rw-rw-rw-  2.0 fat    14096 b- defN 23-Jun-30 11:53 fastdatasets/common/iterable_dataset.py
+-rw-rw-rw-  2.0 fat    10767 b- defN 23-Jun-30 11:53 fastdatasets/common/random_dataset.py
+-rw-rw-rw-  2.0 fat     4251 b- defN 23-Jun-30 11:53 fastdatasets/common/writer.py
+-rw-rw-rw-  2.0 fat     1135 b- defN 23-Jun-30 11:53 fastdatasets/common/writer_object.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-Jun-30 11:53 fastdatasets/leveldb/__init__.py
+-rw-rw-rw-  2.0 fat     7181 b- defN 23-Jul-02 08:21 fastdatasets/leveldb/dataset.py
+-rw-rw-rw-  2.0 fat      206 b- defN 23-Jul-01 11:58 fastdatasets/leveldb/default.py
+-rw-rw-rw-  2.0 fat     4489 b- defN 23-Jun-30 11:53 fastdatasets/leveldb/writer.py
+-rw-rw-rw-  2.0 fat     7588 b- defN 23-Jul-02 16:30 fastdatasets/leveldb/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     4615 b- defN 23-Jul-02 16:30 fastdatasets/leveldb/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      215 b- defN 23-Jun-30 11:53 fastdatasets/lmdb/__init__.py
+-rw-rw-rw-  2.0 fat     7750 b- defN 23-Jul-02 08:21 fastdatasets/lmdb/dataset.py
+-rw-rw-rw-  2.0 fat      602 b- defN 23-Jul-01 12:05 fastdatasets/lmdb/default.py
+-rw-rw-rw-  2.0 fat     4696 b- defN 23-Jun-30 11:53 fastdatasets/lmdb/writer.py
+-rw-rw-rw-  2.0 fat     7848 b- defN 23-Jul-02 16:30 fastdatasets/lmdb/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5541 b- defN 23-Jul-02 16:30 fastdatasets/lmdb/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-Jun-30 11:53 fastdatasets/memory/__init__.py
+-rw-rw-rw-  2.0 fat     6598 b- defN 23-Jul-02 08:21 fastdatasets/memory/dataset.py
+-rw-rw-rw-  2.0 fat     3741 b- defN 23-Jun-30 11:53 fastdatasets/memory/writer.py
+-rw-rw-rw-  2.0 fat     6522 b- defN 23-Jul-02 16:24 fastdatasets/memory/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     4351 b- defN 23-Jul-02 16:04 fastdatasets/memory/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      245 b- defN 23-Jul-02 04:32 fastdatasets/parquet/__init__.py
+-rw-rw-rw-  2.0 fat     8833 b- defN 23-Jul-02 14:52 fastdatasets/parquet/dataset.py
+-rw-rw-rw-  2.0 fat      348 b- defN 23-Jul-02 15:31 fastdatasets/parquet/default.py
+-rw-rw-rw-  2.0 fat     2853 b- defN 23-Jul-02 15:49 fastdatasets/parquet/writer.py
+-rw-rw-rw-  2.0 fat     9545 b- defN 23-Jul-02 16:31 fastdatasets/parquet/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5381 b- defN 23-Jul-02 16:31 fastdatasets/parquet/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-Jun-30 11:53 fastdatasets/record/__init__.py
+-rw-rw-rw-  2.0 fat     8161 b- defN 23-Jul-02 08:16 fastdatasets/record/dataset.py
+-rw-rw-rw-  2.0 fat      188 b- defN 23-Jul-01 12:10 fastdatasets/record/default.py
+-rw-rw-rw-  2.0 fat     3800 b- defN 23-Jun-30 11:53 fastdatasets/record/writer.py
+-rw-rw-rw-  2.0 fat     7768 b- defN 23-Jul-02 16:25 fastdatasets/record/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     6806 b- defN 23-Jul-02 16:32 fastdatasets/record/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5576 b- defN 23-Jun-30 11:53 fastdatasets/torch_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      986 b- defN 23-Jul-02 16:27 fastdatasets/utils/MEMORY.py
+-rw-rw-rw-  2.0 fat       51 b- defN 23-Jun-30 11:53 fastdatasets/utils/__init__.py
+-rw-rw-rw-  2.0 fat    12220 b- defN 23-Jun-30 11:53 fastdatasets/utils/numpyadapter.py
+-rw-rw-rw-  2.0 fat     5457 b- defN 23-Jun-30 11:53 fastdatasets/utils/parallel.py
+-rw-rw-rw-  2.0 fat      346 b- defN 23-Jun-30 11:53 fastdatasets/utils/py_features.py
+-rw-rw-rw-  2.0 fat    15008 b- defN 23-Jul-02 16:33 fastdatasets-0.9.8.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-02 16:33 fastdatasets-0.9.8.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       13 b- defN 23-Jul-02 16:33 fastdatasets-0.9.8.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4698 b- defN 23-Jul-02 16:33 fastdatasets-0.9.8.dist-info/RECORD
+52 files, 231238 bytes uncompressed, 50626 bytes compressed:  78.1%
```

## zipnote {}

```diff
@@ -1,13 +1,31 @@
 Filename: fastdatasets/__init__.py
 Comment: 
 
 Filename: fastdatasets/setup.py
 Comment: 
 
+Filename: fastdatasets/arrow/__init__.py
+Comment: 
+
+Filename: fastdatasets/arrow/dataset.py
+Comment: 
+
+Filename: fastdatasets/arrow/default.py
+Comment: 
+
+Filename: fastdatasets/arrow/writer.py
+Comment: 
+
+Filename: fastdatasets/arrow/iterable_dataset/__init__.py
+Comment: 
+
+Filename: fastdatasets/arrow/random_dataset/__init__.py
+Comment: 
+
 Filename: fastdatasets/common/__init__.py
 Comment: 
 
 Filename: fastdatasets/common/iterable_dataset.py
 Comment: 
 
 Filename: fastdatasets/common/random_dataset.py
@@ -21,14 +39,17 @@
 
 Filename: fastdatasets/leveldb/__init__.py
 Comment: 
 
 Filename: fastdatasets/leveldb/dataset.py
 Comment: 
 
+Filename: fastdatasets/leveldb/default.py
+Comment: 
+
 Filename: fastdatasets/leveldb/writer.py
 Comment: 
 
 Filename: fastdatasets/leveldb/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/leveldb/random_dataset/__init__.py
@@ -36,14 +57,17 @@
 
 Filename: fastdatasets/lmdb/__init__.py
 Comment: 
 
 Filename: fastdatasets/lmdb/dataset.py
 Comment: 
 
+Filename: fastdatasets/lmdb/default.py
+Comment: 
+
 Filename: fastdatasets/lmdb/writer.py
 Comment: 
 
 Filename: fastdatasets/lmdb/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/lmdb/random_dataset/__init__.py
@@ -60,20 +84,41 @@
 
 Filename: fastdatasets/memory/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/memory/random_dataset/__init__.py
 Comment: 
 
+Filename: fastdatasets/parquet/__init__.py
+Comment: 
+
+Filename: fastdatasets/parquet/dataset.py
+Comment: 
+
+Filename: fastdatasets/parquet/default.py
+Comment: 
+
+Filename: fastdatasets/parquet/writer.py
+Comment: 
+
+Filename: fastdatasets/parquet/iterable_dataset/__init__.py
+Comment: 
+
+Filename: fastdatasets/parquet/random_dataset/__init__.py
+Comment: 
+
 Filename: fastdatasets/record/__init__.py
 Comment: 
 
 Filename: fastdatasets/record/dataset.py
 Comment: 
 
+Filename: fastdatasets/record/default.py
+Comment: 
+
 Filename: fastdatasets/record/writer.py
 Comment: 
 
 Filename: fastdatasets/record/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/record/random_dataset/__init__.py
@@ -93,20 +138,20 @@
 
 Filename: fastdatasets/utils/parallel.py
 Comment: 
 
 Filename: fastdatasets/utils/py_features.py
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/METADATA
+Filename: fastdatasets-0.9.8.dist-info/METADATA
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/WHEEL
+Filename: fastdatasets-0.9.8.dist-info/WHEEL
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/top_level.txt
+Filename: fastdatasets-0.9.8.dist-info/top_level.txt
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/RECORD
+Filename: fastdatasets-0.9.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fastdatasets/setup.py

```diff
@@ -16,24 +16,24 @@
 if __name__ == '__main__':
     package_list = setuptools.find_packages('./fastdatasets', exclude=['tests.*'])
     pprint.pprint(package_list)
     package_list = ['fastdatasets'] +  [ 'fastdatasets.' + p for p in package_list]
 
     setuptools.setup(
         name=package_name,
-        version="0.9.7@post0",
+        version="0.9.8",
         author="ssbuild",
         author_email="9727464@qq.com",
         description=title,
         long_description_content_type='text/markdown',
         long_description=project_description_str,
         url="https://github.com/ssbuild/fastdatasets",
         packages=package_list,
         python_requires='>=3, <4',  # python的依赖关系
-        install_requires=['tfrecords >= 0.2.6 , < 0.3','data_serialize >= 0.2.1','numpy'],
+        install_requires=['tfrecords >= 0.2.9 , < 0.3','data_serialize >= 0.2.1','numpy'],
         classifiers=[
             'Development Status :: 5 - Production/Stable',
             'Intended Audience :: Developers',
             'Intended Audience :: Education',
             'Intended Audience :: Science/Research',
             'License :: OSI Approved :: Apache Software License',
             'Programming Language :: C++',
```

## fastdatasets/leveldb/dataset.py

```diff
@@ -1,141 +1,148 @@
 # @Time    : 2022/9/20 21:55
 # @Author  : tk
 # @FileName: dataset.py
-
+import copy
 import typing
 from tfrecords.python.io import gfile
 from tfrecords import LEVELDB as DB
 from typing import Union,List,AnyStr
 
 from .iterable_dataset import SingleLeveldbIterableDataset,MultiLeveldbIterableDataset
 from .random_dataset import SingleLeveldbRandomDataset,MultiLeveldbRandomDataset
+from .default import global_default_options
 
 
 __all__ = [
-           #  "SingleLeveldbIterableDataset",
-           # "MultiLeveldbIterableDataset",
-           # "SingleLeveldbRandomDataset",
-           # "MultiLeveldbRandomDataset",
-           "DB",
-           "load_dataset",
-           "gfile",
-           ]
+    #  "SingleLeveldbIterableDataset",
+    # "MultiLeveldbIterableDataset",
+    # "SingleLeveldbRandomDataset",
+    # "MultiLeveldbRandomDataset",
+    "DB",
+    "load_dataset",
+    "gfile",
+]
+
 
-_DefaultOptions = DB.LeveldbOptions(create_if_missing=False, error_if_exists=False)
 
 def LeveldbIterableDatasetLoader(data_path: Union[List[Union[AnyStr, typing.Iterator]], AnyStr, typing.Iterator],
                                  buffer_size: typing.Optional[int] = 128,
                                  cycle_length=1,
                                  block_length=1,
-                                 options=_DefaultOptions,
-                                 ):
+                                 options=copy.deepcopy(global_default_options)):
     if isinstance(data_path, list):
         if len(data_path) == 1:
-            cls = SingleLeveldbIterableDataset(data_path[0], buffer_size, block_length, options,
-
-                                               )
+            cls = SingleLeveldbIterableDataset(data_path[0],
+                                               buffer_size=buffer_size,
+                                               block_length=block_length,
+                                               options=options)
         else:
-            cls = MultiLeveldbIterableDataset(data_path, buffer_size, cycle_length, block_length, options)
+            cls = MultiLeveldbIterableDataset(data_path,
+                                              buffer_size=buffer_size,
+                                              cycle_length=cycle_length,
+                                              block_length=block_length,
+                                              options= options)
     elif isinstance(data_path, str):
-        cls = SingleLeveldbIterableDataset(data_path, buffer_size, block_length, options,
-
-                                           )
+        cls = SingleLeveldbIterableDataset(data_path,
+                                           buffer_size=buffer_size,
+                                           block_length=block_length,
+                                           options=options)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 def LeveldbRandomDatasetLoader(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                                data_key_prefix_list=('input',),
                                num_key='total_num',
-                               options=_DefaultOptions,
+                               options=copy.deepcopy(global_default_options),
                                ):
     if isinstance(data_path, list):
         if len(data_path) == 1:
-            cls = SingleLeveldbRandomDataset(data_path[0], data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,
-
-                                             )
+            cls = SingleLeveldbRandomDataset(data_path[0],
+                                             data_key_prefix_list=data_key_prefix_list,
+                                             num_key=num_key,
+                                             options=options)
         else:
-            cls = MultiLeveldbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,
-                                            )
+            cls = MultiLeveldbRandomDataset(data_path,
+                                            data_key_prefix_list=data_key_prefix_list,
+                                            num_key=num_key,
+                                            options=options)
     elif isinstance(data_path, str):
-        cls = SingleLeveldbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,
-                                         )
+        cls = SingleLeveldbRandomDataset(data_path,
+                                         data_key_prefix_list=data_key_prefix_list,
+                                         num_key=num_key,
+                                         options=options)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 class load_dataset:
 
     @staticmethod
     def IterableDataset(data_path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=_DefaultOptions,
-                     ):
+                        buffer_size: typing.Optional[int] = 128,
+                        cycle_length=1,
+                        block_length=1,
+                        options=copy.deepcopy(global_default_options)):
         return LeveldbIterableDatasetLoader(data_path,
-                                            buffer_size,
-                                            cycle_length,
-                                            block_length,
+                                            buffer_size=buffer_size,
+                                            cycle_length= cycle_length,
+                                            block_length=block_length,
                                             options=options, )
 
     @staticmethod
-    def SingleIterableDataset( data_path_or_iterator: typing.Union[typing.AnyStr,typing.Iterator],
-                 buffer_size: typing.Optional[int] = 64,
-                 block_length=1,
-                 options=_DefaultOptions,
-                 ):
-
-            return SingleLeveldbIterableDataset(data_path_or_iterator, buffer_size, block_length, options,
-                                                )
+    def SingleIterableDataset(data_path_or_iterator: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 64,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options)):
+
+        return SingleLeveldbIterableDataset(data_path_or_iterator,
+                                            buffer_size = buffer_size,
+                                            block_length = block_length,
+                                            options = options)
 
     @staticmethod
     def MultiIterableDataset(data_path_or_iterator: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = _DefaultOptions,
-                 ):
-
-            return MultiLeveldbIterableDataset(data_path_or_iterator, buffer_size, cycle_length, block_length, options,
-                                               )
+                             buffer_size: typing.Optional[int]=64,
+                             cycle_length=None,
+                             block_length=1,
+                             options=copy.deepcopy(global_default_options)):
+
+        return MultiLeveldbIterableDataset(data_path_or_iterator,
+                                           buffer_size=buffer_size,
+                                           cycle_length=cycle_length,
+                                           block_length=block_length,
+                                           options = options)
 
     @staticmethod
     def RandomDataset(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                       data_key_prefix_list=('input',),
                       num_key='total_num',
-                      options = _DefaultOptions,
-                      ):
+                      options=copy.deepcopy(global_default_options)):
 
         return LeveldbRandomDatasetLoader(data_path,
-                                          data_key_prefix_list,
-                                          num_key, options,
-                                          )
+                                          data_key_prefix_list=data_key_prefix_list,
+                                          num_key=num_key,
+                                          options=options)
 
     @staticmethod
     def SingleRandomDataset(data_path: typing.Union[typing.AnyStr,typing.Sized],
                             data_key_prefix_list=('input',),
                             num_key='total_num',
-                 options=_DefaultOptions
-                 ):
+                            options=copy.deepcopy(global_default_options)):
         return SingleLeveldbRandomDataset(data_path,
-                                          data_key_prefix_list,
-                                          num_key,
+                                          data_key_prefix_list=data_key_prefix_list,
+                                          num_key = num_key,
                                           options=options,
                                           )
 
     @staticmethod
     def MutiRandomDataset(data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
-                        data_key_prefix_list=('input',),
-                        num_key='total_num',
-                        options =_DefaultOptions,
-                        ):
-        return MultiLeveldbRandomDataset(data_path,
-                                         data_key_prefix_list,
-                                         num_key,
-                                         options,
-                                         )
-
-
+                          data_key_prefix_list=('input',),
+                          num_key='total_num',
+                          options=copy.deepcopy(global_default_options)):
 
+        return MultiLeveldbRandomDataset(data_path,
+                                         data_key_prefix_list = data_key_prefix_list,
+                                         num_key=num_key,
+                                         options = options)
```

## fastdatasets/leveldb/iterable_dataset/__init__.py

```diff
@@ -2,43 +2,49 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2022/9/8 15:49
 
 import os
 import warnings
 import typing
 from collections.abc import Iterator
-import tfrecords
 from tfrecords import LEVELDB
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
+from ..default import global_default_options
 import copy
 
-__all__ = ["SingleLeveldbIterableDataset", "MultiLeveldbIterableDataset", "tfrecords", "warnings"]
+__all__ = [
+    "SingleLeveldbIterableDataset",
+    "MultiLeveldbIterableDataset"
+]
 
 class SingleLeveldbIterableDataset(IterableDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
+                 batch_size: typing.Optional[int] = None,
                  block_length=1,
-                 options=LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ):
 
 
         assert block_length > 0
 
+        self.batch_size = batch_size if batch_size is not None else 1
         self.with_share_memory = with_share_memory
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
         self.buffer_size = buffer_size
+        assert self.buffer_size  >0
 
         self.buffer = []
         self.iterator_ = None
         self.iterator_obj = None
         self.reset()
 
     def __del__(self):
@@ -54,16 +60,16 @@
             self.iterator_.close()
             self.iterator_ = None
             self.iterator_obj = None
 
     def __reopen__(self):
         self.block_id = -1
         self.close()
-        if os.path.exists(self.data_path):
-            self.iterator_ = LEVELDB.Leveldb(self.data_path, options=self.options)
+        if os.path.exists(self.path):
+            self.iterator_ = LEVELDB.Leveldb(self.path, options=self.options)
             self.iterator_obj = self.iterator_.get_iterater()
         else:
             self.iterator_ = None
 
         self.repeat_done_num += 1
         return True
 
@@ -80,27 +86,31 @@
         self.block_id += 1
         return it
 
     def __next_ex__(self):
         iterator : LEVELDB.LeveldbIterater = self.iterator_obj
         if iterator is None:
             raise StopIteration
-        if self.buffer_size > 1:
-            if len(self.buffer) == 0:
-                try:
-                    for _ in range(self.buffer_size):
-                        self.buffer.append(next(iterator))
-                except StopIteration:
-                    pass
-            if len(self.buffer) == 0:
-                raise StopIteration
+
+        if len(self.buffer) < self.batch_size:
+            try:
+                for _ in range(max(self.buffer_size,self.batch_size-len(self.buffer) + 1)):
+                    self.buffer.append(next(iterator))
+            except StopIteration:
+                pass
+            except Exception as e:
+                warnings.warn('data corrupted in {} , err {}'.format(self.path, str(e)))
+                pass
+        if len(self.buffer) == 0:
+            raise StopIteration
+
+        if self.batch_size == 1:
             return self.buffer.pop(0)
-        else:
-            result = next(iterator)
-        return result
+
+        return [self.buffer.pop(0) for i in range(min(len(self.buffer), self.batch_size))]
 
 class MultiLeveldbIterableDataset(IterableDatasetBase):
     """Parse (generic) TFTables dataset into `IterableDataset` object,
     which contain `np.ndarrays`s. By default (when `sequence_description`
     is None), it treats the TFTables as containing `tf.Example`.
     Otherwise, it assumes it is a `tf.SequenceExample`.
 
@@ -131,23 +141,23 @@
         if cycle_length is None:
             cycle_length = cpu_count()
 
         self.with_share_memory = with_share_memory
         self.options = options
         self.cycle_length = min(cycle_length,len(data_path))
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.buffer_size = buffer_size
 
         if self.buffer_size is None:
             self.buffer_size = 1
         self.reset()
 
     def reset(self):
-        self.iterators_ = [{"valid": False,"file": self.data_path[i]} for i in range(len(self.data_path))]
+        self.iterators_ = [{"valid": False,"file": self.path[i]} for i in range(len(self.path))]
         self.cicle_iterators_ = []
         self.fresh_iter_ids = False
         self.cur_id = 0
         self.__reopen__()
 
     def close(self):
         for iter_obj in self.iterators_:
@@ -161,20 +171,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleLeveldbIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             self.with_share_memory
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -206,15 +211,19 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         block_length = self.block_length,
+                                                         options = self.options,
+                                                         with_share_memory = self.with_share_memory)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

## fastdatasets/leveldb/random_dataset/__init__.py

```diff
@@ -1,31 +1,33 @@
 # @Time    : 2022/9/18 10:49
 # @Author  : tk
 # @FileName: __init__.py.py
 import logging
 import typing
 import os
 from typing import List
-import tfrecords
 from tfrecords import LEVELDB
 from .. import RandomDatasetBase
+from ..default import global_default_options
 import copy
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleLeveldbRandomDataset", "MultiLeveldbRandomDataset", "tfrecords", "logging"]
+__all__ = [
+    "SingleLeveldbRandomDataset",
+    "MultiLeveldbRandomDataset"
+]
 
 class SingleLeveldbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Sized],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options=LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
-                 
+                 options=copy.deepcopy(global_default_options),
                  ):
         super(SingleLeveldbRandomDataset, self).__init__()
 
         self.data_key_prefix_list = data_key_prefix_list
         self.data_path = data_path
         self.options = options
 
@@ -81,16 +83,15 @@
 
 
 class MultiLeveldbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options = LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
-                 
+                 options=copy.deepcopy(global_default_options),
                  ) -> None:
         super(MultiLeveldbRandomDataset, self).__init__()
 
         self.options = options
         self.data_path = data_path
         self.data_key_prefix_list = data_key_prefix_list
         self.num_key = num_key
@@ -132,10 +133,10 @@
         for i,it_obj in enumerate(self.iterators_):
             tmp_obj = it_obj['inst']
             if item < cur_len + len(tmp_obj):
                 obj = tmp_obj
                 break
             cur_len += len(tmp_obj)
         if obj is None:
-            raise tfrecords.OutOfRangeError
+            raise OverflowError
         real_index =  item - cur_len
         return obj[real_index]
```

## fastdatasets/lmdb/dataset.py

```diff
@@ -1,150 +1,164 @@
 # @Time    : 2022/9/20 21:55
 # @Author  : tk
 # @FileName: dataset.py
-
+import copy
 import typing
 from tfrecords.python.io import gfile
 from tfrecords import LMDB as DB
 from typing import Union,List,AnyStr
 
 from .iterable_dataset import SingleLmdbIterableDataset,MultiLmdbIterableDataset
 from .random_dataset import SingleLmdbRandomDataset,MultiLmdbRandomDataset
+from .default import global_default_options
 
 __all__ = [
-           # "SingleLmdbIterableDataset",
-           # "MultiLmdbIterableDataset",
-           # "SingleLmdbRandomDataset",
-           # "MultiLmdbRandomDataset",
-            "DB",
-           "load_dataset",
-           "gfile",
-           ]
-
-_DefaultOptions = DB.LmdbOptions(env_open_flag = DB.LmdbFlag.MDB_RDONLY,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = DB.LmdbFlag.MDB_RDONLY,
-                dbi_flag = 0,
-                put_flag = 0)
+    # "SingleLmdbIterableDataset",
+    # "MultiLmdbIterableDataset",
+    # "SingleLmdbRandomDataset",
+    # "MultiLmdbRandomDataset",
+    "DB",
+    "load_dataset",
+    "gfile",
+]
+
+
 
 def LmdbIterableDatasetLoader(data_path: Union[List[Union[AnyStr, typing.Iterator]], AnyStr, typing.Iterator],
-                                 buffer_size: typing.Optional[int] = 128,
-                                 cycle_length=1,
-                                 block_length=1,
-                                 options=_DefaultOptions,
-                                 map_size=0,
-                                 ):
+                              buffer_size: typing.Optional[int] = 128,
+                              cycle_length=1,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options),
+                              map_size=0):
     if isinstance(data_path, list):
         if len(data_path) == 1:
-            cls = SingleLmdbIterableDataset(data_path[0], buffer_size, block_length, options=options,map_size=map_size
-                                               )
+            cls = SingleLmdbIterableDataset(data_path[0],
+                                            buffer_size = buffer_size,
+                                            block_length = block_length,
+                                            options=options,
+                                            map_size=map_size)
         else:
-            cls = MultiLmdbIterableDataset(data_path, buffer_size, cycle_length, block_length, options=options,map_size=map_size
-                                           )
+            cls = MultiLmdbIterableDataset(data_path,
+                                           buffer_size = buffer_size,
+                                           cycle_length = cycle_length,
+                                           block_length = block_length,
+                                           options=options,
+                                           map_size=map_size)
     elif isinstance(data_path, str) :
-        cls = SingleLmdbIterableDataset(data_path, buffer_size, block_length, options=options,map_size=map_size
-                                           )
+        cls = SingleLmdbIterableDataset(data_path,
+                                        buffer_size=buffer_size,
+                                        block_length=block_length,
+                                        options=options,
+                                        map_size=map_size)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 def LmdbRandomDatasetLoader(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
-                               data_key_prefix_list=('input',),
-                               num_key='total_num',
-                               options=_DefaultOptions,
-                            map_size=0,
-                               ):
+                            data_key_prefix_list=('input',),
+                            num_key='total_num',
+                            options=copy.deepcopy(global_default_options),
+                            map_size=0):
     if isinstance(data_path, list):
         if len(data_path) == 1:
-            cls = SingleLmdbRandomDataset(data_path[0], data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,map_size=map_size
-                                             )
+            cls = SingleLmdbRandomDataset(data_path[0],
+                                          data_key_prefix_list=data_key_prefix_list,
+                                          num_key=num_key,
+                                          options=options,
+                                          map_size=map_size)
         else:
-            cls = MultiLmdbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,map_size=map_size
-                                            )
+            cls = MultiLmdbRandomDataset(data_path,
+                                         data_key_prefix_list=data_key_prefix_list,
+                                         num_key=num_key,
+                                         options=options,
+                                         map_size=map_size)
     elif isinstance(data_path, str) :
-        cls = SingleLmdbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,map_size=map_size
-                                         )
+        cls = SingleLmdbRandomDataset(data_path,
+                                      data_key_prefix_list=data_key_prefix_list,
+                                      num_key=num_key,
+                                      options=options,
+                                      map_size=map_size)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 class load_dataset:
 
     @staticmethod
     def IterableDataset(data_path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=_DefaultOptions,
-                     map_size=0,
-                     ):
+                        buffer_size: typing.Optional[int] = 128,
+                        cycle_length=1,
+                        block_length=1,
+                        options=copy.deepcopy(global_default_options),
+                        map_size=0):
         return LmdbIterableDatasetLoader(data_path,
-                                            buffer_size,
-                                            cycle_length,
-                                            block_length,
-                                            options=options, 
-                                            map_size=map_size
-                                         )
-
-    @staticmethod
-    def SingleIterableDataset( data_path: typing.Union[typing.AnyStr,typing.Iterator],
-                 buffer_size: typing.Optional[int] = 64,
-                 block_length=1,
-                 options=_DefaultOptions,
-                 map_size=0
-                 ):
-
-            return SingleLmdbIterableDataset(data_path, buffer_size, block_length, options,map_size=map_size,
-                                                )
+                                         buffer_size=buffer_size,
+                                         cycle_length=cycle_length,
+                                         block_length=block_length,
+                                         options=options,
+                                         map_size=map_size)
+
+    @staticmethod
+    def SingleIterableDataset(data_path: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 64,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options),
+                              map_size=0):
+
+        return SingleLmdbIterableDataset(data_path,
+                                         buffer_size=buffer_size,
+                                         block_length=block_length,
+                                         options=options,
+                                         map_size=map_size)
 
     @staticmethod
     def MultiIterableDataset(data_path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
                  cycle_length=None,
                  block_length=1,
-                 options = _DefaultOptions,
+                 options=copy.deepcopy(global_default_options),
                  map_size=0,
                  ):
 
-            return MultiLmdbIterableDataset(data_path, buffer_size, cycle_length, block_length, options=options,map_size=map_size,
-                                               )
+        return MultiLmdbIterableDataset(data_path,
+                                        buffer_size=buffer_size,
+                                        cycle_length=cycle_length,
+                                        block_length=block_length,
+                                        options=options,
+                                        map_size=map_size)
 
     @staticmethod
     def RandomDataset(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                       data_key_prefix_list=('input',),
                       num_key='total_num',
-                      options=_DefaultOptions,
+                      options=copy.deepcopy(global_default_options),
                       map_size=0,
                       ):
 
         return LmdbRandomDatasetLoader(data_path,
-                                          data_key_prefix_list,
-                                          num_key, options=options,map_size=map_size
-                                          )
+                                       data_key_prefix_list=data_key_prefix_list,
+                                       num_key=num_key,
+                                       options=options,map_size=map_size)
 
     @staticmethod
     def SingleRandomDataset(data_path: typing.Union[typing.AnyStr,typing.Sized],
                             data_key_prefix_list=('input',),
                             num_key='total_num',
-                            options=_DefaultOptions,
-                            map_size=0,
-                 ):
+                            options=copy.deepcopy(global_default_options),
+                            map_size=0):
         return SingleLmdbRandomDataset(data_path,
-                                          data_key_prefix_list,
-                                          num_key,
-                                          options=options,
-                                          map_size=map_size
-                                          )
+                                       data_key_prefix_list=data_key_prefix_list,
+                                       num_key=num_key,
+                                       options=options, map_size=map_size)
 
     @staticmethod
     def MutiRandomDataset(data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
-                        data_key_prefix_list=('input',),
-                        num_key='total_num',
-                        options = _DefaultOptions,
-                        map_size=0,
-                        ):
+                          data_key_prefix_list=('input',),
+                          num_key='total_num',
+                          options=copy.deepcopy(global_default_options),
+                          map_size=0):
+
         return MultiLmdbRandomDataset(data_path,
-                                    data_key_prefix_list,
-                                    num_key,
-                                    options=options,
-                                    map_size=map_size)
+                                      data_key_prefix_list=data_key_prefix_list,
+                                      num_key=num_key,
+                                      options=options, map_size=map_size)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## fastdatasets/lmdb/iterable_dataset/__init__.py

```diff
@@ -1,56 +1,55 @@
 """Load tfrecord files into torch datasets."""
 # -*- coding: utf-8 -*-
 # @Time    : 2022/9/8 15:49
 
 import os
 import warnings
 import typing
-import tfrecords
 from tfrecords import LMDB
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
+import copy
+from ..default import global_default_options2 as global_default_options
 
-__all__ = ["SingleLmdbIterableDataset", "MultiLmdbIterableDataset", "tfrecords", "warnings"]
+__all__ = [
+    "SingleLmdbIterableDataset",
+    "MultiLmdbIterableDataset",
+    "warnings"
+]
 
 
 
-DefaultOptions = LMDB.LmdbOptions(env_open_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                dbi_flag = 0,
-                put_flag = 0)
-
-   
 
 class SingleLmdbIterableDataset(IterableDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
                  block_length=1,
-                 options=DefaultOptions,
+                 options=copy.deepcopy(global_default_options),
                  map_size=0,
                  max_readers: int = 128,
                  max_dbs: int = 0
                  ):
 
 
         assert block_length > 0
 
         self.map_size = map_size
         self.max_readers = max_readers
         self.max_dbs = max_dbs
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
         self.buffer_size = buffer_size
+        assert self.buffer_size > 0
 
         self.buffer = []
         self.iterator_ = None
         self.iterator_obj = None
         self.reset()
 
     def __del__(self):
@@ -66,16 +65,16 @@
             self.iterator_.close()
             self.iterator_ = None
             self.iterator_obj = None
 
     def __reopen__(self):
         self.block_id = -1
         self.close()
-        if os.path.exists(self.data_path):
-            self.iterator_ = LMDB.Lmdb(self.data_path,
+        if os.path.exists(self.path):
+            self.iterator_ = LMDB.Lmdb(self.path,
                                        options=self.options,
                                        map_size=self.map_size,
                                        max_readers=self.max_readers,
                                        max_dbs=self.max_dbs)
 
             self.iterator_obj = self.iterator_.get_iterater(reverse=False)
         else:
@@ -97,27 +96,26 @@
         self.block_id += 1
         return it
 
     def __next_ex__(self):
         iterator : LMDB.LmdbIterater = self.iterator_obj
         if iterator is None:
             raise StopIteration
-        if self.buffer_size > 1:
-            if len(self.buffer) == 0:
-                try:
-                    for _ in range(self.buffer_size):
-                        self.buffer.append(next(iterator))
-                except StopIteration:
-                    pass
-            if len(self.buffer) == 0:
-                raise StopIteration
-            return self.buffer.pop(0)
-        else:
-            result = next(iterator)
-        return result
+        if len(self.buffer) == 0:
+            try:
+                for _ in range(self.buffer_size):
+                    self.buffer.append(next(iterator))
+            except StopIteration:
+                pass
+            except Exception as e:
+                warnings.warn('data corrupted in {} , err {}'.format(self.path, str(e)))
+                pass
+        if len(self.buffer) == 0:
+            raise StopIteration
+        return self.buffer.pop(0)
 
 class MultiLmdbIterableDataset(IterableDatasetBase):
     """Parse (generic) TFTables dataset into `IterableDataset` object,
     which contain `np.ndarrays`s. By default (when `sequence_description`
     is None), it treats the TFTables as containing `tf.Example`.
     Otherwise, it assumes it is a `tf.SequenceExample`.
 
@@ -134,15 +132,15 @@
     """
 
     def __init__(self,
                  data_path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
                  cycle_length=None,
                  block_length=1,
-                 options =DefaultOptions,
+                 options = copy.deepcopy(global_default_options),
                  map_size =0,
                  max_readers: int = 128,
                  max_dbs: int = 0
                  ) -> None:
         super(MultiLmdbIterableDataset, self).__init__()
 
         assert block_length > 0
@@ -152,23 +150,23 @@
 
         self.map_size = map_size
         self.max_readers = max_readers
         self.max_dbs = max_dbs
         self.options = options
         self.cycle_length = min(cycle_length,len(data_path))
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.buffer_size = buffer_size
 
         if self.buffer_size is None:
             self.buffer_size = 1
         self.reset()
 
     def reset(self):
-        self.iterators_ = [{"valid": False,"file": self.data_path[i]} for i in range(len(self.data_path))]
+        self.iterators_ = [{"valid": False,"file": self.path[i]} for i in range(len(self.path))]
         self.cicle_iterators_ = []
         self.fresh_iter_ids = False
         self.cur_id = 0
         self.__reopen__()
 
     def close(self):
         for iter_obj in self.iterators_:
@@ -182,22 +180,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleLmdbIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             self.map_size,
-                             self.max_readers,
-                             self.max_dbs,
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -229,15 +220,21 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         block_length = self.block_length,
+                                                         options = self.options,
+                                                         map_size = self.map_size,
+                                                         max_readers = self.max_readers,
+                                                         max_dbs = self.max_dbs,)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## fastdatasets/lmdb/random_dataset/__init__.py

```diff
@@ -1,34 +1,35 @@
 # @Time    : 2022/9/18 10:49
 # @Author  : tk
 # @FileName: __init__.py.py
+import copy
 import logging
 import typing
 import os
 from typing import List
-import tfrecords
 from tfrecords import LMDB
 from .. import RandomDatasetBase
+from ..default import global_default_options
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleLmdbRandomDataset", "MultiLmdbRandomDataset", "tfrecords", "logging"]
+__all__ = [
+    "SingleLmdbRandomDataset",
+    "MultiLmdbRandomDataset"
+]
+
+
 
-DefaultOptions = LMDB.LmdbOptions( env_open_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                dbi_flag = 0,
-                put_flag = 0)
 class SingleLmdbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Sized],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options=DefaultOptions,
+                 options=copy.deepcopy(global_default_options),
                  map_size=0,
                  max_readers: int = 128,
                  max_dbs: int = 0
                  ):
         super(SingleLmdbRandomDataset, self).__init__()
 
         self.data_key_prefix_list = data_key_prefix_list
@@ -96,15 +97,15 @@
 
 
 class MultiLmdbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options = DefaultOptions,
+                 options = copy.deepcopy(global_default_options),
                  map_size=0,
                  max_readers: int = 128,
                  max_dbs: int = 0,
                  ) -> None:
         super(MultiLmdbRandomDataset, self).__init__()
 
         self.options = options
@@ -129,21 +130,20 @@
                 iter_obj["instance"].close()
                 iter_obj["valid"] = False
                 iter_obj["instance"] = None
 
     def __reopen__(self):
         for it_obj in self.iterators_:
             it_obj['inst'] = SingleLmdbRandomDataset(it_obj["file"],
-                                                        data_key_prefix_list=self.data_key_prefix_list,
-                                                        num_key=self.num_key,
-                                                        options=self.options,
-                                                        map_size=self.map_size,
-                                                        max_readers = self.max_readers,
-                                                        max_dbs = self.max_dbs
-                                                     )
+                                                     data_key_prefix_list=self.data_key_prefix_list,
+                                                     num_key=self.num_key,
+                                                     options=self.options,
+                                                     map_size=self.map_size,
+                                                     max_readers = self.max_readers,
+                                                     max_dbs = self.max_dbs)
 
     def __len__(self):
         total_len = 0
         for it_obj in self.iterators_:
             total_len += len(it_obj['inst'])
         return total_len
 
@@ -156,10 +156,10 @@
         for i,it_obj in enumerate(self.iterators_):
             tmp_obj = it_obj['inst']
             if item < cur_len + len(tmp_obj):
                 obj = tmp_obj
                 break
             cur_len += len(tmp_obj)
         if obj is None:
-            raise tfrecords.OutOfRangeError
+            raise OverflowError
         real_index =  item - cur_len
         return obj[real_index]
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## fastdatasets/memory/dataset.py

```diff
@@ -15,102 +15,129 @@
    # "SingleMemoryRandomDataset",
    # "MultiMemoryRandomDataset",
    "load_dataset",
    "gfile"
 ]
 
 def MemoryIterableDatasetLoader(data_list: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=None,
-                     ):
+                                buffer_size: typing.Optional[int] = 128,
+                                cycle_length=1,
+                                block_length=1,
+                                options=None):
     if isinstance(data_list, list):
         if len(data_list) == 1:
-            cls = SingleMemoryIterableDataset(data_list[0], buffer_size, block_length, options)
+            cls = SingleMemoryIterableDataset(data_list[0],
+                                              buffer_size=buffer_size,
+                                              block_length=block_length,
+                                              options=options)
         else:
-            cls = MultiMemoryIterableDataset(data_list, buffer_size, cycle_length, block_length, options)
+            cls = MultiMemoryIterableDataset(data_list,
+                                             buffer_size=buffer_size,
+                                             cycle_length=cycle_length,
+                                             block_length=block_length,
+                                             options=options)
     elif isinstance(data_list, Iterator):
-        cls = SingleMemoryIterableDataset(data_list, buffer_size, block_length, options)
+        cls = SingleMemoryIterableDataset(data_list,
+                                          buffer_size=buffer_size,
+                                          block_length=block_length,
+                                          options=options)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 def MemoryRandomDatasetLoader(data_list: typing.Union[typing.List,typing.AnyStr,typing.Sized],
-                            index_path=None,
-                            use_index_cache=True,
-                            options=None,
-                            ):
+                              index_path=None,
+                              use_index_cache=True,
+                              options=None):
 
     if isinstance(data_list, list):
         if len(data_list) > 0 and isinstance(data_list[0], list):
-            cls = MultiMemoryRandomDataset(data_list, index_path=index_path, use_index_cache=use_index_cache,
+            cls = MultiMemoryRandomDataset(data_list,
+                                           index_path=index_path,
+                                           use_index_cache=use_index_cache,
                                            options=options)
         else:
-            cls = SingleMemoryRandomDataset(data_list, index_path=index_path, use_index_cache=use_index_cache,
-                                        options=options)
+            cls = SingleMemoryRandomDataset(data_list,
+                                            index_path=index_path,
+                                            use_index_cache=use_index_cache,
+                                            options=options)
 
     elif isinstance(data_list, Sized):
-        cls = SingleMemoryRandomDataset(data_list, index_path=index_path, use_index_cache=use_index_cache, options=options)
+        cls = SingleMemoryRandomDataset(data_list,
+                                        index_path=index_path,
+                                        use_index_cache=use_index_cache,
+                                        options=options)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 class load_dataset:
 
     @staticmethod
     def IterableDataset(data_list: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=None,
-                     ):
+                        buffer_size: typing.Optional[int] = 128,
+                        cycle_length=1,
+                        block_length=1,
+                        options=None):
         return MemoryIterableDatasetLoader(data_list,
-                     buffer_size,
-                     cycle_length,
-                     block_length,
-                     options=options)
+                                           buffer_size= buffer_size,
+                                           cycle_length=cycle_length,
+                                           block_length=block_length,
+                                           options=options)
 
     @staticmethod
-    def SingleIterableDataset( data_list: typing.Union[typing.AnyStr,typing.Iterator],
-                                buffer_size: typing.Optional[int] = 64,
-                                block_length=1,
-                                options=None,
-                 ):
-
-            return SingleMemoryIterableDataset(data_list, buffer_size,block_length,options)
+    def SingleIterableDataset(data_list: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 64,
+                              block_length=1,
+                              options=None):
+        return SingleMemoryIterableDataset(data_list,
+                                           buffer_size=buffer_size,
+                                           block_length=block_length,
+                                           options=options)
 
     @staticmethod
     def MultiIterableDataset(data_list: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = None,
-                 ):
-
-            return MultiMemoryIterableDataset(data_list, buffer_size,cycle_length,block_length,options)
+                             buffer_size: typing.Optional[int]=64,
+                             cycle_length=None,
+                             block_length=1,
+                             options = None):
+
+        return MultiMemoryIterableDataset(data_list,
+                                          buffer_size=buffer_size,
+                                          cycle_length=cycle_length,
+                                          block_length=block_length,
+                                          options=options)
 
     @staticmethod
     def RandomDataset(data_list: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                       index_path=None,
                       use_index_cache=True,
                       options=None):
 
-        return MemoryRandomDatasetLoader(data_list,index_path,use_index_cache,options,)
+        return MemoryRandomDatasetLoader(data_list,
+                                         index_path=index_path,
+                                         use_index_cache=use_index_cache,
+                                         options=options)
 
     @staticmethod
     def SingleRandomDataset(data_list: typing.Union[typing.AnyStr,typing.Sized],
-                 index_path: str = None,
-                 use_index_cache=True,
-                 options=None):
-        return SingleMemoryRandomDataset(data_list, index_path, use_index_cache, options=options)
+                            index_path: str = None,
+                            use_index_cache=True,
+                            options=None):
+        return SingleMemoryRandomDataset(data_list,
+                                         index_path=index_path,
+                                         use_index_cache=use_index_cache,
+                                         options=options)
 
     @staticmethod
     def MutiRandomDataset(data_list: List[typing.Union[typing.AnyStr,typing.Sized]],
-                 index_path = None,
-                 use_index_cache=True,
-                 options = None):
-        return MultiMemoryRandomDataset(data_list, index_path, use_index_cache, options)
+                          index_path = None,
+                          use_index_cache=True,
+                          options = None):
+        return MultiMemoryRandomDataset(data_list,
+                                        index_path=index_path,
+                                        use_index_cache=use_index_cache,
+                                        options=options)
```

## fastdatasets/memory/iterable_dataset/__init__.py

```diff
@@ -5,23 +5,25 @@
 import warnings
 import typing
 from collections.abc import Iterator
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
 import copy
 
-__all__ = ["SingleMemoryIterableDataset",'MultiMemoryIterableDataset',"warnings"]
+__all__ = [
+    "SingleMemoryIterableDataset",
+    "MultiMemoryIterableDataset"
+]
 
 class SingleMemoryIterableDataset(IterableDatasetBase):
     def __init__(self,
                  data_iterator: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
                  block_length=1,
                  options:any = None,
-                 
                  ):
 
         assert isinstance(data_iterator,Iterator)
         assert block_length > 0
 
 
         self.block_length = block_length
@@ -108,15 +110,14 @@
 
     def __init__(self,
                  data_iterator: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
                  cycle_length=None,
                  block_length=1,
                  options:any = None,
-                 
                  ) -> None:
         super(MultiMemoryIterableDataset, self).__init__()
 
         assert block_length > 0
 
         if cycle_length is None:
             cycle_length = cpu_count()
@@ -151,19 +152,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleMemoryIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -195,15 +192,18 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         block_length=self.block_length,
+                                                         options=self.options,)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

## fastdatasets/memory/random_dataset/__init__.py

```diff
@@ -9,17 +9,19 @@
 import pickle
 from collections.abc import Sized
 import copy
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleMemoryRandomDataset",
-           "MultiMemoryRandomDataset",
-           "logging"]
+__all__ = [
+   "SingleMemoryRandomDataset",
+   "MultiMemoryRandomDataset",
+   "logging"
+]
 
 
 class SingleMemoryRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_list: typing.Union[typing.AnyStr,typing.Sized],
                  index_path: str = None,
                  use_index_cache=True,
@@ -113,15 +115,16 @@
             if iter_obj["valid"] and "instance" in iter_obj and iter_obj["instance"]:
                 iter_obj["instance"].close()
                 iter_obj["valid"] = False
                 iter_obj["instance"] = None
 
     def __reopen__(self):
         for it_obj in self.iterators_:
-            it_obj['inst'] = SingleMemoryRandomDataset(it_obj["file"], index_path=self.index_path,
+            it_obj['inst'] = SingleMemoryRandomDataset(it_obj["file"],
+                                                       index_path=self.index_path,
                                                        use_index_cache=self.use_index_cache,
                                                        options=self.options,
                                                        with_share_memory=self.with_share_memory)
 
     def __len__(self):
         total_len = 0
         for it_obj in self.iterators_:
```

## fastdatasets/record/dataset.py

```diff
@@ -1,131 +1,161 @@
 # @Time    : 2022/9/20 21:55
 # @Author  : tk
 # @FileName: dataset.py
-
+import copy
 import typing
 from tfrecords.python.io import gfile
 from tfrecords import RECORD
 from collections.abc import Iterator,Sized
 from typing import Union,List,AnyStr
 from .iterable_dataset import SingleRecordIterableDataset,MultiRecordIterableDataset
 from .random_dataset import SingleRecordRandomDataset,MultiRecordRandomDataset
-
+from .default import global_default_options
 
 __all__ = [
            #  "SingleRecordIterableDataset",
            # "MultiRecordIterableDataset",
            # "SingleRecordRandomDataset",
            # "MultiRecordRandomDataset",
            "RECORD",
            "load_dataset",
            "gfile",
            ]
 
 def RecordIterableDatasetLoader(path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                     with_share_memory=False):
+                                buffer_size: typing.Optional[int] = 128,
+                                cycle_length=1,
+                                block_length=1,
+                                options=copy.deepcopy(global_default_options),
+                                with_share_memory=False):
     if isinstance(path, list):
         if len(path) == 1:
-            cls = SingleRecordIterableDataset(path[0], buffer_size, block_length, options,
-                with_share_memory=with_share_memory
-            )
+            cls = SingleRecordIterableDataset(path[0],
+                                              buffer_size=buffer_size,
+                                              block_length=block_length,
+                                              options=options,
+                                              with_share_memory=with_share_memory)
         else:
-            cls = MultiRecordIterableDataset(path, buffer_size, cycle_length, block_length, options)
+            cls = MultiRecordIterableDataset(path,
+                                             buffer_size=buffer_size,
+                                             block_length=block_length,
+                                             cycle_length=cycle_length,
+                                             options=options,
+                                             with_share_memory=with_share_memory)
     elif isinstance(path, str):
-        cls = SingleRecordIterableDataset(path, buffer_size, block_length, options,
-            with_share_memory=with_share_memory
-        )
+        cls = SingleRecordIterableDataset(path,
+                                          buffer_size=buffer_size,
+                                          block_length=block_length,
+                                          options=options,
+                                          with_share_memory=with_share_memory)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 def RecordRandomDatasetLoader(path: typing.Union[typing.List,typing.AnyStr],
-                            index_path=None,
-                            use_index_cache=True,
-                            options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                            with_share_memory=False):
+                              index_path=None,
+                              use_index_cache=True,
+                              options=copy.deepcopy(global_default_options),
+                              with_share_memory=False):
     if isinstance(path, list):
         if len(path) == 1:
-            cls = SingleRecordRandomDataset(path[0], index_path=index_path, use_index_cache=use_index_cache, options=options,
-                                            with_share_memory=with_share_memory
-                                            )
+            cls = SingleRecordRandomDataset(path[0],
+                                            index_path=index_path,
+                                            use_index_cache=use_index_cache,
+                                            options=options, with_share_memory=with_share_memory)
         else:
-            cls = MultiRecordRandomDataset(path, index_path=index_path, use_index_cache=use_index_cache, options=options,
-                                           with_share_memory=with_share_memory)
+            cls = MultiRecordRandomDataset(path,
+                                           index_path=index_path,
+                                           use_index_cache=use_index_cache,
+                                           options=options, with_share_memory=with_share_memory)
     elif isinstance(path, str):
-        cls = SingleRecordRandomDataset(path, index_path=index_path, use_index_cache=use_index_cache, options=options,
-                                        with_share_memory=with_share_memory)
+        cls = SingleRecordRandomDataset(path,
+                                        index_path=index_path,
+                                        use_index_cache=use_index_cache,
+                                        options=options, with_share_memory=with_share_memory)
     else:
         raise Exception('data_path must be list or single string')
     return cls
 
 class load_dataset:
 
     @staticmethod
     def IterableDataset(path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                     with_share_memory=False):
+                        buffer_size: typing.Optional[int] = 128,
+                        cycle_length=1,
+                        block_length=1,
+                        options=copy.deepcopy(global_default_options),
+                        with_share_memory=False):
         return RecordIterableDatasetLoader(path,
-                     buffer_size,
-                     cycle_length,
-                     block_length,
-                     options=options,with_share_memory=with_share_memory)
+                                           buffer_size=buffer_size,
+                                           cycle_length=cycle_length,
+                                           block_length = block_length,
+                                           options=options,
+                                           with_share_memory=with_share_memory)
 
     @staticmethod
-    def SingleIterableDataset( path: typing.Union[typing.AnyStr,typing.Iterator],
-                 buffer_size: typing.Optional[int] = 64,
-                 block_length=1,
-                 options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-
-            return SingleRecordIterableDataset(path, buffer_size,block_length,options,
+    def SingleIterableDataset(path: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 128,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options),
+                              with_share_memory=False):
+
+            return SingleRecordIterableDataset(path,
+                                               buffer_size=buffer_size,
+                                               block_length=block_length,
+                                               options=options,
                                                with_share_memory=with_share_memory)
 
     @staticmethod
     def MultiIterableDataset(path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-
-            return MultiRecordIterableDataset(path, buffer_size,cycle_length,block_length,options,
+                             buffer_size: typing.Optional[int]=64,
+                             cycle_length=None,
+                             block_length=1,
+                             options = RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
+                             with_share_memory=False):
+
+            return MultiRecordIterableDataset(path,
+                                              buffer_size=buffer_size,
+                                              cycle_length=cycle_length,
+                                              block_length=block_length,
+                                              options=options,
                                               with_share_memory=with_share_memory)
 
     @staticmethod
     def RandomDataset(path: typing.Union[typing.List, typing.AnyStr],
                       index_path=None,
                       use_index_cache=True,
-                      options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
+                      options=copy.deepcopy(global_default_options),
                       with_share_memory=False):
 
-        return RecordRandomDatasetLoader(path,index_path,use_index_cache,options,
+        return RecordRandomDatasetLoader(path,
+                                         index_path=index_path,
+                                         use_index_cache= use_index_cache,
+                                         options=options,
                                          with_share_memory=with_share_memory)
 
     @staticmethod
     def SingleRandomDataset(path: typing.Union[typing.AnyStr],
-                 index_path: str = None,
-                 use_index_cache=True,
-                 options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-        return SingleRecordRandomDataset(path, index_path, use_index_cache, options=options,
+                            index_path: str = None,
+                            use_index_cache=True,
+                            options=copy.deepcopy(global_default_options),
+                            with_share_memory=False):
+        return SingleRecordRandomDataset(path,
+                                         index_path=index_path,
+                                         use_index_cache=use_index_cache,
+                                         options=options,
                                          with_share_memory=with_share_memory)
 
     @staticmethod
     def MutiRandomDataset(path: List[typing.Union[typing.AnyStr]],
-                 index_path = None,
-                 use_index_cache=True,
-                 options = RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-        return MultiRecordRandomDataset(path, index_path, use_index_cache, options,
+                          index_path = None,
+                          use_index_cache=True,
+                          options = RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
+                          with_share_memory=False):
+        return MultiRecordRandomDataset(path,
+                                        index_path=index_path,
+                                        use_index_cache=use_index_cache,
+                                        options=options,
                                         with_share_memory=with_share_memory)
 
 
 
-
```

## fastdatasets/record/iterable_dataset/__init__.py

```diff
@@ -6,38 +6,45 @@
 import warnings
 import typing
 # from collections.abc import Iterator
 import tfrecords
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
 import copy
+from ..default import global_default_options
 
-__all__ = ["SingleRecordIterableDataset",'MultiRecordIterableDataset',"tfrecords","warnings"]
+__all__ = [
+    "SingleRecordIterableDataset",
+    "MultiRecordIterableDataset",
+]
 
 class SingleRecordIterableDataset(IterableDatasetBase):
     def __init__(self,
                  path: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
+                 batch_size: typing.Optional[int] = None,
                  block_length=1,
-                 options=tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ):
 
         assert block_length > 0
         self.with_share_memory = with_share_memory
 
-
+        self.batch_size = batch_size if batch_size is not None else 1
+        assert self.batch_size > 0
         self.block_length = block_length
         self.path = path
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
         self.buffer_size = buffer_size
+        assert self.buffer_size > 0
 
         self.buffer = []
         self.iterator_ = None
         self.reset()
 
     def __del__(self):
        self.close()
@@ -54,15 +61,16 @@
 
 
     def __reopen__(self):
         self.block_id = -1
         self.close()
 
         if os.path.exists(self.path):
-            self.iterator_ = tfrecords.tf_record_iterator(self.path, options=self.options,
+            self.iterator_ = tfrecords.tf_record_iterator(self.path,
+                                                          options=self.options,
                                                           with_share_memory=self.with_share_memory)
         else:
             self.iterator_ = None
 
 
         self.repeat_done_num += 1
         return True
@@ -80,32 +88,32 @@
         self.block_id += 1
         return iter
 
     def __next_ex__(self):
         iterator = self.iterator_
         if iterator is None:
             raise StopIteration
-        if self.buffer_size > 1:
-            if len(self.buffer) == 0:
-                try:
-                    for _ in range(self.buffer_size):
-                        self.buffer.append(next(iterator))
-                except StopIteration:
-                    pass
-                except tfrecords.DataLossError:
-                    warnings.warn('data corrupted in {} Is this even a TFRecord file?'.format(self.path))
-                    pass
-                    # warnings.warn("Number of elements in the iterator is less than the "
-                    #               f"queue size (N={self.buffer_size}).")
-            if len(self.buffer) == 0:
-                raise StopIteration
+
+        if len(self.buffer) < self.batch_size:
+            try:
+                for _ in range(max(self.buffer_size,self.batch_size-len(self.buffer) + 1)):
+                    self.buffer.append(next(iterator))
+            except StopIteration:
+                pass
+            except tfrecords.DataLossError:
+                warnings.warn('data corrupted in {} Is this even a TFRecord file?'.format(self.path))
+                pass
+
+        if len(self.buffer) == 0:
+            raise StopIteration
+
+        if self.batch_size == 1:
             return self.buffer.pop(0)
-        else:
-            iterator = next(iterator)
-        return iterator
+
+        return [self.buffer.pop(0) for i in range(min(len(self.buffer), self.batch_size))]
 
 class MultiRecordIterableDataset(IterableDatasetBase):
     """Parse (generic) TFRecords dataset into `IterableDataset` object,
     which contain `np.ndarrays`s. By default (when `sequence_description`
     is None), it treats the TFRecords as containing `tf.Example`.
     Otherwise, it assumes it is a `tf.SequenceExample`.
 
@@ -120,17 +128,18 @@
     block_length: default 1
     options: TFRecordOptions
     """
 
     def __init__(self,
                  path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
+                 batch_size: typing.Optional[int] = 1,
                  cycle_length=None,
                  block_length=1,
-                 options = tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ) -> None:
         super(MultiRecordIterableDataset, self).__init__()
 
         assert block_length > 0
 
         if cycle_length is None:
@@ -138,14 +147,15 @@
 
         self.with_share_memory = with_share_memory
         self.options = options
         self.cycle_length = min(cycle_length,len(path))
         self.block_length = block_length
         self.path = path
         self.buffer_size = buffer_size
+        self.batch_size = batch_size
 
         if self.buffer_size is None:
             self.buffer_size = 1
         self.reset()
 
     def reset(self):
         self.iterators_ = [{"valid": False,"file": self.path[i]} for i in range(len(self.path))]
@@ -166,20 +176,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleRecordIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             self.with_share_memory
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -211,15 +216,20 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](path=iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         batch_size=self.batch_size,
+                                                         block_length= self.block_length,
+                                                         options= self.options,
+                                                         with_share_memory=self.with_share_memory)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

## fastdatasets/record/random_dataset/__init__.py

```diff
@@ -6,33 +6,35 @@
 import os
 from typing import List
 import tfrecords
 from .. import RandomDatasetBase
 import pickle
 # from collections.abc import Sized
 import copy
+from ..default import global_default_options
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleRecordRandomDataset","MultiRecordRandomDataset", "tfrecords", "logging"]
+__all__ = [
+    "SingleRecordRandomDataset",
+    "MultiRecordRandomDataset"
+]
 
 
 class SingleRecordRandomDataset(RandomDatasetBase):
     def __init__(self,
                  path: typing.Union[typing.AnyStr,typing.Sized],
                  index_path: str = None,
                  use_index_cache=True,
-                 options=tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ):
         super(SingleRecordRandomDataset, self).__init__()
 
-
-
         self.with_share_memory = with_share_memory
 
         if index_path is None:
             index_path = os.path.join(os.path.dirname(path), '.' + os.path.basename(path)+ '.INDEX')
         else:
             index_path = os.path.join(index_path, '.' + os.path.basename(path)+ '.INDEX')
 
@@ -139,15 +141,15 @@
 
 
 class MultiRecordRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path_data_list: List[typing.Union[typing.AnyStr,typing.Sized]],
                  index_path = None,
                  use_index_cache=True,
-                 options = tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ) -> None:
         super(MultiRecordRandomDataset, self).__init__()
 
         self.with_share_memory = with_share_memory
         self.options = options
         self.data_path_data_list = data_path_data_list
@@ -167,15 +169,16 @@
             if iter_obj["valid"] and "instance" in iter_obj and iter_obj["instance"]:
                 iter_obj["instance"].close()
                 iter_obj["valid"] = False
                 iter_obj["instance"] = None
 
     def __reopen__(self):
         for it_obj in self.iterators_:
-            it_obj['inst'] = SingleRecordRandomDataset(it_obj["file"], index_path=self.index_path,
+            it_obj['inst'] = SingleRecordRandomDataset(it_obj["file"],
+                                                       index_path=self.index_path,
                                                        use_index_cache=self.use_index_cache,
                                                        options=self.options,
                                                        with_share_memory=self.with_share_memory)
 
     def __len__(self):
         total_len = 0
         for it_obj in self.iterators_:
@@ -191,10 +194,10 @@
         for i,it_obj in enumerate(self.iterators_):
             tmp_obj = it_obj['inst']
             if item < cur_len + len(tmp_obj):
                 obj = tmp_obj
                 break
             cur_len += len(tmp_obj)
         if obj is None:
-            raise tfrecords.OutOfRangeError
+            raise OverflowError
         real_index =  item - cur_len
         return obj[real_index]
```

## fastdatasets/utils/MEMORY.py

```diff
@@ -1,14 +1,15 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2022/11/11 9:04
 import typing
 from .py_features import Final
 
 __all__ = [
-    'MemoryOptions','MemoryWriter'
+    'MemoryOptions',
+    'MemoryWriter'
 ]
 
 
 
 
 
 class MemoryOptions:
```

## Comparing `fastdatasets-0.9.7.post0.dist-info/METADATA` & `fastdatasets-0.9.8.dist-info/METADATA`

 * *Files 26% similar despite different names*

```diff
@@ -1,411 +1,515 @@
-Metadata-Version: 2.1
-Name: fastdatasets
-Version: 0.9.7-post0
-Summary: fastdatasets: datasets for tfrecords
-Home-page: https://github.com/ssbuild/fastdatasets
-Author: ssbuild
-Author-email: 9727464@qq.com
-License: Apache 2.0
-Keywords: fastdatasets,fastdatasets,tfrecords,dataset,datasets
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Education
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Programming Language :: C++
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Mathematics
-Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
-Classifier: Topic :: Software Development
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Requires-Python: >=3, <4
-Description-Content-Type: text/markdown
-Requires-Dist: tfrecords (<0.3,>=0.2.6)
-Requires-Dist: data-serialize (>=0.2.1)
-Requires-Dist: numpy
-
-
-## The update statement 
-
-```text
-2023-04-28: fix lmdb mutiprocess
-2023-02-13: add TopDataset with iterable_dataset and patch
-2022-12-07: modify a bug for randomdataset for batch reminder
-2022-11-07: add numpy writer and parser,add memory writer and parser
-2022-10-29: add kv dataset 
-2022-10-19: update and modify for __all__ module
-```
-
-## usage
-  [numpy_io](https://github.com/ssbuild/numpy_io) 
-
-## Install
-```commandline
-pip install -U fastdatasets
-```
-
-
-### 1. Record Write
-
-```python
-import data_serialize
-from fastdatasets.record import load_dataset, gfile,TFRecordOptions, TFRecordCompressionType, TFRecordWriter
-
-# Example Features结构兼容tensorflow.dataset
-def test_write_featrue():
-    options = TFRecordOptions(compression_type=TFRecordCompressionType.NONE)
-
-    def test_write(filename, N=3, context='aaa'):
-        with TFRecordWriter(filename, options=options) as file_writer:
-            for _ in range(N):
-                val1 = data_serialize.Int64List(value=[1, 2, 3] * 20)
-                val2 = data_serialize.FloatList(value=[1, 2, 3] * 20)
-                val3 = data_serialize.BytesList(value=[b'The china', b'boy'])
-                featrue = data_serialize.Features(feature=
-                {
-                    "item_0": data_serialize.Feature(int64_list=val1),
-                    "item_1": data_serialize.Feature(float_list=val2),
-                    "item_2": data_serialize.Feature(bytes_list=val3)
-                }
-                )
-                example = data_serialize.Example(features=featrue)
-                file_writer.write(example.SerializeToString())
-
-    test_write('d:/example.tfrecords0', 3, 'file0')
-    test_write('d:/example.tfrecords1', 10, 'file1')
-    test_write('d:/example.tfrecords2', 12, 'file2')
-
-
-# 写任意字符串
-def test_write_string():
-    options = TFRecordOptions(compression_type=TFRecordCompressionType.NONE)
-
-    def test_write(filename, N=3, context='aaa'):
-        with TFRecordWriter(filename, options=options) as file_writer:
-            for _ in range(N):
-                # x, y = np.random.random(), np.random.random()
-                file_writer.write(context + '____' + str(_))
-
-    test_write('d:/example.tfrecords0', 3, 'file0')
-    test_write('d:/example.tfrecords1', 10, 'file1')
-    test_write('d:/example.tfrecords2', 12, 'file2')
-
-
-
-```
-
-### 2. record Simple Writer Demo
-
-```python
-# @Time    : 2022/9/18 23:27
-import pickle
-import data_serialize
-import numpy as np
-from fastdatasets.record import load_dataset
-from fastdatasets.record import RECORD, WriterObject,FeatureWriter,StringWriter,PickleWriter,DataType,NumpyWriter
-
-filename= r'd:\\example_writer.record'
-
-def test_writer(filename):
-    print('test_feature ...')
-    options = RECORD.TFRecordOptions(compression_type='GZIP')
-    f = NumpyWriter(filename,options=options)
-
-    values = []
-    n = 30
-    for i in range(n):
-        train_node = {
-            "index": np.asarray(i, dtype=np.int64),
-            'image': np.random.rand(3, 4),
-            'labels': np.random.randint(0, 21128, size=(10), dtype=np.int64),
-            'bdata': np.asarray(b'11111111asdadasdasdaa')
-        }
-
-        values.append(train_node)
-        if (i + 1) % 10000 == 0:
-            f.write_batch( values)
-            values.clear()
-    if len(values):
-        f.write_batch(values)
-    f.close()
-
-def test_iterable(filename):
-    options = RECORD.TFRecordOptions(compression_type='GZIP')
-    datasets = load_dataset.IterableDataset(filename, options=options).parse_from_numpy_writer()
-    for i, d in enumerate(datasets):
-        print(i, d)
-
-def test_random(filename):
-    options = RECORD.TFRecordOptions(compression_type='GZIP')
-    datasets = load_dataset.RandomDataset(filename, options=options).parse_from_numpy_writer()
-    print(len(datasets))
-    for i in range(len(datasets)):
-        d = datasets[i]
-        print(i, d)
-
-test_writer(filename)
-test_iterable(filename)
-```
-
-### 3. IterableDataset demo
-
-```python
-import data_serialize
-from fastdatasets.record import load_dataset, gfile, RECORD
-
-data_path = gfile.glob('d:/example.tfrecords*')
-options = RECORD.TFRecordOptions(compression_type=None)
-base_dataset = load_dataset.IterableDataset(data_path, cycle_length=1,
-                                            block_length=1,
-                                            buffer_size=128,
-                                            options=options,
-                                            with_share_memory=True)
-
-
-def test_batch():
-    num = 0
-    for _ in base_dataset:
-        num += 1
-    print('base_dataset num', num)
-
-    base_dataset.reset()
-    ds = base_dataset.repeat(2).repeat(2).repeat(3).map(lambda x: x + bytes('_aaaaaaaaaaaaaa', encoding='utf-8'))
-    num = 0
-    for _ in ds:
-        num += 1
-
-    print('repeat(2).repeat(2).repeat(3) num ', num)
-
-
-def test_torch():
-    def filter_fn(x):
-        if x == b'file2____2':
-            return True
-        return False
-
-    base_dataset.reset()
-    dataset = base_dataset.filter(filter_fn).interval(2, 0)
-    i = 0
-    for d in dataset:
-        i += 1
-        print(i, d)
-
-    base_dataset.reset()
-    dataset = base_dataset.batch(3)
-    i = 0
-    for d in dataset:
-        i += 1
-        print(i, d)
-
-    # torch.utils.data.IterableDataset
-    from fastdatasets.torch_dataset import IterableDataset
-    dataset.reset()
-    ds = IterableDataset(dataset=dataset)
-    for d in ds:
-        print(d)
-
-
-def test_mutiprocess():
-    print('mutiprocess 0...')
-    base_dataset.reset()
-    dataset = base_dataset.shard(num_shards=3, index=0)
-    i = 0
-    for d in dataset:
-        i += 1
-        print(i, d)
-
-    print('mutiprocess 1...')
-    base_dataset.reset()
-    dataset = base_dataset.shard(num_shards=3, index=1)
-    i = 0
-    for d in dataset:
-        i += 1
-        print(i, d)
-
-    print('mutiprocess 2...')
-    base_dataset.reset()
-    dataset = base_dataset.shard(num_shards=3, index=2)
-    i = 0
-    for d in dataset:
-        i += 1
-        print(i, d)
-
-```
-
-
-
-### 4. RandomDataset demo
-
-```python
-from fastdatasets.record import load_dataset, gfile, RECORD
-
-data_path = gfile.glob('d:/example.tfrecords*')
-options = RECORD.TFRecordOptions(compression_type=None)
-dataset = load_dataset.RandomDataset(data_path, options=options,
-                                     with_share_memory=True)
-
-dataset = dataset.map(lambda x: x + b"adasdasdasd")
-print(len(dataset))
-
-for i in range(len(dataset)):
-    print(i + 1, dataset[i])
-
-print('batch...')
-dataset = dataset.batch(7)
-for i in range(len(dataset)):
-    print(i + 1, dataset[i])
-
-print('unbatch...')
-dataset = dataset.unbatch()
-for i in range(len(dataset)):
-    print(i + 1, dataset[i])
-
-print('shuffle...')
-dataset = dataset.shuffle(10)
-for i in range(len(dataset)):
-    print(i + 1, dataset[i])
-
-print('map...')
-dataset = dataset.map(transform_fn=lambda x: x + b'aa22222222222222222222222222222')
-for i in range(len(dataset)):
-    print(i + 1, dataset[i])
-
-print('torch Dataset...')
-from fastdatasets.torch_dataset import Dataset
-
-d = Dataset(dataset)
-for i in range(len(d)):
-    print(i + 1, d[i])
-
-
-```
-
-
-
-### 5. leveldb dataset
-
-```python
-# @Time    : 2022/10/27 20:37
-# @Author  : tk
-import numpy as np
-from tqdm import tqdm
-from fastdatasets.leveldb import DB,load_dataset,WriterObject,DataType,StringWriter,JsonWriter,FeatureWriter,NumpyWriter
-
-db_path = 'd:\\example_leveldb_numpy'
-
-def test_write(db_path):
-    options = DB.LeveldbOptions(create_if_missing=True,error_if_exists=False)
-    f = NumpyWriter(db_path, options = options)
-    keys,values = [],[]
-    n = 30
-    for i in range(n):
-        train_node = {
-            "index":np.asarray(i,dtype=np.int64),
-            'image': np.random.rand(3,4),
-            'labels': np.random.randint(0,21128,size=(10),dtype=np.int64),
-            'bdata': np.asarray(b'11111111asdadasdasdaa')
-        }
-        keys.append('input{}'.format(i))
-        values.append(train_node)
-        if (i+1) % 10000 == 0:
-            f.put_batch(keys,values)
-            keys.clear()
-            values.clear()
-    if len(keys):
-        f.put_batch(keys, values)
-        
-    f.get_writer.put('total_num',str(n))
-    f.close()
-
-
-
-def test_random(db_path):
-    options = DB.LeveldbOptions(create_if_missing=False, error_if_exists=False)
-    dataset = load_dataset.RandomDataset(db_path,
-                                        data_key_prefix_list=('input',),
-                                        num_key='total_num',
-                                        options = options)
-
-    dataset = dataset.parse_from_numpy_writer().shuffle(10)
-    print(len(dataset))
-    for i in tqdm(range(len(dataset)),total=len(dataset)):
-        d = dataset[i]
-        print(i,d)
-
-test_write(db_path)
-test_random(db_path)
-
-```
-
-
-### 6. lmdb dataset
-
-```python
-# @Time    : 2022/10/27 20:37
-# @Author  : tk
-
-import numpy as np
-from tqdm import tqdm
-from fastdatasets.lmdb import DB,LMDB,load_dataset,WriterObject,DataType,StringWriter,JsonWriter,FeatureWriter,NumpyWriter
-
-db_path = 'd:\\example_lmdb_numpy'
-
-def test_write(db_path):
-    options = DB.LmdbOptions(env_open_flag = 0,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = 0,
-                dbi_flag = 0,
-                put_flag = 0)
-
-    f = NumpyWriter(db_path, options = options,map_size=1024 * 1024 * 1024)
-
-    keys, values = [], []
-    n = 30
-    for i in range(n):
-        train_node = {
-            'image': np.random.rand(3, 4),
-            'labels': np.random.randint(0, 21128, size=(10), dtype=np.int64),
-            'bdata': np.asarray(b'11111111asdadasdasdaa')
-        }
-        keys.append('input{}'.format(i))
-        values.append(train_node)
-        if (i + 1) % 10000 == 0:
-            f.put_batch(keys, values)
-            keys.clear()
-            values.clear()
-    if len(keys):
-        f.put_batch(keys, values)
-
-    f.get_writer.put('total_num',str(n))
-    f.close()
-
-
-
-def test_random(db_path):
-    options = DB.LmdbOptions(env_open_flag=DB.LmdbFlag.MDB_RDONLY,
-                               env_open_mode=0o664,  # 8进制表示
-                               txn_flag=LMDB.LmdbFlag.MDB_RDONLY,
-                               dbi_flag=0,
-                               put_flag=0)
-    dataset = load_dataset.RandomDataset(db_path,
-                                        data_key_prefix_list=('input',),
-                                        num_key='total_num',
-                                        options = options)
-
-    dataset = dataset.parse_from_numpy_writer().shuffle(10)
-    print(len(dataset))
-    for i in tqdm(range(len(dataset)), total=len(dataset)):
-        d = dataset[i]
-        print(d)
-
-test_write(db_path)
-test_random(db_path)
-```
+Metadata-Version: 2.1
+Name: fastdatasets
+Version: 0.9.8
+Summary: fastdatasets: datasets for tfrecords
+Home-page: https://github.com/ssbuild/fastdatasets
+Author: ssbuild
+Author-email: 9727464@qq.com
+License: Apache 2.0
+Keywords: fastdatasets,fastdatasets,tfrecords,dataset,datasets
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Education
+Classifier: Intended Audience :: Science/Research
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Programming Language :: C++
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Mathematics
+Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
+Classifier: Topic :: Software Development
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Requires-Python: >=3, <4
+Description-Content-Type: text/markdown
+Requires-Dist: tfrecords (<0.3,>=0.2.9)
+Requires-Dist: data-serialize (>=0.2.1)
+Requires-Dist: numpy
+
+
+## The update statement 
+
+```text
+2023-07-02: support arrow parquet
+2023-04-28: fix lmdb mutiprocess
+2023-02-13: add TopDataset with iterable_dataset and patch
+2022-12-07: modify a bug for randomdataset for batch reminder
+2022-11-07: add numpy writer and parser,add memory writer and parser
+2022-10-29: add kv dataset 
+2022-10-19: update and modify for __all__ module
+```
+
+## usage
+  [numpy_io](https://github.com/ssbuild/numpy_io) 
+
+## Install
+```commandline
+pip install -U fastdatasets
+```
+
+
+### 1. Record Write
+
+```python
+import data_serialize
+from fastdatasets.record import load_dataset, gfile,TFRecordOptions, TFRecordCompressionType, TFRecordWriter
+
+# Example Features结构兼容tensorflow.dataset
+def test_write_featrue():
+    options = TFRecordOptions(compression_type=TFRecordCompressionType.NONE)
+
+    def test_write(filename, N=3, context='aaa'):
+        with TFRecordWriter(filename, options=options) as file_writer:
+            for _ in range(N):
+                val1 = data_serialize.Int64List(value=[1, 2, 3] * 20)
+                val2 = data_serialize.FloatList(value=[1, 2, 3] * 20)
+                val3 = data_serialize.BytesList(value=[b'The china', b'boy'])
+                featrue = data_serialize.Features(feature=
+                {
+                    "item_0": data_serialize.Feature(int64_list=val1),
+                    "item_1": data_serialize.Feature(float_list=val2),
+                    "item_2": data_serialize.Feature(bytes_list=val3)
+                }
+                )
+                example = data_serialize.Example(features=featrue)
+                file_writer.write(example.SerializeToString())
+
+    test_write('d:/example.tfrecords0', 3, 'file0')
+    test_write('d:/example.tfrecords1', 10, 'file1')
+    test_write('d:/example.tfrecords2', 12, 'file2')
+
+
+# 写任意字符串
+def test_write_string():
+    options = TFRecordOptions(compression_type=TFRecordCompressionType.NONE)
+
+    def test_write(filename, N=3, context='aaa'):
+        with TFRecordWriter(filename, options=options) as file_writer:
+            for _ in range(N):
+                # x, y = np.random.random(), np.random.random()
+                file_writer.write(context + '____' + str(_))
+
+    test_write('d:/example.tfrecords0', 3, 'file0')
+    test_write('d:/example.tfrecords1', 10, 'file1')
+    test_write('d:/example.tfrecords2', 12, 'file2')
+
+
+
+```
+
+### 2. record Simple Writer Demo
+
+```python
+# @Time    : 2022/9/18 23:27
+import pickle
+import data_serialize
+import numpy as np
+from fastdatasets.record import load_dataset
+from fastdatasets.record import RECORD, WriterObject,FeatureWriter,StringWriter,PickleWriter,DataType,NumpyWriter
+
+filename= r'd:\\example_writer.record'
+
+def test_writer(filename):
+    print('test_feature ...')
+    options = RECORD.TFRecordOptions(compression_type='GZIP')
+    f = NumpyWriter(filename,options=options)
+
+    values = []
+    n = 30
+    for i in range(n):
+        train_node = {
+            "index": np.asarray(i, dtype=np.int64),
+            'image': np.random.rand(3, 4),
+            'labels': np.random.randint(0, 21128, size=(10), dtype=np.int64),
+            'bdata': np.asarray(b'11111111asdadasdasdaa')
+        }
+
+        values.append(train_node)
+        if (i + 1) % 10000 == 0:
+            f.write_batch( values)
+            values.clear()
+    if len(values):
+        f.write_batch(values)
+    f.close()
+
+def test_iterable(filename):
+    options = RECORD.TFRecordOptions(compression_type='GZIP')
+    datasets = load_dataset.IterableDataset(filename, options=options).parse_from_numpy_writer()
+    for i, d in enumerate(datasets):
+        print(i, d)
+
+def test_random(filename):
+    options = RECORD.TFRecordOptions(compression_type='GZIP')
+    datasets = load_dataset.RandomDataset(filename, options=options).parse_from_numpy_writer()
+    print(len(datasets))
+    for i in range(len(datasets)):
+        d = datasets[i]
+        print(i, d)
+
+test_writer(filename)
+test_iterable(filename)
+```
+
+### 3. IterableDataset demo
+
+```python
+import data_serialize
+from fastdatasets.record import load_dataset, gfile, RECORD
+
+data_path = gfile.glob('d:/example.tfrecords*')
+options = RECORD.TFRecordOptions(compression_type=None)
+base_dataset = load_dataset.IterableDataset(data_path, cycle_length=1,
+                                            block_length=1,
+                                            buffer_size=128,
+                                            options=options,
+                                            with_share_memory=True)
+
+
+def test_batch():
+    num = 0
+    for _ in base_dataset:
+        num += 1
+    print('base_dataset num', num)
+
+    base_dataset.reset()
+    ds = base_dataset.repeat(2).repeat(2).repeat(3).map(lambda x: x + bytes('_aaaaaaaaaaaaaa', encoding='utf-8'))
+    num = 0
+    for _ in ds:
+        num += 1
+
+    print('repeat(2).repeat(2).repeat(3) num ', num)
+
+
+def test_torch():
+    def filter_fn(x):
+        if x == b'file2____2':
+            return True
+        return False
+
+    base_dataset.reset()
+    dataset = base_dataset.filter(filter_fn).interval(2, 0)
+    i = 0
+    for d in dataset:
+        i += 1
+        print(i, d)
+
+    base_dataset.reset()
+    dataset = base_dataset.batch(3)
+    i = 0
+    for d in dataset:
+        i += 1
+        print(i, d)
+
+    # torch.utils.data.IterableDataset
+    from fastdatasets.torch_dataset import IterableDataset
+    dataset.reset()
+    ds = IterableDataset(dataset=dataset)
+    for d in ds:
+        print(d)
+
+
+def test_mutiprocess():
+    print('mutiprocess 0...')
+    base_dataset.reset()
+    dataset = base_dataset.shard(num_shards=3, index=0)
+    i = 0
+    for d in dataset:
+        i += 1
+        print(i, d)
+
+    print('mutiprocess 1...')
+    base_dataset.reset()
+    dataset = base_dataset.shard(num_shards=3, index=1)
+    i = 0
+    for d in dataset:
+        i += 1
+        print(i, d)
+
+    print('mutiprocess 2...')
+    base_dataset.reset()
+    dataset = base_dataset.shard(num_shards=3, index=2)
+    i = 0
+    for d in dataset:
+        i += 1
+        print(i, d)
+
+```
+
+
+
+### 4. RandomDataset demo
+
+```python
+from fastdatasets.record import load_dataset, gfile, RECORD
+
+data_path = gfile.glob('d:/example.tfrecords*')
+options = RECORD.TFRecordOptions(compression_type=None)
+dataset = load_dataset.RandomDataset(data_path, options=options,
+                                     with_share_memory=True)
+
+dataset = dataset.map(lambda x: x + b"adasdasdasd")
+print(len(dataset))
+
+for i in range(len(dataset)):
+    print(i + 1, dataset[i])
+
+print('batch...')
+dataset = dataset.batch(7)
+for i in range(len(dataset)):
+    print(i + 1, dataset[i])
+
+print('unbatch...')
+dataset = dataset.unbatch()
+for i in range(len(dataset)):
+    print(i + 1, dataset[i])
+
+print('shuffle...')
+dataset = dataset.shuffle(10)
+for i in range(len(dataset)):
+    print(i + 1, dataset[i])
+
+print('map...')
+dataset = dataset.map(transform_fn=lambda x: x + b'aa22222222222222222222222222222')
+for i in range(len(dataset)):
+    print(i + 1, dataset[i])
+
+print('torch Dataset...')
+from fastdatasets.torch_dataset import Dataset
+
+d = Dataset(dataset)
+for i in range(len(d)):
+    print(i + 1, d[i])
+
+
+```
+
+
+
+### 5. leveldb dataset
+
+```python
+# @Time    : 2022/10/27 20:37
+# @Author  : tk
+import numpy as np
+from tqdm import tqdm
+from fastdatasets.leveldb import DB,load_dataset,WriterObject,DataType,StringWriter,JsonWriter,FeatureWriter,NumpyWriter
+
+db_path = 'd:\\example_leveldb_numpy'
+
+def test_write(db_path):
+    options = DB.LeveldbOptions(create_if_missing=True,error_if_exists=False)
+    f = NumpyWriter(db_path, options = options)
+    keys,values = [],[]
+    n = 30
+    for i in range(n):
+        train_node = {
+            "index":np.asarray(i,dtype=np.int64),
+            'image': np.random.rand(3,4),
+            'labels': np.random.randint(0,21128,size=(10),dtype=np.int64),
+            'bdata': np.asarray(b'11111111asdadasdasdaa')
+        }
+        keys.append('input{}'.format(i))
+        values.append(train_node)
+        if (i+1) % 10000 == 0:
+            f.put_batch(keys,values)
+            keys.clear()
+            values.clear()
+    if len(keys):
+        f.put_batch(keys, values)
+        
+    f.get_writer.put('total_num',str(n))
+    f.close()
+
+
+
+def test_random(db_path):
+    options = DB.LeveldbOptions(create_if_missing=False, error_if_exists=False)
+    dataset = load_dataset.RandomDataset(db_path,
+                                        data_key_prefix_list=('input',),
+                                        num_key='total_num',
+                                        options = options)
+
+    dataset = dataset.parse_from_numpy_writer().shuffle(10)
+    print(len(dataset))
+    for i in tqdm(range(len(dataset)),total=len(dataset)):
+        d = dataset[i]
+        print(i,d)
+
+test_write(db_path)
+test_random(db_path)
+
+```
+
+
+### 6. lmdb dataset
+
+```python
+# @Time    : 2022/10/27 20:37
+# @Author  : tk
+
+import numpy as np
+from tqdm import tqdm
+from fastdatasets.lmdb import DB,LMDB,load_dataset,WriterObject,DataType,StringWriter,JsonWriter,FeatureWriter,NumpyWriter
+
+db_path = 'd:\\example_lmdb_numpy'
+
+def test_write(db_path):
+    options = DB.LmdbOptions(env_open_flag = 0,
+                env_open_mode = 0o664, # 8进制表示
+                txn_flag = 0,
+                dbi_flag = 0,
+                put_flag = 0)
+
+    f = NumpyWriter(db_path, options = options,map_size=1024 * 1024 * 1024)
+
+    keys, values = [], []
+    n = 30
+    for i in range(n):
+        train_node = {
+            'image': np.random.rand(3, 4),
+            'labels': np.random.randint(0, 21128, size=(10), dtype=np.int64),
+            'bdata': np.asarray(b'11111111asdadasdasdaa')
+        }
+        keys.append('input{}'.format(i))
+        values.append(train_node)
+        if (i + 1) % 10000 == 0:
+            f.put_batch(keys, values)
+            keys.clear()
+            values.clear()
+    if len(keys):
+        f.put_batch(keys, values)
+
+    f.get_writer.put('total_num',str(n))
+    f.close()
+
+
+
+def test_random(db_path):
+    options = DB.LmdbOptions(env_open_flag=DB.LmdbFlag.MDB_RDONLY,
+                               env_open_mode=0o664,  # 8进制表示
+                               txn_flag=LMDB.LmdbFlag.MDB_RDONLY,
+                               dbi_flag=0,
+                               put_flag=0)
+    dataset = load_dataset.RandomDataset(db_path,
+                                        data_key_prefix_list=('input',),
+                                        num_key='total_num',
+                                        options = options)
+
+    dataset = dataset.parse_from_numpy_writer().shuffle(10)
+    print(len(dataset))
+    for i in tqdm(range(len(dataset)), total=len(dataset)):
+        d = dataset[i]
+        print(d)
+
+test_write(db_path)
+test_random(db_path)
+```
+
+
+
+### 7. arrow dataset 
+
+
+```python
+from fastdatasets.arrow.writer import AnythingWriter
+from fastdatasets.arrow.dataset import load_dataset,arrow
+
+
+path_file = 'd:/tmp/data.arrow'
+
+
+with_stream = False
+def test_write():
+    fs = AnythingWriter(path_file,
+                        schema={'id': 'int32', 'text': 'str', 'text2': 'str'},
+                        with_stream=with_stream,
+                        options=None)
+    for i in range(3):
+        data = {
+            "id": list(range(i * 10,(i+ 1) * 10)),
+            'text': ['asdasdasdas' + str(i) for i in range(10)],
+            'text2': ['asdasdasdas3asdadas' + str(i) for i in range(10)]
+        }
+        # fs.write_batch(data.keys(),data.values())
+        fs.write_table(data.keys(),data.values())
+
+
+    fs.close()
+
+def test_random():
+    dataset = load_dataset.RandomDataset(path_file,with_share_memory=not with_stream)
+    print('total', len(dataset))
+    for i in range(len(dataset)):
+        print(dataset[i])
+
+
+
+def test_read_iter():
+    dataset = load_dataset.IterableDataset(path_file,with_share_memory=not with_stream,batch_size=4)
+    for d in dataset:
+        print(d)
+
+
+test_write()
+
+test_random()
+
+# test_read_iter()
+
+```
+
+### 8. parquet dataset 
+
+```python
+from fastdatasets.parquet.writer import AnythingWriter
+from fastdatasets.parquet.dataset import load_dataset
+from tfrecords.python.io.arrow import ParquetReader,arrow
+
+
+path_file = 'd:/tmp/data.parquet'
+
+
+
+def test_write():
+    fs = AnythingWriter(path_file,
+                        schema={'id': 'int32','text': 'str','text2': 'str'},
+                        parquet_options=dict(write_batch_size = 10))
+    for i in range(3):
+        data = {
+            "id": list(range(i * 10,(i+ 1) * 10)),
+            'text': ['asdasdasdas' + str(i) for i in range(10)],
+            'text2': ['asdasdasdas3asdadas' + str(i) for i in range(10)]
+        }
+        # fs.write_batch(data.keys(),data.values())
+        fs.write_table(data.keys(),data.values())
+
+
+    fs.close()
+
+def test_random():
+    dataset = load_dataset.RandomDataset(path_file)
+    print('total', len(dataset))
+    for i in range(len(dataset)):
+        print(dataset[i])
+
+
+
+def test_read_iter():
+    dataset = load_dataset.IterableDataset(path_file,batch_size=4)
+    for d in dataset:
+        print(d)
+
+
+test_write()
+
+test_random()
+
+# test_read_iter()
+
+```
```

## Comparing `fastdatasets-0.9.7.post0.dist-info/RECORD` & `fastdatasets-0.9.8.dist-info/RECORD`

 * *Files 24% similar despite different names*

```diff
@@ -1,37 +1,52 @@
 fastdatasets/__init__.py,sha256=IWbktO9_fb7lbkdmAGQtoCOcTJqMjDjOdYzqd_eol_I,116
-fastdatasets/setup.py,sha256=Lld9EYqZo12jZaf-wxcC24sQ2-wFKySvtWHNb8Hvhys,2316
+fastdatasets/setup.py,sha256=M716894dnSmL_2AjOdNWrlHnRukVSUSdbKTZZhH9vT4,2310
+fastdatasets/arrow/__init__.py,sha256=rx_XXxtmJaax4fB1e_zL_1WJre3Xbww3io5i-VuLL3c,245
+fastdatasets/arrow/dataset.py,sha256=LOPYK-4VAXOIEKddaxRl8Rtcvi-I9JjbUJH7f1WVsyU,8704
+fastdatasets/arrow/default.py,sha256=TJCrx0bsMaZIx26j5k7Ln6WR0NK8kA1lTytyKBHgHDo,240
+fastdatasets/arrow/writer.py,sha256=tRj0nxE6iPWJc4L5K_ZBLKH2l4rf22Wx2H-DMg3Saws,2832
+fastdatasets/arrow/iterable_dataset/__init__.py,sha256=VJ16xmKQR7P2muFv90y_isGSh7tgeH8yIeFsGKAEjdY,9679
+fastdatasets/arrow/random_dataset/__init__.py,sha256=AV_40VTGgyDg-89_0xhEQWDP95fvdXEPnPyowuDEruI,5842
 fastdatasets/common/__init__.py,sha256=sv6vMerE_nO2Y6W-Fav0GqpVrorbFHt1UN_UjzMW-Lw,76
 fastdatasets/common/iterable_dataset.py,sha256=3c9CqZZGUjZCQd-sxKcyb7_bmjIQ0leZQOFdZijZvSE,14096
 fastdatasets/common/random_dataset.py,sha256=Z7yuPgo9G2mi7lFrfSNYw7OGFOwLZSe83KwhkCeRhd0,10767
 fastdatasets/common/writer.py,sha256=klLfxiMzJ2Ne00lFYqztBwrC8myER18TwKmHGW3Xrug,4251
 fastdatasets/common/writer_object.py,sha256=LZZ7j67e83sAn1rjqGOQHbBSWNq2JPvnpHJ8lodWMKs,1135
 fastdatasets/leveldb/__init__.py,sha256=6SYGU-XQEv88zoG69UiSpLydsLyBAWFz8nYu5oLQq1I,209
-fastdatasets/leveldb/dataset.py,sha256=NFJSACfRBmuR5CKVoxA7_Jg5OtW7gwl5tNNyYlc44e8,6031
+fastdatasets/leveldb/dataset.py,sha256=tX-pjaDqzrvAJQufuUxxFrCpbdOsI_1_O9wpn2p1I6Q,7181
+fastdatasets/leveldb/default.py,sha256=6ZP4RVLhJlzxDd8ixBcX-zFCO25j1D4G8UTy-ysXd30,206
 fastdatasets/leveldb/writer.py,sha256=ho1G57JGpXQE7D3FChHR69PilXqULC2gTRfIww3Lx6E,4489
-fastdatasets/leveldb/iterable_dataset/__init__.py,sha256=eFpT0GTxmtuPOQaPLVyDyKAi6HLbiNHJaQ00TuDi9TI,7148
-fastdatasets/leveldb/random_dataset/__init__.py,sha256=0gQTDFxc__uMdzKOXDNfiQgcEXgzp-aEAi89dj4t3H4,4716
+fastdatasets/leveldb/iterable_dataset/__init__.py,sha256=vRW2wKA48Y_OVIzdwtQJDW0syKhtbSHP2DMr2OOpwpM,7588
+fastdatasets/leveldb/random_dataset/__init__.py,sha256=vDxZm4gPpM0oiBty_9ra5w3USkLIgcshfVy4P2Dcpas,4615
 fastdatasets/lmdb/__init__.py,sha256=L9E4x2kIuVDsOgXXylwQRsURWGtZNw41t-vIxImNwuM,215
-fastdatasets/lmdb/dataset.py,sha256=H2GgEKmMkBf9FTKtvawVPDX_cxFBEwGh1BviUqGdW0Q,6778
+fastdatasets/lmdb/dataset.py,sha256=8ak1YV8Tmk-Gee6MIAudBqRJnCvhWaw6Up-qWoIG2gE,7750
+fastdatasets/lmdb/default.py,sha256=_98NgnaaHI3vhTZ59drIoEXqqo2tfcFsN8l3Ce8yoLs,602
 fastdatasets/lmdb/writer.py,sha256=hFKVyWNM-zlrmnAYi0cjBWIx-atr4XevfDn8i1Ds2rY,4696
-fastdatasets/lmdb/iterable_dataset/__init__.py,sha256=eNFFSJ7bubgga3BjM__fpcXjRj644A-55nt7315rwRE,7765
-fastdatasets/lmdb/random_dataset/__init__.py,sha256=HYRTZqia4uQcjE_OGWBx-xaSPDNAr2lSvIjd386vHxg,5796
+fastdatasets/lmdb/iterable_dataset/__init__.py,sha256=SJ6DuQwZOF2fhJXrPtE3B6PrCn5GRerfH_NZ7caxzt0,7848
+fastdatasets/lmdb/random_dataset/__init__.py,sha256=j4hCSMe2RD2dRDKZB5iMzRvzJKYRtvOafFBHctVjhLA,5541
 fastdatasets/memory/__init__.py,sha256=CYhm3FSbi0vO3UoZwpFtu3b1r4nAqGQrSOYTTF-HYa0,209
-fastdatasets/memory/dataset.py,sha256=_0cUbYO5nx2LQYmC6728Lj1nNBCO9yXYMJemIAbvjrE,4704
+fastdatasets/memory/dataset.py,sha256=tC1HT2fIGni6euoyhxeg7HAxzUUBQ2yxDTr4TyKzL2c,6598
 fastdatasets/memory/writer.py,sha256=BHytyWyoy95ICY7HxpQamyPA70Eybw8mosfiRxLz_4c,3741
-fastdatasets/memory/iterable_dataset/__init__.py,sha256=D6J0wiumaGNS3a-869I7Ceir5VdxNYojZW9sJUkxrv4,6474
-fastdatasets/memory/random_dataset/__init__.py,sha256=NOtCwaZYuKbZBZpU2GIoBqQsiCW9ziwi4a3Zu4mhOj8,4304
+fastdatasets/memory/iterable_dataset/__init__.py,sha256=K4wDaG_V1jMTw9FF8qBfVxTe4ZMvg4yo_SRPwPxU19M,6522
+fastdatasets/memory/random_dataset/__init__.py,sha256=FOcShvVmBPyvj_xWvdV0jusE_XgCm9OjSP-OmeiuMmk,4351
+fastdatasets/parquet/__init__.py,sha256=rx_XXxtmJaax4fB1e_zL_1WJre3Xbww3io5i-VuLL3c,245
+fastdatasets/parquet/dataset.py,sha256=KJ6bBBmr27Uwy-cQ4N8VqR4M8Iv-16iTwc0mV_w6tzU,8833
+fastdatasets/parquet/default.py,sha256=XpypOdIBLXF-RxAJ5kcnfN8f6K-W5mAwv-NE1A_CbWg,348
+fastdatasets/parquet/writer.py,sha256=l2RcNs8xzYudD9rbja4TiNFcpj_xC1dQ_L6RwZATT2A,2853
+fastdatasets/parquet/iterable_dataset/__init__.py,sha256=xv1m8NBUHOwqEWMlC2cp0iAdDzPPlgILYb36TLwzTQ0,9545
+fastdatasets/parquet/random_dataset/__init__.py,sha256=uAsGqesPeQD6P5X_mh2W2PEavl0QLA8GcNajSOu9p_E,5381
 fastdatasets/record/__init__.py,sha256=CYhm3FSbi0vO3UoZwpFtu3b1r4nAqGQrSOYTTF-HYa0,209
-fastdatasets/record/dataset.py,sha256=LpT-C4N5hnLLl8zkF5JSYMYwS0HQExyBJnmFEE5126A,6007
+fastdatasets/record/dataset.py,sha256=LKZEYTD_p6dCRFVRuRWr1EefYwV2bazTZbC6oli9Iv0,8161
+fastdatasets/record/default.py,sha256=QXgeT3BS_gyytuRu-nQiZkvS3Fuzk2p9QcoNkC0Hc1E,188
 fastdatasets/record/writer.py,sha256=O9XxXzh49SZuMEESbuR9EUiTvsWK4on_9smud-uGn24,3800
-fastdatasets/record/iterable_dataset/__init__.py,sha256=2T7quMNkxRH9X0PRJqV4dM1Fmin5T3EHN7HiIVTL97M,7358
-fastdatasets/record/random_dataset/__init__.py,sha256=GGTJ5kIoYrina58Vp7RV9fq9lzYwmn9DYVoRI2Bjjmc,6788
+fastdatasets/record/iterable_dataset/__init__.py,sha256=_ueVa2ATUJvLJ55oONQRYVgfFgCk6ZdPQ7x37K2qNXI,7768
+fastdatasets/record/random_dataset/__init__.py,sha256=uRZR7UozQfZIRdMNl8zYaTzs821D94UW7iKTu51O7sc,6806
 fastdatasets/torch_dataset/__init__.py,sha256=yOgyipcBb3Eciz9RASpVl1Su_y_64DvFr9Za-oCZffs,5576
-fastdatasets/utils/MEMORY.py,sha256=Nbc6_x8BVj7yG8xs4mHVVk-K_dd3tIHXA3bHdyCcZpQ,980
+fastdatasets/utils/MEMORY.py,sha256=vG4QrlMVQneWftedlIsqnSwGlT7umZj_1xrEPttT_ss,986
 fastdatasets/utils/__init__.py,sha256=y8NZKf2Vim0cuGnafXjCDS2OUSDFfmkwcmE2CdS7mgU,51
 fastdatasets/utils/numpyadapter.py,sha256=jE5Oha5DKNhdgOJlBzL7LHhR8G1skj-9UfdeLDDJZTM,12220
 fastdatasets/utils/parallel.py,sha256=7pxBQ7w26H5wo1gHEMJwFwrkl57PDUA7rF3y8GD5kjM,5457
 fastdatasets/utils/py_features.py,sha256=_yPn5zwI6SH5RLC8hTKSMp1el951_5qMDrVmTuP2xmk,346
-fastdatasets-0.9.7.post0.dist-info/METADATA,sha256=RKUj_8TxUDYNiELlxSTb98cgpSTXRGmhI0RJKCF4fgs,12115
-fastdatasets-0.9.7.post0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-fastdatasets-0.9.7.post0.dist-info/top_level.txt,sha256=cSNnK2OJMFJZoleds15VI1qc8MXMYTpgmVdwQ3jItzo,13
-fastdatasets-0.9.7.post0.dist-info/RECORD,,
+fastdatasets-0.9.8.dist-info/METADATA,sha256=oj5AX6pvzH_5Y4eb2cBxXpf2TZAcCWCF19RMdfmEDOc,15008
+fastdatasets-0.9.8.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+fastdatasets-0.9.8.dist-info/top_level.txt,sha256=cSNnK2OJMFJZoleds15VI1qc8MXMYTpgmVdwQ3jItzo,13
+fastdatasets-0.9.8.dist-info/RECORD,,
```

