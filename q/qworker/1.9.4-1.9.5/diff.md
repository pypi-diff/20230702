# Comparing `tmp/qworker-1.9.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip` & `tmp/qworker-1.9.5-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,37 +1,37 @@
-Zip file size: 320118 bytes, number of entries: 35
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qworker.libs/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qw/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qworker-1.9.4.dist-info/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qw/executor/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qw/utils/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qw/queues/
-drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 01:48 qw/wrappers/
--rw-r--r--  2.0 unx     3025 b- defN 23-Jul-02 01:48 qw/conf.py
--rw-r--r--  2.0 unx      622 b- defN 23-Jul-02 01:48 qw/version.py
--rw-r--r--  2.0 unx      137 b- defN 23-Jul-02 01:48 qw/__init__.py
--rw-r--r--  2.0 unx     4129 b- defN 23-Jul-02 01:48 qw/protocols.py
--rw-r--r--  2.0 unx    18681 b- defN 23-Jul-02 01:48 qw/client.py
--rw-r--r--  2.0 unx     2160 b- defN 23-Jul-02 01:48 qw/discovery.py
--rw-r--r--  2.0 unx     8058 b- defN 23-Jul-02 01:48 qw/process.py
--rw-r--r--  2.0 unx    22316 b- defN 23-Jul-02 01:48 qw/server.py
--rw-r--r--  2.0 unx      380 b- defN 23-Jul-02 01:48 qw/decorators.py
--rwxr-xr-x  2.0 unx   568544 b- defN 23-Jul-02 01:48 qw/exceptions.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx     2472 b- defN 23-Jul-02 01:48 qw/__main__.py
--rw-r--r--  2.0 unx     4312 b- defN 23-Jul-02 01:48 qw/executor/__init__.py
--rw-r--r--  2.0 unx       46 b- defN 23-Jul-02 01:48 qw/utils/__init__.py
--rw-r--r--  2.0 unx      512 b- defN 23-Jul-02 01:48 qw/utils/functions.py
--rwxr-xr-x  2.0 unx   434800 b- defN 23-Jul-02 01:48 qw/utils/json.cpython-39-x86_64-linux-gnu.so
--rw-r--r--  2.0 unx      597 b- defN 23-Jul-02 01:48 qw/utils/versions.py
--rw-r--r--  2.0 unx       62 b- defN 23-Jul-02 01:48 qw/queues/__init__.py
--rw-r--r--  2.0 unx     6116 b- defN 23-Jul-02 01:48 qw/queues/manager.py
--rw-r--r--  2.0 unx     1246 b- defN 23-Jul-02 01:48 qw/wrappers/func.py
--rw-r--r--  2.0 unx     4658 b- defN 23-Jul-02 01:48 qw/wrappers/di_task.py
--rw-r--r--  2.0 unx      320 b- defN 23-Jul-02 01:48 qw/wrappers/__init__.py
--rw-r--r--  2.0 unx     1483 b- defN 23-Jul-02 01:48 qw/wrappers/base.py
--rw-r--r--  2.0 unx     3170 b- defN 23-Jul-02 01:48 qworker-1.9.4.dist-info/METADATA
--rw-r--r--  2.0 unx       40 b- defN 23-Jul-02 01:48 qworker-1.9.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx     1070 b- defN 23-Jul-02 01:48 qworker-1.9.4.dist-info/LICENSE
--rw-r--r--  2.0 unx        3 b- defN 23-Jul-02 01:48 qworker-1.9.4.dist-info/top_level.txt
--rw-r--r--  2.0 unx      217 b- defN 23-Jul-02 01:48 qworker-1.9.4.dist-info/WHEEL
--rw-rw-r--  2.0 unx     2189 b- defN 23-Jul-02 01:48 qworker-1.9.4.dist-info/RECORD
-35 files, 1091365 bytes uncompressed, 316012 bytes compressed:  71.0%
+Zip file size: 320191 bytes, number of entries: 35
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qworker.libs/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qw/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qworker-1.9.5.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qw/executor/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qw/utils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qw/queues/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-02 15:20 qw/wrappers/
+-rw-r--r--  2.0 unx     3025 b- defN 23-Jul-02 15:20 qw/conf.py
+-rw-r--r--  2.0 unx      622 b- defN 23-Jul-02 15:20 qw/version.py
+-rw-r--r--  2.0 unx      137 b- defN 23-Jul-02 15:20 qw/__init__.py
+-rw-r--r--  2.0 unx     4129 b- defN 23-Jul-02 15:20 qw/protocols.py
+-rw-r--r--  2.0 unx    18681 b- defN 23-Jul-02 15:20 qw/client.py
+-rw-r--r--  2.0 unx     2160 b- defN 23-Jul-02 15:20 qw/discovery.py
+-rw-r--r--  2.0 unx     8058 b- defN 23-Jul-02 15:20 qw/process.py
+-rw-r--r--  2.0 unx    22807 b- defN 23-Jul-02 15:20 qw/server.py
+-rw-r--r--  2.0 unx      380 b- defN 23-Jul-02 15:20 qw/decorators.py
+-rwxr-xr-x  2.0 unx   568544 b- defN 23-Jul-02 15:20 qw/exceptions.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx     2472 b- defN 23-Jul-02 15:20 qw/__main__.py
+-rw-r--r--  2.0 unx     4223 b- defN 23-Jul-02 15:20 qw/executor/__init__.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Jul-02 15:20 qw/utils/__init__.py
+-rw-r--r--  2.0 unx      512 b- defN 23-Jul-02 15:20 qw/utils/functions.py
+-rwxr-xr-x  2.0 unx   434800 b- defN 23-Jul-02 15:20 qw/utils/json.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx      597 b- defN 23-Jul-02 15:20 qw/utils/versions.py
+-rw-r--r--  2.0 unx       62 b- defN 23-Jul-02 15:20 qw/queues/__init__.py
+-rw-r--r--  2.0 unx     5942 b- defN 23-Jul-02 15:20 qw/queues/manager.py
+-rw-r--r--  2.0 unx     1246 b- defN 23-Jul-02 15:20 qw/wrappers/func.py
+-rw-r--r--  2.0 unx     4939 b- defN 23-Jul-02 15:20 qw/wrappers/di_task.py
+-rw-r--r--  2.0 unx      320 b- defN 23-Jul-02 15:20 qw/wrappers/__init__.py
+-rw-r--r--  2.0 unx     1483 b- defN 23-Jul-02 15:20 qw/wrappers/base.py
+-rw-r--r--  2.0 unx     3170 b- defN 23-Jul-02 15:20 qworker-1.9.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       40 b- defN 23-Jul-02 15:20 qworker-1.9.5.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx     1070 b- defN 23-Jul-02 15:20 qworker-1.9.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx        3 b- defN 23-Jul-02 15:20 qworker-1.9.5.dist-info/top_level.txt
+-rw-r--r--  2.0 unx      217 b- defN 23-Jul-02 15:20 qworker-1.9.5.dist-info/WHEEL
+-rw-rw-r--  2.0 unx     2189 b- defN 23-Jul-02 15:20 qworker-1.9.5.dist-info/RECORD
+35 files, 1091874 bytes uncompressed, 316085 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -1,14 +1,14 @@
 Filename: qworker.libs/
 Comment: 
 
 Filename: qw/
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/
+Filename: qworker-1.9.5.dist-info/
 Comment: 
 
 Filename: qw/executor/
 Comment: 
 
 Filename: qw/utils/
 Comment: 
@@ -81,26 +81,26 @@
 
 Filename: qw/wrappers/__init__.py
 Comment: 
 
 Filename: qw/wrappers/base.py
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/METADATA
+Filename: qworker-1.9.5.dist-info/METADATA
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/entry_points.txt
+Filename: qworker-1.9.5.dist-info/entry_points.txt
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/LICENSE
+Filename: qworker-1.9.5.dist-info/LICENSE
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/top_level.txt
+Filename: qworker-1.9.5.dist-info/top_level.txt
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/WHEEL
+Filename: qworker-1.9.5.dist-info/WHEEL
 Comment: 
 
-Filename: qworker-1.9.4.dist-info/RECORD
+Filename: qworker-1.9.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qw/conf.py

```diff
@@ -9,18 +9,18 @@
     return wl
 
 ### Worker Configuration
 QW_MAX_WORKERS = config.getint('QW_MAX_WORKERS', fallback=10)
 WORKER_DEFAULT_HOST = config.get('WORKER_DEFAULT_HOST', fallback='0.0.0.0')
 WORKER_DEFAULT_PORT = config.getint('WORKER_DEFAULT_PORT', fallback=8888)
 WORKER_DEFAULT_QTY = config.getint('WORKER_DEFAULT_QTY', fallback=4)
-WORKER_QUEUE_SIZE = config.getint('WORKER_QUEUE_SIZE', fallback=8)
+WORKER_QUEUE_SIZE = config.getint('WORKER_QUEUE_SIZE', fallback=4)
 RESOURCE_THRESHOLD = config.getint('RESOURCE_THRESHOLD', fallback=90)
 CHECK_RESOURCE_USAGE = config.getboolean('CHECK_RESOURCE_USAGE', fallback=True)
-WORKER_RETRY_INTERVAL = config.getint('WORKER_RETRY_INTERVAL', fallback=10)
+WORKER_RETRY_INTERVAL = config.getint('WORKER_RETRY_INTERVAL', fallback=60)
 WORKER_RETRY_COUNT = config.getint('WORKER_RETRY_COUNT', fallback=2)
 WORKER_CONCURRENCY_NUMBER = config.getint('WORKER_CONCURRENCY_NUMBER', fallback=8)
 WORKER_TASK_TIMEOUT = config.getint('WORKER_TASK_TIMEOUT', fallback=30)
 
 ## Queue Consumed Callback
 WORKER_QUEUE_CALLBACK = config.get(
     'WORKER_QUEUE_CALLBACK', fallback=None
```

## qw/version.py

```diff
@@ -2,15 +2,15 @@
    QueueWorker is a asyncio-based Worker for distributed functions.
 """
 
 __title__ = 'qworker'
 __description__ = ('QueueWorker is asynchronous Task Queue implementation '
                    'built on top of Asyncio.'
                    'Can you spawn distributed workers to run functions inside workers.')
-__version__ = '1.9.4'
+__version__ = '1.9.5'
 __author__ = 'Jesus Lara'
 __author_email__ = 'jesuslarag@gmail.com'
 __license__ = 'MIT'
 
 def get_version() -> tuple:  # pragma: no cover
     """ Get Queue Worker version as tuple.
     """
```

## qw/server.py

```diff
@@ -452,14 +452,51 @@
             ex = ParserError(
                 f"Error Decoding Serialized Task: {ex}"
             )
             result = cloudpickle.dumps(ex)
             await self.closing_writer(writer, result)
             return False
 
+    async def return_result(self, writer: asyncio.StreamWriter, result, task, uid):
+        if result is None:
+            # Not always a Task returns Value, sometimes returns None.
+            result = [
+                {
+                    "task": task,
+                    "uuid": uid,
+                    "worker": self.name
+                }
+            ]
+        try:
+            if isinstance(result, BaseException):
+                try:
+                    msg = result.message
+                except Exception:
+                    msg = str(result)
+                result = {
+                    "exception": result.__class__,
+                    "error": msg
+                }
+            elif inspect.isgeneratorfunction(result) or isinstance(result, list):
+                try:
+                    result = json_encoder(list(result))
+                except (ValueError, TypeError):
+                    result = f"{result!r}"  # cannot pickle a generator object
+            result = cloudpickle.dumps(result)
+        except Exception as err:  # pylint: disable=W0703
+            error = {
+                "exception": err.__class__,
+                "error": str(err)
+            }
+            result = cloudpickle.dumps(error)
+            self.logger.error(
+                f'Error dumping result: {err!s}'
+            )
+        await self.closing_writer(writer, result)
+
     async def handle_queue_wrapper(
         self,
         task: QueueWrapper,
         uid: uuid.UUID,
         writer: asyncio.StreamWriter
     ):
         """Handle QueueWrapper Tasks.
@@ -468,38 +505,41 @@
         task.debug = self.debug
         if task.queued is True:
             try:
                 task.id = uid
                 await self.queue.put(
                     task, id=task.id
                 )
-                return f'Task {task!s} with id {uid} was queued.'.encode('utf-8')
+                result = f'Task {task!s} with id {uid} was queued.'.encode('utf-8')
+                return await self.return_result(writer, result, task, uid)
             except asyncio.QueueFull:
                 return await self.discard_task(
                     f"Queue in {self.name!s} is Full, discarding Task {task!r}"
                 )
+            except asyncio.TimeoutError:
+                return await self.discard_task(
+                    f"Task {task!r} in {self.name!s} discarded due Timeout"
+                )
         else:
             result = None
             try:
                 # executed and send result to client
                 executor = TaskExecutor(task)
                 result = await executor.run()
+                return await self.return_result(writer, result, task, uid)
             except Exception as err:  # pylint: disable=W0703
                 try:
                     result = cloudpickle.dumps(err)
                 except Exception as ex:  # pylint: disable=W0703
                     result = cloudpickle.dumps(
                         QWException(
-                            f'Error on Deal with Exception: {ex!s}'
+                            f'Error on Task {task!r} with Exception: {ex!s}'
                         )
                     )
                 await self.closing_writer(writer, result)
-                return False
-            finally:
-                return result
 
     async def connection_handler(
             self,
             reader: asyncio.StreamReader,
             writer: asyncio.StreamWriter
     ):
         """ Handler for Function/Task Execution.
@@ -512,15 +552,15 @@
         """
         # # TODO: task can select which executor to use, else use default:
         addr = writer.get_extra_info(
             "peername"
         )
         # first time: check signature authentication of payload:
         if not await self.signature_validation(reader, writer):
-            return False
+            await self.closing_writer(writer, None)
         self.logger.info(
             f"Received Data from {addr!r} to worker {self.name!s} pid: {self._pid}"
         )
         # after: deserialize Task:
         serialized_task = await self._read_task(reader)
         task = None
         result = None
@@ -531,71 +571,37 @@
             self.logger.error(f'No Task was received, received: {serialized_task}')
             await self.closing_writer(writer, result)
             return False
         task_uuid = task.id if task.id else uuid.uuid1(
             node=random.getrandbits(48) | 0x010000000000
         )
         if isinstance(task, QueueWrapper):
-            if not (result := await self.handle_queue_wrapper(task, task_uuid, writer)):
-                await self.closing_writer(writer, result)
-                return False
+            return await self.handle_queue_wrapper(task, task_uuid, writer)
         elif callable(task):
             executor = TaskExecutor(task)
             result = await executor.run()
+            return await self.return_result(writer, result, task, task_uuid)
         else:
             # put work in Queue:
             try:
                 await self.queue.put(task, id=task_uuid)
                 result = f'Task {task!s} was Queued.'.encode('utf-8')
+                return await self.return_result(writer, result, task, task_uuid)
             except asyncio.QueueFull:
                 return await self.discard_task(
                     message=f'Task {task!s} was discarded, queue full',
                     writer=writer
                 )
-        if result is None:
-            # Not always a Task returns Value, sometimes returns None.
-            result = [
-                {
-                    "task": task,
-                    "uuid": task_uuid,
-                    "worker": self.name
-                }
-            ]
-        try:
-            if isinstance(result, BaseException):
-                try:
-                    msg = result.message
-                except Exception:
-                    msg = str(result)
-                result = {
-                    "exception": result.__class__,
-                    "error": msg
-                }
-            elif inspect.isgeneratorfunction(result) or isinstance(result, list):
-                try:
-                    result = json_encoder(list(result))
-                except (ValueError, TypeError):
-                    result = f"{result!r}"  # cannot pickle a generator object
-            result = cloudpickle.dumps(result)
-        except Exception as err:  # pylint: disable=W0703
-            error = {
-                "exception": err.__class__,
-                "error": str(err)
-            }
-            result = cloudpickle.dumps(error)
-            self.logger.error(
-                f'Error dumping result: {err!s}'
-            )
-        await self.closing_writer(writer, result)
 
     async def closing_writer(self, writer: asyncio.StreamWriter, result):
         """Sending results and closing the streamer."""
         try:
-            writer.write(result)
-            await writer.drain()
+            if result:  # Only write non-empty results
+                writer.write(result)
+                await writer.drain()
             if writer.can_write_eof():
                 writer.write_eof()
             writer.close()
         except Exception as e:
             self.logger.error(
                 f"Error while closing writer: {str(e)}"
             )
```

## qw/executor/__init__.py

```diff
@@ -19,41 +19,36 @@
         )
         self.task = task
         self.semaphore = asyncio.Semaphore(WORKER_CONCURRENCY_NUMBER)
 
     async def run_task(self):
         result = None
         self.logger.info(
-            f"Running Task: {self.task!s}"
+            f"Creating Task: {self.task!s}"
         )
         try:
-            task = asyncio.create_task(self.task())
-            task.add_done_callback(self.task_done)
+            await self.task.create()
+            # task = asyncio.create_task(self.task())
+            # task.add_done_callback(self.task_done)
             result = await asyncio.wait_for(
-                task, timeout=WORKER_TASK_TIMEOUT * 60
+                self.task.run(), timeout=WORKER_TASK_TIMEOUT * 60
             )
         except asyncio.TimeoutError:
             raise asyncio.TimeoutError(
                 f"Task {self.task} with id {self.task.id} was cancelled."
             )
         except Exception as err:  # pylint: disable=W0703
             self.logger.error(
                 f"An Error occurred while running Task {self.task}.{self.task.id}: {err}"
             )
             result = err
         finally:
             await self.task.close()
             return result
 
-    def task_done(self, task, *args, **kwargs):
-        self.logger.info(
-            f"Finalized Task {task}"
-        )
-        return True
-
     def get_notify(self):
         # TODO: implement other notify connectors:
         # defining the Default chat object:
         recipient = Chat(
             **{"chat_id": EVENT_CHAT_ID, "chat_name": "Navigator"}
         )
         # send notifications to Telegram bot
```

## qw/queues/manager.py

```diff
@@ -97,22 +97,20 @@
         """put.
 
             Add a Task into the Queue.
         Args:
             task (QueueWrapper): an instance of QueueWrapper
         """
         try:
-            await self.queue.put(task)
+            # await self.queue.put(task)
+            asyncio.create_task(self.queue.put(task))
             await asyncio.sleep(.1)
             self.logger.info(
                 f'Task {task!s} with id {id} was queued at {int(time.time())}'
             )
-            self.logger.debug(
-                f'QUEUE Size: {self.queue.qsize()}'
-            )
             # TODO: Add broadcast event for queued task.
             return True
         except asyncio.queues.QueueFull:
             self.logger.error(
                 f"Worker Queue is Full, discarding Task {task!r}"
             )
             raise
@@ -140,15 +138,15 @@
             )
             ### Process Task:
             try:
                 executor = TaskExecutor(task)
                 result = await executor.run()
                 if type(result) == asyncio.TimeoutError:
                     raise
-                elif type(result) in (NotFound, DataNotFound, FileNotFound, FileError):
+                elif type(result) in (NotFound, DataNotFound):
                     raise
                 elif isinstance(result, BaseException):
                     ## TODO: checking retry info from Task.
                     if task.retries < WORKER_RETRY_COUNT - 1:
                         task.add_retries()
                         self.logger.warning(
                             f"Task {task} failed. Retrying. Retry count: {task.retries}"
@@ -177,10 +175,7 @@
                 raise
             finally:
                 ### Task Completed
                 self.queue.task_done()
                 await self._callback(
                     task, result=result
                 )
-            self.logger.debug(
-                f'QUEUE Size after Work: {self.queue.qsize()}'
-            )
```

## qw/wrappers/di_task.py

```diff
@@ -7,14 +7,17 @@
 from navconfig.logging import logging
 try:
     from flowtask.tasks.task import Task
     from flowtask.exceptions import (
         TaskException,
         TaskNotFound,
         TaskError,
+        FileNotFound,
+        DataNotFound,
+        NotFound,
         TaskFailed
     )
 except ImportError:
     logging.warning(
         "Unable to Load FlowTask Task Component, we can't send Tasks to any Worker."
     )
 from qw.exceptions import QWException
@@ -75,14 +78,20 @@
             )
         except TaskNotFound as ex:
             raise TaskNotFound(
                 f"Task Not Found: {ex}"
             )
         except TaskError:
             raise
+        except (
+            DataNotFound,
+            NotFound
+        ) as err:
+            logging.warning(err)
+            raise
         except Exception as err:
             logging.exception(err, stack_info=True)
             raise QWException(
                 f"{err}"
             ) from err
 
     def __await__(self):
@@ -110,31 +119,30 @@
             )
         finally:
             await self.close()
 
     async def run(self):
         """ Running the Task in the loop."""
         result = None
-        print(f':: Starting Task {self.program}.{self.task}')
         async with self._task as task:
             try:
                 status = await task.start()
                 if not status:
                     raise TaskError(
-                        f'Error on Task: {self.program}.{self.task}'
+                        f'Error starting Task: {self.program}.{self.task}'
                     )
             except Exception as err:
                 logging.error(str(err), exc_info=True)
                 if isinstance(err, TaskException):
                     raise
                 raise TaskFailed(
                     f"{err}"
                 ) from err
-            print(
-                f'Executing Task {self.program}.{self.task}'
+            logging.info(
+                f'Executing Task {self.program}.{self.task} with id {self.id}'
             )
             try:
                 result = await task.run()
             except Exception as err:
                 logging.exception(err, stack_info=False)
                 if isinstance(err, TaskException):
                     raise
@@ -142,14 +150,17 @@
                     raise TaskFailed(
                         f"{err}"
                     ) from err
         return result
 
     async def close(self):
         try:
+            logging.info(
+                f'Closing Task {self.program}.{self.task} with id {self.id}'
+            )
             if self._task:
                 await self._task.close()
                 self._task = None
         except Exception as err:  # pylint: disable=W0703
             logging.error(err)
 
     def __str__(self):
```

## Comparing `qworker-1.9.4.dist-info/METADATA` & `qworker-1.9.5.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qworker
-Version: 1.9.4
+Version: 1.9.5
 Summary: QueueWorker is asynchronous Task Queue implementation built on top of Asyncio.Can you spawn distributed workers to run functions inside workers.
 Home-page: https://github.com/phenobarbital/qworker
 Author: Jesus Lara
 Author-email: jesuslara@phenobarbital.info
 License: MIT
 Project-URL: Source, https://github.com/phenobarbital/qworker
 Project-URL: Funding, https://paypal.me/phenobarbital
```

## Comparing `qworker-1.9.4.dist-info/LICENSE` & `qworker-1.9.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qworker-1.9.4.dist-info/RECORD` & `qworker-1.9.5.dist-info/RECORD`

 * *Files 26% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-qw/conf.py,sha256=Snq4rBPQWBjt8WD2AZihf7ulj-ELqNhJIj-aRQHHUQM,3025
-qw/version.py,sha256=oXNZmLgrn166Wre49mnBQklUyP9jASCu7Tu9PKUvYFI,622
+qw/conf.py,sha256=55ehmQd9JB9QLDSEwMVFSabCNRgjfNlmOZ0USRIAPK4,3025
+qw/version.py,sha256=xnP4Hq46t76bF-bY-rnuTI9H_N2X-edvDbYb7XKjEIU,622
 qw/__init__.py,sha256=awMNjg7WGznbrNuJLoEJV6BbnREk4BPD8k9My8BD7Uo,137
 qw/protocols.py,sha256=I57MsY72OlVlGFT1-ggZCm4nWAd89m4m5YEwGUWGAmg,4129
 qw/client.py,sha256=LxPM7M58e0q2ZoY3D_DtoYtqX8B1zU32XaUDmb_D5v0,18681
 qw/discovery.py,sha256=l_Lb3Bmni6WTTu5fxzj4-9KquiRak1rq8k9I70f_hSI,2160
 qw/process.py,sha256=TnA7JKqMmWaLQCKQEFY3A8Q1poC0cMtpsHuB412OJgw,8058
-qw/server.py,sha256=AayTXWM5HEw4sYndYoMvfMafbaXfJP8j3ByLfTckc8U,22316
+qw/server.py,sha256=xocrWZVA-6pAOBWPmV8lxVs3gObq25cxNHh9Vpu4KT4,22807
 qw/decorators.py,sha256=lL5CN9a7gUi8iDEfOI7kSpABkScu5cE99yj9eYGn69w,380
 qw/exceptions.cpython-39-x86_64-linux-gnu.so,sha256=IYBQ4BGCKA3duo8PEQylQhSS4BTYgAJ_rNjMOWU2nf0,568544
 qw/__main__.py,sha256=H52Hv_ll_8Nwnbj4UAhGfe3bfCbAhYAdH6f5Y3kbpFo,2472
-qw/executor/__init__.py,sha256=a1u8x1LGrY_CJb7huvGipAUXxKsDdgH2lW_Xmn50GbI,4312
+qw/executor/__init__.py,sha256=-ihw1F6p0MS9pdsMKcA7q8KRAJHVnBJB1_UAv_1jw_k,4223
 qw/utils/__init__.py,sha256=bYf_I4ymTf8vsqMjK00NJi2-A-lVijPTwN6HDOVjcjQ,46
 qw/utils/functions.py,sha256=9iXVvYLtQzOK0tRecOV2Oqrl-2raxAHwBCNilLui1xQ,512
 qw/utils/json.cpython-39-x86_64-linux-gnu.so,sha256=jHsLujozPguK1eL9t-Grw2KYTVlZvo4k7rLtF8a6YuU,434800
 qw/utils/versions.py,sha256=d8AdLmhM1bPc82vTAshCJBljGr-Ur3_hiZ_GtTbbORA,597
 qw/queues/__init__.py,sha256=itGqt8q7feqZePbUWJTeA82Q2AwU0B2agUFXRmpLuDc,62
-qw/queues/manager.py,sha256=PfUGOT5o8QwgBQBKwfc619MtLkceh2QpbnNLRRwOc2c,6116
+qw/queues/manager.py,sha256=yShwrALLo3a85GtWBtZGrJ9cR-JKPVDbXZSt_209tJg,5942
 qw/wrappers/func.py,sha256=sL43tB6BWPbz-iOpkySTaOIpGtd6J1F37R8V7WfwcE8,1246
-qw/wrappers/di_task.py,sha256=p1Yfa_rqDJ5w6VR8XtP9fKp8PvvndzJL08Izc1W60OE,4658
+qw/wrappers/di_task.py,sha256=02fRJg_zdSt8Lw2iqrIGGkH0vrJyenm7x4p7FGnoguM,4939
 qw/wrappers/__init__.py,sha256=Ot_f0GTaDB50Za4Hxsz2FZSkZDn4zZoDHjXOvqj9T9k,320
 qw/wrappers/base.py,sha256=2gazsxzyqwJOjopfJZsRX9Lif6o4upusAKkvrUIQoBM,1483
-qworker-1.9.4.dist-info/METADATA,sha256=bdcnRp9KVjlqFKYwt_LUhhfU4Brp0whH5qjznXhqmFU,3170
-qworker-1.9.4.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
-qworker-1.9.4.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
-qworker-1.9.4.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
-qworker-1.9.4.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
-qworker-1.9.4.dist-info/RECORD,,
+qworker-1.9.5.dist-info/METADATA,sha256=FtfRRa9OmNpdVvxcO9UwNC1hodNVuf1be7bEefzOOzw,3170
+qworker-1.9.5.dist-info/entry_points.txt,sha256=ooHTYYyEjHI9Rj79mHQmU_s4HP-PTvD3g3xSa26vIGc,40
+qworker-1.9.5.dist-info/LICENSE,sha256=EW8vB8vWRFvBxGC3soG2HCxkuNrQpOGUgNFd81h7u54,1070
+qworker-1.9.5.dist-info/top_level.txt,sha256=2NxbFCeI_G1kzzf628VnbDchanCX6rcTJORdENY-rHI,3
+qworker-1.9.5.dist-info/WHEEL,sha256=G1-Tt_WO10x9oXjmzv4wXxMLbyVnQaK1ig1QydJbPuA,217
+qworker-1.9.5.dist-info/RECORD,,
```

