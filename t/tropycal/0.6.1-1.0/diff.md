# Comparing `tmp/tropycal-0.6.1.tar.gz` & `tmp/tropycal-1.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "tropycal-0.6.1.tar", last modified: Sun Dec 11 19:25:54 2022, max compression
+gzip compressed data, was "tropycal-1.0.tar", last modified: Sun Jul  2 19:56:54 2023, max compression
```

## Comparing `tropycal-0.6.1.tar` & `tropycal-1.0.tar`

### file list

```diff
@@ -1,144 +1,162 @@
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.936208 tropycal-0.6.1/
--rw-r--r--   0 tomerburg   (501) staff       (20)       66 2022-10-12 23:32:59.000000 tropycal-0.6.1/.gitattributes
--rw-r--r--   0 tomerburg   (501) staff       (20)      113 2022-10-12 23:32:59.000000 tropycal-0.6.1/.gitignore
--rw-r--r--   0 tomerburg   (501) staff       (20)     2485 2022-10-12 23:32:59.000000 tropycal-0.6.1/.travis.yml
--rw-r--r--   0 tomerburg   (501) staff       (20)     1057 2022-10-12 23:32:59.000000 tropycal-0.6.1/LICENSE
--rw-r--r--   0 tomerburg   (501) staff       (20)       16 2022-10-12 23:32:59.000000 tropycal-0.6.1/MANIFEST.in
--rw-r--r--   0 tomerburg   (501) staff       (20)     5064 2022-12-11 19:25:54.936314 tropycal-0.6.1/PKG-INFO
--rw-r--r--   0 tomerburg   (501) staff       (20)     4368 2022-12-11 19:25:29.000000 tropycal-0.6.1/README.md
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.891629 tropycal-0.6.1/ci/
--rw-r--r--   0 tomerburg   (501) staff       (20)     4452 2022-10-12 23:32:59.000000 tropycal-0.6.1/ci/deploy_key.enc
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.893899 tropycal-0.6.1/docs/
--rw-r--r--   0 tomerburg   (501) staff       (20)      642 2022-12-11 19:25:29.000000 tropycal-0.6.1/docs/Makefile
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.911780 tropycal-0.6.1/docs/_static/
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.912966 tropycal-0.6.1/docs/_static/css/
--rw-r--r--   0 tomerburg   (501) staff       (20)     3275 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/badge_only.css
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.922327 tropycal-0.6.1/docs/_static/css/fonts/
--rw-r--r--   0 tomerburg   (501) staff       (20)    87624 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Bold.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)    67312 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Bold.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)    86288 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Regular.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)    66444 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Regular.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)   165742 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.eot
--rw-r--r--   0 tomerburg   (501) staff       (20)   444379 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.svg
--rw-r--r--   0 tomerburg   (501) staff       (20)   165548 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.ttf
--rw-r--r--   0 tomerburg   (501) staff       (20)    98024 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)    77160 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)   323344 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-bold-italic.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)   193308 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-bold-italic.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)   309728 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-bold.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)   184912 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-bold.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)   328412 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-normal-italic.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)   195704 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-normal-italic.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)   309192 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-normal.woff
--rw-r--r--   0 tomerburg   (501) staff       (20)   182708 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/fonts/lato-normal.woff2
--rw-r--r--   0 tomerburg   (501) staff       (20)   135191 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/css/theme.css
--rw-r--r--   0 tomerburg   (501) staff       (20)   530613 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/dorian.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   665534 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/grid_example_1.png
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.923211 tropycal-0.6.1/docs/_static/js/
--rw-r--r--   0 tomerburg   (501) staff       (20)      934 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/js/badge_only.js
--rw-r--r--   0 tomerburg   (501) staff       (20)     5023 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/js/theme.js
--rw-r--r--   0 tomerburg   (501) staff       (20)    11979 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/logo.png
--rw-r--r--   0 tomerburg   (501) staff       (20)    94302 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/logo_32.ico
--rw-r--r--   0 tomerburg   (501) staff       (20)   566670 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/michael_example_1.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   612897 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/michael_example_2.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   565722 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/michael_example_3.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   583690 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/michael_example_4.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   446960 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/storm_example_1.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   401309 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/storm_example_2.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   398432 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/storm_example_3.png
--rw-r--r--   0 tomerburg   (501) staff       (20)      328 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/theme_override.css
--rw-r--r--   0 tomerburg   (501) staff       (20)   726348 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/tornado_example_1.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   307446 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/tornado_example_2.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   172063 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/tornado_example_3.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   152687 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/tornado_example_5.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   277793 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/trackdataset_example_1.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   357620 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/trackdataset_example_2.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   245771 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/trackdataset_example_3.png
--rw-r--r--   0 tomerburg   (501) staff       (20)   217745 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_static/trackdataset_example_4.png
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.888746 tropycal-0.6.1/docs/_templates/
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.924382 tropycal-0.6.1/docs/_templates/autosummary/
--rw-r--r--   0 tomerburg   (501) staff       (20)      181 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_templates/autosummary/base.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)      699 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_templates/autosummary/class.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)     1210 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/_templates/autosummary/module.rst
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.924661 tropycal-0.6.1/docs/_templates/overrides/
--rw-r--r--   0 tomerburg   (501) staff       (20)      722 2022-12-09 19:59:04.000000 tropycal-0.6.1/docs/_templates/overrides/tropycal.utils.rst
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.925027 tropycal-0.6.1/docs/api/
--rw-r--r--   0 tomerburg   (501) staff       (20)      680 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/api/index.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)     7517 2022-12-11 19:25:29.000000 tropycal-0.6.1/docs/conf.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     6077 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/data.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)     1704 2022-12-11 19:25:29.000000 tropycal-0.6.1/docs/index.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)      935 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/install.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)     1108 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/make.bat
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.925757 tropycal-0.6.1/docs/options/
--rw-r--r--   0 tomerburg   (501) staff       (20)     2094 2022-12-09 19:59:04.000000 tropycal-0.6.1/docs/options/domain.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)      102 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/options/gridded_stats.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)    10181 2022-12-09 19:59:04.000000 tropycal-0.6.1/docs/options/map_prop.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)       76 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/requirements.txt
--rw-r--r--   0 tomerburg   (501) staff       (20)      554 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/sample_usage.rst
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.926523 tropycal-0.6.1/docs/samples/
--rw-r--r--   0 tomerburg   (501) staff       (20)     6253 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/samples/tracks.TrackDataset.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)    10328 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/samples/tracks.storm.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)     4359 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/samples/tracks.tornado.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)      505 2022-12-09 19:59:04.000000 tropycal-0.6.1/docs/support.rst
--rw-r--r--   0 tomerburg   (501) staff       (20)      114 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/test.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     7375 2022-10-12 23:32:59.000000 tropycal-0.6.1/docs/tropycal_full_logo.png
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.928600 tropycal-0.6.1/examples/
--rw-r--r--   0 tomerburg   (501) staff       (20)       96 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/README.txt
--rw-r--r--   0 tomerburg   (501) staff       (20)     5534 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/analogs.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     6171 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/customize_storm.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     6297 2022-12-11 19:25:29.000000 tropycal-0.6.1/examples/realtime.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     7105 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/recon.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     5164 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/tc_rainfall.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     8731 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/tracks.TrackDataset.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     2771 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/tracks.season.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     7465 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/tracks.storm.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     4451 2022-10-12 23:32:59.000000 tropycal-0.6.1/examples/tracks.tornado.py
--rw-r--r--   0 tomerburg   (501) staff       (20)      997 2022-12-11 19:25:54.936796 tropycal-0.6.1/setup.cfg
--rw-r--r--   0 tomerburg   (501) staff       (20)      119 2022-10-12 23:32:59.000000 tropycal-0.6.1/setup.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.889292 tropycal-0.6.1/src/
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.929113 tropycal-0.6.1/src/tropycal/
--rw-r--r--   0 tomerburg   (501) staff       (20)      159 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)      447 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/_version.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     4795 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/constants.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.930157 tropycal-0.6.1/src/tropycal/plot/
--rw-r--r--   0 tomerburg   (501) staff       (20)       22 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/plot/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    14329 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/plot/plot.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.930667 tropycal-0.6.1/src/tropycal/rain/
--rw-r--r--   0 tomerburg   (501) staff       (20)      183 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/rain/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    11522 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/rain/dataset.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    26323 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/rain/plot.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.931303 tropycal-0.6.1/src/tropycal/realtime/
--rw-r--r--   0 tomerburg   (501) staff       (20)      131 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/realtime/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    37908 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/realtime/realtime.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    46614 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/realtime/storm.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.932635 tropycal-0.6.1/src/tropycal/recon/
--rw-r--r--   0 tomerburg   (501) staff       (20)      239 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/recon/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)   120512 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/recon/dataset.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    37336 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/recon/plot.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    38784 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/recon/realtime.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    60182 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/recon/tools.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.933627 tropycal-0.6.1/src/tropycal/tornado/
--rw-r--r--   0 tomerburg   (501) staff       (20)      175 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/tornado/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    15955 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/tornado/dataset.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     8951 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/tornado/plot.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     1948 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/tornado/tools.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.934898 tropycal-0.6.1/src/tropycal/tracks/
--rw-r--r--   0 tomerburg   (501) staff       (20)      223 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/tracks/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)   193323 2022-12-11 19:25:29.000000 tropycal-0.6.1/src/tropycal/tracks/dataset.py
--rw-r--r--   0 tomerburg   (501) staff       (20)   122792 2022-12-11 19:25:29.000000 tropycal-0.6.1/src/tropycal/tracks/plot.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    20263 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/tracks/season.py
--rw-r--r--   0 tomerburg   (501) staff       (20)   118996 2022-12-11 19:25:29.000000 tropycal-0.6.1/src/tropycal/tracks/storm.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    33630 2022-12-11 19:25:29.000000 tropycal-0.6.1/src/tropycal/tracks/tools.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.935834 tropycal-0.6.1/src/tropycal/utils/
--rw-r--r--   0 tomerburg   (501) staff       (20)      132 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/utils/__init__.py
--rw-r--r--   0 tomerburg   (501) staff       (20)     1796 2022-10-12 23:32:59.000000 tropycal-0.6.1/src/tropycal/utils/cartopy_utils.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    11311 2022-12-09 19:59:04.000000 tropycal-0.6.1/src/tropycal/utils/colors.py
--rw-r--r--   0 tomerburg   (501) staff       (20)    64886 2022-12-11 19:25:29.000000 tropycal-0.6.1/src/tropycal/utils/generic_utils.py
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.929833 tropycal-0.6.1/src/tropycal.egg-info/
--rw-r--r--   0 tomerburg   (501) staff       (20)     5064 2022-12-11 19:25:54.000000 tropycal-0.6.1/src/tropycal.egg-info/PKG-INFO
--rw-r--r--   0 tomerburg   (501) staff       (20)     3543 2022-12-11 19:25:54.000000 tropycal-0.6.1/src/tropycal.egg-info/SOURCES.txt
--rw-r--r--   0 tomerburg   (501) staff       (20)        1 2022-12-11 19:25:54.000000 tropycal-0.6.1/src/tropycal.egg-info/dependency_links.txt
--rw-r--r--   0 tomerburg   (501) staff       (20)      119 2022-12-11 19:25:54.000000 tropycal-0.6.1/src/tropycal.egg-info/requires.txt
--rw-r--r--   0 tomerburg   (501) staff       (20)        9 2022-12-11 19:25:54.000000 tropycal-0.6.1/src/tropycal.egg-info/top_level.txt
-drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2022-12-11 19:25:54.936026 tropycal-0.6.1/tests/
--rw-r--r--   0 tomerburg   (501) staff       (20)     3777 2022-10-12 23:32:59.000000 tropycal-0.6.1/tests/tracks.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.559933 tropycal-1.0/
+-rw-r--r--   0 tomerburg   (501) staff       (20)       66 2022-10-12 23:32:59.000000 tropycal-1.0/.gitattributes
+-rw-r--r--   0 tomerburg   (501) staff       (20)      113 2022-10-12 23:32:59.000000 tropycal-1.0/.gitignore
+-rw-r--r--   0 tomerburg   (501) staff       (20)     2485 2022-10-12 23:32:59.000000 tropycal-1.0/.travis.yml
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1057 2022-10-12 23:32:59.000000 tropycal-1.0/LICENSE
+-rw-r--r--   0 tomerburg   (501) staff       (20)       16 2022-10-12 23:32:59.000000 tropycal-1.0/MANIFEST.in
+-rw-r--r--   0 tomerburg   (501) staff       (20)     5060 2023-07-02 19:56:54.560047 tropycal-1.0/PKG-INFO
+-rw-r--r--   0 tomerburg   (501) staff       (20)     4366 2023-07-02 19:56:43.000000 tropycal-1.0/README.md
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.498901 tropycal-1.0/ci/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     4452 2022-10-12 23:32:59.000000 tropycal-1.0/ci/deploy_key.enc
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.502188 tropycal-1.0/docs/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      642 2022-12-11 19:25:29.000000 tropycal-1.0/docs/Makefile
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.524254 tropycal-1.0/docs/_static/
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.525757 tropycal-1.0/docs/_static/css/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     3275 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/badge_only.css
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.539221 tropycal-1.0/docs/_static/css/fonts/
+-rw-r--r--   0 tomerburg   (501) staff       (20)    87624 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Bold.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)    67312 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Bold.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)    86288 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Regular.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)    66444 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Regular.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)   165742 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.eot
+-rw-r--r--   0 tomerburg   (501) staff       (20)   444379 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.svg
+-rw-r--r--   0 tomerburg   (501) staff       (20)   165548 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.ttf
+-rw-r--r--   0 tomerburg   (501) staff       (20)    98024 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)    77160 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)   323344 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-bold-italic.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)   193308 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-bold-italic.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)   309728 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-bold.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)   184912 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-bold.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)   328412 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-normal-italic.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)   195704 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-normal-italic.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)   309192 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-normal.woff
+-rw-r--r--   0 tomerburg   (501) staff       (20)   182708 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/fonts/lato-normal.woff2
+-rw-r--r--   0 tomerburg   (501) staff       (20)   135191 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/css/theme.css
+-rw-r--r--   0 tomerburg   (501) staff       (20)   530613 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/dorian.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   665534 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/grid_example_1.png
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.540112 tropycal-1.0/docs/_static/js/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      934 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/js/badge_only.js
+-rw-r--r--   0 tomerburg   (501) staff       (20)     5023 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/js/theme.js
+-rw-r--r--   0 tomerburg   (501) staff       (20)    11979 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/logo.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)    94302 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/logo_32.ico
+-rw-r--r--   0 tomerburg   (501) staff       (20)   566670 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/michael_example_1.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   612897 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/michael_example_2.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   565722 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/michael_example_3.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   583690 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/michael_example_4.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   446960 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/storm_example_1.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   401309 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/storm_example_2.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   398432 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/storm_example_3.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)      328 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/theme_override.css
+-rw-r--r--   0 tomerburg   (501) staff       (20)   726348 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/tornado_example_1.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   307446 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/tornado_example_2.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   172063 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/tornado_example_3.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   152687 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/tornado_example_5.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   277793 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/trackdataset_example_1.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   357620 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/trackdataset_example_2.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   245771 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/trackdataset_example_3.png
+-rw-r--r--   0 tomerburg   (501) staff       (20)   217745 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_static/trackdataset_example_4.png
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.493476 tropycal-1.0/docs/_templates/
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.541432 tropycal-1.0/docs/_templates/autosummary/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      181 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_templates/autosummary/base.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)      699 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_templates/autosummary/class.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1210 2022-10-12 23:32:59.000000 tropycal-1.0/docs/_templates/autosummary/module.rst
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.541638 tropycal-1.0/docs/_templates/overrides/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      772 2023-07-02 19:56:43.000000 tropycal-1.0/docs/_templates/overrides/tropycal.utils.rst
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.542014 tropycal-1.0/docs/api/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      698 2023-07-02 19:56:43.000000 tropycal-1.0/docs/api/index.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)     7515 2023-07-02 19:56:43.000000 tropycal-1.0/docs/conf.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     6077 2022-10-12 23:32:59.000000 tropycal-1.0/docs/data.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1619 2023-07-02 19:56:43.000000 tropycal-1.0/docs/index.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)      935 2022-10-12 23:32:59.000000 tropycal-1.0/docs/install.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1108 2022-10-12 23:32:59.000000 tropycal-1.0/docs/make.bat
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.542842 tropycal-1.0/docs/options/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     2094 2022-12-09 19:59:04.000000 tropycal-1.0/docs/options/domain.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)      102 2022-10-12 23:32:59.000000 tropycal-1.0/docs/options/gridded_stats.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)    10181 2022-12-09 19:59:04.000000 tropycal-1.0/docs/options/map_prop.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)       76 2022-10-12 23:32:59.000000 tropycal-1.0/docs/requirements.txt
+-rw-r--r--   0 tomerburg   (501) staff       (20)      554 2022-10-12 23:32:59.000000 tropycal-1.0/docs/sample_usage.rst
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.543577 tropycal-1.0/docs/samples/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     6253 2022-10-12 23:32:59.000000 tropycal-1.0/docs/samples/tracks.TrackDataset.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)    10328 2022-10-12 23:32:59.000000 tropycal-1.0/docs/samples/tracks.storm.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)     4359 2022-10-12 23:32:59.000000 tropycal-1.0/docs/samples/tracks.tornado.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)      505 2022-12-09 19:59:04.000000 tropycal-1.0/docs/support.rst
+-rw-r--r--   0 tomerburg   (501) staff       (20)      114 2022-12-15 14:18:51.000000 tropycal-1.0/docs/test.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     7375 2022-10-12 23:32:59.000000 tropycal-1.0/docs/tropycal_full_logo.png
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.546879 tropycal-1.0/examples/
+-rw-r--r--   0 tomerburg   (501) staff       (20)       96 2022-10-12 23:32:59.000000 tropycal-1.0/examples/README.txt
+-rw-r--r--   0 tomerburg   (501) staff       (20)     5534 2023-06-28 02:18:34.000000 tropycal-1.0/examples/analogs.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    10897 2023-07-02 19:56:43.000000 tropycal-1.0/examples/cartopy.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     6171 2023-06-28 02:18:34.000000 tropycal-1.0/examples/customize_storm.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     6297 2023-06-28 02:18:34.000000 tropycal-1.0/examples/realtime.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     7105 2023-06-28 02:18:34.000000 tropycal-1.0/examples/recon.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     4456 2023-07-02 19:56:43.000000 tropycal-1.0/examples/ships.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     5164 2023-06-28 02:18:34.000000 tropycal-1.0/examples/tc_rainfall.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     8733 2023-07-02 19:56:43.000000 tropycal-1.0/examples/tracks.TrackDataset.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     2757 2023-07-02 19:56:43.000000 tropycal-1.0/examples/tracks.season.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     7465 2023-06-28 02:18:34.000000 tropycal-1.0/examples/tracks.storm.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     4451 2023-06-28 02:18:34.000000 tropycal-1.0/examples/tracks.tornado.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)      995 2023-07-02 19:56:54.560435 tropycal-1.0/setup.cfg
+-rw-r--r--   0 tomerburg   (501) staff       (20)      121 2023-07-02 19:56:43.000000 tropycal-1.0/setup.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.494749 tropycal-1.0/src/
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.547950 tropycal-1.0/src/tropycal/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      159 2022-12-15 14:18:51.000000 tropycal-1.0/src/tropycal/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)      447 2022-12-15 14:18:51.000000 tropycal-1.0/src/tropycal/_version.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     5103 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/constants.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.549367 tropycal-1.0/src/tropycal/plot/
+-rw-r--r--   0 tomerburg   (501) staff       (20)       23 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/plot/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    14158 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/plot/plot.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.550651 tropycal-1.0/src/tropycal/rain/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      184 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/rain/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    11110 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/rain/dataset.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    27902 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/rain/plot.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.552048 tropycal-1.0/src/tropycal/realtime/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      132 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/realtime/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    37999 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/realtime/realtime.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    47715 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/realtime/storm.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.553976 tropycal-1.0/src/tropycal/recon/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      240 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/recon/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)   122579 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/recon/dataset.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    39097 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/recon/plot.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    39275 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/recon/realtime.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    63621 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/recon/tools.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.554550 tropycal-1.0/src/tropycal/ships/
+-rw-r--r--   0 tomerburg   (501) staff       (20)       85 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/ships/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    15819 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/ships/ships.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.555486 tropycal-1.0/src/tropycal/tornado/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      176 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tornado/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    15765 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tornado/dataset.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     8853 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tornado/plot.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1941 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tornado/tools.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.557065 tropycal-1.0/src/tropycal/tracks/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      224 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/__init__.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.557705 tropycal-1.0/src/tropycal/tracks/data/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1249 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/data/catarina.csv
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1924 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/data/pacific_2006.csv
+-rw-r--r--   0 tomerburg   (501) staff       (20)   213770 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/dataset.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)   129964 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/plot.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    20570 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/season.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)   123468 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/storm.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    31259 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/tracks/tools.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.558530 tropycal-1.0/src/tropycal/utils/
+-rw-r--r--   0 tomerburg   (501) staff       (20)      132 2022-10-12 23:32:59.000000 tropycal-1.0/src/tropycal/utils/__init__.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    10482 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/utils/cartopy_utils.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    11465 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/utils/colors.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)    71423 2023-07-02 19:56:43.000000 tropycal-1.0/src/tropycal/utils/generic_utils.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.548800 tropycal-1.0/src/tropycal.egg-info/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     5060 2023-07-02 19:56:54.000000 tropycal-1.0/src/tropycal.egg-info/PKG-INFO
+-rw-r--r--   0 tomerburg   (501) staff       (20)     3897 2023-07-02 19:56:54.000000 tropycal-1.0/src/tropycal.egg-info/SOURCES.txt
+-rw-r--r--   0 tomerburg   (501) staff       (20)        1 2023-07-02 19:56:54.000000 tropycal-1.0/src/tropycal.egg-info/dependency_links.txt
+-rw-r--r--   0 tomerburg   (501) staff       (20)      119 2023-07-02 19:56:54.000000 tropycal-1.0/src/tropycal.egg-info/requires.txt
+-rw-r--r--   0 tomerburg   (501) staff       (20)        9 2023-07-02 19:56:54.000000 tropycal-1.0/src/tropycal.egg-info/top_level.txt
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.496365 tropycal-1.0/tests/
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.558986 tropycal-1.0/tests/data/
+-rw-r--r--   0 tomerburg   (501) staff       (20)   136603 2023-07-02 19:56:43.000000 tropycal-1.0/tests/data/sample_hurdat.txt
+-rw-r--r--   0 tomerburg   (501) staff       (20)      782 2023-07-02 19:56:43.000000 tropycal-1.0/tests/data/sample_storm.txt
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.559148 tropycal-1.0/tests/ships/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     3566 2023-07-02 19:56:43.000000 tropycal-1.0/tests/ships/test_ships.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.559629 tropycal-1.0/tests/tracks/
+-rw-r--r--   0 tomerburg   (501) staff       (20)     4840 2023-07-02 19:56:43.000000 tropycal-1.0/tests/tracks/test_dataset.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     1599 2023-07-02 19:56:43.000000 tropycal-1.0/tests/tracks/test_season.py
+-rw-r--r--   0 tomerburg   (501) staff       (20)     6755 2023-07-02 19:56:43.000000 tropycal-1.0/tests/tracks/test_storm.py
+drwxr-xr-x   0 tomerburg   (501) staff       (20)        0 2023-07-02 19:56:54.559784 tropycal-1.0/tests/utils/
+-rw-r--r--   0 tomerburg   (501) staff       (20)    19452 2023-07-02 19:56:43.000000 tropycal-1.0/tests/utils/test_utils.py
```

### Comparing `tropycal-0.6.1/.travis.yml` & `tropycal-1.0/.travis.yml`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/LICENSE` & `tropycal-1.0/LICENSE`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/PKG-INFO` & `tropycal-1.0/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tropycal
-Version: 0.6.1
+Version: 1.0
 Summary: Package for retrieving and analyzing tropical cyclone data
 Home-page: https://github.com/tropycal/tropycal
 Author: Tomer Burg, Sam Lillo
 Project-URL: Documentation, https://tropycal.github.io/tropycal/
 Project-URL: Source Code, https://github.com/tropycal/tropycal
 Keywords: meteorology,weather
 Platform: any
@@ -18,15 +18,15 @@
 License-File: LICENSE
 
 # Tropycal
 Tropycal is a Python package intended to simplify the process of retrieving and analyzing tropical cyclone data, both for past storms and in real time, and is geared towards the research and operational meteorology sectors.
 
 Tropycal can read in HURDAT2 and IBTrACS reanalysis data and operational National Hurricane Center (NHC) Best Track data and conform them to the same format, which can be used to perform climatological, seasonal and individual storm analyses. For each individual storm, operational NHC and model forecasts, aircraft reconnaissance data, rainfall data, and any associated tornado activity can be retrieved and plotted.
 
-The latest version of Tropycal is v0.6.1.
+The latest version of Tropycal is v1.0.
 
 ## Installation
 
 
 ### Conda
 
 The currently recommended method of installation is via conda:
```

### Comparing `tropycal-0.6.1/README.md` & `tropycal-1.0/README.md`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Tropycal
 Tropycal is a Python package intended to simplify the process of retrieving and analyzing tropical cyclone data, both for past storms and in real time, and is geared towards the research and operational meteorology sectors.
 
 Tropycal can read in HURDAT2 and IBTrACS reanalysis data and operational National Hurricane Center (NHC) Best Track data and conform them to the same format, which can be used to perform climatological, seasonal and individual storm analyses. For each individual storm, operational NHC and model forecasts, aircraft reconnaissance data, rainfall data, and any associated tornado activity can be retrieved and plotted.
 
-The latest version of Tropycal is v0.6.1.
+The latest version of Tropycal is v1.0.
 
 ## Installation
 
 
 ### Conda
 
 The currently recommended method of installation is via conda:
```

### Comparing `tropycal-0.6.1/ci/deploy_key.enc` & `tropycal-1.0/ci/deploy_key.enc`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/Makefile` & `tropycal-1.0/docs/Makefile`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/badge_only.css` & `tropycal-1.0/docs/_static/css/badge_only.css`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Bold.woff` & `tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Bold.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Bold.woff2` & `tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Bold.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Regular.woff` & `tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Regular.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/Roboto-Slab-Regular.woff2` & `tropycal-1.0/docs/_static/css/fonts/Roboto-Slab-Regular.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.eot` & `tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.eot`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.svg` & `tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.svg`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.ttf` & `tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.ttf`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.woff` & `tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/fontawesome-webfont.woff2` & `tropycal-1.0/docs/_static/css/fonts/fontawesome-webfont.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-bold-italic.woff` & `tropycal-1.0/docs/_static/css/fonts/lato-bold-italic.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-bold-italic.woff2` & `tropycal-1.0/docs/_static/css/fonts/lato-bold-italic.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-bold.woff` & `tropycal-1.0/docs/_static/css/fonts/lato-bold.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-bold.woff2` & `tropycal-1.0/docs/_static/css/fonts/lato-bold.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-normal-italic.woff` & `tropycal-1.0/docs/_static/css/fonts/lato-normal-italic.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-normal-italic.woff2` & `tropycal-1.0/docs/_static/css/fonts/lato-normal-italic.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-normal.woff` & `tropycal-1.0/docs/_static/css/fonts/lato-normal.woff`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/fonts/lato-normal.woff2` & `tropycal-1.0/docs/_static/css/fonts/lato-normal.woff2`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/css/theme.css` & `tropycal-1.0/docs/_static/css/theme.css`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/dorian.png` & `tropycal-1.0/docs/_static/dorian.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/grid_example_1.png` & `tropycal-1.0/docs/_static/grid_example_1.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/js/badge_only.js` & `tropycal-1.0/docs/_static/js/badge_only.js`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/js/theme.js` & `tropycal-1.0/docs/_static/js/theme.js`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/logo.png` & `tropycal-1.0/docs/_static/logo.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/logo_32.ico` & `tropycal-1.0/docs/_static/logo_32.ico`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/michael_example_1.png` & `tropycal-1.0/docs/_static/michael_example_1.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/michael_example_2.png` & `tropycal-1.0/docs/_static/michael_example_2.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/michael_example_3.png` & `tropycal-1.0/docs/_static/michael_example_3.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/michael_example_4.png` & `tropycal-1.0/docs/_static/michael_example_4.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/storm_example_1.png` & `tropycal-1.0/docs/_static/storm_example_1.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/storm_example_2.png` & `tropycal-1.0/docs/_static/storm_example_2.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/storm_example_3.png` & `tropycal-1.0/docs/_static/storm_example_3.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/tornado_example_1.png` & `tropycal-1.0/docs/_static/tornado_example_1.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/tornado_example_2.png` & `tropycal-1.0/docs/_static/tornado_example_2.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/tornado_example_3.png` & `tropycal-1.0/docs/_static/tornado_example_3.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/tornado_example_5.png` & `tropycal-1.0/docs/_static/tornado_example_5.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/trackdataset_example_1.png` & `tropycal-1.0/docs/_static/trackdataset_example_1.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/trackdataset_example_2.png` & `tropycal-1.0/docs/_static/trackdataset_example_2.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/trackdataset_example_3.png` & `tropycal-1.0/docs/_static/trackdataset_example_3.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_static/trackdataset_example_4.png` & `tropycal-1.0/docs/_static/trackdataset_example_4.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_templates/autosummary/class.rst` & `tropycal-1.0/docs/_templates/autosummary/class.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_templates/autosummary/module.rst` & `tropycal-1.0/docs/_templates/autosummary/module.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/_templates/overrides/tropycal.utils.rst` & `tropycal-1.0/docs/_templates/overrides/tropycal.utils.rst`

 * *Files 16% similar despite different names*

```diff
@@ -21,24 +21,27 @@
       get_basin
       get_storm_classification
       get_storm_type
       get_two_current
       get_two_archive
       knots_to_mph
       nhc_cone_radii
+      ships_parser
 
 
 Cartopy Utilities
 -----------------
 
    .. autosummary::
       :toctree: ./
 
       add_tropycal
       plot_storm
+      plot_two
+      plot_cone
 
 
 Colors
 ------
 
    .. autosummary::
       :toctree: ./
```

### Comparing `tropycal-0.6.1/docs/api/index.rst` & `tropycal-1.0/docs/api/index.rst`

 * *Files 2% similar despite different names*

```diff
@@ -9,14 +9,15 @@
    :toctree: generated/
 
    tropycal.tracks
    tropycal.tornado
    tropycal.rain
    tropycal.recon
    tropycal.realtime
+   tropycal.ships
    tropycal.utils
    
 ===============
 Keyword Options
 ===============
 
 Various plotting functions in Tropycal make use of properties to allow the user to customize the plots:
```

### Comparing `tropycal-0.6.1/docs/conf.py` & `tropycal-1.0/docs/conf.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,21 +23,21 @@
 sys.path.insert(0, os.path.abspath('../tornado'))
 sys.path.insert(0, os.path.abspath('../recon'))
 sys.path.insert(0, os.path.abspath('../plot'))
 
 # -- Project information -----------------------------------------------------
 
 project = 'tropycal'
-copyright = '2022, Tropycal Developers'
+copyright = '2023, Tropycal Developers'
 author = 'Tropycal Developers'
 
 # The short X.Y version
 version = ''
 # The full version, including alpha/beta/rc tags
-release = '0.6.1'
+release = '1.0'
 
 
 # -- General configuration ---------------------------------------------------
 
 # If your documentation needs a minimal Sphinx version, state it here.
 #
 needs_sphinx = '2.1'
```

### Comparing `tropycal-0.6.1/docs/data.rst` & `tropycal-1.0/docs/data.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/index.rst` & `tropycal-1.0/docs/index.rst`

 * *Files 9% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 Tropycal is a Python package intended to simplify the process of retrieving and analyzing tropical cyclone data, both for past storms and in real time, and is geared towards the research and operational meteorology sectors.
 
 Tropycal can read in HURDAT2 and IBTrACS reanalysis data and operational National Hurricane Center (NHC) Best Track data and conform them to the same format, which can be used to perform climatological, seasonal and individual storm analyses. For each individual storm, operational NHC forecasts, aircraft reconnaissance data, and any associated tornado activity can be retrieved and plotted.
 
 Tropycal is supported for Python >= 3.6. For examples on how to use Tropycal in a Python script, please refer to the :doc:`examples/index` page. Additional information about the data sources used and their caveats is available in the :doc:`data` page.
 
 .. warning::
-  Tropycal is a new package. The syntax of classes and methods in the library is subject to change in future releases, which will also significantly optimize performance and speed of some functionalities. As such, updates to Tropycal may not be backwards compatible.
+  This version is the first major release of Tropycal. As such, some updates are not backwards compatible. Please refer to the API Reference for the latest documentation for all functions.
 
 .. _Github: https://github.com/tropycal/tropycal/issues
 
 .. toctree::
    :maxdepth: 2
    :hidden:
 
@@ -26,15 +26,15 @@
    api/index
    sample_usage
    data
 
 Latest Version
 ==============
 
-The latest version of Tropycal as of 11 December 2022 is v0.6.1. This documentation is valid for the latest version of Tropycal.
+The latest version of Tropycal as of 2 July 2023 is v1.0. This documentation is valid for the latest version of Tropycal.
 
 Indices and tables
 ==================
 
 * :ref:`genindex`
 * :ref:`modindex`
 * :ref:`search`
```

### Comparing `tropycal-0.6.1/docs/install.rst` & `tropycal-1.0/docs/install.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/make.bat` & `tropycal-1.0/docs/make.bat`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/options/domain.rst` & `tropycal-1.0/docs/options/domain.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/options/map_prop.rst` & `tropycal-1.0/docs/options/map_prop.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/sample_usage.rst` & `tropycal-1.0/docs/sample_usage.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/samples/tracks.TrackDataset.rst` & `tropycal-1.0/docs/samples/tracks.TrackDataset.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/samples/tracks.storm.rst` & `tropycal-1.0/docs/samples/tracks.storm.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/samples/tracks.tornado.rst` & `tropycal-1.0/docs/samples/tracks.tornado.rst`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/docs/tropycal_full_logo.png` & `tropycal-1.0/docs/tropycal_full_logo.png`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/analogs.py` & `tropycal-1.0/examples/analogs.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/customize_storm.py` & `tropycal-1.0/examples/customize_storm.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/realtime.py` & `tropycal-1.0/examples/realtime.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/recon.py` & `tropycal-1.0/examples/recon.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/tc_rainfall.py` & `tropycal-1.0/examples/tc_rainfall.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/tracks.TrackDataset.py` & `tropycal-1.0/examples/tracks.TrackDataset.py`

 * *Files 1% similar despite different names*

```diff
@@ -73,15 +73,15 @@
 # Climatological analyses
 # ~~~~~~~~~~~~~~~~~~~~~~~
 # 
 # A TrackDataset object can also be used to perform various climatological analyses. We'll start off with basic data analysis, then shift gears to plotting functionality.
 #
 # First, let's take a look at the climatology for the basin. The default period is 1991-2020, but this can be customized to any range.
 
-basin.climatology(year_range=(1991,2020))
+basin.climatology(climo_bounds=(1991,2020))
 
 ###########################################
 # Another useful functionality for research or seasonal forecast purposes is to quickly composite multiple hurricane seasons. Simply plug in a list of years, and a year range for the climatology:
 
 basin.season_composite([2004,2005,2008,2010,2017,2020])
 
 ###########################################
```

### Comparing `tropycal-0.6.1/examples/tracks.season.py` & `tropycal-1.0/examples/tracks.season.py`

 * *Files 4% similar despite different names*

```diff
@@ -34,15 +34,15 @@
 # The Season object can be converted to a Pandas DataFrame, which lists a summary of storms during the season:
 
 season.to_dataframe()
 
 ###########################################
 # A more detailed summary of the season can be retrieved using the `summary()` method:
 
-print(season.summary())
+season.summary()
 
 ###########################################
 # Plot Season
 # -----------
 # Plotting a Season object can be quickly done using the ``plot()`` method.
 # 
 # Note that you can pass various arguments to the plot function, such as customizing the map and track aspects. The "Customizing Storm Plots" example script has more examples on how to customize such plots. Read through the documentation for more customization options.
@@ -66,8 +66,8 @@
 # The combined seasons can then be plotted on the same map:
 
 combined.plot()
 
 ###########################################
 # The summary method also generates summaries for all seasons in this object:
 
-print(combined.summary())
+combined.summary()
```

### Comparing `tropycal-0.6.1/examples/tracks.storm.py` & `tropycal-1.0/examples/tracks.storm.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/examples/tracks.tornado.py` & `tropycal-1.0/examples/tracks.tornado.py`

 * *Files identical despite different names*

### Comparing `tropycal-0.6.1/setup.cfg` & `tropycal-1.0/setup.cfg`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 [metadata]
 name = tropycal
-version = 0.6.1
+version = 1.0
 description = Package for retrieving and analyzing tropical cyclone data
 long_description = file: README.md
 long_description_content_type = text/markdown
 author = Tomer Burg, Sam Lillo
 platform = any
 keywords = meteorology, weather
 classifiers =
```

### Comparing `tropycal-0.6.1/src/tropycal/constants.py` & `tropycal-1.0/src/tropycal/constants.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,33 +1,39 @@
 r"""A collection of relevant constants used throughout Tropycal scripts."""
 from matplotlib import path
 
 #Tropical or subtropical storm types (including depressions)
-TROPICAL_STORM_TYPES = frozenset(['SD','SS','TD','TS','HU'])
+TROPICAL_STORM_TYPES = frozenset(['SD','SS','TD','TS','HU','TY','ST'])
 
 #Tropical or subtropical storm types (excluding depressions)
-NAMED_TROPICAL_STORM_TYPES = frozenset(['SS','TS','HU'])
+NAMED_TROPICAL_STORM_TYPES = frozenset(['SS','TS','HU','TY','ST'])
 
 #Tropical only storm types
-TROPICAL_ONLY_STORM_TYPES = frozenset(['TD','TS','HU'])
+TROPICAL_ONLY_STORM_TYPES = frozenset(['TD','TS','HU','TY','ST'])
 
 #Tropical only storm types
 SUBTROPICAL_ONLY_STORM_TYPES = frozenset(['SD','SS'])
 
 #Standard 00/06/12/18 UTC hours
 STANDARD_HOURS = frozenset(['0000','0600','1200','1800'])
 
 #Accepted basins
 ALL_BASINS = frozenset(['north_atlantic','east_pacific','west_pacific','north_indian','south_atlantic','south_indian','australia','south_pacific'])
 
 #Accepted NHC basins
-NHC_BASINS = frozenset(['north_atlantic','east_pacific'])
+NHC_BASINS = frozenset(['north_atlantic','east_pacific','both'])
+
+#South Hemisphere basins
+SOUTH_HEMISPHERE_BASINS = frozenset(['south_atlantic','south_indian','australia','south_pacific'])
+
 
 #NHC Cone Radii, in nautical miles
+#Source: https://www.nhc.noaa.gov/verification/verify3.shtml
 CONE_SIZE_ATL = {
+    2023: [16,26,39,53,67,81,99,145,205],
     2022: [16,26,39,52,67,84,100,142,200],
     2021: [16,27,40,55,69,86,102,148,200],
     2020: [16,26,41,55,69,86,103,151,196],
     2019: [16,26,41,54,68,102,151,198],
     2018: [16,26,43,56,74,103,151,198],
     2017: [16,29,45,63,78,107,159,211],
     2016: [16,30,49,66,84,115,165,237],
@@ -38,14 +44,15 @@
     2011: [16,36,59,79,98,144,190,239],
     2010: [16,36,62,85,108,161,220,285],
     2009: [16,36,62,89,111,167,230,302],
     2008: [16,39,67,92,118,170,233,305],
 }
 
 CONE_SIZE_PAC = {
+    2023: [16,25,38,51,63,78,86,110,137],
     2022: [16,25,38,51,65,79,93,120,146],
     2021: [16,25,37,51,64,77,89,114,138],
     2020: [16,25,38,51,65,78,91,115,138],
     2019: [16,25,38,48,62,88,115,145],
     2018: [16,25,39,50,66,94,125,162],
     2017: [16,25,40,51,66,93,116,151],
     2016: [16,27,42,55,70,100,137,172],
```

### Comparing `tropycal-0.6.1/src/tropycal/plot/plot.py` & `tropycal-1.0/src/tropycal/plot/plot.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,411 +1,426 @@
-import calendar
 import numpy as np
-import pandas as pd
-import re
-import scipy.interpolate as interp
-import urllib
 import warnings
-from datetime import datetime as dt,timedelta
 import pkg_resources
 
 from ..utils import *
 
 try:
     import cartopy.feature as cfeature
     from cartopy import crs as ccrs
     from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
 except:
-    warnings.warn("Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
+    warnings.warn(
+        "Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
 
 try:
     import matplotlib as mlib
-    import matplotlib.lines as mlines
     import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
     import matplotlib.ticker as mticker
-    from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox
 except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+
 
 class Plot:
-    
-    def check_res(self,res):
-        
+
+    def check_res(self, res):
         r"""
         Converts resolution from basemap notation ('l','m','h') to cartopy notation.
-        
+
         Parameters:
         -----------
         res : str
             String representing map resolution ('l','m','h').
-        
+
         Returns:
         --------
         str
             String of the equivalent cartopy map resolution. 
         """
-        
-        #Check input map resolution and return corresponding map resolution
-        compare_dict = {'l':'110m',
-                        'm':'50m',
-                        'h':'10m'}
-        return compare_dict.get(res,'50m')
-    
-    def create_cartopy(self,proj='PlateCarree',mapobj=None,**kwargs):
-        
+
+        # Check input map resolution and return corresponding map resolution
+        compare_dict = {'l': '110m',
+                        'm': '50m',
+                        'h': '10m'}
+        return compare_dict.get(res, '50m')
+
+    def create_cartopy(self, proj='PlateCarree', mapobj=None, **kwargs):
         r"""
         Initialize a cartopy instance passed projection.
-        
+
         Parameters:
         -----------
         projection
             String representing the cartopy map projection.
         ax
             Axis on which to draw on. Default is None.
         mapobj
             Existing cartopy projection. If passed, will be used instead of generating a new one.
         **kwargs
             Additional arguments that are passed to those associated with projection.
         """
-        
-        #Initialize an instance of cartopy if not passed
+
+        # Initialize an instance of cartopy if not passed
         if mapobj is None:
             self.proj = getattr(ccrs, proj)(**kwargs)
         else:
             self.proj = mapobj
-        
-    def create_geography(self,prop):
-        
+
+    def create_geography(self, prop):
         r"""
         Set up the map geography and colors.
-        
+
         Parameters:
         -----------
         prop : dict
             dict entry containing information about the map geography and colors
         """
-        
-        #get resolution corresponding to string in prop
+
+        # get resolution corresponding to string in prop
         res = self.check_res(prop['res'])
-        
-        #Add "hidden" state alpha prop
-        if 'state_alpha' not in prop.keys(): prop['state_alpha'] = 1.0
-        
-        #fill oceans if specified
+
+        # Add "hidden" state alpha prop
+        if 'state_alpha' not in prop.keys():
+            prop['state_alpha'] = 1.0
+
+        # fill oceans if specified
         self.ax.set_facecolor(prop['ocean_color'])
-        ocean_mask = self.ax.add_feature(cfeature.OCEAN.with_scale(res),facecolor=prop['ocean_color'],edgecolor='face')
-        lake_mask = self.ax.add_feature(cfeature.LAKES.with_scale(res),facecolor=prop['ocean_color'],edgecolor='face')
-        continent_mask = self.ax.add_feature(cfeature.LAND.with_scale(res),facecolor=prop['land_color'],edgecolor='face')
-        
-        #draw geography
-        states = self.ax.add_feature(cfeature.STATES.with_scale(res),linewidths=prop['linewidth'],linestyle='solid',edgecolor=prop['linecolor'],alpha=prop['state_alpha'])
-        countries = self.ax.add_feature(cfeature.BORDERS.with_scale(res),linewidths=prop['linewidth'],linestyle='solid',edgecolor=prop['linecolor'])
-        coastlines = self.ax.add_feature(cfeature.COASTLINE.with_scale(res),linewidths=prop['linewidth'],linestyle='solid',edgecolor=prop['linecolor'])
-        
-    def dynamic_map_extent(self,min_lon,max_lon,min_lat,max_lat):
-        
+        ocean_mask = self.ax.add_feature(cfeature.OCEAN.with_scale(
+            res), facecolor=prop['ocean_color'], edgecolor='face')
+        lake_mask = self.ax.add_feature(cfeature.LAKES.with_scale(
+            res), facecolor=prop['ocean_color'], edgecolor='face')
+        continent_mask = self.ax.add_feature(cfeature.LAND.with_scale(
+            res), facecolor=prop['land_color'], edgecolor='face')
+
+        # draw geography
+        states = self.ax.add_feature(cfeature.STATES.with_scale(
+            res), linewidths=prop['linewidth'], linestyle='solid', edgecolor=prop['linecolor'], alpha=prop['state_alpha'])
+        countries = self.ax.add_feature(cfeature.BORDERS.with_scale(
+            res), linewidths=prop['linewidth'], linestyle='solid', edgecolor=prop['linecolor'])
+        coastlines = self.ax.add_feature(cfeature.COASTLINE.with_scale(
+            res), linewidths=prop['linewidth'], linestyle='solid', edgecolor=prop['linecolor'])
+
+    def dynamic_map_extent(self, min_lon, max_lon, min_lat, max_lat):
         r"""
         Sets up a dynamic map extent with an aspect ratio of 3:2 given latitude and longitude bounds.
-        
+
         Parameters:
         -----------
         min_lon : float
             Minimum longitude bound.
         max_lon : float
             Maximum longitude bound.
         min_lat : float
             Minimum latitude bound.
         max_lat : float
             Maximum latitude bound.
-        
+
         Returns:
         --------
         list
             List containing new west, east, north, south map bounds, respectively.
         """
-        
-        return dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-    
-    def plot_lat_lon_lines(self,bounds,zorder=None):
-        
+
+        return dynamic_map_extent(min_lon, max_lon, min_lat, max_lat)
+
+    def plot_lat_lon_lines(self, bounds, zorder=None):
         r"""
         Plots parallels and meridians that are constrained by the map bounds.
-        
+
         Parameters:
         -----------
         bounds : list
             List containing map bounds.
         """
-        
-        #Suppress gridliner warnings
+
+        # Suppress gridliner warnings
         warnings.filterwarnings("ignore")
-        
-        #Retrieve bounds from list
-        bound_w,bound_e,bound_s,bound_n = bounds
-        
+
+        # Retrieve bounds from list
+        bound_w, bound_e, bound_s, bound_n = bounds
+
         new_xrng = abs(bound_w-bound_e)
         new_yrng = abs(bound_n-bound_s)
-        
-        #function to round to nearest number
+
+        # function to round to nearest number
         def rdown(num, divisor):
-            return num - (num%divisor)
+            return num - (num % divisor)
+
         def rup(num, divisor):
-            return divisor + (num - (num%divisor))
-        
-        #Calculate parallels and meridians
+            return divisor + (num - (num % divisor))
+
+        # Calculate parallels and meridians
         rthres = 20
         if new_yrng < 160.0 or new_xrng < 160.0:
             rthres = 10
         if new_yrng < 40.0 or new_xrng < 40.0:
             rthres = 5
         if new_yrng < 25.0 or new_xrng < 25.0:
             rthres = 2
         if new_yrng < 9.0 or new_xrng < 9.0:
             rthres = 1
-        parallels = np.arange(rdown(bound_s,rthres),rup(bound_n,rthres)+rthres,rthres)
-        meridians = np.arange(rdown(bound_w,rthres),rup(bound_e,rthres)+rthres,rthres)
-        
+        parallels = np.arange(rdown(bound_s, rthres),
+                              rup(bound_n, rthres)+rthres, rthres)
+        meridians = np.arange(rdown(bound_w, rthres),
+                              rup(bound_e, rthres)+rthres, rthres)
+
         add_kwargs = {}
         if zorder is not None:
-            add_kwargs = {'zorder':zorder}
-        
-        #Fix for dateline crossing
+            add_kwargs = {'zorder': zorder}
+
+        # Fix for dateline crossing
         if self.proj.proj4_params['lon_0'] == 180.0:
-            
-            #Recalculate parallels and meridians
-            parallels = np.arange(rup(bound_s,rthres),rdown(bound_n,rthres)+rthres,rthres)
-            meridians = np.arange(rup(bound_w,rthres),rdown(bound_e,rthres)+rthres,rthres)
+
+            # Recalculate parallels and meridians
+            parallels = np.arange(rup(bound_s, rthres), rdown(
+                bound_n, rthres)+rthres, rthres)
+            meridians = np.arange(rup(bound_w, rthres), rdown(
+                bound_e, rthres)+rthres, rthres)
             meridians2 = np.copy(meridians)
-            meridians2[meridians2>180.0] = meridians2[meridians2>180.0]-360.0
-            all_meridians = np.arange(-180.0,180.0+rthres,rthres)
-            all_parallels = np.arange(rdown(-90.0,rthres),90.0+rthres,rthres)
-            
-            #First call with no labels but gridlines plotted
-            gl1 = self.ax.gridlines(crs=ccrs.PlateCarree(),draw_labels=False,xlocs=all_meridians,ylocs=all_parallels,linewidth=1.0,color='k',alpha=0.5,linestyle='dotted',**add_kwargs)
-            #Second call with labels but no gridlines
-            gl = self.ax.gridlines(crs=ccrs.PlateCarree(),draw_labels=True,xlocs=meridians,ylocs=parallels,linewidth=0.0,color='k',alpha=0.0,linestyle='dotted',**add_kwargs)
-            
-            #this syntax is deprecated in newer functions of cartopy
+            meridians2[meridians2 >
+                       180.0] = meridians2[meridians2 > 180.0]-360.0
+            all_meridians = np.arange(-180.0, 180.0+rthres, rthres)
+            all_parallels = np.arange(
+                rdown(-90.0, rthres), 90.0+rthres, rthres)
+
+            # First call with no labels but gridlines plotted
+            gl1 = self.ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=False, xlocs=all_meridians,
+                                    ylocs=all_parallels, linewidth=1.0, color='k', alpha=0.5, linestyle='dotted', **add_kwargs)
+            # Second call with labels but no gridlines
+            gl = self.ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, xlocs=meridians,
+                                   ylocs=parallels, linewidth=0.0, color='k', alpha=0.0, linestyle='dotted', **add_kwargs)
+
+            # this syntax is deprecated in newer functions of cartopy
             try:
                 gl.xlabels_top = False
                 gl.ylabels_right = False
             except:
                 gl.top_labels = False
                 gl.right_labels = False
-            
+
             gl.xlocator = mticker.FixedLocator(meridians2)
             gl.ylocator = mticker.FixedLocator(parallels)
             gl.xformatter = LONGITUDE_FORMATTER
             gl.yformatter = LATITUDE_FORMATTER
 
         else:
-            #Add meridians and parallels
-            gl = self.ax.gridlines(crs=ccrs.PlateCarree(),draw_labels=True,linewidth=1.0,color='k',alpha=0.5,linestyle='dotted',**add_kwargs)
-            
-            #this syntax is deprecated in newer functions of cartopy
+            # Add meridians and parallels
+            gl = self.ax.gridlines(crs=ccrs.PlateCarree(
+            ), draw_labels=True, linewidth=1.0, color='k', alpha=0.5, linestyle='dotted', **add_kwargs)
+
+            # this syntax is deprecated in newer functions of cartopy
             try:
                 gl.xlabels_top = False
                 gl.ylabels_right = False
             except:
                 gl.top_labels = False
                 gl.right_labels = False
-            
+
             gl.xlocator = mticker.FixedLocator(meridians)
             gl.ylocator = mticker.FixedLocator(parallels)
             gl.xformatter = LONGITUDE_FORMATTER
             gl.yformatter = LATITUDE_FORMATTER
-        
-        #Reset plot bounds
-        self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-        
-    def plot_init(self,ax,map_prop,plot_geography=True):
-        
+
+        # Reset plot bounds
+        self.ax.set_extent(
+            [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+    def plot_init(self, ax, map_prop, plot_geography=True):
         r"""
         Initializes the plot by creating a cartopy and axes instance, if one hasn't been created yet, and adds geography.
-        
+
         Parameters:
         -----------
         ax : axes
             Instance of axes
         map_prop : dict
             Dictionary of map properties
         """
 
-        #create cartopy projection, if none existing
+        # create cartopy projection, if none existing
         if self.proj is None:
-            self.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-        
-        #create figure
+            self.create_cartopy(proj='PlateCarree', central_longitude=0.0)
+
+        # create figure
         if ax is None:
-            self.fig = plt.figure(figsize=map_prop['figsize'],dpi=map_prop['dpi'])
+            self.fig = plt.figure(
+                figsize=map_prop['figsize'], dpi=map_prop['dpi'])
             self.ax = plt.axes(projection=self.proj)
         else:
-            fig_numbers = [x.num for x in mlib._pylab_helpers.Gcf.get_all_fig_managers()]
+            fig_numbers = [
+                x.num for x in mlib._pylab_helpers.Gcf.get_all_fig_managers()]
             if len(fig_numbers) > 0:
                 self.fig = plt.figure(fig_numbers[-1])
             else:
-                self.fig = plt.figure(figsize=map_prop['figsize'],dpi=map_prop['dpi'])
+                self.fig = plt.figure(
+                    figsize=map_prop['figsize'], dpi=map_prop['dpi'])
             self.ax = ax
-        
-        #Attach geography to plot, lat/lon lines, etc.
+
+        # Attach geography to plot, lat/lon lines, etc.
         if plot_geography:
             self.create_geography(map_prop)
 
-    
-    def add_prop(self,input_prop,default_prop):
-        
+    def add_prop(self, input_prop, default_prop):
         r"""
         Overrides default property dictionary elements with those passed as input arguments.
-        
+
         Parameters:
         -----------
         input_prop : dict
             Dictionary to use for overriding default entries.
         default_prop : dict
             Dictionary containing default entries.
-        
+
         Returns:
         --------
         dict
             Default dictionary overriden by entries in input_prop.
         """
-        
-        #add kwargs to prop and map_prop
-        for key in input_prop.keys(): default_prop[key] = input_prop[key]
-            
-        #Return prop
+
+        # add kwargs to prop and map_prop
+        for key in input_prop.keys():
+            default_prop[key] = input_prop[key]
+
+        # Return prop
         return default_prop
-    
-    def set_projection(self,domain):
-        
+
+    def set_projection(self, domain):
         r"""
         Sets a predefined map projection domain.
-        
+
         Parameters
         ----------
         domain : str
             Name of map projection to domain over.
         """
-        
-        #North Atlantic plot domain
+
+        # North Atlantic plot domain
         if domain == "both":
             bound_w = -179.0+360.0
             bound_e = -15.0+360.0
             bound_s = 0.0
             bound_n = 70.0
-        
-        #North Atlantic plot domain
+
+        # North Atlantic plot domain
         elif domain == "north_atlantic":
             bound_w = -105.0
             bound_e = -5.0
             bound_s = 0.0
             bound_n = 65.0
-        
-        #South Atlantic plot domain
+
+        # South Atlantic plot domain
         elif domain == "south_atlantic":
             bound_w = -105.0
             bound_e = -5.0
             bound_s = -65.0
             bound_n = 0.0
-            
-        #East Pacific plot domain
+
+        # East Pacific plot domain
         elif domain == "east_pacific":
-            bound_w = -180.0+360.0 
-            bound_e = -80+360.0 
+            bound_w = -180.0+360.0
+            bound_e = -80+360.0
             bound_s = 0.0
             bound_n = 65.0
-            
-        #West Pacific plot domain
+
+        # West Pacific plot domain
         elif domain == "west_pacific":
             bound_w = 90.0
             bound_e = 180.0
             bound_s = 0.0
             bound_n = 58.0
-            
-        #North Indian plot domain
+
+        # North Indian plot domain
         elif domain == "north_indian":
             bound_w = 35.0
             bound_e = 110.0
             bound_s = -5.0
             bound_n = 42.0
-            
-        #South Indian plot domain
+
+        # South Indian plot domain
         elif domain == "south_indian":
             bound_w = 20.0
             bound_e = 110.0
             bound_s = -50.0
             bound_n = 5.0
-            
-        #Australia plot domain
+
+        # Australia plot domain
         elif domain == "australia":
             bound_w = 90.0
             bound_e = 180.0
             bound_s = -60.0
             bound_n = 0.0
-            
-        #South Pacific plot domain
+
+        # South Pacific plot domain
         elif domain == "south_pacific":
             bound_w = 140.0
             bound_e = -120.0+360.0
             bound_s = -65.0
             bound_n = 0.0
-            
-        #Global plot domain
+
+        # Global plot domain
         elif domain == "all":
             bound_w = 0.1
             bound_e = 360.0
             bound_s = -90.0
             bound_n = 90.0
-            
-        #CONUS plot domain
+
+        # CONUS plot domain
         elif domain == "conus":
             bound_w = -130.0
             bound_e = -65.0
             bound_s = 20.0
             bound_n = 50.0
 
-        #CONUS plot domain
+        # CONUS plot domain
         elif domain == "east_conus":
             bound_w = -105.0
             bound_e = -60.0
             bound_s = 20.0
             bound_n = 48.0
-        
-        #Custom domain
+
+        # Custom domain
         else:
-            
-            #Error check
-            if isinstance(domain,dict) == False:
+
+            # Error check
+            if not isinstance(domain, dict):
                 msg = "Custom domains must be of type dict."
                 raise TypeError(msg)
-            
-            #Retrieve map bounds
+
+            # Retrieve map bounds
             keys = domain.keys()
             check = [False, False, False, False]
             for key in keys:
-                if key[0].lower() == 'n': check[0] = True; bound_n = domain[key]
-                if key[0].lower() == 's': check[1] = True; bound_s = domain[key]
-                if key[0].lower() == 'e': check[2] = True; bound_e = domain[key]
-                if key[0].lower() == 'w': check[3] = True; bound_w = domain[key]
+                if key[0].lower() == 'n':
+                    check[0] = True
+                    bound_n = domain[key]
+                if key[0].lower() == 's':
+                    check[1] = True
+                    bound_s = domain[key]
+                if key[0].lower() == 'e':
+                    check[2] = True
+                    bound_e = domain[key]
+                if key[0].lower() == 'w':
+                    check[3] = True
+                    bound_w = domain[key]
             if False in check:
                 msg = "Custom domains must be of type dict with arguments for 'n', 's', 'e' and 'w'."
                 raise ValueError(msg)
-            
-        #Set map extent
-        self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
+
+        # Set map extent
+        self.ax.set_extent(
+            [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
         return bound_w, bound_e, bound_s, bound_n
-    
+
     def plot_credit(self):
-        
+
         return "Plot generated using troPYcal"
-    
-    def add_credit(self,text):
-        
+
+    def add_credit(self, text):
+
         if self.use_credit:
-            a = self.ax.text(0.99,0.01,text,fontsize=10,color='k',alpha=0.7,fontweight='bold',
-                    transform=self.ax.transAxes,ha='right',va='bottom',zorder=10)
+            a = self.ax.text(0.99, 0.01, text, fontsize=10, color='k', alpha=0.7, fontweight='bold',
+                             transform=self.ax.transAxes, ha='right', va='bottom', zorder=10)
             a.set_path_effects([path_effects.Stroke(linewidth=5, foreground='white'),
-                           path_effects.Normal()])
-    
-    
+                                path_effects.Normal()])
```

### Comparing `tropycal-0.6.1/src/tropycal/rain/dataset.py` & `tropycal-1.0/src/tropycal/rain/dataset.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,34 +1,23 @@
 r"""Functionality for reading and analyzing SPC tornado dataset."""
 
 import numpy as np
+import xarray as xr
 import pandas as pd
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt
 from scipy.interpolate import griddata
-import matplotlib.dates as mdates
 import warnings
 
-import matplotlib.pyplot as plt
-import matplotlib.lines as mlines
-import matplotlib.colors as mcolors
-import matplotlib.patheffects as patheffects
-
-try:
-    import cartopy.feature as cfeature
-    from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
-except:
-    warnings.warn("Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
-
 from .plot import RainPlot
 
 from ..utils import *
 
+
 class RainDataset():
-    
+
     r"""
     Creates an instance of a RainDataset object containing tropical cyclone rainfall data, courtesy of the Weather Prediction Center (WPC).
 
     Parameters
     ----------
     data_path : str
         Source to read tropical cyclone rainfall data from. Default is "wpc", which reads from the online Weather Prediction Center (WPC) database. Can change this to a local file.
@@ -37,122 +26,119 @@
     -------
     RainDataset
         An instance of RainDataset.
     """
 
     def __init__(self, data_path='wpc'):
 
-        #Start timer
+        # Start timer
         timer_start = dt.now()
         print(f'--> Starting to read in rainfall data')
-        
-        #Read in storm rainfall dataset
+
+        # Read in storm rainfall dataset
         if data_path == 'wpc':
             data_path = "https://www.wpc.ncep.noaa.gov/tropical/rain/CONUS_rainfall_obs_1900-2020.csv"
         self.rain_df = pd.read_csv(data_path)
 
-        #Update user on duration
-        print(f'--> Completed reading in rainfall data (%.2f seconds)' % (dt.now()-timer_start).total_seconds())
-        
-    def get_storm_rainfall(self,storm):
-        
+        # Update user on duration
+        print(f'--> Completed reading in rainfall data (%.2f seconds)' %
+              (dt.now()-timer_start).total_seconds())
+
+    def get_storm_rainfall(self, storm):
         r"""
         Retrieves all rainfall observations in inches associated with a tropical cyclone.
-        
+
         Parameters
         ----------
         storm : tropycal.tracks.Storm
             Instance of a Storm object.
 
         Returns
         -------
         pandas.DataFrame
             Pandas DataFrame object containing rainfall data associated with this tropical cyclone, in inches.
         """
-        
-        #Filter dataset to this specific storm
+
+        # Filter dataset to this specific storm
         name_1 = f"{storm.name.title()} {storm.year}"
         name_2 = f"{storm.id} {storm.year}"
-        df_storm = self.rain_df.loc[(self.rain_df['Storm'] == name_1) | (self.rain_df['Storm'] == name_2)]
+        df_storm = self.rain_df.loc[(self.rain_df['Storm'] == name_1) | (
+            self.rain_df['Storm'] == name_2)]
+
+        # Drop Storm and Year column, no longer necessary
+        df_storm = df_storm.drop(columns=['Storm', 'Year'])
+
+        # Remove NaN entries
+        df_storm = df_storm.loc[~np.isnan(df_storm['Lat']) & ~np.isnan(
+            df_storm['Lon']) & ~np.isnan(df_storm['Total'])]
 
-        #Drop Storm and Year column, no longer necessary
-        df_storm = df_storm.drop(columns=['Storm','Year'])
-        
-        #Remove NaN entries
-        df_storm = df_storm.loc[~np.isnan(df_storm['Lat']) & ~np.isnan(df_storm['Lon']) & ~np.isnan(df_storm['Total'])]
-        
-        #Check if data is empty
+        # Check if data is empty
         if len(df_storm) == 0:
             raise RuntimeError("No rainfall data is available for this storm.")
-        
-        #Return dataframe
+
+        # Return dataframe
         return df_storm
-    
-    def interpolate_to_grid(self,storm,grid_res=0.1,method='linear',return_xarray=False):
-        
+
+    def interpolate_to_grid(self, storm, grid_res=0.1, method='linear', return_xarray=False):
         r"""
         Interpolates storm rainfall data to a horizontal grid.
-        
+
         Interpolation is performed using Scipy's `scipy.interpolate.griddata()` interpolation function.
-        
+
         Parameters
         ----------
         storm : tropycal.tracks.Storm
             Instance of a Storm object.
         grid_res : int or float
             Horizontal resolution of the desired grid in degrees. Default is 0.1 degrees.
         method : str
             Method for interpolation to pass to scipy's interpolation function. Default is "linear".
         return_xarray : bool
             If True, output is returned as an xarray DataArray with coordinates included. Default is false.
-            
+
         Returns
         -------
         dict or xarray.DataArray
             If return_xarray is True, an xarray DataArray is returned. Otherwise, a dict including the grid lat, lon and grid values is returned.
         """
-        
-        #Check if xarray is installed
-        try:
-            import xarray as xr
-        except ImportError as e:
-            return_xarray = False
-            msg = "xarray is not installed in your python environment. Defaulting to return_xarray=False."
-            warnings.warn(msg)
-        
-        #Check if Storm object contains rainfall data
+
+        # Check if Storm object contains rainfall data
         try:
             storm.rain
         except:
             storm.rain = self.get_storm_rainfall(storm)
-        
-        #Create grid to interpolate observations to
-        grid_lon = np.arange(-140,-60+grid_res,grid_res)
-        grid_lat = np.arange(20,50+grid_res,grid_res)
-        
-        #Retrieve data for interpolation
+
+        # Create grid to interpolate observations to
+        grid_lon = np.arange(-140, -60+grid_res, grid_res)
+        grid_lat = np.arange(20, 50+grid_res, grid_res)
+
+        # Retrieve data for interpolation
         rainfall = storm.rain['Total'].values
         lat = storm.rain['Lat'].values
         lon = storm.rain['Lon'].values
 
-        #Perform the interpolation
-        grid = griddata((lat, lon), rainfall, (grid_lat[None,:], grid_lon[:,None]), method=method)
+        # Perform the interpolation
+        grid = griddata((lat, lon), rainfall,
+                        (grid_lat[None, :], grid_lon[:, None]), method=method)
         grid = np.transpose(grid)
 
-        #Return data
+        # Return data
         if return_xarray:
-            return xr.DataArray(grid,coords=[grid_lat,grid_lon],dims=['lat','lon'])
+            return xr.DataArray(grid, coords=[grid_lat, grid_lon], dims=['lat', 'lon'])
         else:
-            return {'grid':grid,'lat':grid_lat,'lon':grid_lon}
+            return {
+                'grid': grid,
+                'lat': grid_lat,
+                'lon': grid_lon
+            }
 
-    def plot_rain_grid(self,storm,grid,levels=None,cmap=None,domain="dynamic",plot_all_dots=False,ax=None,cartopy_proj=None,save_path=None,prop={},map_prop={}):
-        
+    def plot_rain_grid(self, storm, grid, levels=None, cmap=None, domain="dynamic", plot_all_dots=False, ax=None, cartopy_proj=None, save_path=None, prop={}, map_prop={}):
         r"""
         Creates a plot of a storm track and its associated rainfall (gridded).
-        
+
         Parameters
         ----------
         storm : tropycal.tracks.Storm
             Storm object to be plotted.
         grid : dict or xarray.DataArray
             Output from `interpolate_to_grid()` to be plotted. Can also be any dict with "lat", "lon" and "grid" entries, or an xarray DataArray with "lat" and "lon" dimensions.
         levels : list or numpy.ndarray
@@ -165,59 +151,62 @@
             Whether to plot dots for all observations along the track. If false, dots will be plotted every 6 hours. Default is false.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Check if Storm object contains rainfall data
+
+        # Check if Storm object contains rainfall data
         try:
             storm.rain
         except:
             storm.rain = self.get_storm_rainfall(storm)
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = RainPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         elif max(storm['lon']) > 150 or min(storm['lon']) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storm(storm,grid,levels,cmap,domain,plot_all_dots,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storm(storm, grid, levels, cmap,
+                                           domain, plot_all_dots, ax=ax,
+                                           save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
 
-    def plot_rain(self,storm,ms=7.5,mec=None,mew=0.5,minimum_threshold=1.0,levels=None,cmap=None,domain="dynamic",plot_all_dots=False,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+    def plot_rain(self, storm, ms=7.5, mec=None, mew=0.5, minimum_threshold=1.0, levels=None, cmap=None, domain="dynamic", plot_all_dots=False, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of a storm track and its associated rainfall (individual dots).
-        
+
         Parameters
         ----------
         storm : tropycal.tracks.Storm
             Storm object to be plotted.
         ms : float or int
             Marker size for individual rainfall observations. Default is 7.5.
         mec : str or rgb tuple
@@ -236,49 +225,62 @@
             Whether to plot dots for all observations along the track. If false, dots will be plotted every 6 hours. Default is false.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Check if Storm object contains rainfall data
+
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Check if Storm object contains rainfall data
         try:
             storm.rain
         except:
             storm.rain = self.get_storm_rainfall(storm)
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = RainPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         elif max(storm['lon']) > 150 or min(storm['lon']) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storm(storm,{'ms':ms,'minimum_threshold':minimum_threshold,'mew':mew,'mec':mec},levels,cmap,domain,plot_all_dots,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Format plot settings
+        plot_settings = {
+            'ms': ms,
+            'minimum_threshold': minimum_threshold,
+            'mew': mew,
+            'mec': mec
+        }
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storm(storm, plot_settings, levels,
+                                           cmap, domain, plot_all_dots,
+                                           ax=ax, save_path=save_path,
+                                           prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
```

### Comparing `tropycal-0.6.1/src/tropycal/rain/plot.py` & `tropycal-1.0/src/tropycal/rain/plot.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,50 +1,44 @@
-import calendar
 import numpy as np
-import pandas as pd
-import re
-import scipy.interpolate as interp
-import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt
 
 from ..plot import Plot
-
 from ..utils import *
+from .. import constants
 
 try:
     import cartopy.feature as cfeature
     from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
-except:
-    warnings.warn("Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
 
 try:
     import matplotlib as mlib
     import matplotlib.lines as mlines
     import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
     import matplotlib.colors as col
-    import matplotlib.ticker as mticker
-    import matplotlib.patches as mpatches
 
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+
 
 class RainPlot(Plot):
-    
+
     def __init__(self):
-        
+
         self.use_credit = True
-    
-    def plot_storm(self,storm,grid,levels,cmap,domain="dynamic",plot_all_dots=False,ax=None,track_labels=False,save_path=None,prop={},map_prop={}):
-        
+
+    def plot_storm(self, storm, grid, levels, cmap, domain="dynamic", plot_all_dots=False, ax=None, track_labels=False, save_path=None, prop={}, map_prop={}):
         r"""
         Creates a plot of a single storm track.
-        
+
         Parameters
         ----------
         storm : str, tuple or dict
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), tuple with storm name and year (e.g., ("Matthew",2016)), or a dict entry.
         domain : str
             Domain for the plot. Can be one of the following:
             "dynamic" - default. Dynamically focuses the domain using the storm track(s) plotted.
@@ -56,455 +50,531 @@
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        #Set default properties
-        default_prop={'dots':True,'fillcolor':'category','cmap':None,'levels':None,'linecolor':'k','linewidth':1.0,'ms':7.5}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop,plot_geography=False)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #get resolution corresponding to string in prop
+
+        # Set default properties
+        default_prop = {'dots': True, 'fillcolor': 'category', 'cmap': None,
+                        'levels': None, 'linecolor': 'k', 'linewidth': 1.0, 'ms': 7.5}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop, plot_geography=False)
+
+        # --------------------------------------------------------------------------------------
+
+        # get resolution corresponding to string in prop
         res = self.check_res(map_prop['res'])
-        
-        #fill oceans if specified
+
+        # fill oceans if specified
         self.ax.set_facecolor(map_prop['ocean_color'])
-        ocean_mask = self.ax.add_feature(cfeature.OCEAN.with_scale(res),facecolor=map_prop['ocean_color'],edgecolor='face',zorder=3)
-        lake_mask = self.ax.add_feature(cfeature.LAKES.with_scale(res),facecolor=map_prop['ocean_color'],edgecolor='face',zorder=1)
-        continent_mask = self.ax.add_feature(cfeature.LAND.with_scale(res),facecolor=map_prop['land_color'],edgecolor='face',zorder=0)
-        
-        #draw geography
-        states = self.ax.add_feature(cfeature.STATES.with_scale(res),linewidths=map_prop['linewidth'],
-                                     linestyle='solid',edgecolor=map_prop['linecolor'],zorder=4)
-        countries = self.ax.add_feature(cfeature.BORDERS.with_scale(res),linewidths=map_prop['linewidth'],
-                                        linestyle='solid',edgecolor=map_prop['linecolor'],zorder=4)
-        coastlines = self.ax.add_feature(cfeature.COASTLINE.with_scale(res),linewidths=map_prop['linewidth'],
-                                         linestyle='solid',edgecolor=map_prop['linecolor'],zorder=4)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Retrieve grid coordinates and values
+        ocean_mask = self.ax.add_feature(cfeature.OCEAN.with_scale(
+            res), facecolor=map_prop['ocean_color'], edgecolor='face', zorder=3)
+        lake_mask = self.ax.add_feature(cfeature.LAKES.with_scale(
+            res), facecolor=map_prop['ocean_color'], edgecolor='face', zorder=1)
+        continent_mask = self.ax.add_feature(cfeature.LAND.with_scale(
+            res), facecolor=map_prop['land_color'], edgecolor='face', zorder=0)
+
+        # draw geography
+        states = self.ax.add_feature(cfeature.STATES.with_scale(res), linewidths=map_prop['linewidth'],
+                                     linestyle='solid', edgecolor=map_prop['linecolor'], zorder=4)
+        countries = self.ax.add_feature(cfeature.BORDERS.with_scale(res), linewidths=map_prop['linewidth'],
+                                        linestyle='solid', edgecolor=map_prop['linecolor'], zorder=4)
+        coastlines = self.ax.add_feature(cfeature.COASTLINE.with_scale(res), linewidths=map_prop['linewidth'],
+                                         linestyle='solid', edgecolor=map_prop['linecolor'], zorder=4)
+
+        # --------------------------------------------------------------------------------------
+
+        # Retrieve grid coordinates and values
         plot_grid = True
-        if isinstance(grid,dict):
+        if isinstance(grid, dict):
             if 'lat' in grid.keys():
                 grid_lat = grid['lat']
                 grid_lon = grid['lon']
                 grid_val = grid['grid']
             else:
                 ms = grid['ms']
                 mec = grid['mec']
                 mew = grid['mew']
                 minimum_threshold = grid['minimum_threshold']
                 plot_grid = False
         else:
             grid_lat = grid.lat.values
             grid_lon = grid.lon.values
             grid_val = grid.values
-        
-        #Determine levels and colormap
+
+        # Determine levels and colormap
         if levels is None:
-            levels = [1,2,3,4,6,8,10,12,16,20]
+            levels = [1, 2, 3, 4, 6, 8, 10, 12, 16, 20]
         if cmap is None:
             cmap = plt.cm.YlGn
-        norm = col.BoundaryNorm(levels,cmap.N)
-        
-        #Contour fill grid if requested
+        norm = col.BoundaryNorm(levels, cmap.N)
+
+        # Contour fill grid if requested
         if plot_grid:
-            self.ax.contourf(grid_lon, grid_lat, grid, levels, cmap=cmap, norm=norm, zorder=2)
-        
-        #Plot dots if requested
+            self.ax.contourf(grid_lon, grid_lat, grid, levels,
+                             cmap=cmap, norm=norm, zorder=2)
+
+        # Plot dots if requested
         else:
-            
-            #Iterate over sorted data (so highest totals show up on top)
+
+            # Iterate over sorted data (so highest totals show up on top)
             iter_df = storm.rain.sort_values('Total')
-            for _,row in iter_df.iterrows():
-                
-                #Retrieve rain total and determine color
+            for _, row in iter_df.iterrows():
+
+                # Retrieve rain total and determine color
                 rain_value = row['Total']
-                if rain_value < minimum_threshold: continue
-                color = self.rgb(cmap(norm(rain_value),bytes=True)[:-1])
-                
-                #Specify additional kwargs
+                if rain_value < minimum_threshold:
+                    continue
+                color = rgb_tuple_to_str(
+                    cmap(norm(rain_value), bytes=True)[:-1])
+
+                # Specify additional kwargs
                 ms_kwargs = {}
-                if mec is not None: ms_kwargs = {'mec':mec,'mew':mew}
-                self.ax.plot(row['Lon'],row['Lat'],'o',ms=ms,color=color,**ms_kwargs,transform=ccrs.PlateCarree())
-        
-        #Produce colorbar
+                if mec is not None:
+                    ms_kwargs = {'mec': mec, 'mew': mew}
+                self.ax.plot(row['Lon'], row['Lat'], 'o', ms=ms,
+                             color=color, **ms_kwargs, transform=ccrs.PlateCarree())
+
+        # Produce colorbar
         cs = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
         cs.set_array([])
-        self.fig.colorbar(cs,ax=self.ax)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Keep record of lat/lon coordinate extrema
+        self.fig.colorbar(cs, ax=self.ax)
+
+        # --------------------------------------------------------------------------------------
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = np.nanmax(storm.rain['Lat'].values)
         min_lat = np.nanmin(storm.rain['Lat'].values)
         max_lon = np.nanmax(storm.rain['Lon'].values)
         min_lon = np.nanmin(storm.rain['Lon'].values)
 
-        #Get storm data
+        # Get storm data
         storm_data = storm.dict
 
-        #Retrieve storm data
+        # Retrieve storm data
         lats = storm_data['lat']
         lons = storm_data['lon']
         vmax = storm_data['vmax']
         styp = storm_data['type']
-        sdate = storm_data['date']
-                
-        #Account for cases crossing dateline
+        sdate = storm_data['time']
+
+        # Account for cases crossing dateline
         if self.proj.proj4_params['lon_0'] == 180.0:
             new_lons = np.array(lons)
-            new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+            new_lons[new_lons < 0] = new_lons[new_lons < 0]+360.0
             lons = new_lons.tolist()
 
-        #Force dynamic_tropical to tropical if an invest
+        # Force dynamic_tropical to tropical if an invest
         invest_bool = False
         if 'invest' in storm_data.keys() and storm_data['invest']:
             invest_bool = True
-            if domain == 'dynamic_tropical': domain = 'dynamic'
-        
-        #Add to coordinate extrema
+            if domain == 'dynamic_tropical':
+                domain = 'dynamic'
+
+        # Add to coordinate extrema
         if domain == 'dynamic_tropical':
             type_array = np.array(storm_data['type'])
-            idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+            idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+                type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
             use_lats = (np.array(storm_data['lat'])[idx]).tolist()
             use_lons = (np.array(lons)[idx]).tolist()
         else:
             use_lats = storm_data['lat']
             use_lons = np.copy(lons).tolist()
 
-        #Iterate over storm data to plot
-        for i,(i_lat,i_lon,i_vmax,i_mslp,i_date,i_type) in enumerate(zip(storm_data['lat'],lons,storm_data['vmax'],storm_data['mslp'],storm_data['date'],storm_data['type'])):
+        # Iterate over storm data to plot
+        for i, (i_lat, i_lon, i_vmax, i_mslp, i_time, i_type) in enumerate(zip(storm_data['lat'], lons, storm_data['vmax'], storm_data['mslp'], storm_data['time'], storm_data['type'])):
 
-            #Determine line color, with SSHWS scale used as default
+            # Determine line color, with SSHWS scale used as default
             if prop['linecolor'] == 'category':
                 segmented_colors = True
                 line_color = get_colors_sshws(np.nan_to_num(i_vmax))
 
-            #Use user-defined colormap if another storm variable
-            elif isinstance(prop['linecolor'],str) == True and prop['linecolor'] in ['vmax','mslp']:
+            # Use user-defined colormap if another storm variable
+            elif isinstance(prop['linecolor'], str) and prop['linecolor'] in ['vmax', 'mslp']:
                 segmented_colors = True
                 color_variable = storm_data[prop['linecolor']]
-                if prop['levels'] is None: #Auto-determine color levels if needed
-                    prop['levels'] = [np.nanmin(color_variable),np.nanmax(color_variable)]
-                cmap,levels = get_cmap_levels(prop['linecolor'],prop['cmap'],prop['levels'])
-                line_color = cmap((color_variable-min(levels))/(max(levels)-min(levels)))[i]
+                if prop['levels'] is None:  # Auto-determine color levels if needed
+                    prop['levels'] = [
+                        np.nanmin(color_variable), np.nanmax(color_variable)]
+                cmap, levels = get_cmap_levels(
+                    prop['linecolor'], prop['cmap'], prop['levels'])
+                line_color = cmap((color_variable-min(levels)) /
+                                  (max(levels)-min(levels)))[i]
 
-            #Otherwise go with user input as is
+            # Otherwise go with user input as is
             else:
                 segmented_colors = False
                 line_color = prop['linecolor']
 
-            #For tropical/subtropical types, color-code if requested
+            # For tropical/subtropical types, color-code if requested
             if i > 0:
-                if i_type in ['SD','TD','SS','TS','HU']:
+                if i_type in constants.TROPICAL_STORM_TYPES:
 
-                    #Plot underlying black and overlying colored line
-                    self.ax.plot([lons[i-1],lons[i]],[storm_data['lat'][i-1],storm_data['lat'][i]],'-',
-                                  linewidth=prop['linewidth']*1.33,color='k',zorder=5,
-                                  transform=ccrs.PlateCarree())
-                    self.ax.plot([lons[i-1],lons[i]],[storm_data['lat'][i-1],storm_data['lat'][i]],'-',
-                                  linewidth=prop['linewidth'],color=line_color,zorder=6,
-                                  transform=ccrs.PlateCarree())
+                    # Plot underlying black and overlying colored line
+                    self.ax.plot([lons[i-1], lons[i]], [storm_data['lat'][i-1], storm_data['lat'][i]], '-',
+                                 linewidth=prop['linewidth']*1.33, color='k', zorder=5,
+                                 transform=ccrs.PlateCarree())
+                    self.ax.plot([lons[i-1], lons[i]], [storm_data['lat'][i-1], storm_data['lat'][i]], '-',
+                                 linewidth=prop['linewidth'], color=line_color, zorder=6,
+                                 transform=ccrs.PlateCarree())
 
-                #For non-tropical types, plot dotted lines
+                # For non-tropical types, plot dotted lines
                 else:
 
-                    #Restrict line width to 1.5 max
+                    # Restrict line width to 1.5 max
                     line_width = prop['linewidth'] + 0.0
-                    if line_width > 1.5: line_width = 1.5
+                    if line_width > 1.5:
+                        line_width = 1.5
 
-                    #Plot dotted line
-                    self.ax.plot([lons[i-1],lons[i]],[storm_data['lat'][i-1],storm_data['lat'][i]],':',
-                                  linewidth=line_width,color=line_color,zorder=6,
-                                  transform=ccrs.PlateCarree(),
-                                  path_effects=[path_effects.Stroke(linewidth=line_width*1.33, foreground='k'),
-                                                path_effects.Normal()])
-
-            #Plot dots if requested
-            if prop['dots'] == True:
-                
-                #Skip if plot_all_dots == False and not in 0,6,12,18z
-                if plot_all_dots == False:
-                    if i_date.strftime('%H%M') not in ['0000','0600','1200','1800']: continue
+                    # Plot dotted line
+                    self.ax.plot([lons[i-1], lons[i]], [storm_data['lat'][i-1], storm_data['lat'][i]], ':',
+                                 linewidth=line_width, color=line_color, zorder=6,
+                                 transform=ccrs.PlateCarree(),
+                                 path_effects=[path_effects.Stroke(linewidth=line_width*1.33, foreground='k'),
+                                               path_effects.Normal()])
+
+            # Plot dots if requested
+            if prop['dots']:
+
+                # Skip if plot_all_dots is False and not in 0,6,12,18z
+                if not plot_all_dots:
+                    if i_time.strftime('%H%M') not in ['0000', '0600', '1200', '1800']:
+                        continue
 
-                #Determine fill color, with SSHWS scale used as default
+                # Determine fill color, with SSHWS scale used as default
                 if prop['fillcolor'] == 'category':
                     segmented_colors = True
                     fill_color = get_colors_sshws(np.nan_to_num(i_vmax))
 
-                #Use user-defined colormap if another storm variable
-                elif isinstance(prop['fillcolor'],str) == True and prop['fillcolor'] in ['vmax','mslp']:
+                # Use user-defined colormap if another storm variable
+                elif isinstance(prop['fillcolor'], str) and prop['fillcolor'] in ['vmax', 'mslp']:
                     segmented_colors = True
                     color_variable = storm_data[prop['fillcolor']]
-                    if prop['levels'] is None: #Auto-determine color levels if needed
-                        prop['levels'] = [np.nanmin(color_variable),np.nanmax(color_variable)]
-                    cmap,levels = get_cmap_levels(prop['fillcolor'],prop['cmap'],prop['levels'])
-                    fill_color = cmap((color_variable-min(levels))/(max(levels)-min(levels)))[i]
+                    if prop['levels'] is None:  # Auto-determine color levels if needed
+                        prop['levels'] = [
+                            np.nanmin(color_variable), np.nanmax(color_variable)]
+                    cmap, levels = get_cmap_levels(
+                        prop['fillcolor'], prop['cmap'], prop['levels'])
+                    fill_color = cmap(
+                        (color_variable-min(levels))/(max(levels)-min(levels)))[i]
 
-                #Otherwise go with user input as is
+                # Otherwise go with user input as is
                 else:
                     segmented_colors = False
                     fill_color = prop['fillcolor']
 
-                #Determine dot type
+                # Determine dot type
                 marker_type = '^'
-                if i_type in ['SD','SS']:
+                if i_type in constants.SUBTROPICAL_ONLY_STORM_TYPES:
                     marker_type = 's'
-                elif i_type in ['TD','TS','HU']:
+                elif i_type in constants.TROPICAL_ONLY_STORM_TYPES:
                     marker_type = 'o'
 
-                #Plot marker
-                self.ax.plot(i_lon,i_lat,marker_type,mfc=fill_color,mec='k',mew=0.5,
-                             zorder=7,ms=prop['ms'],transform=ccrs.PlateCarree())
+                # Plot marker
+                self.ax.plot(i_lon, i_lat, marker_type, mfc=fill_color, mec='k', mew=0.5,
+                             zorder=7, ms=prop['ms'], transform=ccrs.PlateCarree())
 
-            #Label track dots
+            # Label track dots
             if track_labels in ['valid_utc']:
                 if track_labels == 'valid_utc':
                     strformat = '%H UTC \n%-m/%-d'
-                    labels = {t.strftime(strformat):(x,y) for t,x,y in zip(sdate,lons,lats) if t.hour==0}
-                    track = {t.strftime(strformat):(x,y) for t,x,y in zip(sdate,lons,lats)}
+                    labels = {t.strftime(strformat): (x, y) for t, x, y in zip(
+                        sdate, lons, lats) if t.hour == 0}
+                    track = {t.strftime(strformat): (x, y)
+                             for t, x, y in zip(sdate, lons, lats)}
                 self.plot_track_labels(self.ax, labels, track, k=.9)
 
-        #--------------------------------------------------------------------------------------
-        
-        #Storm-centered plot domain
+        # --------------------------------------------------------------------------------------
+
+        # Storm-centered plot domain
         if domain == "dynamic" or domain == "dynamic_tropical":
-            
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
+
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
 
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         try:
-            self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n],zorder=9)
+            self.plot_lat_lon_lines(
+                [bound_w, bound_e, bound_s, bound_n], zorder=9)
         except:
             pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
+
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
         type_array = np.array(storm_data['type'])
-        if invest_bool == False:
-            idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+        if not invest_bool:
+            idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+                type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
             tropical_vmax = np.array(storm_data['vmax'])[idx]
-            
-            #Coerce to include non-TC points if storm hasn't been designated yet
+
+            # Coerce to include non-TC points if storm hasn't been designated yet
             add_ptc_flag = False
             if len(tropical_vmax) == 0:
                 add_ptc_flag = True
                 idx = np.where((type_array == 'LO') | (type_array == 'DB'))
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             subtrop = classify_subtropical(np.array(storm_data['type']))
             peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
             peak_basin = storm_data['wmo_basin'][peak_idx]
-            storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-            if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
+            storm_type = get_storm_classification(
+                np.nanmax(tropical_vmax), subtrop, peak_basin)
+            if add_ptc_flag:
+                storm_type = "Potential Tropical Cyclone"
             left_title_string = f"{storm_type} {storm_data['name']}"
         else:
-            #Use all indices for invests
+            # Use all indices for invests
             idx = np.array([True for i in type_array])
             add_ptc_flag = False
             tropical_vmax = np.array(storm_data['vmax'])
-            
-            #Determine letter in front of invest
+
+            # Determine letter in front of invest
             add_letter = 'L'
             if storm_data['id'][0] == 'C':
                 add_letter = 'C'
             elif storm_data['id'][0] == 'E':
                 add_letter = 'E'
-            
-            #Add title
+
+            # Add title
             left_title_string = f"INVEST {storm_data['id'][2:4]}{add_letter}"
 
-        #Add left title
-        if plot_grid == True:
+        # Add left title
+        if plot_grid:
             left_title_string += "\nInterpolated WPC Storm Rainfall (in)"
         else:
             if minimum_threshold > 1:
                 left_title_string += f"\nWPC Storm Rainfall (>{np.round(minimum_threshold,2)} inch)"
             else:
                 left_title_string += f"\nWPC Storm Rainfall (inch)"
-        self.ax.set_title(left_title_string,loc='left',fontsize=16,fontweight='bold')
-        
-        #Add right title
+        self.ax.set_title(left_title_string, loc='left',
+                          fontsize=16, fontweight='bold')
+
+        # Add right title
         ace = storm_data['ace']
-        if add_ptc_flag == True: ace = 0.0
+        if add_ptc_flag:
+            ace = 0.0
         type_array = np.array(storm_data['type'])
-        
-        #Get storm extrema for display
+
+        # Get storm extrema for display
         mslp_key = 'mslp' if 'wmo_mslp' not in storm_data.keys() else 'wmo_mslp'
-        if all_nan(np.array(storm_data[mslp_key])[idx]) == True:
+        if all_nan(np.array(storm_data[mslp_key])[idx]):
             min_pres = "N/A"
         else:
-            min_pres = int(np.nan_to_num(np.nanmin(np.array(storm_data[mslp_key])[idx])))
-        if all_nan(np.array(storm_data['vmax'])[idx]) == True:
+            min_pres = int(np.nan_to_num(
+                np.nanmin(np.array(storm_data[mslp_key])[idx])))
+        if all_nan(np.array(storm_data['vmax'])[idx]):
             max_wind = "N/A"
         else:
-            max_wind = int(np.nan_to_num(np.nanmax(np.array(storm_data['vmax'])[idx])))
-        start_date = dt.strftime(np.array(storm_data['date'])[idx][0],'%d %b %Y')
-        end_date = dt.strftime(np.array(storm_data['date'])[idx][-1],'%d %b %Y')
+            max_wind = int(np.nan_to_num(
+                np.nanmax(np.array(storm_data['vmax'])[idx])))
+        start_time = dt.strftime(
+            np.array(storm_data['time'])[idx][0], '%d %b %Y')
+        end_time = dt.strftime(np.array(storm_data['time'])[
+                               idx][-1], '%d %b %Y')
         endash = u"\u2013"
         dot = u"\u2022"
-        self.ax.set_title(f"{start_date} {endash} {end_date}\n{max_wind} kt {dot} {min_pres} hPa {dot} {ace:.1f} ACE",loc='right',fontsize=13)
+        self.ax.set_title(
+            f"{start_time} {endash} {end_time}\n{max_wind} kt {dot} {min_pres} hPa {dot} {ace:.1f} ACE", loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
 
-        #--------------------------------------------------------------------------------------
-        
-        #Add plot credit
-        warning_text=""
+        # Add plot credit
+        warning_text = ""
         if storm_data['source'] == 'ibtracs' and storm_data['source_info'] == 'World Meteorological Organization (official)':
             warning_text = f"This plot uses 10-minute averaged WMO official wind data converted\nto 1-minute average (factor of 0.88). Use this wind data with caution.\n\n"
 
-            self.ax.text(0.99,0.01,warning_text,fontsize=9,color='k',alpha=0.7,
-            transform=self.ax.transAxes,ha='right',va='bottom',zorder=10)
-        
+            self.ax.text(0.99, 0.01, warning_text, fontsize=9, color='k', alpha=0.7,
+                         transform=self.ax.transAxes, ha='right', va='bottom', zorder=10)
+
         credit_text = "Rainfall data (inch) from\nWeather Prediction Center (WPC)\n\n"
-        self.ax.text(0.99,0.01,credit_text,fontsize=9,color='k',alpha=0.7,
-                     transform=self.ax.transAxes,ha='right',va='bottom',zorder=10)
-        
+        self.ax.text(0.99, 0.01, credit_text, fontsize=9, color='k', alpha=0.7,
+                     transform=self.ax.transAxes, ha='right', va='bottom', zorder=10)
+
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
-        
-        #--------------------------------------------------------------------------------------
-                
-        #Add legend
-        if prop['fillcolor'] == 'category' and prop['dots'] == True:
-            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color='w')
-            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color='w')
-            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical Depression', marker='o', color=get_colors_sshws(33))
-            ts = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical Storm', marker='o', color=get_colors_sshws(34))
-            c1 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 1', marker='o', color=get_colors_sshws(64))
-            c2 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 2', marker='o', color=get_colors_sshws(83))
-            c3 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 3', marker='o', color=get_colors_sshws(96))
-            c4 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 4', marker='o', color=get_colors_sshws(113))
-            c5 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 5', marker='o', color=get_colors_sshws(137))
-            self.ax.legend(handles=[ex,sb,td,ts,c1,c2,c3,c4,c5], prop={'size':11.5})
-
-        elif prop['linecolor'] == 'category' and prop['dots'] == False:
-            ex = mlines.Line2D([], [], linestyle='dotted', label='Non-Tropical', color='k')
-            td = mlines.Line2D([], [], linestyle='solid', label='Sub/Tropical Depression', color=get_colors_sshws(33))
-            ts = mlines.Line2D([], [], linestyle='solid', label='Sub/Tropical Storm', color=get_colors_sshws(34))
-            c1 = mlines.Line2D([], [], linestyle='solid', label='Category 1', color=get_colors_sshws(64))
-            c2 = mlines.Line2D([], [], linestyle='solid', label='Category 2', color=get_colors_sshws(83))
-            c3 = mlines.Line2D([], [], linestyle='solid', label='Category 3', color=get_colors_sshws(96))
-            c4 = mlines.Line2D([], [], linestyle='solid', label='Category 4', color=get_colors_sshws(113))
-            c5 = mlines.Line2D([], [], linestyle='solid', label='Category 5', color=get_colors_sshws(137))
-            self.ax.legend(handles=[ex,td,ts,c1,c2,c3,c4,c5], prop={'size':11.5}).set_zorder(10)
-
-        elif prop['dots'] and segmented_colors == False:
-            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color=prop['fillcolor'])
-            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color=prop['fillcolor'])
-            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical', marker='o', color=prop['fillcolor'])
-            handles=[ex,sb,td]
-            self.ax.legend(handles=handles,fontsize=11.5).set_zorder(10)
-
-        elif prop['dots'] == False and segmented_colors == False:
-            ex = mlines.Line2D([], [], linestyle='dotted',label='Non-Tropical', color=prop['linecolor'])
-            td = mlines.Line2D([], [], linestyle='solid',label='Tropical', color=prop['linecolor'])
-            handles=[ex,td]
-            self.ax.legend(handles=handles,fontsize=11.5).set_zorder(10)
-
-        elif prop['dots'] == True:
-            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color='w')
-            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color='w')
-            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical', marker='o', color='w')
-            handles=[ex,sb,td]
+
+        # --------------------------------------------------------------------------------------
+
+        # Add legend
+        if prop['fillcolor'] == 'category' and prop['dots']:
+            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Non-Tropical', marker='^', color='w')
+            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Subtropical', marker='s', color='w')
+            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k', mew=0.5,
+                               label='Tropical Depression', marker='o', color=get_colors_sshws(33))
+            ts = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Tropical Storm', marker='o', color=get_colors_sshws(34))
+            c1 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 1', marker='o', color=get_colors_sshws(64))
+            c2 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 2', marker='o', color=get_colors_sshws(83))
+            c3 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 3', marker='o', color=get_colors_sshws(96))
+            c4 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 4', marker='o', color=get_colors_sshws(113))
+            c5 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 5', marker='o', color=get_colors_sshws(137))
+            self.ax.legend(handles=[ex, sb, td, ts, c1,
+                           c2, c3, c4, c5], prop={'size': 11.5})
+
+        elif prop['linecolor'] == 'category' and not prop['dots']:
+            ex = mlines.Line2D([], [], linestyle='dotted',
+                               label='Non-Tropical', color='k')
+            td = mlines.Line2D([], [], linestyle='solid',
+                               label='Sub/Tropical Depression', color=get_colors_sshws(33))
+            ts = mlines.Line2D([], [], linestyle='solid',
+                               label='Sub/Tropical Storm', color=get_colors_sshws(34))
+            c1 = mlines.Line2D([], [], linestyle='solid',
+                               label='Category 1', color=get_colors_sshws(64))
+            c2 = mlines.Line2D([], [], linestyle='solid',
+                               label='Category 2', color=get_colors_sshws(83))
+            c3 = mlines.Line2D([], [], linestyle='solid',
+                               label='Category 3', color=get_colors_sshws(96))
+            c4 = mlines.Line2D([], [], linestyle='solid',
+                               label='Category 4', color=get_colors_sshws(113))
+            c5 = mlines.Line2D([], [], linestyle='solid',
+                               label='Category 5', color=get_colors_sshws(137))
+            self.ax.legend(handles=[ex, td, ts, c1, c2, c3, c4, c5], prop={
+                           'size': 11.5}).set_zorder(10)
+
+        elif prop['dots'] and not segmented_colors:
+            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Non-Tropical', marker='^', color=prop['fillcolor'])
+            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Subtropical', marker='s', color=prop['fillcolor'])
+            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Tropical', marker='o', color=prop['fillcolor'])
+            handles = [ex, sb, td]
+            self.ax.legend(handles=handles, fontsize=11.5).set_zorder(10)
+
+        elif not prop['dots'] and not segmented_colors:
+            ex = mlines.Line2D([], [], linestyle='dotted',
+                               label='Non-Tropical', color=prop['linecolor'])
+            td = mlines.Line2D([], [], linestyle='solid',
+                               label='Tropical', color=prop['linecolor'])
+            handles = [ex, td]
+            self.ax.legend(handles=handles, fontsize=11.5).set_zorder(10)
+
+        elif prop['dots']:
+            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Non-Tropical', marker='^', color='w')
+            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Subtropical', marker='s', color='w')
+            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Tropical', marker='o', color='w')
+            handles = [ex, sb, td]
             for _ in range(7):
-                handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-            l=self.ax.legend(handles=handles,fontsize=11.5).set_zorder(10)
+                handles.append(mlines.Line2D(
+                    [], [], linestyle='-', label='', lw=0))
+            l = self.ax.legend(handles=handles, fontsize=11.5).set_zorder(10)
             plt.draw()
-            
-            #Get the bbox
+
+            # Get the bbox
             try:
                 bb = l.legendPatch.get_bbox().inverse_transformed(self.fig.transFigure)
             except:
                 bb = l.legendPatch.get_bbox().transformed(self.fig.transFigure.inverted())
-                
-            #Define colorbar axis
-            cax = self.fig.add_axes([bb.x0+0.47*bb.width, bb.y0+.057*bb.height, 0.015, .65*bb.height])
+
+            # Define colorbar axis
+            cax = self.fig.add_axes(
+                [bb.x0+0.47*bb.width, bb.y0+.057*bb.height, 0.015, .65*bb.height])
             norm = mlib.colors.Normalize(vmin=min(levels), vmax=max(levels))
             cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-            cbar = self.fig.colorbar(cbmap,cax=cax,orientation='vertical',\
+            cbar = self.fig.colorbar(cbmap, cax=cax, orientation='vertical',
                                      ticks=levels)
-            
+
             cax.tick_params(labelsize=11.5)
             cax.yaxis.set_ticks_position('left')
-            cbar.set_label(prop['fillcolor'],fontsize=11.5,rotation=90)
-        
+            cbar.set_label(prop['fillcolor'], fontsize=11.5, rotation=90)
+
             rect_offset = 0.0
             if prop['cmap'] == 'category' and prop['fillcolor'] == 'vmax':
-                cax.yaxis.set_ticks(np.linspace(min(levels),max(levels),len(levels)))
+                cax.yaxis.set_ticks(np.linspace(
+                    min(levels), max(levels), len(levels)))
                 cax.yaxis.set_ticklabels(levels)
                 cax2 = cax.twinx()
                 cax2.yaxis.set_ticks_position('right')
-                cax2.yaxis.set_ticks((np.linspace(0,1,len(levels))[:-1]+np.linspace(0,1,len(levels))[1:])*.5)
-                cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+                cax2.yaxis.set_ticks((np.linspace(0, 1, len(levels))[
+                                     :-1]+np.linspace(0, 1, len(levels))[1:])*.5)
+                cax2.set_yticklabels(
+                    ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
                 cax2.tick_params('both', length=0, width=0, which='major')
                 cax.yaxis.set_ticks_position('left')
                 rect_offset = 0.7
-            if prop['fillcolor'] == 'date':
-                cax.set_yticklabels([f'{mdates.num2date(i):%b %-d}' for i in clevs],fontsize=11.5)
-                
+            if prop['fillcolor'] == 'time':
+                cax.set_yticklabels(
+                    [f'{mdates.num2date(i):%b %-d}' for i in clevs], fontsize=11.5)
+
         else:
-            ex = mlines.Line2D([], [], linestyle='dotted',label='Non-Tropical', color='k')
-            td = mlines.Line2D([], [], linestyle='solid',label='Tropical', color='k')
-            handles=[ex,td]
+            ex = mlines.Line2D([], [], linestyle='dotted',
+                               label='Non-Tropical', color='k')
+            td = mlines.Line2D([], [], linestyle='solid',
+                               label='Tropical', color='k')
+            handles = [ex, td]
             for _ in range(7):
-                handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-            l=self.ax.legend(handles=handles,fontsize=11.5).set_zorder(10)
+                handles.append(mlines.Line2D(
+                    [], [], linestyle='-', label='', lw=0))
+            l = self.ax.legend(handles=handles, fontsize=11.5).set_zorder(10)
             plt.draw()
-            
-            #Get the bbox
+
+            # Get the bbox
             try:
                 bb = l.legendPatch.get_bbox().inverse_transformed(self.fig.transFigure)
             except:
                 bb = l.legendPatch.get_bbox().transformed(self.fig.transFigure.inverted())
-                
-            #Define colorbar axis
-            cax = self.fig.add_axes([bb.x0+0.47*bb.width, bb.y0+.057*bb.height, 0.015, .65*bb.height])
+
+            # Define colorbar axis
+            cax = self.fig.add_axes(
+                [bb.x0+0.47*bb.width, bb.y0+.057*bb.height, 0.015, .65*bb.height])
             norm = mlib.colors.Normalize(vmin=min(levels), vmax=max(levels))
             cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-            cbar = self.fig.colorbar(cbmap,cax=cax,orientation='vertical',\
+            cbar = self.fig.colorbar(cbmap, cax=cax, orientation='vertical',
                                      ticks=levels)
-            
+
             cax.tick_params(labelsize=11.5)
             cax.yaxis.set_ticks_position('left')
-            cbarlab = make_var_label(prop['linecolor'],storm_data)            
-            cbar.set_label(cbarlab,fontsize=11.5,rotation=90)
-        
+            cbarlab = make_var_label(prop['linecolor'], storm_data)
+            cbar.set_label(cbarlab, fontsize=11.5, rotation=90)
+
             rect_offset = 0.0
             if prop['cmap'] == 'category' and prop['linecolor'] == 'vmax':
-                cax.yaxis.set_ticks(np.linspace(min(levels),max(levels),len(levels)))
+                cax.yaxis.set_ticks(np.linspace(
+                    min(levels), max(levels), len(levels)))
                 cax.yaxis.set_ticklabels(levels)
                 cax2 = cax.twinx()
                 cax2.yaxis.set_ticks_position('right')
-                cax2.yaxis.set_ticks((np.linspace(0,1,len(levels))[:-1]+np.linspace(0,1,len(levels))[1:])*.5)
-                cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+                cax2.yaxis.set_ticks((np.linspace(0, 1, len(levels))[
+                                     :-1]+np.linspace(0, 1, len(levels))[1:])*.5)
+                cax2.set_yticklabels(
+                    ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
                 cax2.tick_params('both', length=0, width=0, which='major')
                 cax.yaxis.set_ticks_position('left')
                 rect_offset = 0.7
-            if prop['linecolor'] == 'date':
-                cax.set_yticklabels([f'{mdates.num2date(i):%b %-d}' for i in clevs],fontsize=11.5)
-                
-        #-----------------------------------------------------------------------------------------
-        
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return axis if specified, otherwise display figure
+            if prop['linecolor'] == 'time':
+                cax.set_yticklabels(
+                    [f'{mdates.num2date(i):%b %-d}' for i in clevs], fontsize=11.5)
+
+        # -----------------------------------------------------------------------------------------
+
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return axis if specified, otherwise display figure
         return self.ax
```

### Comparing `tropycal-0.6.1/src/tropycal/realtime/realtime.py` & `tropycal-1.0/src/tropycal/realtime/realtime.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,769 +1,820 @@
 r"""Functionality for managing real-time tropical cyclone data."""
 
-import calendar
-import numpy as np
-import pandas as pd
 import re
-import scipy.interpolate as interp
+import ssl
+import numpy as np
 import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 
 try:
     import shapefile
     import zipfile
-    from io import StringIO, BytesIO
-except:
+    from io import BytesIO
+except ImportError:
     warn_message = "Warning: The libraries necessary for online NHC forecast retrieval aren't available (shapefile, gzip, io)."
     warnings.warn(warn_message)
 
-try:
-    import cartopy.feature as cfeature
-    from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
-except:
-    warn_message = "Warning: Cartopy is not installed in your python environment. Plotting functions will not work."
-    warnings.warn(warn_message)
-
-#Internal imports
+# Internal imports
 from .storm import RealtimeStorm
 from ..utils import *
 from .. import constants
 from ..tracks.plot import TrackPlot
 
+
 class Realtime():
-    
+
     r"""
     Creates an instance of a Realtime object containing currently active tropical cyclones and invests.
-    
+
     Realtime objects are used to retrieve RealtimeStorm objects, which are created for every active tropical cyclone globally upon creating an instance of Realtime.
-    
+
     Data sources are as follows:
     National Hurricane Center (NHC): https://ftp.nhc.noaa.gov/atcf/btk/
     Joint Typhoon Warning Center (JTWC): (read notes for more details)
-    
+
     Parameters
     ----------
     jtwc : bool, optional
         Flag determining whether to read JTWC data in. If True, specify the JTWC data source using "jtwc_source". Default is False.
     jtwc_source : str, optional
         If jtwc is set to True, this specifies the JTWC data source to read from. Available options are "noaa", "ucar" or "jtwc". Default is "jtwc". Read the notes for more details.
     ssl_certificate : boolean, optional
         If jtwc is set to True, this determines whether to disable SSL certificate when retrieving data from JTWC. Default is True. Use False *ONLY* if True causes an SSL certification error.
-    
+
     Returns
     -------
     tropycal.realtime.Realtime
         An instance of Realtime.
-    
+
     Notes
     -----
     During 2021, the multiple sources offering a Best Track archive of raw JTWC tropical cyclone data experienced frequent outages and/or severe slowdowns, hampering the ability to easily retrieve this data. As such, JTWC data has been optional in Realtime objects since v0.2.7. There are three JTWC sources available:
-    
+
     * **jtwc** - This is currently the default JTWC source if JTWC data is read in. As of June 2022, this source is working, but reading data is somewhat slow (can take up to 2 minutes).
     * **ucar** - As of September 2021, this source is available and fairly quick to read in, but offers a less compherensive storm history than the "jtwc" source. Between July and September 2021, this source did not update any active tropical cyclones outside of NHC's domain. If using this source, check to make sure it is in fact retrieving current global tropical cyclones.
     * **noaa** - This source was inactive from July 2021 through early 2022, and appears to be back online again. The code retains the ability to read in data from this source should it go back offline then return online again.
-    
+
     .. warning::
 
         JTWC's SSL certificate appears to have expired sometime in early 2022. If using JTWC data with the ``jtwc=True`` argument, this will result in Realtime functionality crashing by default. To avoid this, add a ``ssl_certificate=False`` argument to both creating an instance of Realtime and to any method that retrieves JTWC forecast.
-        
+
         Affected functions include ``Realtime.plot_summary()``, ``RealtimeStorm.get_forecast_realtime()``, and ``RealtimeStorm.plot_forecast_realtime()``.
-    
+
     The following block of code creates an instance of a Realtime() object and stores it in a variable called "realtime_obj":
-    
+
     .. code-block:: python
-    
+
         from tropycal import realtime
         realtime_obj = realtime.Realtime()
-        
+
     With an instance of Realtime created, any of the methods listed below can be accessed via the "realtime_obj" variable. All active storms and invests are stored as attributes of this instance, and can be simply retrieved:
-    
+
     .. code-block:: python
-        
+
         #This stores an instance of a RealtimeStorm object for AL012021 in the "storm" variable.
         storm = realtime_obj.AL012021
-        
+
         #From there, you can use any method of a Storm object:
         storm.plot()
-    
+
     RealtimeStorm objects contain all the methods of Storm objects, in addition to special methods available only for storms retrieved via a Realtime object. As of Tropycal v0.3, this now includes invests, though Storm and RealtimeStorm objects retrieved for invests are blocked from performing functionality that does not apply to invests operationally (e.g., forecast discussions, official NHC/JTWC track forecasts).
-    
+
     To check whether a RealtimeStorm object contains an invest, check the "invest" boolean stored in it:
-    
+
     .. code-block:: python
-        
+
         if storm.invest == True:
             print("This is an invest!")
         else:
             print("This is not an invest!")
-    
+
     Realtime objects have several attributes, including the time the object was last updated, which can be accessed in dictionary format as follows:
-    
+
     >>> realtime_obj.attrs
     """
-    
+
     def __repr__(self):
-         
+
         summary = ["<tropycal.realtime.Realtime>"]
 
-        #Add general summary
-        
-        #Add dataset summary
+        # Add general summary
+
+        # Add dataset summary
         summary.append("Dataset Summary:")
         summary.append(f'{" "*4}Numbers of active storms: {len(self.storms)}')
-        summary.append(f'{" "*4}Time Updated: {self.time.strftime("%H%M UTC %d %B %Y")}')
-        
+        summary.append(
+            f'{" "*4}Time Updated: {self.time.strftime("%H%M UTC %d %B %Y")}')
+
         if len(self.storms) > 0:
             summary.append("\nActive Storms:")
             for key in self.storms:
-                if self[key]['invest'] == False:
+                if not self[key]['invest']:
                     summary.append(f'{" "*4}{key}')
-            
+
             summary.append("\nActive Invests:")
             for key in self.storms:
-                if self[key]['invest'] == True:
+                if self[key]['invest']:
                     summary.append(f'{" "*4}{key}')
-        
+
         return "\n".join(summary)
 
     def __setitem__(self, key, value):
         self.__dict__[key] = value
-        
+
     def __getitem__(self, key):
         return self.__dict__[key]
-    
-    def __init__(self,jtwc=False,jtwc_source='ucar',ssl_certificate=True):
-        
-        #Define empty dict to store track data in
+
+    def __init__(self, jtwc=False, jtwc_source='ucar', ssl_certificate=True):
+
+        # Define empty dict to store track data in
         self.data = {}
         self.jtwc = jtwc
         self.jtwc_source = jtwc_source
         self.ssl_certificate = ssl_certificate
-        
-        #Time data reading
+
+        # Time data reading
         start_time = dt.now()
         print("--> Starting to read in current storm data")
-        
-        #Read in best track data
+
+        # Read in best track data
         self.__read_btk()
-        if jtwc_source not in ['ucar','noaa','jtwc']:
+        if jtwc_source not in ['ucar', 'noaa', 'jtwc']:
             msg = "\"jtwc_source\" must be either \"ucar\", \"noaa\", or \"jtwc\"."
             raise ValueError(msg)
-        if jtwc: self.__read_btk_jtwc(jtwc_source,ssl_certificate)
-        
-        #Determine time elapsed
+        if jtwc:
+            self.__read_btk_jtwc(jtwc_source, ssl_certificate)
+
+        # Determine time elapsed
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
+        tsec = str(round(time_elapsed.total_seconds(), 2))
         print(f"--> Completed reading in current storm data ({tsec} seconds)")
-        
-        #Remove storms that haven't been active in 18 hours
+
+        # Remove storms that haven't been active in 18 hours
         all_keys = [k for k in self.data.keys()]
         for key in all_keys:
-            
-            #Filter for storm duration
-            if len(self.data[key]['date']) == 0:
+
+            # Filter for storm duration
+            if len(self.data[key]['time']) == 0:
                 del self.data[key]
                 continue
-            
-            #Get last date
-            last_date = self.data[key]['date'][-1]
-            current_date = dt.utcnow()
-            
-            #Get time difference
-            hours_diff = (current_date - last_date).total_seconds() / 3600.0
+
+            # Get last time
+            last_time = self.data[key]['time'][-1]
+            current_time = dt.utcnow()
+
+            # Get time difference
+            hours_diff = (current_time - last_time).total_seconds() / 3600.0
             if hours_diff >= 15.0 or (self.data[key]['invest'] and hours_diff >= 9.0):
                 del self.data[key]
             if hours_diff <= -48.0:
                 del self.data[key]
-        
-        #Remove invests that have been classified as TCs
+
+        # Remove invests that have been classified as TCs
         all_keys = [k for k in self.data.keys()]
         for key in all_keys:
-            
-            #Only keep invests
+
+            # Only keep invests
             try:
-                if self.data[key]['invest'] == False: continue
+                if not self.data[key]['invest']:
+                    continue
             except:
                 continue
-            
-            #Iterate through all storms
+
+            # Iterate through all storms
             match = False
             for key_storm in self.data.keys():
-                if self.data[key_storm]['invest'] == True: continue
-                
-                #Check for overlap in lons
-                if self.data[key_storm]['lon'][0] == self.data[key]['lon'][0] and self.data[key_storm]['date'][0] == self.data[key]['date'][0]: match = True
-            
-            if match == True: del self.data[key]
-        
-        #For each storm remaining, create a Storm object
+                if self.data[key_storm]['invest']:
+                    continue
+
+                # Check for overlap in lons
+                if self.data[key_storm]['lon'][0] == self.data[key]['lon'][0] and self.data[key_storm]['time'][0] == self.data[key]['time'][0]:
+                    match = True
+
+            if match:
+                del self.data[key]
+
+        # For each storm remaining, create a Storm object
         if len(self.data) > 0:
             self.__read_nhc_shapefile()
-            
-            #Add probability attributes for storms where it's unavailable
+
+            # Add probability attributes for storms where it's unavailable
             for key in self.data.keys():
-                if key[0:2] in ['AL','EP'] and 'prob_2day' not in self.data[key].keys():
+                if key[0:2] in ['AL', 'EP'] and 'prob_2day' not in self.data[key].keys():
                     self.data[key]['prob_2day'] = 'N/A'
-                    self.data[key]['prob_5day'] = 'N/A'
+                    self.data[key]['prob_7day'] = 'N/A'
                     self.data[key]['risk_2day'] = 'N/A'
-                    self.data[key]['risk_5day'] = 'N/A'
-                if key[0:2] not in ['AL','EP']:
+                    self.data[key]['risk_7day'] = 'N/A'
+                if key[0:2] not in ['AL', 'EP']:
                     self.data[key]['prob_2day'] = 'N/A'
-                    self.data[key]['prob_5day'] = 'N/A'
+                    self.data[key]['prob_7day'] = 'N/A'
                     self.data[key]['risk_2day'] = 'N/A'
-                    self.data[key]['risk_5day'] = 'N/A'
+                    self.data[key]['risk_7day'] = 'N/A'
                 if self.data[key]['type'][-1] in constants.TROPICAL_STORM_TYPES:
                     self.data[key]['prob_2day'] = 'N/A'
-                    self.data[key]['prob_5day'] = 'N/A'
+                    self.data[key]['prob_7day'] = 'N/A'
                     self.data[key]['risk_2day'] = 'N/A'
-                    self.data[key]['risk_5day'] = 'N/A'
+                    self.data[key]['risk_7day'] = 'N/A'
                 self[key] = RealtimeStorm(self.data[key])
 
-            #Delete data dict while retaining active storm keys
+            # Delete data dict while retaining active storm keys
             self.storms = [k for k in self.data.keys()]
             del self.data
         else:
-            
-            #Create an empty list signaling no active storms
+
+            # Create an empty list signaling no active storms
             self.storms = []
             del self.data
-        
-        #Save current time
+
+        # Save current time
         self.time = dt.now()
-        
-        #Set attributes
+
+        # Set attributes
         self.attrs = {
-            'jtwc':jtwc,
-            'jtwc_source':jtwc_source,
-            'time':self.time
+            'jtwc': jtwc,
+            'jtwc_source': jtwc_source,
+            'time': self.time
         }
-    
+
     def __read_btk(self):
-        
         r"""
         Reads in best track data into the Dataset object.
         """
-        
-        #Get current year
+
+        # Get current year
         current_year = (dt.now()).year
 
-        #Get list of files in online directory
+        # Get list of files in online directory
         use_ftp = False
         try:
-            urlpath = urllib.request.urlopen('https://ftp.nhc.noaa.gov/atcf/btk/')
+            urlpath = urllib.request.urlopen(
+                'https://ftp.nhc.noaa.gov/atcf/btk/')
             string = urlpath.read().decode('utf-8')
         except:
             use_ftp = True
-            urlpath = urllib.request.urlopen('ftp://ftp.nhc.noaa.gov/atcf/btk/')
+            urlpath = urllib.request.urlopen(
+                'ftp://ftp.nhc.noaa.gov/atcf/btk/')
             string = urlpath.read().decode('utf-8')
 
-        #Get relevant filenames from directory
+        # Get relevant filenames from directory
         files = []
         search_pattern = f'b[aec][lp][012349][0123456789]{current_year}.dat'
 
         pattern = re.compile(search_pattern)
         filelist = pattern.findall(string)
         for filename in filelist:
-            if filename not in files: files.append(filename)
+            if filename not in files:
+                files.append(filename)
 
-        #For each file, read in file content and add to hurdat dict
+        # For each file, read in file content and add to hurdat dict
         for file in files:
 
-            #Get file ID
+            # Get file ID
             stormid = ((file.split(".dat")[0])[1:]).upper()
-            
-            #Check for invest status
+
+            # Check for invest status
             invest_bool = False
             if int(stormid[2]) == 9:
                 invest_bool = True
 
-            #Determine basin
+            # Determine basin
             add_basin = 'north_atlantic'
             if stormid[0] == 'C':
                 add_basin = 'east_pacific'
             elif stormid[0] == 'E':
                 add_basin = 'east_pacific'
 
-            #add empty entry into dict
-            self.data[stormid] = {'id':stormid,'operational_id':stormid,'name':'','year':int(stormid[4:8]),'season':int(stormid[4:8]),'basin':add_basin,'source_info':'NHC Hurricane Database','realtime':True,'invest':invest_bool,'source_method':"NHC's Automated Tropical Cyclone Forecasting System (ATCF)",'source_url':"https://ftp.nhc.noaa.gov/atcf/btk/"}
+            # add empty entry into dict
+            self.data[stormid] = {
+                'id': stormid,
+                'operational_id': stormid,
+                'name': '',
+                'year': int(stormid[4:8]),
+                'season': int(stormid[4:8]),
+                'basin': add_basin,
+                'source_info': 'NHC Hurricane Database',
+                'realtime': True,
+                'invest': invest_bool,
+                'source_method': "NHC's Automated Tropical Cyclone Forecasting System (ATCF)",
+                'source_url': "https://ftp.nhc.noaa.gov/atcf/btk/",
+            }
             self.data[stormid]['source'] = 'hurdat'
             self.data[stormid]['jtwc_source'] = 'N/A'
 
-            #add empty lists
-            for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
+            # add empty lists
+            for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin']:
                 self.data[stormid][val] = []
             self.data[stormid]['ace'] = 0.0
 
-            #Read in file
+            # Read in file
             if use_ftp:
                 url = f"ftp://ftp.nhc.noaa.gov/atcf/btk/{file}"
             else:
                 url = f"https://ftp.nhc.noaa.gov/atcf/btk/{file}"
             f = urllib.request.urlopen(url)
             content = f.read()
             content_full = content.decode("utf-8")
             content = content_full.split("\n")
-            content = [(i.replace(" ","")).split(",") for i in content]
+            content = [(i.replace(" ", "")).split(",") for i in content]
             f.close()
-            
-            #Check if transition is in keywords for invests
-            if invest_bool == True and 'TRANSITION' in content_full:
+
+            # Check if transition is in keywords for invests
+            if invest_bool and 'TRANSITION' in content_full:
                 del self.data[stormid]
                 continue
 
-            #iterate through file lines
+            # iterate through file lines
             for line in content:
 
-                if len(line) < 28: continue
+                if len(line) < 28:
+                    continue
 
-                #Get date of obs
-                date = dt.strptime(line[2],'%Y%m%d%H')
-                if date.strftime('%H%M') not in constants.STANDARD_HOURS: continue
+                # Get time of obs
+                time = dt.strptime(line[2], '%Y%m%d%H')
+                if time.strftime('%H%M') not in constants.STANDARD_HOURS:
+                    continue
+
+                # Ensure obs aren't being repeated
+                if time in self.data[stormid]['time']:
+                    continue
 
-                #Ensure obs aren't being repeated
-                if date in self.data[stormid]['date']: continue
-
-                #Get latitude into number
+                # Get latitude into number
                 btk_lat_temp = line[6].split("N")[0]
-                btk_lat = np.round(float(btk_lat_temp) * 0.1,1)
+                btk_lat = np.round(float(btk_lat_temp) * 0.1, 1)
 
-                #Get longitude into number
+                # Get longitude into number
                 if "W" in line[7]:
                     btk_lon_temp = line[7].split("W")[0]
                     btk_lon = float(btk_lon_temp) * -0.1
                 elif "E" in line[7]:
                     btk_lon_temp = line[7].split("E")[0]
-                    btk_lon = np.round(float(btk_lon_temp) * 0.1,1)
+                    btk_lon = np.round(float(btk_lon_temp) * 0.1, 1)
 
-                #Get other relevant variables
+                # Get other relevant variables
                 btk_wind = int(line[8])
                 btk_mslp = int(line[9])
                 btk_type = line[10]
                 name = line[27]
-                
-                #Get last tropical date
+
+                # Get last tropical time
                 if btk_type in constants.TROPICAL_STORM_TYPES:
-                    last_tropical_date = date + timedelta(hours=0)
+                    last_tropical_time = time + timedelta(hours=0)
 
-                #Replace with NaNs
-                if btk_wind > 250 or btk_wind < 10: btk_wind = np.nan
-                if btk_mslp > 1040 or btk_mslp < 800: btk_mslp = np.nan
+                # Replace with NaNs
+                if btk_wind > 250 or btk_wind < 10:
+                    btk_wind = np.nan
+                if btk_mslp > 1040 or btk_mslp < 800:
+                    btk_mslp = np.nan
 
-                #Add extra obs
+                # Add extra obs
                 self.data[stormid]['extra_obs'].append(0)
 
-                #Append into dict
-                self.data[stormid]['date'].append(date)
+                # Append into dict
+                self.data[stormid]['time'].append(time)
                 self.data[stormid]['special'].append('')
                 self.data[stormid]['type'].append(btk_type)
-                self.data[stormid]['lat'].append(round(btk_lat,1))
-                self.data[stormid]['lon'].append(round(btk_lon,1))
+                self.data[stormid]['lat'].append(round(btk_lat, 1))
+                self.data[stormid]['lon'].append(round(btk_lon, 1))
                 self.data[stormid]['vmax'].append(btk_wind)
                 self.data[stormid]['mslp'].append(btk_mslp)
-                
-                #Add basin
+
+                # Add basin
                 origin_basin = add_basin + ''
                 if add_basin == 'east_pacific':
-                    check_basin = get_basin(self.data[stormid]['lat'][0],self.data[stormid]['lon'][0],add_basin)
-                    if check_basin != add_basin: origin_basin = 'north_atlantic'
-                self.data[stormid]['wmo_basin'].append(get_basin(btk_lat,btk_lon,origin_basin))
-
-                #Calculate ACE & append to storm total
-                if np.isnan(btk_wind) == False:
-                    ace = (10**-4) * (btk_wind**2)
+                    check_basin = get_basin(
+                        self.data[stormid]['lat'][0], self.data[stormid]['lon'][0], add_basin)
+                    if check_basin != add_basin:
+                        origin_basin = 'north_atlantic'
+                self.data[stormid]['wmo_basin'].append(
+                    get_basin(btk_lat, btk_lon, origin_basin))
+
+                # Calculate ACE & append to storm total
+                if not np.isnan(btk_wind):
+                    ace = accumulated_cyclone_energy(btk_wind)
                     if btk_type in constants.NAMED_TROPICAL_STORM_TYPES:
-                        self.data[stormid]['ace'] += np.round(ace,4)
+                        self.data[stormid]['ace'] += np.round(ace, 4)
 
-            #Determine storm name for invests
-            if invest_bool == True:
-                
-                #Determine letter in front of invest
+            # Determine storm name for invests
+            if invest_bool:
+
+                # Determine letter in front of invest
                 add_letter = 'L'
                 if stormid[0] == 'C':
                     add_letter = 'C'
                 elif stormid[0] == 'E':
                     add_letter = 'E'
                 elif stormid[0] == 'W':
                     add_letter = 'W'
                 elif stormid[0] == 'I':
                     add_letter = 'I'
                 elif stormid[0] == 'S':
                     add_letter = 'S'
                 name = stormid[2:4] + add_letter
-            
-            #Add storm name
+
+            # Add storm name
             self.data[stormid]['name'] = name
-            
-            #Check if storm is still tropical, if not an invest.
-            #Re-designate as an invest if has not been a TC for over 18 hours.
+
+            # Check if storm is still tropical, if not an invest.
+            # Re-designate as an invest if has not been a TC for over 18 hours.
             if any(type in self.data[stormid]['type'] for type in constants.TROPICAL_STORM_TYPES):
-                current_date = dt.utcnow()
-                date_diff = (current_date - last_tropical_date).total_seconds() / 3600
-                if date_diff > 18:
+                current_time = dt.utcnow()
+                hour_diff = (current_time -
+                             last_tropical_time).total_seconds() / 3600
+                if hour_diff > 18:
                     self.data[stormid]['invest'] = True
-        
-        
-    def __read_btk_jtwc(self,source,ssl_certificate):
-        
+
+    def __read_btk_jtwc(self, source, ssl_certificate):
         r"""
         Reads in b-deck data from the Tropical Cyclone Guidance Project (TCGP) into the Dataset object.
         """
 
-        #Get current year
+        # Get current year
         current_year = (dt.now()).year
 
-        #Get list of files in online directory
+        # Get list of files in online directory
         url = f'https://www.nrlmry.navy.mil/atcf_web/docs/tracks/{current_year}/'
-        if source == 'noaa': url = f'https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/'
-        if source == 'ucar': url = f'http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{current_year}/'
-        if ssl_certificate == False and source in ['jtwc','noaa']:
-            import ssl
-            urlpath = urllib.request.urlopen(url,context=ssl._create_unverified_context())
+        if source == 'noaa':
+            url = f'https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/'
+        if source == 'ucar':
+            url = f'http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{current_year}/'
+        if not ssl_certificate and source in ['jtwc', 'noaa']:
+            urlpath = urllib.request.urlopen(
+                url, context=ssl._create_unverified_context())
         else:
             urlpath = urllib.request.urlopen(url)
         string = urlpath.read().decode('utf-8')
 
-        #Get relevant filenames from directory
+        # Get relevant filenames from directory
         files = []
         search_pattern = f'b[isw][ohp][012349][0123456789]{current_year}.dat'
 
         pattern = re.compile(search_pattern)
         filelist = pattern.findall(string)
         for filename in filelist:
-            if filename not in files: files.append(filename)
-        
-        #Search for following year (for SH storms)
+            if filename not in files:
+                files.append(filename)
+
+        # Search for following year (for SH storms)
         search_pattern = f'b[isw][ohp][012349][0123456789]{current_year+1}.dat'
 
         pattern = re.compile(search_pattern)
         filelist = pattern.findall(string)
         for filename in filelist:
-            if filename not in files: files.append(filename)
-        
-        if source in ['jtwc','ucar','noaa']:
+            if filename not in files:
+                files.append(filename)
+
+        if source in ['jtwc', 'ucar', 'noaa']:
             try:
-                if ssl_certificate == False and source in ['jtwc','noaa']:
-                    urlpath_nextyear = urllib.request.urlopen(url.replace(str(current_year),str(current_year+1)),context=ssl._create_unverified_context())
+                if not ssl_certificate and source in ['jtwc', 'noaa']:
+                    urlpath_nextyear = urllib.request.urlopen(url.replace(str(current_year), str(
+                        current_year+1)), context=ssl._create_unverified_context())
                     string_nextyear = urlpath_nextyear.read().decode('utf-8')
                 else:
-                    urlpath_nextyear = urllib.request.urlopen(url.replace(str(current_year),str(current_year+1)))
+                    urlpath_nextyear = urllib.request.urlopen(
+                        url.replace(str(current_year), str(current_year+1)))
                     string_nextyear = urlpath_nextyear.read().decode('utf-8')
 
                 pattern = re.compile(search_pattern)
                 filelist = pattern.findall(string_nextyear)
                 for filename in filelist:
-                    if filename not in files: files.append(filename)
+                    if filename not in files:
+                        files.append(filename)
             except:
                 pass
 
-        #For each file, read in file content and add to hurdat dict
+        # For each file, read in file content and add to hurdat dict
         for file in files:
 
-            #Get file ID
+            # Get file ID
             stormid = ((file.split(".dat")[0])[1:]).upper()
-            
-            #Check for invest status
+
+            # Check for invest status
             invest_bool = False
             if int(stormid[2]) == 9:
                 invest_bool = True
 
-            #Determine basin based on where storm developed
+            # Determine basin based on where storm developed
             add_basin = 'west_pacific'
             if stormid[0] == 'I':
                 add_basin = 'north_indian'
             elif stormid[0] == 'S':
                 add_basin = ''
 
-            #add empty entry into dict
-            self.data[stormid] = {'id':stormid,'operational_id':stormid,'name':'','year':int(stormid[4:8]),'season':int(stormid[4:8]),'basin':add_basin,'source_info':'Joint Typhoon Warning Center','realtime':True,'invest':invest_bool}
+            # add empty entry into dict
+            self.data[stormid] = {
+                'id': stormid,
+                'operational_id': stormid,
+                'name': '',
+                'year': int(stormid[4:8]),
+                'season': int(stormid[4:8]),
+                'basin': add_basin,
+                'source_info': 'Joint Typhoon Warning Center',
+                'realtime': True,
+                'invest': invest_bool,
+            }
             self.data[stormid]['source'] = 'jtwc'
             self.data[stormid]['jtwc_source'] = source
-            
-            #Add source info
+
+            # Add source info
             self.data[stormid]['source_method'] = "JTWC ATCF"
-            self.data[stormid]['source_url'] = f'https://www.nrlmry.navy.mil/atcf_web/docs/tracks/{current_year}/'
+            self.data[stormid][
+                'source_url'] = f'https://www.nrlmry.navy.mil/atcf_web/docs/tracks/{current_year}/'
             if source == 'noaa':
                 self.data[stormid]['source_method'] = "NOAA SSD"
                 self.data[stormid]['source_url'] = f'https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/'
             if source == 'ucar':
-                self.data[stormid]['source_method'] = "UCAR's Tropical Cyclone Guidance Project (TCGP)"
+                self.data[stormid][
+                    'source_method'] = "UCAR's Tropical Cyclone Guidance Project (TCGP)"
                 self.data[stormid]['source_url'] = f'http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/'
 
-            #add empty lists
-            for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
+            # add empty lists
+            for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin']:
                 self.data[stormid][val] = []
             self.data[stormid]['ace'] = 0.0
 
-            #Read in file
+            # Read in file
             url = f"https://www.nrlmry.navy.mil/atcf_web/docs/tracks/{current_year}/{file}"
-            if source == 'noaa': url = f"https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/{file}"
-            if source == 'ucar': url = f"http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{current_year}/{file}"
-            if f"{current_year+1}.dat" in url: url = url.replace(str(current_year),str(current_year+1))
-            
-            if ssl_certificate == False and source in ['jtwc','noaa']:
-                f = urllib.request.urlopen(url,context=ssl._create_unverified_context())
+            if source == 'noaa':
+                url = f"https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/{file}"
+            if source == 'ucar':
+                url = f"http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{current_year}/{file}"
+            if f"{current_year+1}.dat" in url:
+                url = url.replace(str(current_year), str(current_year+1))
+
+            if not ssl_certificate and source in ['jtwc', 'noaa']:
+                f = urllib.request.urlopen(
+                    url, context=ssl._create_unverified_context())
                 content = f.read()
                 content = content.decode("utf-8")
                 content = content.split("\n")
-                content = [(i.replace(" ","")).split(",") for i in content]
+                content = [(i.replace(" ", "")).split(",") for i in content]
                 f.close()
             else:
                 f = urllib.request.urlopen(url)
                 content = read_url(url)
 
-            #iterate through file lines
+            # iterate through file lines
             for line in content:
 
-                if len(line) < 28: continue
-
-                #Get date of obs
-                date = dt.strptime(line[2],'%Y%m%d%H')
-                if date.strftime('%H%M') not in constants.STANDARD_HOURS: continue
+                if len(line) < 28:
+                    continue
 
-                #Ensure obs aren't being repeated
-                if date in self.data[stormid]['date']: continue
+                # Get time of obs
+                time = dt.strptime(line[2], '%Y%m%d%H')
+                if time.strftime('%H%M') not in constants.STANDARD_HOURS:
+                    continue
+
+                # Ensure obs aren't being repeated
+                if time in self.data[stormid]['time']:
+                    continue
 
-                #Get latitude into number
+                # Get latitude into number
                 if "N" in line[6]:
                     btk_lat_temp = line[6].split("N")[0]
-                    btk_lat = np.round(float(btk_lat_temp) * 0.1,1)
+                    btk_lat = np.round(float(btk_lat_temp) * 0.1, 1)
                 elif "S" in line[6]:
                     btk_lat_temp = line[6].split("S")[0]
-                    btk_lat = np.round(float(btk_lat_temp) * -0.1,1)
-                
-                #Get longitude into number
+                    btk_lat = np.round(float(btk_lat_temp) * -0.1, 1)
+
+                # Get longitude into number
                 if "W" in line[7]:
                     btk_lon_temp = line[7].split("W")[0]
-                    btk_lon = np.round(float(btk_lon_temp) * -0.1,1)
+                    btk_lon = np.round(float(btk_lon_temp) * -0.1, 1)
                 elif "E" in line[7]:
                     btk_lon_temp = line[7].split("E")[0]
-                    btk_lon = np.round(float(btk_lon_temp) * 0.1,1)
+                    btk_lon = np.round(float(btk_lon_temp) * 0.1, 1)
 
-                #Determine basin if unknown
+                # Determine basin if unknown
                 if add_basin == '':
-                    add_basin = get_basin(btk_lat,btk_lon)
+                    add_basin = get_basin(btk_lat, btk_lon)
                     self.data[stormid]['basin'] = add_basin
-                
-                #Get other relevant variables
+
+                # Get other relevant variables
                 btk_wind = int(line[8])
                 btk_mslp = int(line[9])
                 if source == 'ucar':
-                    btk_type = get_storm_type(btk_wind,False)
+                    btk_type = get_storm_type(btk_wind, False)
                 else:
                     btk_type = line[10]
-                    if btk_type == "TY" or btk_type == "ST": btk_type = "HU"
                 name = line[27]
 
-                #Replace with NaNs
-                if btk_wind > 250 or btk_wind < 10: btk_wind = np.nan
-                if btk_mslp > 1040 or btk_mslp < 800: btk_mslp = np.nan
+                # Replace with NaNs
+                if btk_wind > 250 or btk_wind < 10:
+                    btk_wind = np.nan
+                if btk_mslp > 1040 or btk_mslp < 800:
+                    btk_mslp = np.nan
 
-                #Add extra obs
+                # Add extra obs
                 self.data[stormid]['extra_obs'].append(0)
 
-                #Append into dict
-                self.data[stormid]['date'].append(date)
+                # Append into dict
+                self.data[stormid]['time'].append(time)
                 self.data[stormid]['special'].append('')
                 self.data[stormid]['type'].append(btk_type)
                 self.data[stormid]['lat'].append(btk_lat)
                 self.data[stormid]['lon'].append(btk_lon)
                 self.data[stormid]['vmax'].append(btk_wind)
                 self.data[stormid]['mslp'].append(btk_mslp)
-                
-                #Add basin
-                self.data[stormid]['wmo_basin'].append(add_basin)
-
-                #Calculate ACE & append to storm total
-                if np.isnan(btk_wind) == False:
-                    ace = (10**-4) * (btk_wind**2)
+
+                # Add basin
+                self.data[stormid]['wmo_basin'].append(
+                    get_basin(btk_lat, btk_lon, add_basin))
+
+                # Calculate ACE & append to storm total
+                if not np.isnan(btk_wind):
+                    ace = accumulated_cyclone_energy(btk_wind)
                     if btk_type in constants.NAMED_TROPICAL_STORM_TYPES:
-                        self.data[stormid]['ace'] += np.round(ace,4)
+                        self.data[stormid]['ace'] += np.round(ace, 4)
 
-            #Determine storm name for invests
-            if invest_bool == True:
-                
-                #Determine letter in front of invest
+            # Determine storm name for invests
+            if invest_bool:
+
+                # Determine letter in front of invest
                 add_letter = 'L'
                 if stormid[0] == 'C':
                     add_letter = 'C'
                 elif stormid[0] == 'E':
                     add_letter = 'E'
                 elif stormid[0] == 'W':
                     add_letter = 'W'
                 elif stormid[0] == 'I':
                     add_letter = 'I'
                 elif stormid[0] == 'S':
                     add_letter = 'S'
                 name = stormid[2:4] + add_letter
-            
-            #Add storm name
+
+            # Add storm name
             self.data[stormid]['name'] = name
-            
+
     def __read_nhc_shapefile(self):
-        
+
         try:
-            
-            #Read in shapefile zip from NHC
+
+            # Read in shapefile zip from NHC
             url = 'https://www.nhc.noaa.gov/xgtwo/gtwo_shapefiles.zip'
             request = urllib.request.Request(url)
             response = urllib.request.urlopen(request)
             file_like_object = BytesIO(response.read())
             tar = zipfile.ZipFile(file_like_object)
 
-            #Get file list (points, areas)
+            # Get file list (points, areas)
             members = '\n'.join([i for i in tar.namelist()])
             nums = "[0123456789]"
             search_pattern = f'gtwo_points_202{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}.shp'
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(members)
             files = []
             for file in filelist:
-                if file not in files: files.append(file.split(".shp")[0]) #remove duplicates
+                if file not in files:
+                    files.append(file.split(".shp")[0])  # remove duplicates
 
-            #Retrieve necessary components for shapefile
+            # Retrieve necessary components for shapefile
             members = tar.namelist()
             members_names = [i for i in members]
-            data = {'shp':0,'dbf':0,'prj':0,'shx':0}
+            data = {
+                'shp': 0,
+                'dbf': 0,
+                'prj': 0,
+                'shx': 0,
+            }
             for key in data.keys():
                 idx = members_names.index(files[0]+"."+key)
                 data[key] = BytesIO(tar.read(members[idx]))
 
-            #Read in shapefile
-            orig_reader = shapefile.Reader(shp=data['shp'], dbf=data['dbf'], prj=data['prj'], shx=data['shx'])
+            # Read in shapefile
+            orig_reader = shapefile.Reader(
+                shp=data['shp'], dbf=data['dbf'], prj=data['prj'], shx=data['shx'])
             shp = BasicReader(orig_reader)
 
-            #Iterate through all areas to match to existing invests
+            # Iterate through all areas to match to existing invests
             for record, point in zip(shp.records(), shp.geometries()):
 
-                #Read relevant data
+                # Read relevant data
                 lon = (list(point.coords)[0][0])
                 lat = (list(point.coords)[0][1])
                 prob_2day = record.attributes['PROB2DAY']
-                prob_5day = record.attributes['PROB5DAY']
+                prob_7day = record.attributes['PROB7DAY']
                 risk_2day = record.attributes['RISK2DAY']
-                risk_5day = record.attributes['RISK5DAY']
-                
-                #Match to existing invests
-                distances = [great_circle((lat,lon),(self.data[storm_id]['lat'][-1],self.data[storm_id]['lon'][-1])).miles for storm_id in self.data.keys()]
+                risk_7day = record.attributes['RISK7DAY']
+
+                # Match to existing invests
+                distances = [great_circle((lat, lon), (self.data[storm_id]['lat'][-1],
+                                          self.data[storm_id]['lon'][-1])).miles for storm_id in self.data.keys()]
                 min_distance = np.min(distances)
                 idx = distances.index(min_distance)
                 storm_id = [k for k in self.data.keys()][idx]
                 if min_distance <= 150:
                     self.data[storm_id]['prob_2day'] = prob_2day
-                    self.data[storm_id]['prob_5day'] = prob_5day
+                    self.data[storm_id]['prob_7day'] = prob_7day
                     self.data[storm_id]['risk_2day'] = risk_2day
-                    self.data[storm_id]['risk_5day'] = risk_5day
-            
+                    self.data[storm_id]['risk_7day'] = risk_7day
+
         except:
-            
+
             msg = "Error in retrieving NHC invest data."
             warnings.warn(msg)
-    
+
     def update(self):
-        
         r"""
         Update with the latest realtime data.
-        
+
         Notes
         -----
         This function has no return value, but simply updates the Realtime object with the latest data.
         """
-        
-        self.__init__(self.jtwc,self.jtwc_source,self.ssl_certificate)
-    
-    def list_active_storms(self,basin='all'):
-        
+
+        self.__init__(self.jtwc, self.jtwc_source, self.ssl_certificate)
+
+    def list_active_storms(self, basin='all'):
         r"""
         Produces a list of storms currently stored in Realtime.
-        
+
         Parameters
         ----------
         basin : str
             Basin for which to return active storms for. Default is 'all'.
-        
+
         Returns
         -------
         list
             List containing the storm IDs for currently active storms in the requested basin. Each ID has a Storm object stored as an attribute of Realtime.
         """
-        
+
         if basin == 'all':
             return self.storms
-        
+
         keys = []
         for key in self.storms:
-            if self[key].basin == basin: keys.append(key)
-        
+            if self[key].basin == basin:
+                keys.append(key)
+
         return keys
-    
-    def get_storm(self,storm):
-        
+
+    def get_storm(self, storm):
         r"""
         Returns a RealtimeStorm object for the requested storm ID.
-        
+
         Parameters
         ----------
         storm : str
             Storm ID for the requested storm (e.g., "AL012020").
-        
+
         Returns
         -------
         tropycal.realtime.RealtimeStorm
             An instance of RealtimeStorm.
         """
-        
-        #Check to see if storm is available
-        if isinstance(storm,str) == False:
+
+        # Check to see if storm is available
+        if not isinstance(storm, str):
             msg = "\"storm\" must be of type str."
             raise TypeError(msg)
         if storm not in self.storms:
             msg = "Requested storm ID is not contained in this object."
             raise RuntimeError(msg)
-        
-        #Return RealtimeStorm object
+
+        # Return RealtimeStorm object
         return self[storm]
 
-    def plot_summary(self,domain='all',ax=None,cartopy_proj=None,save_path=None,ssl_certificate=True,**kwargs):
-        
+    def plot_summary(self, domain='all', ax=None, cartopy_proj=None, save_path=None, ssl_certificate=True, **kwargs):
         r"""
         Plot a summary map of ongoing tropical cyclone and potential development activity.
-        
+
         Parameters
         ----------
         domain : str
             Domain for the plot. Default is "all". Please refer to :ref:`options-domain` for available domain options.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs, optional
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str, optional
             Relative or full path of directory to save the image in. If none, image will not be saved.
         ssl_certificate : boolean, optional
             If a JTWC forecast, this determines whether to disable SSL certificate when retrieving data from JTWC. Default is True. Use False *ONLY* if True causes an SSL certification error.
-        
+
         Other Parameters
         ----------------
         two_prop : dict
             Customization properties of NHC Tropical Weather Outlook (TWO). Please refer to :ref:`options-summary` for available options.
         invest_prop : dict
             Customization properties of active invests. Please refer to :ref:`options-summary` for available options.
         storm_prop : dict
             Customization properties of active storms. Please refer to :ref:`options-summary` for available options.
         cone_prop : dict
             Customization properties of cone of uncertainty. Please refer to :ref:`options-summary` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
 
         The following properties are available for plotting NHC Tropical Weather Outlook (TWO), via ``two_prop``.
 
         .. list-table:: 
            :widths: 25 75
@@ -845,48 +896,50 @@
            * - fillcolor
              - Fill color for forecast dots. Default is color by SSHWS category ("category").
            * - label_category
              - Boolean for whether to plot SSHWS category on top of forecast dots. Default is True.
            * - ms
              - Marker size for forecast dots. Default is 12.
         """
-        
-        #Retrieve NHC shapefiles for development areas
+
+        # Retrieve NHC shapefiles for development areas
         shapefiles = get_two_current()
-        
-        #Retrieve kwargs
-        two_prop = kwargs.pop('two_prop',{})
-        invest_prop = kwargs.pop('invest_prop',{})
-        storm_prop = kwargs.pop('storm_prop',{})
-        cone_prop = kwargs.pop('cone_prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Create instance of plot object
+
+        # Retrieve kwargs
+        two_prop = kwargs.pop('two_prop', {})
+        invest_prop = kwargs.pop('invest_prop', {})
+        storm_prop = kwargs.pop('storm_prop', {})
+        cone_prop = kwargs.pop('cone_prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0) #0.0
-        
-        #Get realtime forecasts
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)  # 0.0
+
+        # Get realtime forecasts
         forecasts = []
         for key in self.storms:
-            if self[key].invest == False:
+            if not self[key].invest:
                 try:
-                    forecasts.append(self.get_storm(key).get_forecast_realtime(ssl_certificate))
+                    forecasts.append(self.get_storm(
+                        key).get_forecast_realtime(ssl_certificate))
                 except:
                     forecasts.append({})
             else:
                 forecasts.append({})
-        forecasts = [entry if 'init' in entry.keys() and (dt.utcnow() - entry['init']).total_seconds() / 3600.0 <= 12 else {} for entry in forecasts]
-        
-        #Plot
-        ax = self.plot_obj.plot_summary([self.get_storm(key) for key in self.storms],forecasts,
-                                        shapefiles,dt.utcnow(),domain,ax,save_path,two_prop,invest_prop,storm_prop,cone_prop,map_prop)
-        
+        forecasts = [entry if 'init' in entry.keys() and (dt.utcnow(
+        ) - entry['init']).total_seconds() / 3600.0 <= 12 else {} for entry in forecasts]
+
+        # Plot
+        ax = self.plot_obj.plot_summary([self.get_storm(key) for key in self.storms], forecasts,
+                                        shapefiles, dt.utcnow(), domain, ax, save_path, two_prop, invest_prop, storm_prop, cone_prop, map_prop)
+
         return ax
-
```

### Comparing `tropycal-0.6.1/src/tropycal/realtime/storm.py` & `tropycal-1.0/src/tropycal/realtime/storm.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,90 +1,71 @@
 r"""Functionality for storing and analyzing an individual realtime storm."""
 
-import calendar
 import numpy as np
-import pandas as pd
-import re
 import scipy.interpolate as interp
 import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 import requests
 from ftplib import FTP
 
 from ..tracks import *
 from ..tracks.tools import *
 from ..tracks.plot import TrackPlot
 from ..utils import *
 from .. import constants
 from ..recon import ReconDataset
 
-try:
-    import zipfile
-    import gzip
-    from io import StringIO, BytesIO
-    import tarfile
-except:
-    warnings.warn("Warning: The libraries necessary for online NHC forecast retrieval aren't available (gzip, io, tarfile).")
-
-try:
-    import matplotlib.lines as mlines
-    import matplotlib.dates as mdates
-    import matplotlib.patheffects as path_effects
-    import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
 
 class RealtimeStorm(Storm):
-    
+
     r"""
     Initializes an instance of RealtimeStorm. This object inherits all the methods and functionality of ``tropycal.tracks.Storm``, but with additional methods unique to this object, all containing the word "realtime" as part of the function name.
 
     Parameters
     ----------
     storm : dict
         Dict entry of the requested storm.
-    
+
     Returns
     -------
     RealtimeStorm
         Instance of a RealtimeStorm object.
-    
+
     Notes
     -----
     A RealtimeStorm object is retrieved from a Realtime object's ``get_storm()`` method, or directly as an attribute of the Realtime object. For example, if an active storm has an ID of 'EP012022', it can be retrieved as such:
-    
+
     .. code-block:: python
-    
+
         from tropycal import realtime
         realtime_obj = realtime.Realtime()
         storm = realtime_obj.get_storm('EP012022')
-    
+
     Now this storm's data is stored in the variable ``storm``, which is an instance of RealtimeStorm and can access all of the methods and attributes of a RealtimeStorm object.
-    
-    All the variables associated with a RealtimeStorm object (e.g., lat, lon, date, vmax) can be accessed in two ways. The first is directly from the RealtimeStorm object:
-    
+
+    All the variables associated with a RealtimeStorm object (e.g., lat, lon, time, vmax) can be accessed in two ways. The first is directly from the RealtimeStorm object:
+
     >>> storm.lat
     array([ 9.8, 10.3, 10.8, 11.4, 11.9, 12.1, 12.2, 12.4, 12.6, 12.8, 13. ,
            12.9, 12.8, 12.9, 13.2, 13.6, 13.8, 13.9, 14. , 14. , 14.3, 14.6,
            15.1, 15.4])
-    
+
     The second is via ``storm.vars``, which returns a dictionary of the variables associated with the RealtimeStorm object. This is also a quick way to access all of the variables associated with a RealtimeStorm object:
-    
+
     >>> variable_dict = storm.vars
     >>> lat = variable_dict['lat']
     >>> lon = variable_dict['lon']
     >>> print(variable_dict.keys())
-    dict_keys(['date', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin'])
-    
+    dict_keys(['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin'])
+
     RealtimeStorm objects also have numerous attributes with information about the storm. ``storm.attrs`` returns a dictionary of the attributes for this RealtimeStorm object.
-    
+
     It should be noted that RealtimeStorm objects have additional attributes that Storm objects do not, specifically for 2 and 5 day NHC formation probability. These only display values for invests within NHC's area of responsibility; tropical cyclones or invests in JTWC's area of responsibility display "N/A".
-    
+
     >>> print(storm.attrs)
     {'id': 'EP012022',
      'operational_id': 'EP012022',
      'name': 'AGATHA',
      'year': 2022,
      'season': 2022,
      'basin': 'east_pacific',
@@ -95,927 +76,1020 @@
      'source': 'hurdat',
      'ace': 6.055,
      'prob_2day': 'N/A',
      'prob_5day': 'N/A',
      'risk_2day': 'N/A',
      'risk_5day': 'N/A',
      'realtime': True}
-    
+
     """
-    
+
     def __setitem__(self, key, value):
         self.__dict__[key] = value
-        
+
     def __getitem__(self, key):
         return self.__dict__[key]
-    
+
     def __repr__(self):
-         
-        #Label object
+
+        # Label object
         summary = ["<tropycal.realtime.RealtimeStorm>"]
-        
-        #Format keys for summary
+
+        # Format keys for summary
         type_array = np.array(self.dict['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))[0]
-        if self.invest and len(idx) == 0: idx = np.array([True for i in type_array])
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))[0]
+        if self.invest and len(idx) == 0:
+            idx = np.array([True for i in type_array])
         if len(idx) == 0:
-            start_date = 'N/A'
-            end_date = 'N/A'
+            start_time = 'N/A'
+            end_time = 'N/A'
             max_wind = 'N/A'
             min_mslp = 'N/A'
         else:
-            time_tropical = np.array(self.dict['date'])[idx]
-            start_date = time_tropical[0].strftime("%H00 UTC %d %B %Y")
-            end_date = time_tropical[-1].strftime("%H00 UTC %d %B %Y")
-            max_wind = 'N/A' if all_nan(np.array(self.dict['vmax'])[idx]) == True else np.nanmax(np.array(self.dict['vmax'])[idx])
-            min_mslp = 'N/A' if all_nan(np.array(self.dict['mslp'])[idx]) == True else np.nanmin(np.array(self.dict['mslp'])[idx])
-        summary_keys = {'Maximum Wind':f"{max_wind} knots",
-                        'Minimum Pressure':f"{min_mslp} hPa",
-                        'Start Date':start_date,
-                        'End Date':end_date}
-        
-        #Format keys for coordinates
+            time_tropical = np.array(self.dict['time'])[idx]
+            start_time = time_tropical[0].strftime("%H00 UTC %d %B %Y")
+            end_time = time_tropical[-1].strftime("%H00 UTC %d %B %Y")
+            max_wind = 'N/A' if all_nan(np.array(self.dict['vmax'])[idx]) else np.nanmax(np.array(self.dict['vmax'])[idx])
+            min_mslp = 'N/A' if all_nan(np.array(self.dict['mslp'])[idx]) else np.nanmin(np.array(self.dict['mslp'])[idx])
+        summary_keys = {
+            'Maximum Wind': f"{max_wind} knots",
+            'Minimum Pressure': f"{min_mslp} hPa",
+            'Start Time': start_time,
+            'End Time': end_time,
+        }
+
+        # Format keys for coordinates
         variable_keys = {}
         for key in self.vars.keys():
             dtype = type(self.vars[key][0]).__name__
-            dtype = dtype.replace("_","")
+            dtype = dtype.replace("_", "")
             variable_keys[key] = f"({dtype}) [{self.vars[key][0]} .... {self.vars[key][-1]}]"
 
-        #Add storm summary
+        # Add storm summary
         summary.append("Storm Summary:")
         add_space = np.max([len(key) for key in summary_keys.keys()])+3
         for key in summary_keys.keys():
             key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
-        #Add coordinates
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
+        # Add coordinates
         summary.append("\nVariables:")
         add_space = np.max([len(key) for key in variable_keys.keys()])+3
         for key in variable_keys.keys():
             key_name = key
-            summary.append(f'{" "*4}{key_name:<{add_space}}{variable_keys[key]}')
-        
-        #Add additional information
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{variable_keys[key]}')
+
+        # Add additional information
         summary.append("\nMore Information:")
         add_space = np.max([len(key) for key in self.attrs.keys()])+3
         for key in self.attrs.keys():
             key_name = key+":"
-            val = '%0.1f'%(self.attrs[key]) if key == 'ace' else self.attrs[key]
+            val = '%0.1f' % (
+                self.attrs[key]) if key == 'ace' else self.attrs[key]
             summary.append(f'{" "*4}{key_name:<{add_space}}{val}')
 
         return "\n".join(summary)
-    
-    def __init__(self,storm,stormTors=None):
-        
-        #Save the dict entry of the storm
+
+    def __init__(self, storm, stormTors=None):
+
+        # Save the dict entry of the storm
         self.dict = storm
-        
-        #Add other attributes about the storm
+
+        # Add other attributes about the storm
         keys = self.dict.keys()
         self.attrs = {}
         self.vars = {}
         for key in keys:
-            if key == 'realtime': continue
-            if isinstance(self.dict[key], list) == False and isinstance(self.dict[key], dict) == False:
+            if key == 'realtime':
+                continue
+            if not isinstance(self.dict[key], list) and not isinstance(self.dict[key], dict):
                 self[key] = self.dict[key]
                 self.attrs[key] = self.dict[key]
-            if isinstance(self.dict[key], list) == True and isinstance(self.dict[key], dict) == False:
+            if isinstance(self.dict[key], list) and not isinstance(self.dict[key], dict):
                 self.vars[key] = np.array(self.dict[key])
                 self[key] = np.array(self.dict[key])
-                
-        #Assign tornado data
-        if stormTors is not None and isinstance(stormTors,dict):
+
+        # Assign tornado data
+        if stormTors is not None and isinstance(stormTors, dict):
             self.stormTors = stormTors['data']
             self.tornado_dist_thresh = stormTors['dist_thresh']
             self.attrs['Tornado Count'] = len(stormTors['data'])
-        
-        #Get Archer track data for this storm, if it exists
+
+        # Get Archer track data for this storm, if it exists
         try:
             self.get_archer()
         except:
             pass
-    
-        #Initialize recon dataset instance
+
+        # Initialize recon dataset instance
         self.recon = ReconDataset(storm=self)
 
-        #Determine if storm object was retrieved via realtime object
+        # Determine if storm object was retrieved via realtime object
         if 'realtime' in keys and self.dict['realtime']:
             self.realtime = True
             self.attrs['realtime'] = True
         else:
             self.realtime = False
             self.attrs['realtime'] = False
-            
+
     def get_realtime_formation_prob(self):
-        
         r"""
         Retrieve the latest NHC formation probability. Only valid for invests within NHC's area of responsibility.
-        
+
         Returns
         -------
         dict
             Dictionary containing latest NHC forecast formation probability, if available. If none, defaults to zero or N/A.
         """
-        
+
         return {
             'prob_2day': self.dict['prob_2day'],
             'risk_2day': self.dict['risk_2day'],
-            'prob_5day': self.dict['prob_5day'],
-            'risk_5day': self.dict['risk_5day']
+            'prob_7day': self.dict['prob_7day'],
+            'risk_7day': self.dict['risk_7day']
         }
-    
-    def download_graphic_realtime(self,save_path=""):
-        
+
+    def download_graphic_realtime(self, save_path=""):
         r"""
         Download the latest official forecast track graphic. Available for both NHC and JTWC sources.
-        
+
         Parameters
         ----------
         save_path : str
             Filepath to save the image in. If blank, default is current working directory.
         """
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Determine data source
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Determine data source
         if self.source == 'hurdat':
             part1 = f"AT{self.id[2:4]}" if self.id[0:2] == "AL" else self.id[0:4]
             url = f"https://www.nhc.noaa.gov/storm_graphics/{part1}/{self.id}_5day_cone_with_line_and_wind.png"
         else:
             url = f"https://www.nrlmry.navy.mil/atcf_web/docs/current_storms/{self.id.lower()}.gif"
         url_ext = url.split(".")[-1]
-        
-        #Try to download file
-        if requests.get(url).status_code != 200: raise RuntimeError("Official forecast graphic is unavailable for this storm.")
-        
-        #Download file
+
+        # Try to download file
+        if requests.get(url).status_code != 200:
+            raise RuntimeError(
+                "Official forecast graphic is unavailable for this storm.")
+
+        # Download file
         response = requests.get(url)
-        full_path = os.path.join(save_path,f"Forecast_{self.id}.{url_ext}")
+        full_path = os.path.join(save_path, f"Forecast_{self.id}.{url_ext}")
         with open(full_path, 'wb') as f:
             f.write(response.content)
 
-    
     def get_discussion_realtime(self):
-        
         r"""
         Retrieve the latest available forecast discussion. For JTWC storms, the Prognostic Reasoning product is retrieved.
-        
+
         Returns
         -------
         dict
             Dict entry containing the latest official forecast discussion.
         """
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Get latest forecast discussion for HURDAT source storm objects
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Get latest forecast discussion for HURDAT source storm objects
         if self.source == "hurdat":
             return self.get_nhc_discussion(forecast=-1)
-        
-        #Get latest forecast discussion for JTWC source storm objects
+
+        # Get latest forecast discussion for JTWC source storm objects
         elif self.source == 'jtwc':
-            
-            #Read in discussion file
+
+            # Read in discussion file
             url = f"https://www.metoc.navy.mil/jtwc/products/{self.id[0:2].lower()}{self.id[2:4]}{self.id[6:8]}prog.txt"
             f = urllib.request.urlopen(url)
             content = f.read()
             content = content.decode("utf-8")
             f.close()
             return content
-        
-        #Otherwise, return error message
+
+        # Otherwise, return error message
         else:
             msg = "No realtime forecast discussion is available for this storm."
             raise RuntimeError(msg)
-    
-    def get_forecast_realtime(self,ssl_certificate=True):
-        
+
+    def get_forecast_realtime(self, ssl_certificate=True):
         r"""
         Retrieve a dictionary containing the latest official forecast. Available for both NHC and JTWC sources.
-        
+
         Parameters
         ----------
         ssl_certificate : boolean, optional
             If a JTWC forecast, this determines whether to disable SSL certificate when retrieving data from JTWC. Default is True. Use False *ONLY* if True causes an SSL certification error.
-        
+
         Returns
         -------
         dict
             Dictionary containing the latest official forecast.
-        
+
         Notes
         -----
         This dictionary includes a calculation for accumulated cyclone energy (ACE), cumulatively for the storm's lifespan through each forecast hour. This is done by linearly interpolating the forecast to 6-hour intervals and calculating 6-hourly ACE at each interval. For storms where forecast tropical cyclone type is available, ACE is not calculated for forecast periods that are neither tropical nor subtropical.
         """
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #NHC forecast data
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # NHC forecast data
         if self.source == 'hurdat':
-        
-            #Get forecast for this storm
+
+            # Get forecast for this storm
             try:
-                content = read_url(f"https://ftp.nhc.noaa.gov/atcf/fst/{self.id.lower()}.fst")
+                content = read_url(
+                    f"https://ftp.nhc.noaa.gov/atcf/fst/{self.id.lower()}.fst")
             except:
                 try:
-                    content = read_url(f"ftp://ftp.nhc.noaa.gov/atcf/fst/{self.id.lower()}.fst")
+                    content = read_url(
+                        f"ftp://ftp.nhc.noaa.gov/atcf/fst/{self.id.lower()}.fst")
                 except:
-                    raise RuntimeError("NHC forecast data is unavailable for this storm.")
+                    raise RuntimeError(
+                        "NHC forecast data is unavailable for this storm.")
 
-            #Iterate through every line in content:
+            # Iterate through every line in content:
             forecasts = {}
 
             for line in content:
 
-                #Get basic components
-                lineArray = [i.replace(" ","") for i in line]
-                if len(lineArray) < 11: continue
+                # Get basic components
+                lineArray = [i.replace(" ", "") for i in line]
+                if len(lineArray) < 11:
+                    continue
                 try:
-                    basin,number,run_init,n_a,model,fhr,lat,lon,vmax,mslp,stype,rad,windcode,neq,seq,swq,nwq = lineArray[:17]
+                    basin, number, run_init, n_a, model, fhr, lat, lon, vmax, mslp, stype, rad, windcode, neq, seq, swq, nwq = lineArray[:17]
                     use_wind = True
                 except:
-                    basin,number,run_init,n_a,model,fhr,lat,lon,vmax,mslp,stype = lineArray[:11]
+                    basin, number, run_init, n_a, model, fhr, lat, lon, vmax, mslp, stype = lineArray[:11]
                     use_wind = False
-                if model not in ["OFCL","OFCI"]: continue
+                if model not in ["OFCL", "OFCI"]:
+                    continue
 
                 if len(forecasts) == 0:
                     forecasts = {
-                        'init':dt.strptime(run_init,'%Y%m%d%H'),'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],'type':[],
-                        'windrad':[],'cumulative_ace':[],'cumulative_ace_fhr':[]
+                        'init': dt.strptime(run_init, '%Y%m%d%H'),
+                        'fhr': [],
+                        'lat': [],
+                        'lon': [],
+                        'vmax': [],
+                        'mslp': [],
+                        'type': [],
+                        'windrad': [],
+                        'cumulative_ace': [],
+                        'cumulative_ace_fhr': [],
                     }
 
-                #Format lat & lon
+                # Format lat & lon
                 fhr = int(fhr)
                 if "N" in lat:
                     lat_temp = lat.split("N")[0]
-                    lat = np.round(float(lat_temp) * 0.1,1)
+                    lat = np.round(float(lat_temp) * 0.1, 1)
                 elif "S" in lat:
                     lat_temp = lat.split("S")[0]
-                    lat = np.round(float(lat_temp) * -0.1,1)
+                    lat = np.round(float(lat_temp) * -0.1, 1)
                 if "W" in lon:
                     lon_temp = lon.split("W")[0]
-                    lon = np.round(float(lon_temp) * -0.1,1)
+                    lon = np.round(float(lon_temp) * -0.1, 1)
                 elif "E" in lon:
                     lon_temp = lon.split("E")[0]
-                    lon = np.round(float(lon_temp) * 0.1,1)
+                    lon = np.round(float(lon_temp) * 0.1, 1)
 
-                #Format vmax & MSLP
+                # Format vmax & MSLP
                 if vmax == '':
                     vmax = np.nan
                 else:
                     vmax = int(vmax)
-                    if vmax < 10 or vmax > 300: vmax = np.nan
+                    if vmax < 10 or vmax > 300:
+                        vmax = np.nan
                 if mslp == '':
                     mslp = np.nan
                 else:
                     mslp = int(mslp)
-                    if mslp < 1: mslp = np.nan
-                    
-                #Format wind radii
+                    if mslp < 1:
+                        mslp = np.nan
+
+                # Format wind radii
                 if use_wind:
                     try:
                         rad = int(rad)
-                        if rad in [0,35]: rad = 34
-                        neq = np.nan if windcode=='' else int(neq)
-                        seq = np.nan if windcode in ['','AAA'] else int(seq)
-                        swq = np.nan if windcode in ['','AAA'] else int(swq)
-                        nwq = np.nan if windcode in ['','AAA'] else int(nwq)
+                        if rad in [0, 35]:
+                            rad = 34
+                        neq = np.nan if windcode == '' else int(neq)
+                        seq = np.nan if windcode in ['', 'AAA'] else int(seq)
+                        swq = np.nan if windcode in ['', 'AAA'] else int(swq)
+                        nwq = np.nan if windcode in ['', 'AAA'] else int(nwq)
                     except:
                         rad = 34
                         neq = np.nan
                         seq = np.nan
                         swq = np.nan
                         nwq = np.nan
                 else:
                     rad = 34
                     neq = np.nan
                     seq = np.nan
                     swq = np.nan
                     nwq = np.nan
-                
-                #Add forecast data to dict if forecast hour isn't already there
+
+                # Add forecast data to dict if forecast hour isn't already there
                 if fhr not in forecasts['fhr']:
-                    if model in ['OFCL','OFCI'] and fhr > 120:
+                    if model in ['OFCL', 'OFCI'] and fhr > 120:
                         pass
                     else:
                         if lat == 0.0 and lon == 0.0:
                             continue
                         forecasts['fhr'].append(fhr)
                         forecasts['lat'].append(lat)
                         forecasts['lon'].append(lon)
                         forecasts['vmax'].append(vmax)
                         forecasts['mslp'].append(mslp)
-                        forecasts['windrad'].append({rad:[neq,seq,swq,nwq]})
+                        forecasts['windrad'].append(
+                            {rad: [neq, seq, swq, nwq]})
 
-                        #Get storm type, if it can be determined
-                        if stype in ['','DB'] and vmax != 0 and np.isnan(vmax) == False:
-                            stype = get_storm_type(vmax,False)
+                        # Get storm type, if it can be determined
+                        if stype in ['', 'DB'] and vmax != 0 and not np.isnan(vmax):
+                            stype = get_storm_type(vmax, False)
                         forecasts['type'].append(stype)
                 else:
                     ifhr = forecasts['fhr'].index(fhr)
-                    forecasts['windrad'][ifhr][rad] = [neq,seq,swq,nwq]
-                    
-        #Retrieve JTWC forecast otherwise
+                    forecasts['windrad'][ifhr][rad] = [neq, seq, swq, nwq]
+
+        # Retrieve JTWC forecast otherwise
         else:
-            
-            #Get forecast for this storm
-            if self.jtwc_source in ['jtwc','ucar']:
+
+            # Get forecast for this storm
+            if self.jtwc_source in ['jtwc', 'ucar']:
                 url = f"https://www.nrlmry.navy.mil/atcf_web/docs/current_storms/{self.id.lower()}.sum"
             else:
                 url = f"https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/{self.id.lower()}.fst"
-            if ssl_certificate == False and self.jtwc_source in ['jtwc','ucar']:
+            if not ssl_certificate and self.jtwc_source in ['jtwc', 'ucar']:
                 import ssl
-                if requests.get(url,verify=False).status_code != 200:
-                    raise RuntimeError("JTWC forecast data is unavailable for this storm.")
+                if requests.get(url, verify=False).status_code != 200:
+                    raise RuntimeError(
+                        "JTWC forecast data is unavailable for this storm.")
             else:
-                if requests.get(url).status_code != 200: raise RuntimeError("JTWC forecast data is unavailable for this storm.")
+                if requests.get(url).status_code != 200:
+                    raise RuntimeError(
+                        "JTWC forecast data is unavailable for this storm.")
 
-            #Read file content
-            if ssl_certificate == False and self.jtwc_source in ['jtwc','ucar']:
+            # Read file content
+            if not ssl_certificate and self.jtwc_source in ['jtwc', 'ucar']:
                 import ssl
-                f = urllib.request.urlopen(url,context=ssl._create_unverified_context())
+                f = urllib.request.urlopen(
+                    url, context=ssl._create_unverified_context())
             else:
                 f = urllib.request.urlopen(url)
             content = f.read()
             content = content.decode("utf-8")
             content = content.split("\n")
             f.close()
 
-            if self.jtwc_source in ['jtwc','ucar']:
-                #Iterate through every line in content:
-                run_init = content[2].split(" ")[0]
+            # Find starting index
+            start_idx = 0
+            for idx, line in enumerate(content):
+                lineArray = line.split(" ")
+                if 'WARNING' in lineArray[0] and len(lineArray) > 2:
+                    start_idx = idx
+                    break
+
+            if self.jtwc_source in ['jtwc', 'ucar']:
+                # Iterate through every line in content:
+                run_init = content[start_idx+2].split(" ")[0]
                 forecasts = {}
 
-                for line in content[3:]:
+                for line in content[start_idx+3:]:
 
-                    #Exit once done retrieving forecast
-                    if line == "AMP": break
+                    # Exit once done retrieving forecast
+                    if line == "AMP":
+                        break
 
-                    #Get basic components
+                    # Get basic components
                     lineArray = line.split(" ")
-                    if len(lineArray) < 4: continue
+                    if len(lineArray) < 4:
+                        continue
 
-                    #Exit once done retrieving forecast
-                    if lineArray[0] == "AMP": break
+                    # Exit once done retrieving forecast
+                    if lineArray[0] == "AMP":
+                        break
 
                     if len(forecasts) == 0:
                         forecasts = {
-                            'init':dt.strptime(run_init,'%Y%m%d%H'),'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],
-                            'windrad':[],'cumulative_ace':[],'cumulative_ace_fhr':[],'type':[]
+                            'init': dt.strptime(run_init, '%Y%m%d%H'),
+                            'fhr': [],
+                            'lat': [],
+                            'lon': [],
+                            'vmax': [],
+                            'mslp': [],
+                            'windrad': [],
+                            'cumulative_ace': [],
+                            'cumulative_ace_fhr': [],
+                            'type': []
                         }
 
-                    #Forecast hour
+                    # Forecast hour
                     fhr = int(lineArray[0].split("T")[1])
 
-                    #Format lat & lon
+                    # Format lat & lon
                     lat = lineArray[1]
                     lon = lineArray[2]
                     if "N" in lat:
                         lat_temp = lat.split("N")[0]
-                        lat = np.round(float(lat_temp) * 0.1,1)
+                        lat = np.round(float(lat_temp) * 0.1, 1)
                     elif "S" in lat:
                         lat_temp = lat.split("S")[0]
-                        lat = np.round(float(lat_temp) * -0.1,1)
+                        lat = np.round(float(lat_temp) * -0.1, 1)
                     if "W" in lon:
                         lon_temp = lon.split("W")[0]
-                        lon = np.round(float(lon_temp) * -0.1,1)
+                        lon = np.round(float(lon_temp) * -0.1, 1)
                     elif "E" in lon:
                         lon_temp = lon.split("E")[0]
-                        lon = np.round(float(lon_temp) * 0.1,1)
+                        lon = np.round(float(lon_temp) * 0.1, 1)
 
-                    #Format vmax & MSLP
+                    # Format vmax & MSLP
                     vmax = int(lineArray[3])
-                    if vmax < 10 or vmax > 300: vmax = np.nan
+                    if vmax < 10 or vmax > 300:
+                        vmax = np.nan
                     mslp = np.nan
 
-                    #Format wind radii
+                    # Format wind radii
                     windrad = {}
-                    for rad in (34,50,64):
+                    for rad in (34, 50, 64):
                         try:
                             irad = list(lineArray).index(f'R{rad:03}')
-                            windrad[rad] = [int(lineArray[irad+j]) for j in (1,4,7,10)]
+                            windrad[rad] = [int(lineArray[irad+j])
+                                            for j in (1, 4, 7, 10)]
                         except:
                             continue
 
-                    #Add forecast data to dict if forecast hour isn't already there
+                    # Add forecast data to dict if forecast hour isn't already there
                     if fhr not in forecasts['fhr']:
-                        if lat == 0.0 and lon == 0.0: continue
+                        if lat == 0.0 and lon == 0.0:
+                            continue
                         forecasts['fhr'].append(fhr)
                         forecasts['lat'].append(lat)
                         forecasts['lon'].append(lon)
                         forecasts['vmax'].append(vmax)
                         forecasts['mslp'].append(mslp)
                         forecasts['windrad'].append(windrad)
 
-                        #Get storm type, if it can be determined
-                        stype = get_storm_type(vmax,False)
+                        # Get storm type, if it can be determined
+                        stype = get_storm_type(vmax, False)
                         forecasts['type'].append(stype)
-            
+
             else:
-                
-                content = [(i.replace(" ","")).split(",") for i in content]
-                
-                #Iterate through every line in content:
+
+                content = [(i.replace(" ", "")).split(",") for i in content]
+
+                # Iterate through every line in content:
                 forecasts = {}
 
                 for line in content:
 
-                    #Get basic components
-                    lineArray = [i.replace(" ","") for i in line]
-                    if len(lineArray) < 11: continue
+                    # Get basic components
+                    lineArray = [i.replace(" ", "") for i in line]
+                    if len(lineArray) < 11:
+                        continue
                     try:
-                        basin,number,run_init,n_a,model,fhr,lat,lon,vmax,mslp,stype,rad,windcode,neq,seq,swq,nwq = lineArray[:17]
+                        basin, number, run_init, n_a, model, fhr, lat, lon, vmax, mslp, stype, rad, windcode, neq, seq, swq, nwq = lineArray[ :17]
                         use_wind = True
                     except:
-                        basin,number,run_init,n_a,model,fhr,lat,lon,vmax,mslp,stype = lineArray[:11]
+                        basin, number, run_init, n_a, model, fhr, lat, lon, vmax, mslp, stype = lineArray[:11]
                         use_wind = False
-                    if model not in ["JTWC"]: continue
+                    if model not in ["JTWC"]:
+                        continue
 
                     if len(forecasts) == 0:
                         forecasts = {
-                            'init':dt.strptime(run_init,'%Y%m%d%H'),'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],'type':[],
-                            'windrad':[],'cumulative_ace':[],'cumulative_ace_fhr':[]
+                            'init': dt.strptime(run_init, '%Y%m%d%H'),
+                            'fhr': [],
+                            'lat': [],
+                            'lon': [],
+                            'vmax': [],
+                            'mslp': [],
+                            'type': [],
+                            'windrad': [],
+                            'cumulative_ace': [],
+                            'cumulative_ace_fhr': []
                         }
 
-                    #Format lat & lon
+                    # Format lat & lon
                     fhr = int(fhr)
                     if "N" in lat:
                         lat_temp = lat.split("N")[0]
-                        lat = np.round(float(lat_temp) * 0.1,1)
+                        lat = np.round(float(lat_temp) * 0.1, 1)
                     elif "S" in lat:
                         lat_temp = lat.split("S")[0]
-                        lat = np.round(float(lat_temp) * -0.1,1)
+                        lat = np.round(float(lat_temp) * -0.1, 1)
                     if "W" in lon:
                         lon_temp = lon.split("W")[0]
-                        lon = np.round(float(lon_temp) * -0.1,1)
+                        lon = np.round(float(lon_temp) * -0.1, 1)
                     elif "E" in lon:
                         lon_temp = lon.split("E")[0]
-                        lon = np.round(float(lon_temp) * 0.1,1)
+                        lon = np.round(float(lon_temp) * 0.1, 1)
 
-                    #Format vmax & MSLP
+                    # Format vmax & MSLP
                     if vmax == '':
                         vmax = np.nan
                     else:
                         vmax = int(vmax)
-                        if vmax < 10 or vmax > 300: vmax = np.nan
+                        if vmax < 10 or vmax > 300:
+                            vmax = np.nan
                     if mslp == '':
                         mslp = np.nan
                     else:
                         mslp = int(mslp)
-                        if mslp < 1: mslp = np.nan
+                        if mslp < 1:
+                            mslp = np.nan
 
-                    #Format wind radii
+                    # Format wind radii
                     if use_wind:
                         try:
                             rad = int(rad)
-                            if rad in [0,35]: rad = 34
-                            neq = np.nan if windcode=='' else int(neq)
-                            seq = np.nan if windcode in ['','AAA'] else int(seq)
-                            swq = np.nan if windcode in ['','AAA'] else int(swq)
-                            nwq = np.nan if windcode in ['','AAA'] else int(nwq)
+                            if rad in [0, 35]:
+                                rad = 34
+                            neq = np.nan if windcode == '' else int(neq)
+                            seq = np.nan if windcode in [
+                                '', 'AAA'] else int(seq)
+                            swq = np.nan if windcode in [
+                                '', 'AAA'] else int(swq)
+                            nwq = np.nan if windcode in [
+                                '', 'AAA'] else int(nwq)
                         except:
                             rad = 34
                             neq = np.nan
                             seq = np.nan
                             swq = np.nan
                             nwq = np.nan
                     else:
                         rad = 34
                         neq = np.nan
                         seq = np.nan
                         swq = np.nan
                         nwq = np.nan
 
-                    #Add forecast data to dict if forecast hour isn't already there
+                    # Add forecast data to dict if forecast hour isn't already there
                     if fhr not in forecasts['fhr']:
-                        if model in ['OFCL','OFCI'] and fhr > 120:
+                        if model in ['OFCL', 'OFCI'] and fhr > 120:
                             pass
                         else:
                             if lat == 0.0 and lon == 0.0:
                                 continue
                             forecasts['fhr'].append(fhr)
                             forecasts['lat'].append(lat)
                             forecasts['lon'].append(lon)
                             forecasts['vmax'].append(vmax)
                             forecasts['mslp'].append(mslp)
-                            forecasts['windrad'].append({rad:[neq,seq,swq,nwq]})
+                            forecasts['windrad'].append(
+                                {rad: [neq, seq, swq, nwq]})
 
-                            #Get storm type, if it can be determined
-                            if stype in ['','DB'] and vmax != 0 and np.isnan(vmax) == False:
-                                stype = get_storm_type(vmax,False)
-                            if stype == 'TY': stype = 'HU'
-                            if stype == 'ST': stype = 'HU'
+                            # Get storm type, if it can be determined
+                            if stype in ['', 'DB'] and vmax != 0 and not np.isnan(vmax):
+                                stype = get_storm_type(vmax, False)
                             forecasts['type'].append(stype)
                     else:
                         ifhr = forecasts['fhr'].index(fhr)
-                        forecasts['windrad'][ifhr][rad]=[neq,seq,swq,nwq]
-                    
-        #Determine ACE thus far (prior to initial forecast hour)
+                        forecasts['windrad'][ifhr][rad] = [neq, seq, swq, nwq]
+
+        # Determine ACE thus far (prior to initial forecast hour)
         ace = 0.0
-        for i in range(len(self.date)):
-            if self.date[i] >= forecasts['init']: continue
-            if self.type[i] not in constants.NAMED_TROPICAL_STORM_TYPES: continue
-            ace += accumulated_cyclone_energy(self.vmax[i],hours=6)
-        
-        #Add initial forecast hour ACE
-        ace += accumulated_cyclone_energy(forecasts['vmax'][0],hours=6)
+        for i in range(len(self.time)):
+            if self.time[i] >= forecasts['init']:
+                continue
+            if self.type[i] not in constants.NAMED_TROPICAL_STORM_TYPES:
+                continue
+            ace += accumulated_cyclone_energy(self.vmax[i], hours=6)
+
+        # Add initial forecast hour ACE
+        ace += accumulated_cyclone_energy(forecasts['vmax'][0], hours=6)
         forecasts['cumulative_ace_fhr'].append(0)
-        forecasts['cumulative_ace'].append(np.round(ace,1))
-        
-        #Interpolate forecast to 6-hour increments
+        forecasts['cumulative_ace'].append(np.round(ace, 1))
+
+        # Interpolate forecast to 6-hour increments
         def temporal_interpolation(value, orig_times, target_times, kind='linear'):
-            f = interp.interp1d(orig_times,value,kind=kind,fill_value='extrapolate')
+            f = interp.interp1d(orig_times, value, kind=kind,
+                                fill_value='extrapolate')
             ynew = f(target_times)
             return ynew
-        interp_fhr = range(0,forecasts['fhr'][-1]+1,6) #Construct a 6-hour time range
-        interp_vmax = temporal_interpolation(forecasts['vmax'],forecasts['fhr'],interp_fhr)
-        
-        #Interpolate storm type
+        # Construct a 6-hour time range
+        interp_fhr = range(0, forecasts['fhr'][-1]+1, 6)
+        interp_vmax = temporal_interpolation(
+            forecasts['vmax'], forecasts['fhr'], interp_fhr)
+
+        # Interpolate storm type
         interp_type = []
-        for dummy_i,(i_hour,i_vmax) in enumerate(zip(interp_fhr,interp_vmax)):
+        for dummy_i, (i_hour, i_vmax) in enumerate(zip(interp_fhr, interp_vmax)):
             use_i = 0
             for i in range(len(forecasts['fhr'])):
                 if forecasts['fhr'][i] > i_hour:
                     break
                 use_i = int(i + 0.0)
             i_type = forecasts['type'][use_i]
-            if i_type in constants.TROPICAL_STORM_TYPES: i_type = get_storm_type(i_vmax,False)
+            if i_type in constants.TROPICAL_STORM_TYPES:
+                i_type = get_storm_type(i_vmax, False)
             interp_type.append(i_type)
-        
-        #Add forecast ACE
-        for i,(i_fhr,i_vmax,i_type) in enumerate(zip(interp_fhr[1:],interp_vmax[1:],interp_type[1:])):
-            
-            #Add ACE if storm is a TC
+
+        # Add forecast ACE
+        for i, (i_fhr, i_vmax, i_type) in enumerate(zip(interp_fhr[1:], interp_vmax[1:], interp_type[1:])):
+
+            # Add ACE if storm is a TC
             if i_type in constants.NAMED_TROPICAL_STORM_TYPES:
-                ace += accumulated_cyclone_energy(i_vmax,hours=6)
-            
-            #Add ACE to array
+                ace += accumulated_cyclone_energy(i_vmax, hours=6)
+
+            # Add ACE to array
             if i_fhr in forecasts['fhr']:
-                forecasts['cumulative_ace'].append(np.round(ace,1))
+                forecasts['cumulative_ace'].append(np.round(ace, 1))
                 forecasts['cumulative_ace_fhr'].append(i_fhr)
-        
-        #Save forecast as attribute
+
+        # Save forecast as attribute
         self.latest_forecast = forecasts
         return self.latest_forecast
 
-    def plot_forecast_realtime(self,track_labels='fhr',cone_days=5,domain="dynamic_forecast",
-                                   ax=None,cartopy_proj=None,save_path=None,ssl_certificate=True,**kwargs):
-        
+    def plot_forecast_realtime(self, track_labels='fhr', cone_days=5, domain="dynamic_forecast",
+                               ax=None, cartopy_proj=None, save_path=None, ssl_certificate=True, **kwargs):
         r"""
         Plots the latest available official forecast. Available for both NHC and JTWC sources.
-        
+
         Parameters
         ----------
         track_labels : str
             Label forecast hours with the following methods:
-            
+
             * **""** = no label
             * **"fhr"** = forecast hour (default)
             * **"valid_utc"** = UTC valid time
             * **"valid_edt"** = EDT valid time
         cone_days : int
             Number of days to plot the forecast cone. Default is 5 days. Can select 2, 3, 4 or 5 days.
         domain : str
             Domain for the plot. Default is "dynamic_forecast". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         ssl_certificate : boolean, optional
             If a JTWC forecast, this determines whether to disable SSL certificate when retrieving data from JTWC. Default is True. Use False *ONLY* if True causes an SSL certification error.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Check to ensure storm is not an invest
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Create instance of plot object
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
             if max(self.dict['lon']) > 140 or min(self.dict['lon']) < -140:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=180.0)
             else:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Get forecast for this storm
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
+
+        # Get forecast for this storm
         try:
             nhc_forecasts = (self.latest_forecast).copy()
         except:
-            nhc_forecasts = self.get_forecast_realtime(ssl_certificate=ssl_certificate)
-        
-        #Add other info to forecast dict
+            nhc_forecasts = self.get_forecast_realtime(
+                ssl_certificate=ssl_certificate)
+
+        # Add other info to forecast dict
         nhc_forecasts['advisory_num'] = -1
         nhc_forecasts['basin'] = self.basin
-        if self.source != "hurdat": nhc_forecasts['cone'] = False
-        
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storm_nhc(nhc_forecasts,self.dict,track_labels,cone_days,domain,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+        if self.source != "hurdat":
+            nhc_forecasts['cone'] = False
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storm_nhc(
+            nhc_forecasts, self.dict, track_labels, cone_days, domain, ax=ax, save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-        
-    def get_realtime_info(self,source='all'):
-        
+
+    def get_realtime_info(self, source='all'):
         r"""
         Returns a dict containing the latest available information about the storm. This function uses NHC Public Advisories, so it will differ from available Best Track data.
-        
+
         Parameters
         ----------
         source : str
             Data source to use. Default is "all". Available options are:
-            
+
             * **"all"** = Latest from either public advisory or best track. Both NHC & JTWC.
             * **"public_advisory"** = Latest public advisory. NHC only.
             * **"best_track"** = Latest Best Track file data. Both NHC & JTWC.
-        
+
         Returns
         -------
         dict
             Dictionary containing current storm information.
         """
-        
-        #Error check
-        if isinstance(source,str) == False:
+
+        # Error check
+        if not isinstance(source, str):
             msg = "\"source\" must be of type str."
             raise TypeError(msg)
-        if source not in ['all','public_advisory','best_track']:
+        if source not in ['all', 'public_advisory', 'best_track']:
             msg = "\"source\" must be 'all', 'public_advisory', or 'best_track'."
             raise ValueError(msg)
         if source == 'public_advisory' and self.source != 'hurdat':
             msg = "A source of 'public_advisory' can only be used for storms in NHC's area of responsibility."
             raise RuntimeError(msg)
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
             if self.source == 'hurdat':
                 msg = "NHC does not issue public advisories on invests. Defaulting to best track method."
                 warnings.warn(msg)
             source = 'best_track'
-        
-        #Declare empty dict
+
+        # Declare empty dict
         current_advisory = {}
-        
-        #If source is all, determine which method to use
+
+        # If source is all, determine which method to use
         if source == 'all':
             if self.source == 'hurdat':
-                #Check to see which is the latest advisory
-                latest_btk = self.date[-1]
-                
-                #Get latest available public advisory
+                # Check to see which is the latest advisory
+                latest_btk = self.time[-1]
+
+                # Get latest available public advisory
                 try:
-                    content = read_url(f"https://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml",subsplit=False)
+                    content = read_url(
+                        f"https://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml", subsplit=False)
                 except:
-                    content = read_url(f"ftp://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml",subsplit=False)
-                
-                #Get UTC time of advisory
+                    content = read_url(
+                        f"ftp://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml", subsplit=False)
+
+                # Get UTC time of advisory
                 results = [i for i in content if 'messageDateTimeUTC' in i][0]
                 result = (results.split(">")[1]).split("<")[0]
-                latest_advisory = dt.strptime(result,'%Y%m%d %I:%M:%S %p UTC')
-                
-                #Check which one to use
+                latest_advisory = dt.strptime(result, '%Y%m%d %I:%M:%S %p UTC')
+
+                # Check which one to use
                 if latest_btk > latest_advisory:
                     source = 'best_track'
                 else:
                     source = 'public_advisory'
             else:
                 source = 'best_track'
-        
-        #If public advisory, retrieve this data
+
+        # If public advisory, retrieve this data
         if source == 'public_advisory':
-            
-            #Add source
+
+            # Add source
             current_advisory['source'] = 'NHC Public Advisory'
 
-            #Get latest available public advisory
+            # Get latest available public advisory
             try:
-                content = read_url(f"https://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml",subsplit=False)
+                content = read_url(
+                    f"https://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml", subsplit=False)
             except:
-                content = read_url(f"ftp://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml",subsplit=False)
+                content = read_url(
+                    f"ftp://ftp.nhc.noaa.gov/atcf/adv/{self.id.lower()}_info.xml", subsplit=False)
 
-            #Get public advisory number
+            # Get public advisory number
             results = [i for i in content if 'advisoryNumber' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['advisory_number'] = result
 
-            #Get UTC time of advisory
+            # Get UTC time of advisory
             results = [i for i in content if 'messageDateTimeUTC' in i][0]
             result = (results.split(">")[1]).split("<")[0]
-            result = dt.strptime(result,'%Y%m%d %I:%M:%S %p UTC')
+            result = dt.strptime(result, '%Y%m%d %I:%M:%S %p UTC')
             current_advisory['time_utc'] = result
 
-            #Get storm type
+            # Get storm type
             results = [i for i in content if 'systemType' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['type'] = result.title()
 
-            #Get storm name
+            # Get storm name
             results = [i for i in content if 'systemName' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['name'] = result.title()
 
-            #Get coordinates
+            # Get coordinates
             results = [i for i in content if 'centerLocLatitude' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['lat'] = float(result)
             results = [i for i in content if 'centerLocLongitude' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['lon'] = float(result)
 
-            #Get sustained wind speed
+            # Get sustained wind speed
             results = [i for i in content if 'systemIntensityMph' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['wind_mph'] = int(result)
             results = [i for i in content if 'systemIntensityKph' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['wind_kph'] = int(result)
             results = [i for i in content if 'systemIntensityKts' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['wind_kt'] = int(result)
 
-            #Get MSLP
+            # Get MSLP
             results = [i for i in content if 'systemMslpMb' in i][0]
             result = (results.split(">")[1]).split("<")[0]
-            current_advisory['mslp'] = np.int(result)
+            current_advisory['mslp'] = int(result)
 
-            #Get storm category
-            current_advisory['category'] = wind_to_category(current_advisory['wind_kt'])
+            # Get storm category
+            current_advisory['category'] = wind_to_category(
+                current_advisory['wind_kt'])
 
-            #Get storm direction
+            # Get storm direction
             results = [i for i in content if 'systemDirectionOfMotion' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['motion_direction'] = result.split(" OR ")[0]
             results = [i for i in content if 'systemDirectionOfMotion' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             try:
-                current_advisory['motion_direction_degrees'] = int((result.split(" OR ")[1]).split(" DEGREES")[0])
+                current_advisory['motion_direction_degrees'] = int(
+                    (result.split(" OR ")[1]).split(" DEGREES")[0])
             except:
                 current_advisory['motion_direction_degrees'] = 0
 
-            #Get storm speed
+            # Get storm speed
             results = [i for i in content if 'systemSpeedMph' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['motion_mph'] = int(result)
             results = [i for i in content if 'systemSpeedKph' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['motion_kph'] = int(result)
             results = [i for i in content if 'systemSpeedKts' in i][0]
             result = (results.split(">")[1]).split("<")[0]
             current_advisory['motion_kt'] = int(result)
-            
-        #Best track data
+
+        # Best track data
         else:
-            
-            #Add source
+
+            # Add source
             if self.source == 'hurdat':
                 current_advisory['source'] = 'NHC Best Track'
             else:
                 current_advisory['source'] = 'JTWC Best Track'
 
-            #Get public advisory number
+            # Get public advisory number
             current_advisory['advisory_number'] = 'n/a'
 
-            #Get UTC time of advisory
-            current_advisory['time_utc'] = self.date[-1]
+            # Get UTC time of advisory
+            current_advisory['time_utc'] = self.time[-1]
 
-            #Get storm type
+            # Get storm type
             subtrop_flag = self.type[-1] in constants.SUBTROPICAL_ONLY_STORM_TYPES
-            current_advisory['type'] = get_storm_classification(self.vmax[-1],subtrop_flag,self.basin)
-            
-            #Check for non-tropical storm types
+            current_advisory['type'] = get_storm_classification(
+                self.vmax[-1], subtrop_flag, self.wmo_basin[-1])
+
+            # Check for non-tropical storm types
             if self.type[-1] not in constants.TROPICAL_STORM_TYPES:
                 if all(type not in self.type for type in constants.TROPICAL_STORM_TYPES):
                     if self.invest:
                         current_advisory['type'] = 'Invest'
                     else:
                         current_advisory['type'] = 'Potential Tropical Cyclone'
                 else:
                     current_advisory['type'] = 'Post-Tropical Cyclone'
-            elif self.source in ['jtwc','ucar']:
+            elif self.source in ['jtwc', 'ucar']:
                 if self.invest:
                     current_advisory['type'] = 'Invest'
 
-            #Get storm name
+            # Get storm name
             current_advisory['name'] = self.name.title()
 
-            #Get coordinates
+            # Get coordinates
             current_advisory['lat'] = self.lat[-1]
             current_advisory['lon'] = self.lon[-1]
 
-            #Get sustained wind speed
+            # Get sustained wind speed
             current_advisory['wind_mph'] = knots_to_mph(self.vmax[-1])
             current_advisory['wind_kph'] = int(self.vmax[-1] * 1.852)
             current_advisory['wind_kt'] = self.vmax[-1]
 
-            #Get MSLP
-            current_advisory['mslp'] = np.int(self.mslp[-1])
+            # Get MSLP
+            current_advisory['mslp'] = int(self.mslp[-1])
 
-            #Get storm category
-            current_advisory['category'] = wind_to_category(current_advisory['wind_kt'])
+            # Get storm category
+            current_advisory['category'] = wind_to_category(
+                current_advisory['wind_kt'])
 
-            #Determine motion direction and degrees
+            # Determine motion direction and degrees
             try:
-                
-                #Cannot calculate motion if there's only one data point
+
+                # Cannot calculate motion if there's only one data point
                 if len(self.lon) == 1:
-                    
-                    #Get storm direction
+
+                    # Get storm direction
                     current_advisory['motion_direction'] = 'n/a'
                     current_advisory['motion_direction_degrees'] = 'n/a'
 
-                    #Get storm speed
+                    # Get storm speed
                     current_advisory['motion_mph'] = 'n/a'
                     current_advisory['motion_kph'] = 'n/a'
                     current_advisory['motion_kt'] = 'n/a'
-                
-                #Otherwise, use great_circle to calculate
+
+                # Otherwise, use great_circle to calculate
                 else:
-                    
-                    #Get points
-                    start_point = (self.lat[-2],self.lon[-2])
-                    end_point = (self.lat[-1],self.lon[-1])
-                    
-                    #Get time since last update
-                    hour_diff = (self.date[-1] - self.date[-2]).total_seconds() / 3600.0
-                    
-                    #Calculate zonal and meridional position change in km
-                    x = great_circle((self.lat[-2],self.lon[-2]), (self.lat[-2],self.lon[-1])).kilometers
-                    if self.lon[-1] < self.lon[-2]: x = x * -1
-
-                    y = great_circle((self.lat[-2],self.lon[-2]), (self.lat[-1],self.lon[-2])).kilometers
-                    if self.lat[-1] < self.lat[-2]: y = y * -1
-
-                    #Calculate motion direction vector
-                    idir = np.degrees(np.arctan2(x,y))
-                    if idir < 0: idir += 360.0
-                    
-                    #Calculate motion direction string
+
+                    # Get points
+                    start_point = (self.lat[-2], self.lon[-2])
+                    end_point = (self.lat[-1], self.lon[-1])
+
+                    # Get time since last update
+                    hour_diff = (
+                        self.time[-1] - self.time[-2]).total_seconds() / 3600.0
+
+                    # Calculate zonal and meridional position change in km
+                    x = great_circle(
+                        (self.lat[-2], self.lon[-2]), (self.lat[-2], self.lon[-1])).kilometers
+                    if self.lon[-1] < self.lon[-2]:
+                        x = x * -1
+
+                    y = great_circle(
+                        (self.lat[-2], self.lon[-2]), (self.lat[-1], self.lon[-2])).kilometers
+                    if self.lat[-1] < self.lat[-2]:
+                        y = y * -1
+
+                    # Calculate motion direction vector
+                    idir = np.degrees(np.arctan2(x, y))
+                    if idir < 0:
+                        idir += 360.0
+
+                    # Calculate motion direction string
                     def deg_str(d):
                         dirs = ["N", "NNE", "NE", "ENE", "E", "ESE", "SE", "SSE",
                                 "S", "SSW", "SW", "WSW", "W", "WNW", "NW", "NNW"]
                         ix = int((d + 11.25)/22.5)
                         return dirs[ix % 16]
                     dirs = deg_str(idir)
 
-                    #Update storm direction
+                    # Update storm direction
                     current_advisory['motion_direction'] = dirs
-                    current_advisory['motion_direction_degrees'] = int(np.round(idir,0))
+                    current_advisory['motion_direction_degrees'] = int(
+                        np.round(idir, 0))
+
+                    # Get storm speed
+                    current_advisory['motion_mph'] = int(
+                        np.round(great_circle(start_point, end_point).miles / float(hour_diff), 0))
+                    current_advisory['motion_kph'] = int(np.round(great_circle(
+                        start_point, end_point).kilometers / float(hour_diff), 0))
+                    current_advisory['motion_kt'] = int(
+                        np.round(current_advisory['motion_mph'] * 0.868976, 0))
 
-                    #Get storm speed
-                    current_advisory['motion_mph'] = int(np.round(great_circle(start_point,end_point).miles / float(hour_diff),0))
-                    current_advisory['motion_kph'] = int(np.round(great_circle(start_point,end_point).kilometers / float(hour_diff),0))
-                    current_advisory['motion_kt'] = int(np.round(current_advisory['motion_mph'] * 0.868976,0))
-                
-            #Otherwise, can't calculate motion
+            # Otherwise, can't calculate motion
             except:
-                
-                #Get storm direction
+
+                # Get storm direction
                 current_advisory['motion_direction'] = 'n/a'
                 current_advisory['motion_direction_degrees'] = 'n/a'
 
-                #Get storm speed
+                # Get storm speed
                 current_advisory['motion_mph'] = 'n/a'
                 current_advisory['motion_kph'] = 'n/a'
                 current_advisory['motion_kt'] = 'n/a'
-        
-        #Return dict
+
+        # Return dict
         return current_advisory
-            
-        
+
     def __get_public_advisory(self):
 
-        #Get list of all public advisories for this storm
+        # Get list of all public advisories for this storm
         url_disco = 'https://ftp.nhc.noaa.gov/atcf/pub/'
         try:
             page = requests.get(url_disco).text
             content = page.split("\n")
             files = []
             for line in content:
                 if ".public" in line and self.id.lower() in line:
@@ -1026,62 +1100,64 @@
         except:
             ftp = FTP('ftp.nhc.noaa.gov')
             ftp.login()
             ftp.cwd('atcf/pub')
             files = ftp.nlst()
             out = ftp.quit()
 
-        #Keep only largest number
+        # Keep only largest number
         numbers = [int(i.split(".")[-1]) for i in files]
         max_number = np.nanmax(numbers)
         if max_number >= 100:
             max_number = str(max_number)
         elif max_number >= 10:
             max_number = f"0{max_number}"
         else:
             max_number = f"00{max_number}"
         files = [i for i in files if f".{max_number}" in i]
 
-        #Determine if there's an intermediate advisory available
+        # Determine if there's an intermediate advisory available
         if len(files) > 1:
             advisory_letter = []
             for file in files:
                 if 'public_' in file:
                     letter = (file.split("public_")[1]).split(".")[0]
                     advisory_letter.append(letter)
             max_letter = max(advisory_letter)
             files = [i for i in files if f".public_{max_letter}" in i]
 
-        #Read file containing advisory
-        content = read_url(url_disco + files[0],subsplit=False)
+        # Read file containing advisory
+        content = read_url(url_disco + files[0], subsplit=False)
 
-        #Figure out time issued
+        # Figure out time issued
         hr = content[6].split(" ")[0]
         zone = content[6].split(" ")[2]
         disco_time = num_to_str2(int(hr)) + ' '.join(content[6].split(" ")[1:])
 
         format_time = content[6].split(" ")[0]
-        if len(format_time) == 3: format_time = "0" + format_time
-        format_time = format_time + " " +  ' '.join(content[6].split(" ")[1:])
-        disco_date = dt.strptime(format_time,f'%I00 %p {zone} %a %b %d %Y')
+        if len(format_time) == 3:
+            format_time = "0" + format_time
+        format_time = format_time + " " + ' '.join(content[6].split(" ")[1:])
+        disco_time = dt.strptime(format_time, f'%I00 %p {zone} %a %b %d %Y')
 
         time_zones = {
-        'ADT':-3,
-        'AST':-4,
-        'EDT':-4,
-        'EST':-5,
-        'CDT':-5,
-        'CST':-6,
-        'MDT':-6,
-        'MST':-7,
-        'PDT':-7,
-        'PST':-8,
-        'HDT':-9,
-        'HST':-10}
-        offset = time_zones.get(zone,0)
-        disco_date = disco_date + timedelta(hours=offset*-1)
-        
+            'ADT': -3,
+            'AST': -4,
+            'EDT': -4,
+            'EST': -5,
+            'CDT': -5,
+            'CST': -6,
+            'MDT': -6,
+            'MST': -7,
+            'PDT': -7,
+            'PST': -8,
+            'HDT': -9,
+            'HST': -10
+        }
+        offset = time_zones.get(zone, 0)
+        disco_time = disco_time + timedelta(hours=offset*-1)
+
     def __get_ensembles_eps(self):
-        #This function is currently not functioning. The path to retrieve EPS ensemble data is:
-        #ftp://wmo:essential@dissemination.ecmwf.int/20200518120000/
-        #A_JSXX01ECEP181200_C_ECMP_20200518120000_tropical_cyclone_track_AMPHAN_86p3degE_14degN_bufr4.bin
+        # This function is currently not functioning. The path to retrieve EPS ensemble data is:
+        # ftp://wmo:essential@dissemination.ecmwf.int/20200518120000/
+        # A_JSXX01ECEP181200_C_ECMP_20200518120000_tropical_cyclone_track_AMPHAN_86p3degE_14degN_bufr4.bin
         return
```

### Comparing `tropycal-0.6.1/src/tropycal/recon/dataset.py` & `tropycal-1.0/src/tropycal/recon/dataset.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,903 +1,955 @@
 import os
-import sys
 import numpy as np
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 import pandas as pd
 import requests
 import pickle
 import copy
 import urllib3
 
 from scipy.interpolate import interp1d
-from scipy.ndimage import gaussian_filter as gfilt,gaussian_filter1d as gfilt1d
-from scipy.ndimage.filters import minimum_filter
+from scipy.ndimage import gaussian_filter1d as gfilt1d
+from scipy.ndimage import minimum_filter
 import matplotlib.dates as mdates
 
 try:
-    import matplotlib as mlib
-    import matplotlib.lines as mlines
     import matplotlib.colors as mcolors
-    import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
     import cartopy.crs as ccrs
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
 
 from .plot import *
 from ..tracks.plot import TrackPlot
 from .realtime import Mission
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 
+
 class ReconDataset:
 
     r"""
     Creates an instance of a ReconDataset object containing all recon data for a single storm.
-    
+
     Parameters
     ----------
     storm : tropycal.tracks.Storm
         Requested Storm object.
-    
+
     Returns
     -------
     ReconDataset
         An instance of ReconDataset.
-    
+
     Notes
     -----
     .. warning::
-    
+
         Recon data is currently only available from 1989 onwards.
-    
+
     ReconDataset and its subclasses (hdobs, dropsondes and vdms) consist the **storm-centric** part of the recon module, meaning that recon data is retrieved specifically for tropical cyclones, and all recon missions for the requested storm are additionally transformed to storm-centric coordinates. This differs from realtime recon functionality, which is **mission-centric**.
-    
+
     This storm-centric functionality allows for additional recon analysis and visualization functions, such as derived hovmollers and spatial maps for example. As of Tropycal v0.4, Recon data can only be retrieved for tropical cyclones, not for invests.
-    
+
     ReconDataset will contain nothing the first time it's initialized, but contains methods to retrieve the three sub-classes of recon:
-    
+
     .. list-table:: 
        :widths: 25 75
        :header-rows: 1
 
        * - Class
          - Description
        * - hdobs
          - Class containing all High Density Observations (HDOBs) for this Storm.
        * - dropsondes
          - Class containing all dropsondes for this Storm.
        * - vdms
          - Class containing all Vortex Data Messages (VDMs) for this Storm.
-    
+
     Each of these sub-classes can be initialized as a sub-class of ReconDataset as follows. Note that this may take some time, especially for storms with many recon missions.
-    
+
     .. code-block:: python
-    
+
         #Retrieve Hurricane Michael (2018) from TrackDataset
         basin = tracks.TrackDataset()
         storm = basin.get_storm(('michael',2018))
-    
+
         #Retrieve all HDOBs for this storm
         storm.recon.get_hdobs()
-        
+
         #Retrieve all dropsondes for this storm
         storm.recon.get_dropsondes()
-        
+
         #Retrieve all VDMs for this storm
         storm.recon.get_vdms()
-    
+
     Once this data has been read in, these subclasses and their associated methods and attributes can be accessed from within the `recon` object as follows, using HDOBs for example:
-    
+
     .. code-block:: python
-    
+
         #Retrieve Pandas DataFrame of all HDOB observations
         storm.recon.hdobs.data
-        
+
         #Plot all HDOB points
         storm.recon.hdobs.plot_points()
-        
+
         #Plot derived hovmoller from HDOB points
         storm.recon.hdobs.plot_hovmoller()
-    
+
     Individual missions can also be retrieved as ``Mission`` objects. For example, this code retrieves a ``Mission`` object for the second mission of this storm:
-    
+
     .. code-block:: python
-    
+
         #This line prints all available mission IDs from this storm
         print(storm.recon.find_mission())
-        
+
         #This line retrieves the 2nd mission from this storm
         mission = storm.recon.get_mission(2)
-    
+
     """
 
     def __repr__(self):
-        
+
         info = []
-        for name in ['hdobs','dropsondes','vdms']:
+        for name in ['hdobs', 'dropsondes', 'vdms']:
             try:
                 info.append(self.__dict__[name].__repr__())
             except:
                 info.append('')
         return '\n'.join(info)
-    
+
     def __init__(self, storm):
-        
+
         self.source = 'https://www.nhc.noaa.gov/archive/recon/'
         self.storm = storm
 
-    def get_hdobs(self,data=None):
-        
+    def get_hdobs(self, data=None):
         r"""
         Retrieve High Density Observations (HDOBs) for this storm.
-        
+
         Parameters
         ----------
         data : str, optional
             String representing the path of a pickle file containing HDOBs data, saved via ``hdobs.to_pickle()``. If none, data is read from NHC.
-        
+
         Notes
         -----
         This function has no return value, but stores the resulting HDOBs object within this ReconDataset instance. All of its methods can then be accessed as follows, for the following example storm:
-        
+
         .. code-block:: python
-    
+
             from tropycal import tracks
-            
+
             #Read basin dataset
             basin = tracks.TrackDataset()
-            
+
             #Read storm object
             storm = basin.get_storm(('michael',2018))
-            
+
             #Read hdobs data
             storm.recon.get_hdobs()
-            
+
             #Plot all HDOB points
             storm.recon.hdobs.plot_points()
         """
-        
-        self.hdobs = hdobs(self.storm,data)
-    
-    def get_dropsondes(self,data=None):
-        
+
+        self.hdobs = hdobs(self.storm, data)
+
+    def get_dropsondes(self, data=None):
         r"""
         Retrieve dropsondes for this storm.
-        
+
         Parameters
         ----------
         data : str, optional
             String representing the path of a pickle file containing dropsonde data, saved via ``dropsondes.to_pickle()``. If none, data is read from NHC.
-        
+
         Notes
         -----
         This function has no return value, but stores the resulting dropsondes object within this ReconDataset instance. All of its methods can then be accessed as follows, for the following example storm:
-        
+
         .. code-block:: python
-    
+
             from tropycal import tracks
-            
+
             #Read basin dataset
             basin = tracks.TrackDataset()
-            
+
             #Read storm object
             storm = basin.get_storm(('michael',2018))
-            
+
             #Read dropsondes data
             storm.recon.get_dropsondes()
-            
+
             #Plot all dropsonde points
             storm.recon.dropsondes.plot_points()
         """
-        
-        self.dropsondes = dropsondes(self.storm,data)
-        
-    def get_vdms(self,data=None):
-        
+
+        self.dropsondes = dropsondes(self.storm, data)
+
+    def get_vdms(self, data=None):
         r"""
         Retrieve Vortex Data Messages (VDMs) for this storm.
-        
+
         Parameters
         ----------
         data : str, optional
             String representing the path of a pickle file containing VDM data, saved via ``vdms.to_pickle()``. If none, data is read from NHC.
-        
+
         Notes
         -----
         This function has no return value, but stores the resulting VDMs object within this ReconDataset instance. All of its methods can then be accessed as follows, for the following example storm:
-        
+
         .. code-block:: python
-    
+
             from tropycal import tracks
-            
+
             #Read basin dataset
             basin = tracks.TrackDataset()
-            
+
             #Read storm object
             storm = basin.get_storm(('michael',2018))
-            
+
             #Read VDM data
             storm.recon.get_vdms()
-            
+
             #Plot all VDM points
             storm.recon.vdms.plot_points()
         """
-        
-        self.vdms = vdms(self.storm,data)
-    
-    def get_mission(self,number):
-        
+
+        self.vdms = vdms(self.storm, data)
+
+    def get_mission(self, number):
         r"""
         Retrieve a Mission object for a given mission number for this storm.
-        
+
         Parameters
         ----------
         number : int or str
             Requested mission number. Can be an integer (1) or a string with two characters ("01").
-        
+
         Returns
         -------
         Mission
             Instance of a Mission object for the requested mission.
         """
-        
+
         def str2(number):
-            if isinstance(number,str): return number
-            if number < 10: return f"0{number}"
+            if isinstance(number, str):
+                return number
+            if number < 10:
+                return f"0{number}"
             return str(number)
-       
-        #Automatically retrieve data if not already available
+
+        # Automatically retrieve data if not already available
         try:
             self.vdms
         except:
             self.get_vdms()
         try:
             self.hdobs
         except:
             self.get_hdobs()
         try:
             self.dropsondes
         except:
             self.get_dropsondes()
-        
-        #Search through all missions to find the full mission ID
+
+        # Search through all missions to find the full mission ID
         missions = []
         for mission in np.unique(self.hdobs.data['mission']):
             try:
                 missions.append(int(mission))
             except:
                 pass
         missions = list(np.sort(missions))
-        if isinstance(number,str): missions = [str2(i) for i in missions]
+        if isinstance(number, str):
+            missions = [str2(i) for i in missions]
         if number not in missions:
             raise ValueError("Requested mission ID is not available.")
-        
-        #Retrieve data for mission
-        hdobs_mission = self.hdobs.data.loc[self.hdobs.data['mission']==str2(number)]
+
+        # Retrieve data for mission
+        hdobs_mission = self.hdobs.data.loc[self.hdobs.data['mission'] == str2(
+            number)]
         mission_id = hdobs_mission['mission_id'].values[0]
-        vdms_mission = [i for i in self.vdms.data if i['mission_id'] == mission_id]
-        dropsondes_mission = [i for i in self.dropsondes.data if i['mission_id'] == mission_id]
+        vdms_mission = [
+            i for i in self.vdms.data if i['mission_id'] == mission_id]
+        dropsondes_mission = [
+            i for i in self.dropsondes.data if i['mission_id'] == mission_id]
 
         mission_dict = {
-            'hdobs':hdobs_mission,
-            'vdms':vdms_mission,
-            'dropsondes':dropsondes_mission,
-            'aircraft':mission_id.split("-")[0],
-            'storm_name':mission_id.split("-")[2]
+            'hdobs': hdobs_mission,
+            'vdms': vdms_mission,
+            'dropsondes': dropsondes_mission,
+            'aircraft': mission_id.split("-")[0],
+            'storm_name': mission_id.split("-")[2]
         }
-        
-        #Get sources
+
+        # Get sources
         try:
-            sources = list(np.unique([self.vdms.source,self.hdobs.source,self.dropsondes.source]))
-            if len(sources) == 1: sources = sources[0]
+            sources = list(
+                np.unique([self.vdms.source, self.hdobs.source, self.dropsondes.source]))
+            if len(sources) == 1:
+                sources = sources[0]
         except:
             sources = 'National Hurricane Center (NHC)'
         mission_dict['source'] = sources
-        
-        return Mission(mission_dict,mission_id)
-        
+
+        return Mission(mission_dict, mission_id)
+
     def update(self):
-        
         r"""
         Update with the latest data for an ongoing storm.
-        
+
         Notes
         -----
         This function has no return value, but simply updates all existing sub-classes of ReconDataset.
         """
-        
-        for name in ['hdobs','dropsondes','vdms']:
+
+        for name in ['hdobs', 'dropsondes', 'vdms']:
             try:
                 self.__dict__[name].update()
             except:
                 print(f'No {name} object to update')
 
-    def get_track(self,time=None):
-        
+    def get_track(self, time=None):
         r"""
         Retrieve coordinates of recon track for one or more times.
-        
+
         Parameters
         ----------
         time : datetime.datetime or list, optional
             Datetime object or list of datetime objects representing the requested time.
-        
+
         Returns
         -------
         tuple
             (lon,lat) coordinates.
-        
+
         Notes
         -----
         The track from which coordinate(s) are returned is generated by an optimal combination of Best Track and Recon (VDMs and/or HDOBs) tracks.
         """
-                
+
         if time is None or 'trackfunc' not in self.__dict__.keys():
-            btk = self.storm.to_dataframe()[['date','lon','lat']].rename(columns={'date':'time'})
-                      
+            btk = self.storm.to_dataframe()[['time', 'lon', 'lat']]
+
             try:
                 if 'vdms' not in self.__dict__.keys():
                     print('Getting VDMs for track')
-                    self.get_vdms()  
-                rec = pd.DataFrame([{k:d[k] for k in ('time','lon','lat')} for d in self.vdms.data])
+                    self.get_vdms()
+                rec = pd.DataFrame(
+                    [{k: d[k] for k in ('time', 'lon', 'lat')} for d in self.vdms.data])
             except:
                 try:
                     rec = storm.recon.hdobs.sel(iscenter=1).data
                 except:
                     rec = None
 
             if rec is None:
                 track = copy.copy(btk)
             else:
                 track = copy.copy(rec)
-                for i,row in btk.iterrows():
-                    if min(abs(row['time']-rec['time']))>timedelta(hours=3):
+                for i, row in btk.iterrows():
+                    if min(abs(row['time'] - rec['time'])) > timedelta(hours=3):
                         track.loc[len(track.index)] = row
             track = track.sort_values(by='time').reset_index(drop=True)
 
-            #Interpolate center position to time of each ob
+            # Interpolate center position to time of each ob
             datenum = [mdates.date2num(t) for t in track['time']]
-            f1 = interp1d(datenum,track['lon'],fill_value='extrapolate',kind='quadratic')
-            f2 = interp1d(datenum,track['lat'],fill_value='extrapolate',kind='quadratic')
-            self.trackfunc = (f1,f2)
+            f1 = interp1d(
+                datenum, track['lon'], fill_value='extrapolate', kind='quadratic')
+            f2 = interp1d(
+                datenum, track['lat'], fill_value='extrapolate', kind='quadratic')
+            self.trackfunc = (f1, f2)
 
         if time is not None:
             datenum = []
             for t in time:
                 try:
                     datenum.append(mdates.date2num(t))
                 except:
                     datenum.append(np.nan)
             track = tuple([f(datenum) for f in self.trackfunc])
             return track
-                
-    def find_mission(self,time=None,distance=None):
-        
+
+    def find_mission(self, time=None, distance=None):
         r"""
         Returns the name of a mission or list of recon missions for this storm.
-        
+
         Parameters
         ----------
         time : datetime.datetime or list, optional
             Datetime object or list of datetime objects representing the time of the requested mission. If none, all missions will be returned.
         distance : int, optional
             Distance from storm center, in kilometers.
-        
+
         Returns
         -------
         list
             The IDs of any/all missions that had in-storm observations during the specified time.
-        
+
         Notes
         -----
         Especially in earlier years, missions are not always numbered sequentially (e.g., the first mission might not have an ID of "01").
-        
+
         To get a ``Mission`` object for one or more mission IDs, use the mission ID as an argument in ``ReconDataset.get_mission()``. For example, to retrieve a Mission object for every mission valid at a requested time, assuming that ``ReconDataset`` is linked to a Storm object:
-        
+
         .. code-block:: python
-    
+
             #Enter a requested time here
             import datetime as dt
             requested_time = dt.datetime(2020,8,12,12) #enter your requested time here
-            
+
             #Get all active mission ID(s), if any, for this time
             mission_ids = storm.recon.find_mission(requested_time)
-            
+
             #Get Mission object for each mission
             for mission_id in mission_ids:
                 mission = storm.recon.get_mission(mission_id)
                 print(mission)
         """
-        
-        #Return all missions if time is None
+
+        # Return all missions if time is None
         if time is None:
             if distance is None:
                 data = self.hdobs.data
             else:
                 data = self.hdobs.sel(distance=distance).data
             missions = np.unique(data['mission'].values)
             return list(missions)
-        
-        #Filter temporally
-        if isinstance(time,list):
-            t1=min(time)
-            t2=max(time)
+
+        # Filter temporally
+        if isinstance(time, list):
+            t1 = min(time)
+            t2 = max(time)
         else:
             t1 = t2 = time
-        
-        #Filter spatially
+
+        # Filter spatially
         if distance is None:
             data = self.hdobs.data
         else:
             data = self.hdobs.sel(distance=distance).data
 
-        #Find and return missions
+        # Find and return missions
         selected = []
         mission_groups = data.groupby('mission')
         for g in mission_groups:
-            t_start,t_end = (min(g[1]['time']),max(g[1]['time']))
-            if t_start<=t1<=t_end or t_start<=t2<=t_end or t1<t_start<t2:
+            t_start, t_end = (min(g[1]['time']), max(g[1]['time']))
+            if t_start <= t1 <= t_end or t_start <= t2 <= t_end or t1 < t_start < t2:
                 selected.append(g[0])
         return selected
 
-    def plot_summary(self,mission=None,save_path=None):
-        
+    def plot_summary(self, mission=None, save_path=None):
         r"""
         Plot summary map of all recon data.
-        
+
         Parameters
         ----------
         mission : str, optional
             String with mission name. Will plot summary for the specified mission, otherwise plots for all missions (default).
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot.
-        
+
         Notes
         -----
         HDOB data needs to be read into the recon object to use this function. To do so, use the ``ReconDataset.get_hdobs()`` function.
         """
-        
-        #Error check
+
+        # Error check
         if 'hdobs' not in self.__dict__.keys():
-            raise RuntimeError("hdobs needs to be read into the 'recon' object first. Use the 'ReconDataset.get_hdobs()' method to read in HDOBs data.")
-        
-        prop = {'hdobs':{'ms':5,'marker':'o'},\
-                'dropsondes':{'ms':25,'marker':'v'},\
-                'vdms':{'ms':100,'marker':'s'}}
-        
+            raise RuntimeError(
+                "hdobs needs to be read into the 'recon' object first. Use the 'ReconDataset.get_hdobs()' method to read in HDOBs data.")
+
+        prop = {
+            'hdobs': {'ms': 5, 'marker': 'o'},
+            'dropsondes': {'ms': 25, 'marker': 'v'},
+            'vdms': {'ms': 100, 'marker': 's'}
+        }
+
         hdobs = self.hdobs.sel(mission=mission)
-        ax = hdobs.plot_points('pkwnd',prop={'cmap':{1:'firebrick',2:'tomato',4:'gold',6:'lemonchiffon'},'levels':(0,200),'ms':2})
-        
+        ax = hdobs.plot_points('pkwnd', prop={'cmap': {
+                               1: 'firebrick', 2: 'tomato', 4: 'gold', 6: 'lemonchiffon'}, 'levels': (0, 200), 'ms': 2})
+
         if 'dropsondes' in self.__dict__.keys():
             dropsondes = self.dropsondes.sel(mission=mission)
             drop_lons = []
             drop_lats = []
             for drop in dropsondes.data:
                 if np.isnan(drop['TOPlon']):
                     drop_lons.append(drop['lon'])
                 else:
                     drop_lons.append(drop['TOPlon'])
                 if np.isnan(drop['TOPlat']):
                     drop_lats.append(drop['lat'])
                 else:
                     drop_lats.append(drop['TOPlat'])
-            ax.scatter(drop_lons,drop_lats,s=50,marker='v',edgecolor='w',linewidth=0.5,color='darkblue',transform=ccrs.PlateCarree())
-        
+            ax.scatter(drop_lons, drop_lats, s=50, marker='v', edgecolor='w',
+                       linewidth=0.5, color='darkblue', transform=ccrs.PlateCarree())
+
         if 'vdms' in self.__dict__.keys():
             vdms = self.vdms.sel(mission=mission)
-            ax.scatter(*zip(*[(d['lon'],d['lat']) for d in vdms.data]),s=80,marker='H',edgecolor='w',linewidth=1,color='k',transform=ccrs.PlateCarree())
-        
+            ax.scatter(*zip(*[(d['lon'], d['lat']) for d in vdms.data]), s=80, marker='H',
+                       edgecolor='w', linewidth=1, color='k', transform=ccrs.PlateCarree())
+
         title_left = ax.get_title(loc='left').split('\n')
-        newtitle = title_left[0]+'\nRecon summary'+['',f' for mission {mission}'][mission is not None]
-        ax.set_title(newtitle,fontsize=17,fontweight='bold',loc='left')
-        
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight')
-        
+        newtitle = title_left[0] + '\nRecon summary' + \
+            ['', f' for mission {mission}'][mission is not None]
+        ax.set_title(newtitle, fontsize=17, fontweight='bold', loc='left')
+
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
         return ax
-    
+
 
 class hdobs:
 
     r"""
     Creates an instance of an HDOBs object containing all recon High Density Observations (HDOBs) for a single storm.
-    
+
     Parameters
     ----------
     storm : tropycal.tracks.Storm
         Requested storm.
     data : str, optional
         Filepath of pickle file containing HDOBs data retrieved from ``hdobs.to_pickle()``. If provided, data will be retrieved from the local pickle file instead of the NHC server.
     update : bool
         If True, search for new data, following existing data in the dropsonde object, and concatenate. Default is False.
-        
+
     Returns
     -------
     Dataset
         An instance of HDOBs, initialized with a dataframe of HDOB.
-    
+
     Notes
     -----
     .. warning::
-    
+
         Recon data is currently only available from 1989 onwards.
-    
+
     There are two recommended ways of retrieving an hdob object. Since the ``ReconDataset``, ``hdobs``, ``dropsondes`` and ``vdms`` classes are **storm-centric**, a Storm object is required for both methods.
-    
+
     .. code-block:: python
-    
+
         #Retrieve Hurricane Michael (2018) from TrackDataset
         basin = tracks.TrackDataset()
         storm = basin.get_storm(('michael',2018))
-    
+
     The first method is to use the empty instance of ReconDataset already initialized in the Storm object, which has a ``get_hdobs()`` method thus allowing all of the hdobs attributes and methods to be accessed from the Storm object. As a result, a Storm object does not need to be provided as an argument.
-    
+
     .. code-block:: python
-    
+
         #Retrieve all HDOBs for this storm
         storm.recon.get_hdobs()
-        
+
         #Retrieve the raw HDOBs data
         storm.recon.hdobs.data
-        
+
         #Use the plot_points() method of hdobs
         storm.recon.hdobs.plot_points()
-    
+
     The second method is to use the hdobs class independently of the other recon classes:
-    
+
     .. code-block:: python
-    
+
         from tropycal.recon import hdobs
-        
+
         #Retrieve all HDOBs for this storm, passing the Storm object as an argument
         hdobs_obj = hdobs(storm)
-        
+
         #Retrieve the raw HDOBs data
         hdobs_obj.data
-        
+
         #Use the plot_points() method of hdobs
         hdobs_obj.plot_points()
     """
 
     def __repr__(self):
-         
+
         summary = ["<tropycal.recon.hdobs>"]
-        
-        #Find maximum wind and minimum pressure
+
+        # Find maximum wind and minimum pressure
         max_wspd = np.nanmax(self.data['wspd'])
         max_pkwnd = np.nanmax(self.data['pkwnd'])
         max_sfmr = np.nanmax(self.data['sfmr'])
         min_psfc = np.nanmin(self.data['p_sfc'])
-        time_range = [pd.to_datetime(t) for t in (np.nanmin(self.data['time']),np.nanmax(self.data['time']))]
+        time_range = [pd.to_datetime(t) for t in (
+            np.nanmin(self.data['time']), np.nanmax(self.data['time']))]
 
-        #Add general summary
+        # Add general summary
         emdash = '\u2014'
-        summary_keys = {'Storm':f'{self.storm.name} {self.storm.year}',\
-                        'Missions':len(set(self.data['mission'])),
-                        'Time range':f"{time_range[0]:%b-%d %H:%M} {emdash} {time_range[1]:%b-%d %H:%M}",
-                        'Max 30sec flight level wind':f"{max_wspd} knots",
-                        'Max 10sec flight level wind':f"{max_pkwnd} knots",
-                        'Max SFMR wind':f"{max_sfmr} knots",
-                        'Min surface pressure':f"{min_psfc} hPa",
-                        'Source':self.source}
+        summary_keys = {
+            'Storm': f'{self.storm.name} {self.storm.year}',
+            'Missions': len(set(self.data['mission'])),
+            'Time range': f"{time_range[0]:%b-%d %H:%M} {emdash} {time_range[1]:%b-%d %H:%M}",
+            'Max 30sec flight level wind': f"{max_wspd} knots",
+            'Max 10sec flight level wind': f"{max_pkwnd} knots",
+            'Max SFMR wind': f"{max_sfmr} knots",
+            'Min surface pressure': f"{min_psfc} hPa",
+            'Source': self.source
+        }
 
-        #Add dataset summary
+        # Add dataset summary
         summary.append("Dataset Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
         return "\n".join(summary)
-    
+
     def __init__(self, storm, data=None, update=False):
 
         self.storm = storm
         self.data = None
         self.format = 1
         self.source = 'National Hurricane Center (NHC)'
-        
-        #Get URL based on storm year
+
+        # Get URL based on storm year
         if storm.year >= 2012:
             self.format = 1
             if storm.basin == 'north_atlantic':
-                archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/AHONT1/']
+                archive_url = [
+                    f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/AHONT1/']
             else:
-                archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/AHOPN1/']
+                archive_url = [
+                    f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/AHOPN1/']
         elif storm.year <= 2011 and storm.year >= 2008:
             self.format = 2
             if storm.basin == 'north_atlantic':
                 archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/NOAA/URNT15/',
                                f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/USAF/URNT15/']
             else:
                 archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/NOAA/URPN15/',
                                f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/USAF/URPN15/']
         elif storm.year == 2007:
             self.format = 3
             archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/NOAA/',
                            f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/USAF/']
         elif storm.year == 2006:
             self.format = 4
-            archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/']
+            archive_url = [
+                f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/HDOB/']
         elif storm.year >= 2002:
             self.format = 5
-            archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/{self.storm.name.upper()}/']
+            archive_url = [
+                f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/{self.storm.name.upper()}/']
         elif storm.year <= 2001 and storm.year >= 1989:
             self.format = 6
-            archive_url = [f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/{self.storm.name.lower()}/']
+            archive_url = [
+                f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/{self.storm.name.lower()}/']
         else:
             raise RuntimeError("Recon data is not available prior to 1989.")
 
-        if isinstance(data,str):
+        if isinstance(data, str):
             with open(data, 'rb') as f:
                 self.data = pickle.load(f)
         elif data is not None:
             self.data = data
-        
+
         if data is None or update:
             try:
                 start_time = max(self.data['time'])
             except:
-                start_time = min(self.storm.dict['date'])-timedelta(hours=12)
-            end_time = max(self.storm.dict['date'])+timedelta(hours=12)
+                start_time = min(self.storm.dict['time']) - timedelta(hours=12)
+            end_time = max(self.storm.dict['time']) + timedelta(hours=12)
 
-            timestr = [f'{start_time:%Y%m%d}']+\
-                      [f'{t:%Y%m%d}' for t in self.storm.dict['date'] if t>start_time]+\
+            timestr = [f'{start_time:%Y%m%d}'] +\
+                      [f'{t:%Y%m%d}' for t in self.storm.dict['time'] if t > start_time] +\
                       [f'{end_time:%Y%m%d}']
 
-            #Retrieve list of files in URL(s) and filter by storm dates
-            if self.format in [1,3,4]:
+            # Retrieve list of files in URL(s) and filter by storm dates
+            if self.format in [1, 3, 4]:
                 page = requests.get(archive_url[0]).text
                 content = page.split("\n")
                 files = []
                 for line in content:
-                    if ".txt" in line: files.append(((line.split('txt">')[1]).split("</a>")[0]).split("."))
+                    if ".txt" in line:
+                        files.append(
+                            ((line.split('txt">')[1]).split("</a>")[0]).split("."))
                 del content
-                files = sorted([i for i in files if i[1][:8] in timestr],key=lambda x: x[1])
-                linksub = [archive_url[0]+'.'.join(l) for l in files]
+                files = sorted([i for i in files if i[1][:8]
+                               in timestr], key=lambda x: x[1])
+                linksub = [archive_url[0] + '.'.join(l) for l in files]
             elif self.format == 2:
                 linksub = []
                 for url in archive_url:
                     files = []
                     page = requests.get(url).text
                     content = page.split("\n")
                     for line in content:
-                        if ".txt" in line: files.append(((line.split('txt">')[1]).split("</a>")[0]).split("."))
+                        if ".txt" in line:
+                            files.append(
+                                ((line.split('txt">')[1]).split("</a>")[0]).split("."))
                     del content
-                    linksub += sorted([url+'.'.join(i) for i in files if i[1][:8] in timestr],key=lambda x: x[1])
+                    linksub += sorted([url + '.'.join(i)
+                                      for i in files if i[1][:8] in timestr], key=lambda x: x[1])
                 linksub = sorted(linksub)
             elif self.format == 5:
                 page = requests.get(archive_url[0]).text
                 content = page.split("\n")
                 files = []
                 for line in content:
-                    if ".txt" in line and 'HDOBS' in line: files.append(((line.split('txt">')[1]).split("</a>")[0]))
+                    if ".txt" in line and 'HDOBS' in line:
+                        files.append(
+                            ((line.split('txt">')[1]).split("</a>")[0]))
                 del content
-                linksub = [archive_url[0]+l for l in files]
+                linksub = [archive_url[0] + l for l in files]
             elif self.format == 6:
                 page = requests.get(archive_url[0]).text
                 content = page.split("\n")
                 files = []
                 for line in content:
-                    if ".txt" in line: files.append(((line.split('txt">')[1]).split("</a>")[0]))
+                    if ".txt" in line:
+                        files.append(
+                            ((line.split('txt">')[1]).split("</a>")[0]))
                 del content
-                linksub = [archive_url[0]+l for l in files if l[0] in ['H','h','M','m']]
+                linksub = [archive_url[0] + l for l in files if l[0]
+                           in ['H', 'h', 'M', 'm']]
 
-            #Initiate urllib3
+            # Initiate urllib3
             urllib3.disable_warnings()
             http = urllib3.PoolManager()
-            
-            #Read through all files
+
+            # Read through all files
             timer_start = dt.now()
-            print(f'Searching through recon HDOB files between {timestr[0]} and {timestr[-1]} ...')
-            filecount,unreadable = 0,0
+            print(
+                f'Searching through recon HDOB files between {timestr[0]} and {timestr[-1]} ...')
+            filecount, unreadable = 0, 0
             found = False
             for link in linksub:
-                
-                #Read URL
-                response = http.request('GET',link)
+
+                # Read URL
+                response = http.request('GET', link)
                 content = response.data.decode('utf-8')
-                
-                #Find mission name line
+
+                # Find mission name line
                 row = 3 if self.format <= 4 else 0
-                while len(content.split("\n")[row]) < 3 or content.split("\n")[row][:3] in ["SXX","URN","URP","YYX"]:
+                while len(content.split("\n")[row]) < 3 or content.split("\n")[row][:3] in ["SXX", "URN", "URP", "YYX"]:
                     row += 1
-                    if row >= 100: break
-                if row >= 100: continue
+                    if row >= 100:
+                        break
+                if row >= 100:
+                    continue
                 missionname = [i.split() for i in content.split('\n')][row][1]
-                
-                #Check for mission name to storm match by format
+
+                # Check for mission name to storm match by format
                 if self.format != 6:
-                    check = missionname[2:5] == self.storm.operational_id[2:4]+self.storm.operational_id[0]
+                    check = missionname[2:5] == self.storm.operational_id[2:4] + \
+                        self.storm.operational_id[0]
                 else:
                     check = True
-                
-                #Read HDOBs if this file matches the requested storm
-                if check == True:
+
+                # Read HDOBs if this file matches the requested storm
+                if check:
                     filecount += 1
 
                     try:
 
-                        #Decode HDOBs by format
+                        # Decode HDOBs by format
                         if self.format <= 3:
-                            iter_hdob = decode_hdob(content,mission_row=row)
+                            iter_hdob = decode_hdob(content, mission_row=row)
                         elif self.format == 4:
                             strdate = (link.split('.')[-2])[:8]
-                            iter_hdob = decode_hdob_2006(content,strdate,mission_row=row)
+                            iter_hdob = decode_hdob_2006(
+                                content, strdate, mission_row=row)
                         elif self.format == 6:
-                            #Check for date
-                            day = int(content.split("\n")[row-1].split()[2][:2])
-                            for iter_date in storm.dict['date']:
+                            # Check for date
+                            day = int(content.split("\n")[
+                                      row - 1].split()[2][:2])
+                            for iter_date in storm.dict['time']:
                                 found_date = False
                                 if iter_date.day == day:
-                                    date = dt(iter_date.year,iter_date.month,iter_date.day)
+                                    date = dt(iter_date.year,
+                                              iter_date.month, iter_date.day)
                                     strdate = date.strftime('%Y%m%d')
                                     found_date = True
                                     break
-                            if found_date == False: continue
-                            iter_hdob = decode_hdob_2006(content,strdate,mission_row=row)
+                            if not found_date:
+                                continue
+                            iter_hdob = decode_hdob_2006(
+                                content, strdate, mission_row=row)
                         elif self.format == 5:
-                            #Split content by 10/20 minute blocks
+                            # Split content by 10/20 minute blocks
                             strdate = (link.split('.')[-3]).split("_")[-1]
                             content_split = content.split("NNNN")
                             iter_hdob = None
                             for iter_content in content_split:
                                 iter_split = iter_content.split("\n")
-                                if len(iter_split) < 10: continue
+                                if len(iter_split) < 10:
+                                    continue
 
-                                #Search for starting line of data within sub-block
+                                # Search for starting line of data within sub-block
                                 found = False
                                 for line in iter_split:
-                                    if missionname in line: found = True
+                                    if missionname in line:
+                                        found = True
                                 temp_row = 0
-                                while len(iter_split[temp_row]) < 3 or iter_split[temp_row][:3] in ["SXX","URN","URP"]:
+                                while len(iter_split[temp_row]) < 3 or iter_split[temp_row][:3] in ["SXX", "URN", "URP"]:
                                     temp_row += 1
-                                    if temp_row >= 100: break
-                                if temp_row >= 100: break
+                                    if temp_row >= 100:
+                                        break
+                                if temp_row >= 100:
+                                    break
 
-                                #Parse data by format
+                                # Parse data by format
                                 if 'NOAA' in link:
-                                    iter_hdob_loop = decode_hdob_2005_noaa(iter_content,strdate,temp_row)
+                                    iter_hdob_loop = decode_hdob_2005_noaa(
+                                        iter_content, strdate, temp_row)
                                 else:
-                                    iter_hdob_loop = decode_hdob_2006(iter_content,strdate,temp_row)
+                                    iter_hdob_loop = decode_hdob_2006(
+                                        iter_content, strdate, temp_row)
 
-                                #Append HDOBs to full data
+                                # Append HDOBs to full data
                                 if iter_hdob is None:
                                     iter_hdob = copy.copy(iter_hdob_loop)
                                 elif max(iter_hdob_loop['time']) > start_time:
-                                    iter_hdob = pd.concat([iter_hdob,iter_hdob_loop])
+                                    iter_hdob = pd.concat(
+                                        [iter_hdob, iter_hdob_loop])
                                 else:
                                     pass
 
-                        #Append HDOBs to full data
+                        # Append HDOBs to full data
                         if self.data is None:
                             self.data = copy.copy(iter_hdob)
                         elif max(iter_hdob['time']) > start_time:
-                            self.data = pd.concat([self.data,iter_hdob])
+                            self.data = pd.concat([self.data, iter_hdob])
                         else:
                             pass
 
                     except:
                         unreadable += 1
 
-            print(f'--> Completed reading in recon HDOB files ({(dt.now()-timer_start).total_seconds():.1f} seconds)'+\
-                  f'\nRead {filecount} files'+\
+            print(f'--> Completed reading in recon HDOB files ({(dt.now()-timer_start).total_seconds():.1f} seconds)' +
+                  f'\nRead {filecount} files' +
                   f'\nUnable to decode {unreadable} files')
-        
-        #This code will crash if no HDOBs are available
+
+        # This code will crash if no HDOBs are available
         try:
-            
-            #Sort data by time
-            self.data.sort_values(['time'],inplace=True)
-            
-            #Recenter
+
+            # Sort data by time
+            self.data.sort_values(['time'], inplace=True)
+
+            # Recenter
             self._recenter()
             self.keys = list(self.data.keys())
-            
+
         except:
             self.keys = []
 
     def update(self):
         r"""
         Update with the latest data for an ongoing storm.
-        
+
         Notes
         -----
         This function has no return value, but simply updates the internal HDOB data with new observations since the object was created.
         """
-        
-        self = self.__init__(storm=self.storm,data=self.data,update=True)
-        
-    def _find_centers(self,data=None):
-        
+
+        self = self.__init__(storm=self.storm, data=self.data, update=True)
+
+    def _find_centers(self, data=None):
+
         if data is None:
             data = self.data
-        data = data.sort_values(['mission','time'])
-                
+        data = data.sort_values(['mission', 'time'])
+
         def fill_nan(A):
-            #Interpolate to fill nan values
+            # Interpolate to fill nan values
             A = np.array(A)
             inds = np.arange(len(A))
             good = np.where(np.isfinite(A))
-            good_grad = np.interp(inds,good[0],np.gradient(good[0]))
-            if len(good[0])>=3:
-                f = interp1d(inds[good], A[good],bounds_error=False,kind='quadratic')
-                B = np.where((np.isfinite(A)[good[0][0]:good[0][-1]+1]) | (good_grad[good[0][0]:good[0][-1]+1]>3),
-                             A[good[0][0]:good[0][-1]+1],
-                             f(inds[good[0][0]:good[0][-1]+1]))
-                return [np.nan]*good[0][0]+list(B)+[np.nan]*(inds[-1]-good[0][-1])
+            good_grad = np.interp(inds, good[0], np.gradient(good[0]))
+            if len(good[0]) >= 3:
+                f = interp1d(inds[good], A[good],
+                             bounds_error=False, kind='quadratic')
+                B = np.where((np.isfinite(A)[good[0][0]:good[0][-1] + 1]) | (good_grad[good[0][0]:good[0][-1] + 1] > 3),
+                             A[good[0][0]:good[0][-1] + 1],
+                             f(inds[good[0][0]:good[0][-1] + 1]))
+                return [np.nan] * good[0][0] + list(B) + [np.nan] * (inds[-1] - good[0][-1])
             else:
-                return [np.nan]*len(A)
+                return [np.nan] * len(A)
 
         missiondata = data.groupby('mission')
         dfs = []
         for group in missiondata:
             mdata = group[1]
-            #Check that sfc pressure spread is big enough to identify real minima
-            if np.nanpercentile(mdata['p_sfc'],95)-np.nanpercentile(mdata['p_sfc'],5)>8:
-                p_sfc_interp = fill_nan(mdata['p_sfc']) #Interp p_sfc across missing data
-                wspd_interp = fill_nan(mdata['wspd']) #Interp wspd across missing data
-                #Smooth p_sfc and wspd
-                p_sfc_smooth = [np.nan]*1+list(np.convolve(p_sfc_interp,[1/3]*3,mode='valid'))+[np.nan]*1
-                wspd_smooth = [np.nan]*1+list(np.convolve(wspd_interp,[1/3]*3,mode='valid'))+[np.nan]*1
-                #Add wspd to p_sfc to encourage finding p mins with wspd mins 
-                #and prevent finding p mins in intense thunderstorms
-                pw_test = np.array(p_sfc_smooth)+np.array(wspd_smooth)*.1
-                #Find mins in 20-minute windows
-                imin = np.nonzero(pw_test == minimum_filter(pw_test,40))[0]
-                #Only use mins if below 10th %ile of mission p_sfc data and when plane p is 550-950mb
-                #and not in takeoff and landing time windows
+            # Check that sfc pressure spread is big enough to identify real minima
+            if np.nanpercentile(mdata['p_sfc'], 95) - np.nanpercentile(mdata['p_sfc'], 5) > 8:
+                # Interp p_sfc across missing data
+                p_sfc_interp = fill_nan(mdata['p_sfc'])
+                # Interp wspd across missing data
+                wspd_interp = fill_nan(mdata['wspd'])
+                # Smooth p_sfc and wspd
+                p_sfc_smooth = [
+                    np.nan] * 1 + list(np.convolve(p_sfc_interp, [1 / 3] * 3, mode='valid')) + [np.nan] * 1
+                wspd_smooth = [
+                    np.nan] * 1 + list(np.convolve(wspd_interp, [1 / 3] * 3, mode='valid')) + [np.nan] * 1
+                # Add wspd to p_sfc to encourage finding p mins with wspd mins
+                # and prevent finding p mins in intense thunderstorms
+                pw_test = np.array(p_sfc_smooth) + np.array(wspd_smooth) * .1
+                # Find mins in 20-minute windows
+                imin = np.nonzero(pw_test == minimum_filter(pw_test, 40))[0]
+                # Only use mins if below 10th %ile of mission p_sfc data and when plane p is 550-950mb
+                # and not in takeoff and landing time windows
                 plane_p = fill_nan(mdata['plane_p'])
-                imin = [i for i in imin if 800<p_sfc_interp[i]<np.nanpercentile(mdata['p_sfc'],10) and \
-                        550<plane_p[i]<950 and i>60 and i<len(mdata)-60]
+                imin = [i for i in imin if 800 < p_sfc_interp[i] < np.nanpercentile(mdata['p_sfc'], 10) and
+                        550 < plane_p[i] < 950 and i > 60 and i < len(mdata) - 60]
             else:
-                imin=[]
-            mdata['iscenter'] = np.array([1 if i in imin else 0 for i in range(len(mdata))])
+                imin = []
+            mdata['iscenter'] = np.array(
+                [1 if i in imin else 0 for i in range(len(mdata))])
             dfs.append(mdata)
 
         data = pd.concat(dfs)
         numcenters = sum(data['iscenter'])
         print(f'Found {numcenters} center passes')
         return data
-        
-    def _recenter(self): 
+
+    def _recenter(self):
         data = copy.copy(self.data)
-        #Interpolate center position to time of each ob
-        interp_clon,interp_clat = self.storm.recon.get_track(data['time'])
+        # Interpolate center position to time of each ob
+        interp_clon, interp_clat = self.storm.recon.get_track(data['time'])
 
-        #Get x,y distance of each ob from coinciding interped center position
-        data['xdist'] = [great_circle( (interp_clat[i],interp_clon[i]), \
-            (interp_clat[i],data['lon'].values[i]) ).kilometers* \
-            [1,-1][int(data['lon'].values[i] < interp_clon[i])] for i in range(len(data))]
-        data['ydist'] = [great_circle( (interp_clat[i],interp_clon[i]), \
-            (data['lat'].values[i],interp_clon[i]) ).kilometers* \
-            [1,-1][int(data['lat'].values[i] < interp_clat[i])] for i in range(len(data))]
-        data['distance'] = [(i**2+j**2)**.5 for i,j in zip(data['xdist'],data['ydist'])]
-        
-        imin = np.nonzero(data['distance'].values == minimum_filter(data['distance'].values,40))[0]
-        data['iscenter'] = np.array([1 if i in imin and data['distance'].values[i]<10 else 0 for i in range(len(data))])        
+        # Get x,y distance of each ob from coinciding interped center position
+        data['xdist'] = [great_circle((interp_clat[i], interp_clon[i]),
+                                      (interp_clat[i], data['lon'].values[i])).kilometers *
+                         [1, -1][int(data['lon'].values[i] < interp_clon[i])] for i in range(len(data))]
+        data['ydist'] = [great_circle((interp_clat[i], interp_clon[i]),
+                                      (data['lat'].values[i], interp_clon[i])).kilometers *
+                         [1, -1][int(data['lat'].values[i] < interp_clat[i])] for i in range(len(data))]
+        data['distance'] = [(i**2 + j**2)**.5 for i,
+                            j in zip(data['xdist'], data['ydist'])]
+
+        imin = np.nonzero(data['distance'].values ==
+                          minimum_filter(data['distance'].values, 40))[0]
+        data['iscenter'] = np.array(
+            [1 if i in imin and data['distance'].values[i] < 10 else 0 for i in range(len(data))])
 
-        #print('Completed hdob center-relative coordinates')
+        # print('Completed hdob center-relative coordinates')
         self.data = data
-    
-    def sel(self,mission=None,time=None,domain=None,plane_p=None,plane_z=None,p_sfc=None,\
-            temp=None,dwpt=None,wdir=None,wspd=None,pkwnd=None,sfmr=None,noflag=None,\
-            iscenter=None,distance=None):
+
+    def sel(self, mission=None, time=None, domain=None, plane_p=None, plane_z=None, p_sfc=None,
+            temp=None, dwpt=None, wdir=None, wspd=None, pkwnd=None, sfmr=None, noflag=None,
+            iscenter=None, distance=None):
         r"""
         Select a subset of HDOBs by any of its parameters and return a new hdobs object.
-        
+
         Parameters
         ----------
         mission : str
             Mission name (number + storm id), e.g. mission 7 for AL05 is '0705L'
         time : list/tuple of datetimes
             list/tuple of start time and end time datetime objects.
             Default is None, which returns all points
@@ -914,127 +966,139 @@
         -------
         hdobs object
             A new hdobs object that satisfies the intersection of all subsetting.
         """
 
         NEW_DATA = copy.copy(self.data)
 
-        #Apply mission filter
+        # Apply mission filter
         if mission is not None:
             mission = str(mission)
-            NEW_DATA = NEW_DATA.loc[NEW_DATA['mission']==mission]
+            NEW_DATA = NEW_DATA.loc[NEW_DATA['mission'] == mission]
 
-        #Apply time filter
+        # Apply time filter
         if time is not None:
-            bounds = get_bounds(NEW_DATA['time'],time)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['time']>bounds[0]) & (NEW_DATA['time']<bounds[1])]
-        
-        #Apply domain filter
+            bounds = get_bounds(NEW_DATA['time'], time)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['time'] > bounds[0]) & (
+                NEW_DATA['time'] < bounds[1])]
+
+        # Apply domain filter
         if domain is not None:
-            tmp = {k[0].lower():v for k,v in domain.items()}
-            domain = {'n':90,'s':-90,'e':359.99,'w':0}
+            tmp = {k[0].lower(): v for k, v in domain.items()}
+            domain = {'n': 90, 's': -90, 'e': 359.99, 'w': 0}
             domain.update(tmp)
-            bounds = get_bounds(NEW_DATA['lon']%360,(domain['w']%360,domain['e']%360))
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lon']%360>=bounds[0]) & (NEW_DATA['lon']%360<=bounds[1])]
-            bounds = get_bounds(NEW_DATA['lat'],(domain['s'],domain['n']))
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lat']>=bounds[0]) & (NEW_DATA['lat']<=bounds[1])]
-        
-        #Apply flight pressure filter
+            bounds = get_bounds(
+                NEW_DATA['lon'] % 360, (domain['w'] % 360, domain['e'] % 360))
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lon'] % 360 >= bounds[0]) & (
+                NEW_DATA['lon'] % 360 <= bounds[1])]
+            bounds = get_bounds(NEW_DATA['lat'], (domain['s'], domain['n']))
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lat'] >= bounds[0]) & (
+                NEW_DATA['lat'] <= bounds[1])]
+
+        # Apply flight pressure filter
         if plane_p is not None:
-            bounds = get_bounds(NEW_DATA['plane_p'],plane_p)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['plane_p']>bounds[0]) & (NEW_DATA['plane_p']<bounds[1])]
-            
-        #Apply flight height filter
+            bounds = get_bounds(NEW_DATA['plane_p'], plane_p)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['plane_p'] > bounds[0]) & (
+                NEW_DATA['plane_p'] < bounds[1])]
+
+        # Apply flight height filter
         if plane_z is not None:
-            bounds = get_bounds(NEW_DATA['plane_z'],plane_z)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['plane_z']>bounds[0]) & (NEW_DATA['plane_z']<bounds[1])]
-        
-        #Apply surface pressure filter
+            bounds = get_bounds(NEW_DATA['plane_z'], plane_z)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['plane_z'] > bounds[0]) & (
+                NEW_DATA['plane_z'] < bounds[1])]
+
+        # Apply surface pressure filter
         if p_sfc is not None:
-            bounds = get_bounds(NEW_DATA['p_sfc'],p_sfc)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['p_sfc']>bounds[0]) & (NEW_DATA['p_sfc']<bounds[1])]
-            
-        #Apply temperature filter
+            bounds = get_bounds(NEW_DATA['p_sfc'], p_sfc)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['p_sfc'] > bounds[0]) & (
+                NEW_DATA['p_sfc'] < bounds[1])]
+
+        # Apply temperature filter
         if temp is not None:
-            bounds = get_bounds(NEW_DATA['temp'],temp)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['temp']>bounds[0]) & (NEW_DATA['temp']<bounds[1])]
-        
-        #Apply dew point filter
+            bounds = get_bounds(NEW_DATA['temp'], temp)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['temp'] > bounds[0]) & (
+                NEW_DATA['temp'] < bounds[1])]
+
+        # Apply dew point filter
         if dwpt is not None:
-            bounds = get_bounds(NEW_DATA['dwpt'],dwpt)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['dwpt']>bounds[0]) & (NEW_DATA['dwpt']<bounds[1])]
-            
-        #Apply wind direction filter
+            bounds = get_bounds(NEW_DATA['dwpt'], dwpt)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['dwpt'] > bounds[0]) & (
+                NEW_DATA['dwpt'] < bounds[1])]
+
+        # Apply wind direction filter
         if wdir is not None:
-            bounds = get_bounds(NEW_DATA['wdir'],wdir)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['wdir']>bounds[0]) & (NEW_DATA['wdir']<bounds[1])]
-            
-        #Apply wind speed filter
+            bounds = get_bounds(NEW_DATA['wdir'], wdir)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['wdir'] > bounds[0]) & (
+                NEW_DATA['wdir'] < bounds[1])]
+
+        # Apply wind speed filter
         if wspd is not None:
-            bounds = get_bounds(NEW_DATA['wspd'],wspd)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['wspd']>bounds[0]) & (NEW_DATA['wspd']<bounds[1])]
-            
-        #Apply peak wind filter
+            bounds = get_bounds(NEW_DATA['wspd'], wspd)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['wspd'] > bounds[0]) & (
+                NEW_DATA['wspd'] < bounds[1])]
+
+        # Apply peak wind filter
         if pkwnd is not None:
-            bounds = get_bounds(NEW_DATA['pkwnd'],pkwnd)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['pkwnd']>bounds[0]) & (NEW_DATA['pkwnd']<bounds[1])]
-            
-        #Apply sfmr filter
+            bounds = get_bounds(NEW_DATA['pkwnd'], pkwnd)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['pkwnd'] > bounds[0]) & (
+                NEW_DATA['pkwnd'] < bounds[1])]
+
+        # Apply sfmr filter
         if sfmr is not None:
-            bounds = get_bounds(NEW_DATA['sfmr'],sfmr)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['sfmr']>bounds[0]) & (NEW_DATA['sfmr']<bounds[1])]
-        
-        #Apply iscenter filter
+            bounds = get_bounds(NEW_DATA['sfmr'], sfmr)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['sfmr'] > bounds[0]) & (
+                NEW_DATA['sfmr'] < bounds[1])]
+
+        # Apply iscenter filter
         if iscenter is not None:
-            NEW_DATA = NEW_DATA.loc[NEW_DATA['iscenter']==iscenter]
-        
-        #Apply distance filter
+            NEW_DATA = NEW_DATA.loc[NEW_DATA['iscenter'] == iscenter]
+
+        # Apply distance filter
         if distance is not None:
-            NEW_DATA = NEW_DATA.loc[NEW_DATA['distance']<distance]
-        
-        NEW_OBJ = hdobs(storm=self.storm,data=NEW_DATA)
-        
+            NEW_DATA = NEW_DATA.loc[NEW_DATA['distance'] < distance]
+
+        NEW_OBJ = hdobs(storm=self.storm, data=NEW_DATA)
+
         return NEW_OBJ
-        
-    def to_pickle(self,filename):
+
+    def to_pickle(self, filename):
         r"""
         Save HDOB data (Pandas dataframe) to a pickle file.
-        
+
         Parameters
         ----------
         filename : str
             name of file to save pickle file to.
-        
+
         Notes
         -----
         This method saves the HDOBs data as a pickle within the current working directory, given a filename as an argument.
-        
+
         For example, assume ``hdobs`` was retrieved from a Storm object (using the first method described in the ``hdobs`` class documentation). The HDOBs data would be saved to a pickle file as follows:
-        
+
         >>> storm.recon.hdobs.to_pickle("mystorm_hdobs.pickle")
-        
+
         Now the HDOBs data is saved locally, and next time recon data for this storm needs to be analyzed, this allows to bypass re-reading the HDOBs data from the NHC server by providing the pickle file as an argument:
-        
+
         >>> storm.recon.get_hdobs("mystorm_hdobs.pickle")
-        
+
         """
-        
-        with open(filename,'wb') as f:
-            pickle.dump(self.data,f)
-    
-    def plot_time_series(self,varname=('p_sfc','wspd'),mission=None,time=None,realtime=False,**kwargs):
-        
+
+        with open(filename, 'wb') as f:
+            pickle.dump(self.data, f)
+
+    def plot_time_series(self, varname=('p_sfc', 'wspd'), mission=None, time=None, realtime=False, **kwargs):
         r"""
         Plots a time series of one or two variables on an axis.
-        
+
         Parameters
         ----------
         varname : str or tuple
             If one variable to plot, varname is a string of the variable name. If two variables to plot, varname is a tuple of the left and right variable names, respectively. Available varnames are:
-            
+
             * **p_sfc** - Mean Sea Level Pressure (hPa)
             * **temp** - Flight Level Temperature (C)
             * **dwpt** - Flight Level Dewpoint (C)
             * **wspd** - Flight Level Wind (kt)
             * **sfmr** - Surface Wind (kt)
             * **pkwnd** - Peak Wind Gust (kt)
             * **rain** - Rain Rate (mm/hr)
@@ -1042,27 +1106,27 @@
             * **plane_p** - Pressure (hPa)
         mission : int
             Mission number to plot. If None, all missions for this storm are plotted.
         time : tuple
             Tuple of start and end times (datetime.datetime) to plot. If None, all times available are plotted.
         realtime : bool
             If True, the most recent 2 hours of the mission will plot, overriding the time argument. Default is False.
-        
+
         Other Parameters
         ----------------
         left_prop : dict
             Dictionary of properties for the left line. Scroll down for more information.
         right_prop : dict
             Dictionary of properties for the right line. Scroll down for more information.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         The following properties are available for customizing the plot, via ``left_prop`` and ``right_prop``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
@@ -1072,349 +1136,389 @@
            * - ms
              - Marker size. If zero, none will be plotted. Default is zero.
            * - color
              - Color of lines (and markers if used). Default varies per varname.
            * - linewidth
              - Line width. Default is 1.0.
         """
-        
-        #Pop kwargs
-        left_prop = kwargs.pop('left_prop',{})
-        right_prop = kwargs.pop('right_prop',{})
-        
-        #Retrieve variables
+
+        # Pop kwargs
+        left_prop = kwargs.pop('left_prop', {})
+        right_prop = kwargs.pop('right_prop', {})
+
+        # Retrieve variables
         twin_ax = False
-        if isinstance(varname,tuple):
+        if isinstance(varname, tuple):
             varname_right = varname[1]
             varname = varname[0]
             twin_ax = True
             varname_right_info = time_series_plot(varname_right)
         varname_info = time_series_plot(varname)
-        
-        #Filter by mission
+
+        # Filter by mission
         def str2(number):
-            if number < 10: return f'0{number}'
+            if number < 10:
+                return f'0{number}'
             return str(number)
         if mission is not None:
             df = self.data.loc[self.data['mission'] == str2(mission)]
-            if len(df) == 0: raise ValueError("Mission number provided is invalid.")
+            if len(df) == 0:
+                raise ValueError("Mission number provided is invalid.")
         else:
             df = self.data
-        
-        #Filter by time or realtime flag
+
+        # Filter by time or realtime flag
         if realtime:
             end_time = pd.to_datetime(df['time'].values[-1])
-            df = df.loc[(df['time'] >= end_time-timedelta(hours=2)) & (df['time'] <= end_time)]
+            df = df.loc[(df['time'] >= end_time - timedelta(hours=2))
+                        & (df['time'] <= end_time)]
         elif time is not None:
             df = df.loc[(df['time'] >= time[0]) & (df['time'] <= time[1])]
-        if len(df) == 0: raise ValueError("Time range provided is invalid.")
-        
-        #Filter by default kwargs
-        left_prop_default = {'ms':0,'color':varname_info['color'],'linewidth':1}
-        for key in left_prop.keys(): left_prop_default[key] = left_prop[key]
+        if len(df) == 0:
+            raise ValueError("Time range provided is invalid.")
+
+        # Filter by default kwargs
+        left_prop_default = {
+            'ms': 0,
+            'color': varname_info['color'],
+            'linewidth': 1
+        }
+        for key in left_prop.keys():
+            left_prop_default[key] = left_prop[key]
         left_prop = left_prop_default
         if twin_ax:
-            right_prop_default = {'ms':0,'color':varname_right_info['color'],'linewidth':1}
-            for key in right_prop.keys(): right_prop_default[key] = right_prop[key]
+            right_prop_default = {
+                'ms': 0,
+                'color': varname_right_info['color'],
+                'linewidth': 1
+            }
+            for key in right_prop.keys():
+                right_prop_default[key] = right_prop[key]
             right_prop = right_prop_default
-        
-        #----------------------------------------------------------------------------------
-        
-        #Create figure
-        fig,ax = plt.subplots(figsize=(9,6),dpi=200)
+
+        # ----------------------------------------------------------------------------------
+
+        # Create figure
+        fig, ax = plt.subplots(figsize=(9, 6), dpi=200)
         if twin_ax:
             ax.grid(axis='x')
         else:
             ax.grid()
-        
-        #Plot line
-        line1 = ax.plot(df['time'],df[varname],color=left_prop['color'],linewidth=left_prop['linewidth'],label=varname_info['name'])
+
+        # Plot line
+        line1 = ax.plot(df['time'], df[varname], color=left_prop['color'],
+                        linewidth=left_prop['linewidth'], label=varname_info['name'])
         ax.set_ylabel(varname_info['full_name'])
-        
-        #Plot dots
+
+        # Plot dots
         if left_prop['ms'] >= 1:
             plot_times = df['time'].values
             plot_var = df[varname].values
-            plot_times = [plot_times[i] for i in range(len(plot_times)) if varname not in df['flag'].values[i]]
-            plot_var = [plot_var[i] for i in range(len(plot_var)) if varname not in df['flag'].values[i]]
-            ax.plot(plot_times,plot_var,'o',color=left_prop['color'],ms=left_prop['ms'])
-        
-        #Format x-axis dates
+            plot_times = [plot_times[i] for i in range(
+                len(plot_times)) if varname not in df['flag'].values[i]]
+            plot_var = [plot_var[i] for i in range(
+                len(plot_var)) if varname not in df['flag'].values[i]]
+            ax.plot(plot_times, plot_var, 'o',
+                    color=left_prop['color'], ms=left_prop['ms'])
+
+        # Format x-axis dates
         ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%Mz\n%m/%d'))
-        
-        #Add twin axis
+
+        # Add twin axis
         if twin_ax:
             ax2 = ax.twinx()
-            
-            #Plot line
-            line2 = ax2.plot(df['time'],df[varname_right],color=right_prop['color'],linewidth=right_prop['linewidth'],label=varname_right_info['name'])
+
+            # Plot line
+            line2 = ax2.plot(df['time'], df[varname_right], color=right_prop['color'],
+                             linewidth=right_prop['linewidth'], label=varname_right_info['name'])
             ax2.set_ylabel(varname_right_info['full_name'])
-            
-            #Plot dots
+
+            # Plot dots
             if right_prop['ms'] >= 1:
                 plot_times = df['time'].values
                 plot_var = df[varname_right].values
-                plot_times = [plot_times[i] for i in range(len(plot_times)) if varname_right not in df['flag'].values[i]]
-                plot_var = [plot_var[i] for i in range(len(plot_var)) if varname_right not in df['flag'].values[i]]
-                ax2.plot(plot_times,plot_var,'o',color=right_prop['color'],ms=right_prop['ms'])
+                plot_times = [plot_times[i] for i in range(
+                    len(plot_times)) if varname_right not in df['flag'].values[i]]
+                plot_var = [plot_var[i] for i in range(
+                    len(plot_var)) if varname_right not in df['flag'].values[i]]
+                ax2.plot(plot_times, plot_var, 'o',
+                         color=right_prop['color'], ms=right_prop['ms'])
 
-            #Add legend
+            # Add legend
             lines = line1 + line2
             labels = [l.get_label() for l in lines]
-            ax.legend(lines,labels)
-        
-            #Special handling if both are in units of Celsius
+            ax.legend(lines, labels)
+
+            # Special handling if both are in units of Celsius
             same_unit = False
-            if varname in ['temp','dwpt'] and varname_right in ['temp','dwpt']: same_unit = True
-            if varname in ['sfmr','wspd','pkwnd'] and varname_right in ['sfmr','wspd','pkwnd']: same_unit = True
+            if varname in ['temp', 'dwpt'] and varname_right in ['temp', 'dwpt']:
+                same_unit = True
+            if varname in ['sfmr', 'wspd', 'pkwnd'] and varname_right in ['sfmr', 'wspd', 'pkwnd']:
+                same_unit = True
             if same_unit:
-                min_val = np.nanmin([np.nanmin(df[varname]),np.nanmin(df[varname_right])])
-                max_val = np.nanmax([np.nanmax(df[varname]),np.nanmax(df[varname_right])])*1.05
+                min_val = np.nanmin(
+                    [np.nanmin(df[varname]), np.nanmin(df[varname_right])])
+                max_val = np.nanmax(
+                    [np.nanmax(df[varname]), np.nanmax(df[varname_right])]) * 1.05
                 min_val = min_val * 1.05 if min_val < 0 else min_val * 0.95
-                if np.isnan(min_val): min_val = 0
-                if np.isnan(max_val): max_val = 0
+                if np.isnan(min_val):
+                    min_val = 0
+                if np.isnan(max_val):
+                    max_val = 0
                 if min_val == max_val:
                     min_val = 0
                     max_val = 10
-                ax.set_ylim(min_val,max_val)
-                ax2.set_ylim(min_val,max_val)
-        
-        #Add titles
+                ax.set_ylim(min_val, max_val)
+                ax2.set_ylim(min_val, max_val)
+
+        # Add titles
         storm_data = self.storm.dict
         type_array = np.array(storm_data['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
-        if ('invest' in storm_data.keys() and storm_data['invest'] == False) or len(idx[0]) > 0:
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
+        if ('invest' in storm_data.keys() and not storm_data['invest']) or len(idx[0]) > 0:
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             add_ptc_flag = False
             if len(tropical_vmax) == 0:
                 add_ptc_flag = True
                 idx = np.where((type_array == 'LO') | (type_array == 'DB'))
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             subtrop = classify_subtropical(np.array(storm_data['type']))
             peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
             peak_basin = storm_data['wmo_basin'][peak_idx]
-            storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-            if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
-        
-        #Plot title
+            storm_type = get_storm_classification(
+                np.nanmax(tropical_vmax), subtrop, peak_basin)
+            if add_ptc_flag:
+                storm_type = "Potential Tropical Cyclone"
+
+        # Plot title
         title_string = f"{storm_type} {storm_data['name']}"
         if mission is None:
             title_string += f"\nRecon Aircraft HDOBs | All Missions"
         else:
             title_string += f"\nRecon Aircraft HDOBs | Mission #{mission}"
-        ax.set_title(title_string,loc='left',fontweight='bold')
-        ax.set_title("Plot generated using Tropycal",fontsize=8,loc='right')
-        
-        #Return plot
+        ax.set_title(title_string, loc='left', fontweight='bold')
+        ax.set_title("Plot generated using Tropycal", fontsize=8, loc='right')
+
+        # Return plot
         return ax
-    
-    def plot_points(self,varname='wspd',domain="dynamic",radlim=None,barbs=False,ax=None,cartopy_proj=None,**kwargs):
-        
+
+    def plot_points(self, varname='wspd', domain="dynamic", radlim=None, barbs=False, ax=None, cartopy_proj=None, **kwargs):
         r"""
         Creates a plot of recon data points.
-        
+
         Parameters
         ----------
         varname : str
             Variable to plot. Can be one of the following keys in dataframe:
-            
+
             * **"sfmr"** = SFMR surface wind
             * **"wspd"** = 30-second flight level wind (default)
             * **"pkwnd"** = 10-second flight level wind
             * **"p_sfc"** = extrapolated surface pressure
         domain : str
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         radlim : int
             Radius (in km) away from storm center to include points. If none (default), all points are plotted.
         barbs : bool
             If True, plots wind barbs. If False (default), plots dots.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of recon plot. Please refer to :ref:`options-prop-recon-plot` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         1. Plotting wind barbs only works for wind related variables. ``barbs`` will be automatically set to False for non-wind variables.
-        
+
         2. The special colormap **category_recon** can be used in the prop dict (``prop={'cmap':'category_recon'}``). This uses the standard SSHWS colormap, but with a new color for wind between 50 and 64 knots.
         """
-        
-        #Change barbs
-        if varname == 'p_sfc': barbs = False
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-                
-        #Get plot data
+
+        # Change barbs
+        if varname == 'p_sfc':
+            barbs = False
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Get plot data
         dfRecon = self.data
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         self.plot_obj = ReconPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
             cartopy_proj = self.plot_obj.proj
-        
-        #Plot recon
-        plot_ax = self.plot_obj.plot_points(self.storm,dfRecon,domain,varname=varname,radlim=radlim,barbs=barbs,ax=ax,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+
+        # Plot recon
+        plot_ax = self.plot_obj.plot_points(
+            self.storm, dfRecon, domain, varname=varname, radlim=radlim, barbs=barbs, ax=ax, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-        
-    def plot_hovmoller(self,varname='wspd',radlim=None,window=6,align='center',ax=None,**kwargs):
-        
+
+    def plot_hovmoller(self, varname='wspd', radlim=None, window=6, align='center', ax=None, **kwargs):
         r"""
         Creates a hovmoller plot of azimuthally-averaged recon data.
-        
+
         Parameters
         ----------
         varname : str
             Variable to average and plot. Available variable names are:
-            
+
             * **"sfmr"** = SFMR surface wind
             * **"wspd"** = 30-second flight level wind (default)
             * **"pkwnd"** = 10-second flight level wind
             * **"p_sfc"** = extrapolated surface pressure
         radlim : int, optional
             Radius from storm center, in kilometers, to plot in hovmoller. Default is 200 km.
         window : int, optional
             Window of hours to interpolate between observations. Default is 6 hours.
         align : str, optional
             Alignment of window. Default is 'center'.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties for recon plot. Please refer to :ref:`options-prop-recon-hovmoller` for available options.
         track_dict : dict, optional
             Storm track dictionary. If None (default), internal storm center track is used.
-        
+
         Returns
         -------
         ax
             Axes instance containing the plot.
-        
+
         Notes
         -----
         The special colormap **category_recon** can be used in the prop dict (``prop={'cmap':'category_recon'}``). This uses the standard SSHWS colormap, but with a new color for wind between 50 and 64 knots.
         """
-        
-        #Pop kwargs
-        track_dict = kwargs.pop('track_dict',None)
-        prop = kwargs.pop('prop',{})
-        default_prop = {'cmap':'category','levels':None,'smooth_contourf':False}
+
+        # Pop kwargs
+        track_dict = kwargs.pop('track_dict', None)
+        prop = kwargs.pop('prop', {})
+        default_prop = {
+            'cmap': 'category',
+            'levels': None,
+            'smooth_contourf': False
+        }
         for key in default_prop.keys():
             if key not in prop.keys():
-                prop[key]=default_prop[key]
+                prop[key] = default_prop[key]
 
-        #Get recon data
+        # Get recon data
         dfRecon = self.data
 
-        #Retrieve track dictionary if none is specified
+        # Retrieve track dictionary if none is specified
         if track_dict is None:
             track_dict = self.storm.dict
-        
-        #Interpolate recon data to a hovmoller
-        iRecon = interpRecon(dfRecon,varname,radlim,window=window,align=align)
+
+        # Interpolate recon data to a hovmoller
+        iRecon = interpRecon(dfRecon, varname, radlim,
+                             window=window, align=align)
         Hov_dict = iRecon.interpHovmoller(track_dict)
 
-        #title = get_recon_title(varname) #may not be necessary
-        #If no contour levels specified, generate levels based on data min and max
+        # title = get_recon_title(varname) #may not be necessary
+        # If no contour levels specified, generate levels based on data min and max
         if prop['levels'] is None:
-            prop['levels'] = (np.nanmin(Hov_dict['hovmoller']),np.nanmax(Hov_dict['hovmoller']))
-        
-        #Retrieve updated contour levels and colormap based on input arguments and variable type
-        cmap,clevs = get_cmap_levels(varname,prop['cmap'],prop['levels'])
-        
-        #Retrieve hovmoller times, radii and data
+            prop['levels'] = (np.nanmin(Hov_dict['hovmoller']),
+                              np.nanmax(Hov_dict['hovmoller']))
+
+        # Retrieve updated contour levels and colormap based on input arguments and variable type
+        cmap, clevs = get_cmap_levels(varname, prop['cmap'], prop['levels'])
+
+        # Retrieve hovmoller times, radii and data
         time = Hov_dict['time']
         radius = Hov_dict['radius']
         vardata = Hov_dict['hovmoller']
-        
-        #Error check time
-        time = [dt.strptime((i.strftime('%Y%m%d%H%M')),'%Y%m%d%H%M') for i in time]
-        
-        #------------------------------------------------------------------------------
-        
-        #Create plot
-        plt.figure(figsize=(9,9),dpi=150)
+
+        # Error check time
+        time = [dt.strptime((i.strftime('%Y%m%d%H%M')), '%Y%m%d%H%M')
+                for i in time]
+
+        # ------------------------------------------------------------------------------
+
+        # Create plot
+        plt.figure(figsize=(9, 9), dpi=150)
         ax = plt.subplot()
-        
-        #Plot surface category colors individually, necessitating normalizing colormap
-        if varname in ['vmax','sfmr','wspd','fl_to_sfc'] and prop['cmap'] in ['category','category_recon']:
-            norm = mcolors.BoundaryNorm(clevs,cmap.N)
-            cf = ax.contourf(radius,time,gfilt1d(vardata,sigma=3,axis=1),
-                             levels=clevs,cmap=cmap,norm=norm)
-        
-        #Multiple clevels or without smooth contouring
-        elif len(prop['levels']) > 2 or prop['smooth_contourf'] == False:
-            cf = ax.contourf(radius,time,gfilt1d(vardata,sigma=3,axis=1),
-                             levels=clevs,cmap=cmap)
-        
-        #Automatically generated levels with smooth contouring
+
+        # Plot surface category colors individually, necessitating normalizing colormap
+        if varname in ['vmax', 'sfmr', 'wspd', 'fl_to_sfc'] and prop['cmap'] in ['category', 'category_recon']:
+            norm = mcolors.BoundaryNorm(clevs, cmap.N)
+            cf = ax.contourf(radius, time, gfilt1d(vardata, sigma=3, axis=1),
+                             levels=clevs, cmap=cmap, norm=norm)
+
+        # Multiple clevels or without smooth contouring
+        elif len(prop['levels']) > 2 or not prop['smooth_contourf']:
+            cf = ax.contourf(radius, time, gfilt1d(vardata, sigma=3, axis=1),
+                             levels=clevs, cmap=cmap)
+
+        # Automatically generated levels with smooth contouring
         else:
-            cf = ax.contourf(radius,time,gfilt1d(vardata,sigma=3,axis=1),
-                             cmap=cmap,levels=np.linspace(min(prop['levels']),max(prop['levels']),256))
-        ax.axis([0,max(radius),min(time),max(time)])
-        
-        #Plot colorbar
-        cbar = plt.colorbar(cf,orientation='horizontal',pad=0.1)
-        
-        #Format y-label ticks and labels as dates
+            cf = ax.contourf(radius, time, gfilt1d(vardata, sigma=3, axis=1),
+                             cmap=cmap, levels=np.linspace(min(prop['levels']), max(prop['levels']), 256))
+        ax.axis([0, max(radius), min(time), max(time)])
+
+        # Plot colorbar
+        cbar = plt.colorbar(cf, orientation='horizontal', pad=0.1)
+
+        # Format y-label ticks and labels as dates
         ax.yaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H'))
         for tick in ax.xaxis.get_major_ticks():
-                tick.label.set_fontsize(14)
+            tick.label.set_fontsize(14)
         for tick in ax.yaxis.get_major_ticks():
-                tick.label.set_fontsize(14)
-        
-        #Set axes labels
-        ax.set_ylabel('UTC Time (MM-DD HH)',fontsize=15)
-        ax.set_xlabel('Radius (km)',fontsize=15)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Generate left and right title strings
-        title_left, title_right = hovmoller_plot_title(self.storm,Hov_dict,varname)
-        ax.set_title(title_left,loc='left',fontsize=16,fontweight='bold')
-        ax.set_title(title_right,loc='right',fontsize=12)
-        
-        #Return axis
+            tick.label.set_fontsize(14)
+
+        # Set axes labels
+        ax.set_ylabel('UTC Time (MM-DD HH)', fontsize=15)
+        ax.set_xlabel('Radius (km)', fontsize=15)
+
+        # --------------------------------------------------------------------------------------
+
+        # Generate left and right title strings
+        title_left, title_right = hovmoller_plot_title(
+            self.storm, Hov_dict, varname)
+        ax.set_title(title_left, loc='left', fontsize=16, fontweight='bold')
+        ax.set_title(title_right, loc='right', fontsize=12)
+
+        # Return axis
         return ax
 
-    def plot_maps(self,time=None,varname='wspd',recon_stats=None,domain="dynamic",\
-                  window=6,align='center',radlim=None,ax=None,cartopy_proj=None,save_dir=None,**kwargs):
+    def plot_maps(self, time=None, varname='wspd', recon_stats=None, domain="dynamic",
+                  window=6, align='center', radlim=None, ax=None, cartopy_proj=None, save_dir=None, **kwargs):
         r"""
         Creates maps of interpolated recon data. 
-        
+
         Parameters
         ----------
         time : datetime.datetime or list
             Single datetime object, or list/tuple of datetime objects containing the start and end times to plot between. If None (default), all times will be plotted.
         varname : str or tuple
             Variable to plot. Can be one of the following keys in dataframe:
-            
+
             * **"sfmr"** = SFMR surface wind
             * **"wspd"** = 30-second flight level wind (default)
             * **"pkwnd"** = 10-second flight level wind
             * **"p_sfc"** = extrapolated surface pressure
         domain : str
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         radlim : int, optional
@@ -1425,491 +1529,550 @@
             Alignment of window. Default is 'center'.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs, optional
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_dir : str, optional
             Directory to save output images in. If None, images will not be saved. Default is None.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of recon plot. Please refer to :ref:`options-prop-recon-swath` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
         track_dict : dict, optional
             Storm track dictionary. If None (default), internal storm center track is used.
         """
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        track_dict = kwargs.pop('track_dict',None)
-        
-        #Get plot data
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+        track_dict = kwargs.pop('track_dict', None)
+
+        # Get plot data
         ONE_MAP = False
         if time is None:
-            dfRecon = self.data        
-        elif isinstance(time,(tuple,list)):
-            dfRecon = self.sel(time = time).data
-        elif isinstance(time,dt):
-            dfRecon = self.sel(time = (time-timedelta(hours=6),time+timedelta(hours=6))).data
+            dfRecon = self.data
+        elif isinstance(time, (tuple, list)):
+            dfRecon = self.sel(time=time).data
+        elif isinstance(time, dt):
+            dfRecon = self.sel(
+                time=(time - timedelta(hours=6), time + timedelta(hours=6))).data
             ONE_MAP = True
-        
-        MULTIVAR=False
-        if isinstance(varname,(tuple,list)):
-            MULTIVAR=True                    
-        
+
+        MULTIVAR = False
+        if isinstance(varname, (tuple, list)):
+            MULTIVAR = True
+
         if track_dict is None:
             track_dict = self.storm.dict
-            
-            #Error check for time dimension name
+
+            # Error check for time dimension name
             if 'time' not in track_dict.keys():
-                track_dict['time'] = track_dict['date']
-         
+                track_dict['time'] = track_dict['time']
+
         if ONE_MAP:
-            f = interp1d(mdates.date2num(track_dict['time']),track_dict['lon'], fill_value='extrapolate')
+            f = interp1d(mdates.date2num(
+                track_dict['time']), track_dict['lon'], fill_value='extrapolate')
             clon = f(mdates.date2num(time))
-            f = interp1d(mdates.date2num(track_dict['time']),track_dict['lat'], fill_value='extrapolate')
+            f = interp1d(mdates.date2num(
+                track_dict['time']), track_dict['lat'], fill_value='extrapolate')
             clat = f(mdates.date2num(time))
-            
-            #clon = np.interp(mdates.date2num(recon_select),mdates.date2num(track_dict['time']),track_dict['lon'])
-            #clat = np.interp(mdates.date2num(recon_select),mdates.date2num(track_dict['time']),track_dict['lat'])
-            track_dict = {'time':time,'lon':clon,'lat':clat}
-        
+
+            # clon = np.interp(mdates.date2num(recon_select),mdates.date2num(track_dict['time']),track_dict['lon'])
+            # clat = np.interp(mdates.date2num(recon_select),mdates.date2num(track_dict['time']),track_dict['lat'])
+            track_dict = {
+                'time': time,
+                'lon': clon,
+                'lat': clat
+            }
+
         if MULTIVAR:
-            Maps=[]
+            Maps = []
             for v in varname:
-                iRecon = interpRecon(dfRecon,v,radlim,window=window,align=align)
+                iRecon = interpRecon(dfRecon, v, radlim,
+                                     window=window, align=align)
                 tmpMaps = iRecon.interpMaps(track_dict)
                 Maps.append(tmpMaps)
         else:
-            iRecon = interpRecon(dfRecon,varname,radlim,window=window,align=align)
+            iRecon = interpRecon(dfRecon, varname, radlim,
+                                 window=window, align=align)
             Maps = iRecon.interpMaps(track_dict)
-                
-        #titlename,units = get_recon_title(varname)
-        
+
+        # titlename,units = get_recon_title(varname)
+
         if 'levels' not in prop.keys() or 'levels' in prop.keys() and prop['levels'] is None:
-            prop['levels'] = np.arange(np.floor(np.nanmin(Maps['maps'])/10)*10,
-                             np.ceil(np.nanmax(Maps['maps'])/10)*10+1,10)
-        
+            prop['levels'] = np.arange(np.floor(np.nanmin(Maps['maps']) / 10) * 10,
+                                       np.ceil(np.nanmax(Maps['maps']) / 10) * 10 + 1, 10)
+
         if not ONE_MAP:
-            
+
             if save_dir is True:
                 save_dir = f'{self.storm}{self.year}_maps'
             try:
                 os.system(f'mkdir {save_dir}')
             except:
                 pass
-            
+
             if MULTIVAR:
                 Maps2 = Maps[1]
                 Maps = Maps[0]
-            
-                print(np.nanmax(Maps['maps']),np.nanmin(Maps2['maps']))
-            
+
+                print(np.nanmax(Maps['maps']), np.nanmin(Maps2['maps']))
+
             figs = []
-            for i,t in enumerate(Maps['time']):
-                Maps_sub = {'time':t,'grid_x':Maps['grid_x'],'grid_y':Maps['grid_y'],'maps':Maps['maps'][i],\
-                            'center_lon':Maps['center_lon'][i],'center_lat':Maps['center_lat'][i],'stats':Maps['stats']}
+            for i, t in enumerate(Maps['time']):
+                Maps_sub = {
+                    'time': t,
+                    'grid_x': Maps['grid_x'],
+                    'grid_y': Maps['grid_y'],
+                    'maps': Maps['maps'][i],
+                    'center_lon': Maps['center_lon'][i],
+                    'center_lat': Maps['center_lat'][i],
+                    'stats': Maps['stats']
+                }
 
-                #Create instance of plot object
+                # Create instance of plot object
                 self.plot_obj = ReconPlot()
-                
-                #Create cartopy projection
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+
+                # Create cartopy projection
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
                 cartopy_proj = self.plot_obj.proj
-                
-                #Maintain the same lat / lon dimensions for all dynamic maps
-                #Determined by the dynamic domain from the first map
-                if i>0 and domain == 'dynamic':
-                    d1 = {'n':Maps_sub['center_lat']+dlat,\
-                          's':Maps_sub['center_lat']-dlat,\
-                          'e':Maps_sub['center_lon']+dlon,\
-                          'w':Maps_sub['center_lon']-dlon}
+
+                # Maintain the same lat / lon dimensions for all dynamic maps
+                # Determined by the dynamic domain from the first map
+                if i > 0 and domain == 'dynamic':
+                    d1 = {
+                        'n': Maps_sub['center_lat'] + dlat,
+                        's': Maps_sub['center_lat'] - dlat,
+                        'e': Maps_sub['center_lon'] + dlon,
+                        'w': Maps_sub['center_lon'] - dlon
+                    }
                 else:
                     d1 = domain
-                
-                #Plot recon
-                
+
+                # Plot recon
+
                 if MULTIVAR:
                     Maps_sub1 = dict(Maps_sub)
                     Maps_sub2 = dict(Maps_sub)
-                    Maps_sub = [Maps_sub1,Maps_sub2]
+                    Maps_sub = [Maps_sub1, Maps_sub2]
                     Maps_sub[1]['maps'] = Maps2['maps'][i]
-                    
-                    print(np.nanmax(Maps_sub[0]['maps']),np.nanmin(Maps_sub[1]['maps']))
-                    
-                plot_ax,d0 = self.plot_obj.plot_maps(self.storm,Maps_sub,varname,recon_stats,\
-                                                    domain=d1,ax=ax,return_domain=True,prop=prop,map_prop=map_prop)
-                
-                #Get domain dimensions from the first map
-                if i==0:
-                    dlat = .5*(d0['n']-d0['s'])
-                    dlon = .5*(d0['e']-d0['w'])
-                
+
+                    print(np.nanmax(Maps_sub[0]['maps']),
+                          np.nanmin(Maps_sub[1]['maps']))
+
+                plot_ax, d0 = self.plot_obj.plot_maps(self.storm, Maps_sub, varname, recon_stats,
+                                                      domain=d1, ax=ax, return_domain=True, prop=prop, map_prop=map_prop)
+
+                # Get domain dimensions from the first map
+                if i == 0:
+                    dlat = .5 * (d0['n'] - d0['s'])
+                    dlon = .5 * (d0['e'] - d0['w'])
+
                 figs.append(plot_ax)
-                
+
                 if save_dir is not None:
-                    plt.savefig(f'{save_dir}/{t.strftime("%Y%m%d%H%M")}.png',bbox_inches='tight')
+                    plt.savefig(
+                        f'{save_dir}/{t.strftime("%Y%m%d%H%M")}.png', bbox_inches='tight')
                 plt.close()
-                
+
             if save_dir is None:
                 return figs
 
         else:
-            #Create instance of plot object
+            # Create instance of plot object
             self.plot_obj = ReconPlot()
-            
-            #Create cartopy projection
+
+            # Create cartopy projection
             if cartopy_proj is None:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
                 cartopy_proj = self.plot_obj.proj
-            
-            #Plot recon
-            plot_ax = self.plot_obj.plot_maps(self.storm,Maps,varname,recon_stats,domain,ax,prop=prop,map_prop=map_prop)
-            
-            #Return axis
+
+            # Plot recon
+            plot_ax = self.plot_obj.plot_maps(
+                self.storm, Maps, varname, recon_stats, domain, ax, prop=prop, map_prop=map_prop)
+
+            # Return axis
             return plot_ax
 
-    def plot_swath(self,varname='wspd',domain="dynamic",ax=None,cartopy_proj=None,**kwargs):
-        
+    def plot_swath(self, varname='wspd', domain="dynamic", ax=None, cartopy_proj=None, **kwargs):
         r"""
         Creates a map plot of a swath of interpolated recon data.
-        
+
         Parameters
         ----------
         varname : str
             Variable to plot. Can be one of the following keys in dataframe:
-            
+
             * **"sfmr"** = SFMR surface wind
             * **"wspd"** = 30-second flight level wind (default)
             * **"pkwnd"** = 10-second flight level wind
             * **"p_sfc"** = extrapolated surface pressure
-        
+
         domain : str
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of recon plot. Please refer to :ref:`options-prop-recon-swath` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
         track_dict : dict, optional
             Storm track dictionary. If None (default), internal storm center track is used.
         swathfunc : function
             Function to operate on interpolated recon data (e.g., np.max, np.min, or percentile function). Default is np.min for pressure, otherwise np.max.
         """
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        track_dict = kwargs.pop('track_dict',None)
-        swathfunc = kwargs.pop('swathfunc',None)
-        
-        #Get plot data
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+        track_dict = kwargs.pop('track_dict', None)
+        swathfunc = kwargs.pop('swathfunc', None)
+
+        # Get plot data
         dfRecon = self.data
 
         if track_dict is None:
             track_dict = self.storm.dict
-        
+
         if swathfunc is None:
             if varname == 'p_sfc':
                 swathfunc = np.min
             else:
                 swathfunc = np.max
-        
-        iRecon = interpRecon(dfRecon,varname)
-        Maps = iRecon.interpMaps(track_dict,interval=.2)
-        
-        #Create instance of plot object
+
+        iRecon = interpRecon(dfRecon, varname)
+        Maps = iRecon.interpMaps(track_dict, interval=.2)
+
+        # Create instance of plot object
         self.plot_obj = ReconPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
             cartopy_proj = self.plot_obj.proj
-        
-        #Plot recon
-        plot_ax = self.plot_obj.plot_swath(self.storm,Maps,varname,swathfunc,track_dict,domain,ax,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+
+        # Plot recon
+        plot_ax = self.plot_obj.plot_swath(
+            self.storm, Maps, varname, swathfunc, track_dict, domain, ax, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-    
-    
-    def gridded_stats(self,request,thresh={},binsize=1,domain="dynamic",ax=None,
-                      return_array=False,cartopy_proj=None,prop={},map_prop={}):
-        
+
+    def gridded_stats(self, request, thresh={}, binsize=1, domain="dynamic", ax=None,
+                      return_array=False, cartopy_proj=None, prop={}, map_prop={}):
         r"""
         Creates a plot of gridded statistics.
-        
+
         Parameters
         ----------
         request : str
             This string is a descriptor for what you want to plot.
             It will be used to define the variable (e.g. 'wind' --> 'vmax') and the function (e.g. 'maximum' --> np.max()).
             This string is also used as the plot title.
-            
+
             Variable words to use in request:
-                
+
             * **wind** - (kt). Sustained wind.
             * **pressure** - (hPa). Minimum pressure.
             * **wind change** - (kt/time). Must be followed by an integer value denoting the length of the time window '__ hours' (e.g., "wind change in 24 hours").
             * **pressure change** - (hPa/time). Must be followed by an integer value denoting the length of the time window '__ hours' (e.g., "pressure change in 24 hours").
             * **storm motion** - (km/hour). Can be followed a length of time window. Otherwise defaults to 24 hours.
-            
+
             Units of all wind variables are knots and pressure variables are hPa. These are added into the title.
-            
+
             Function words to use in request:
-                
+
             * **maximum**
             * **minimum**
             * **average** 
             * **percentile** - Percentile must be preceded by an integer [0,100].
             * **number** - Number of storms in grid box satisfying filter thresholds.
-            
+
             Example usage: "maximum wind change in 24 hours", "50th percentile wind", "number of storms"
-            
+
         thresh : dict, optional
             Keywords in self.keys
-            
+
             Units of all wind variables = kt, and pressure variables = hPa. These are added to the subtitle.
 
         binsize : float, optional
             Grid resolution in degrees. Default is 1 degree.
         domain : str, optional
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
         return_array : bool, optional
             If True, returns the gridded 2D array used to generate the plot. Default is False.
         cartopy_proj : ccrs, optional
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-        
+
         Other Parameters
         ----------------
         prop : dict, optional
             Customization properties of plot. Please refer to :ref:`options-prop-gridded` for available options.
         map_prop : dict, optional
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         By default, the plot axes is returned. If "return_array" are set to True, a dictionary is returned containing both the axes and data array.
         """
 
-        default_prop = {'smooth':None}
+        default_prop = {
+            'smooth': None
+        }
         for key in prop.keys():
             default_prop[key] = prop[key]
         prop = default_prop
-        
-        #Update thresh based on input
-        default_thresh={'sample_min':1,'p_max':np.nan,'v_min':np.nan,'dv_min':np.nan,'dp_max':np.nan,'dv_max':np.nan,'dp_min':np.nan,'dt_window':24,'dt_align':'middle'}
+
+        # Update thresh based on input
+        default_thresh = {
+            'sample_min': 1,
+            'p_max': np.nan,
+            'v_min': np.nan,
+            'dv_min': np.nan,
+            'dp_max': np.nan,
+            'dv_max': np.nan,
+            'dp_min': np.nan,
+            'dt_window': 24,
+            'dt_align': 'middle'
+        }
         for key in thresh:
             default_thresh[key] = thresh[key]
         thresh = default_thresh
-        
-        #Retrieve the requested function, variable for computing stats, and plot title. These modify thresh if necessary.
-        thresh,func = find_func(request,thresh)
-        thresh,varname = find_var(request,thresh)
-        
-        #---------------------------------------------------------------------------------------------------
+
+        # Retrieve the requested function, variable for computing stats, and plot title. These modify thresh if necessary.
+        thresh, func = find_func(request, thresh)
+        thresh, varname = find_var(request, thresh)
+
+        # ---------------------------------------------------------------------------------------------------
 
         points = self.data
-        #Round lat/lon points down to nearest bin
-        to_bin = lambda x: np.floor(x / binsize) * binsize
+        # Round lat/lon points down to nearest bin
+        def to_bin(x): return np.floor(x / binsize) * binsize
         points["latbin"] = points.lat.map(to_bin)
         points["lonbin"] = points.lon.map(to_bin)
 
-        #---------------------------------------------------------------------------------------------------
+        # ---------------------------------------------------------------------------------------------------
 
-        #Group by latbin,lonbin,stormid
+        # Group by latbin,lonbin,stormid
         print("--> Grouping by lat/lon")
-        groups = points.groupby(["latbin","lonbin"])
+        groups = points.groupby(["latbin", "lonbin"])
 
-        #Loops through groups, and apply stat func to obs
-        #Constructs a new dataframe containing the lat/lon bins and plotting variable
-        new_df = {'latbin':[],'lonbin':[],varname:[]}
+        # Loops through groups, and apply stat func to obs
+        # Constructs a new dataframe containing the lat/lon bins and plotting variable
+        new_df = {
+            'latbin': [],
+            'lonbin': [],
+            'varname': []
+        }
         for g in groups:
-            new_df[varname].append(func(g[1][varname].values))                    
+            new_df[varname].append(func(g[1][varname].values))
             new_df['latbin'].append(g[0][0])
             new_df['lonbin'].append(g[0][1])
         new_df = pd.DataFrame.from_dict(new_df)
 
-        #---------------------------------------------------------------------------------------------------
+        # ---------------------------------------------------------------------------------------------------
 
-        #Group again by latbin,lonbin
-        #Construct two 1D lists: zi (grid values) and coords, that correspond to the 2D grid
+        # Group again by latbin,lonbin
+        # Construct two 1D lists: zi (grid values) and coords, that correspond to the 2D grid
         groups = new_df.groupby(["latbin", "lonbin"])
 
-        zi = [func(g[1][varname]) if len(g[1]) >= thresh['sample_min'] else np.nan for g in groups]
+        zi = [func(g[1][varname]) if len(g[1]) >=
+              thresh['sample_min'] else np.nan for g in groups]
 
-        #Construct a 1D array of coordinates
+        # Construct a 1D array of coordinates
         coords = [g[0] for g in groups]
 
-        #Construct a 2D longitude and latitude grid, using the specified binsize resolution
+        # Construct a 2D longitude and latitude grid, using the specified binsize resolution
         if prop['smooth'] is not None:
-            all_lats = [(round(l/binsize)*binsize) for l in self.data['lat']]
-            all_lons = [(round(l/binsize)*binsize)%360 for l in self.data['lon']]
-            xi = np.arange(min(all_lons)-binsize,max(all_lons)+2*binsize,binsize)
-            yi = np.arange(min(all_lats)-binsize,max(all_lats)+2*binsize,binsize)
+            all_lats = [(round(l / binsize) * binsize)
+                        for l in self.data['lat']]
+            all_lons = [(round(l / binsize) * binsize) %
+                        360 for l in self.data['lon']]
+            xi = np.arange(min(all_lons) - binsize,
+                           max(all_lons) + 2 * binsize, binsize)
+            yi = np.arange(min(all_lats) - binsize,
+                           max(all_lats) + 2 * binsize, binsize)
         else:
-            xi = np.arange(np.nanmin(points["lonbin"])-binsize,np.nanmax(points["lonbin"])+2*binsize,binsize)
-            yi = np.arange(np.nanmin(points["latbin"])-binsize,np.nanmax(points["latbin"])+2*binsize,binsize)
-        grid_x, grid_y = np.meshgrid(xi,yi)
-
-        #Construct a 2D grid for the z value, depending on whether vector or scalar quantity
-        grid_z = np.ones(grid_x.shape)*np.nan
-        for c,z in zip(coords,zi):
-            grid_z[np.where((grid_y==c[0]) & (grid_x==c[1]))] = z
+            xi = np.arange(np.nanmin(
+                points["lonbin"]) - binsize, np.nanmax(points["lonbin"]) + 2 * binsize, binsize)
+            yi = np.arange(np.nanmin(
+                points["latbin"]) - binsize, np.nanmax(points["latbin"]) + 2 * binsize, binsize)
+        grid_x, grid_y = np.meshgrid(xi, yi)
+
+        # Construct a 2D grid for the z value, depending on whether vector or scalar quantity
+        grid_z = np.ones(grid_x.shape) * np.nan
+        for c, z in zip(coords, zi):
+            grid_z[np.where((grid_y == c[0]) & (grid_x == c[1]))] = z
 
-        #---------------------------------------------------------------------------------------------------
+        # ---------------------------------------------------------------------------------------------------
 
-        #Create instance of plot object
+        # Create instance of plot object
         plot_obj = TrackPlot()
-        
-        #Create cartopy projection using basin
+
+        # Create cartopy projection using basin
         if cartopy_proj is None:
             if max(points['lon']) > 150 or min(points['lon']) < -150:
-                plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+                plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=180.0)
             else:
-                plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+                plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
+
+        prop['title_L'], prop['title_R'] = self.storm.name, 'things'
 
-        prop['title_L'],prop['title_R'] = self.storm.name,'things'
-        
         if domain == "dynamic":
-            domain = {'W':min(self.data['lon']),'E':max(self.data['lon']),'S':min(self.data['lat']),'N':max(self.data['lat'])}
-        
-        #Plot gridded field
-        plot_ax = plot_obj.plot_gridded(grid_x,grid_y,grid_z,varname,domain=domain,ax=ax,prop=prop,map_prop=map_prop)
-        
-        #Format grid into xarray if specified
+            domain = {
+                'W': min(self.data['lon']),
+                'E': max(self.data['lon']),
+                'S': min(self.data['lat']),
+                'N': max(self.data['lat'])
+            }
+
+        # Plot gridded field
+        plot_ax = plot_obj.plot_gridded(
+            grid_x, grid_y, grid_z, varname, domain=domain, ax=ax, prop=prop, map_prop=map_prop)
+
+        # Format grid into xarray if specified
         if return_array:
             try:
-                #Import xarray and construct DataArray, replacing NaNs with zeros
+                # Import xarray and construct DataArray, replacing NaNs with zeros
                 import xarray as xr
-                arr = xr.DataArray(np.nan_to_num(grid_z),coords=[grid_y.T[0],grid_x[0]],dims=['lat','lon'])
+                arr = xr.DataArray(np.nan_to_num(grid_z), coords=[
+                                   grid_y.T[0], grid_x[0]], dims=['lat', 'lon'])
                 return arr
             except ImportError as e:
-                raise RuntimeError("Error: xarray is not available. Install xarray in order to use the 'return_array' flag.") from e
+                raise RuntimeError(
+                    "Error: xarray is not available. Install xarray in order to use the 'return_array' flag.") from e
 
-        #Return axis
+        # Return axis
         if return_array:
-            return {'ax':plot_ax,'array':arr}
+            return {'ax': plot_ax, 'array': arr}
         else:
             return plot_ax
 
-        
+
 class dropsondes:
 
     r"""
     Creates an instance of a Dropsondes object containing all dropsonde data for a single storm.
-    
+
     Parameters
     ----------
     storm : tropycal.tracks.Storm
         Requested storm.
     data : str, optional
         Filepath of pickle file containing dropsondes data retrieved from ``dropsondes.to_pickle()``. If provided, data will be retrieved from the local pickle file instead of the NHC server.
     update : bool
         True = search for new data, following existing data in the dropsonde object, and concatenate.
 
     Returns
     -------
     Dataset
         An instance of dropsondes.
-    
+
     Notes
     -----
     .. warning::
-    
+
         Recon data is currently only available from 2006 onwards.
-    
+
     There are two recommended ways of retrieving a dropsondes object. Since the ``ReconDataset``, ``hdobs``, ``dropsondes`` and ``vdms`` classes are **storm-centric**, a Storm object is required for both methods.
-    
+
     .. code-block:: python
-    
+
         #Retrieve Hurricane Michael (2018) from TrackDataset
         basin = tracks.TrackDataset()
         storm = basin.get_storm(('michael',2018))
-    
+
     The first method is to use the empty instance of ReconDataset already initialized in the Storm object, which has a ``get_dropsondes()`` method thus allowing all of the dropsondes attributes and methods to be accessed from the Storm object. As a result, a Storm object does not need to be provided as an argument.
-    
+
     .. code-block:: python
-    
+
         #Retrieve all dropsondes for this storm
         storm.recon.get_dropsondes()
-        
+
         #Retrieve the raw dropsondes data
         storm.recon.dropsondes.data
-        
+
         #Use the plot_points() method of dropsondes
         storm.recon.dropsondes.plot_points()
-    
+
     The second method is to use the dropsondes class independently of the other recon classes:
-    
+
     .. code-block:: python
-    
+
         from tropycal.recon import dropsondes
-        
+
         #Retrieve all dropsondes for this storm, passing the Storm object as an argument
         dropsondes_obj = dropsondes(storm)
-        
+
         #Retrieve the raw dropsondes data
         dropsondes_obj.data
-        
+
         #Use the plot_points() method of dropsondes
         dropsondes_obj.plot_points()
     """
 
     def __repr__(self):
-        
+
         summary = ["<tropycal.recon.dropsondes>"]
-        
-        def isNA(x,units):
+
+        def isNA(x, units):
             if np.isnan(x):
                 return 'N/A'
             else:
                 return f'{x} {units}'
-        #Find maximum wind and minimum pressure
-        max_MBLspd = isNA(np.nanmax([i['MBLspd'] for i in self.data]),'knots')
-        max_DLMspd = isNA(np.nanmax([i['DLMspd'] for i in self.data]),'knots')
-        max_WL150spd = isNA(np.nanmax([i['WL150spd'] for i in self.data]),'knots')
-        min_slp = isNA(np.nanmin([i['slp'] for i in self.data]),'hPa')
+        # Find maximum wind and minimum pressure
+        max_MBLspd = isNA(np.nanmax([i['MBLspd'] for i in self.data]), 'knots')
+        max_DLMspd = isNA(np.nanmax([i['DLMspd'] for i in self.data]), 'knots')
+        max_WL150spd = isNA(np.nanmax([i['WL150spd']
+                            for i in self.data]), 'knots')
+        min_slp = isNA(np.nanmin([i['slp'] for i in self.data]), 'hPa')
         missions = set([i['mission'] for i in self.data])
-        
-        #Add general summary
+
+        # Add general summary
         emdash = '\u2014'
-        summary_keys = {'Storm':f'{self.storm.name} {self.storm.year}',\
-                        'Missions':len(missions),
-                        'Dropsondes':len(self.data),
-                        'Max 500m-avg wind':max_MBLspd,
-                        'Max 150m-avg wind':max_WL150spd,
-                        'Min sea level pressure':min_slp,
-                        'Source':self.source}
+        summary_keys = {
+            'Storm': f'{self.storm.name} {self.storm.year}',
+            'Missions': len(missions),
+            'Dropsondes': len(self.data),
+            'Max 500m-avg wind': max_MBLspd,
+            'Max 150m-avg wind': max_WL150spd,
+            'Min sea level pressure': min_slp,
+            'Source': self.source
+        }
 
-        #Add dataset summary
+        # Add dataset summary
         summary.append("Dataset Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
         return "\n".join(summary)
-    
+
     def __init__(self, storm, data=None, update=False):
 
         self.storm = storm
         self.source = 'National Hurricane Center (NHC)'
-        
+
         if storm.year >= 2006:
             self.format = 1
             if storm.basin == 'north_atlantic':
                 archive_url = f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/REPNT3/'
             else:
                 archive_url = f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/REPPN3/'
         elif storm.year >= 2002 and storm.year <= 2005:
@@ -1918,215 +2081,237 @@
         elif storm.year >= 1989 and storm.year <= 2001:
             self.format = 3
             archive_url = f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/{self.storm.name.lower()}/'
         else:
             raise RuntimeError("Recon data is not available prior to 1989.")
         self.data = None
 
-        if isinstance(data,str):
+        if isinstance(data, str):
             with open(data, 'rb') as f:
                 self.data = pickle.load(f)
         elif data is not None:
             self.data = data
-        
+
         if data is None or update:
             try:
                 start_time = max(self.data['time'])
             except:
-                start_time = min(self.storm.dict['date'])-timedelta(days=1)
-            end_time = max(self.storm.dict['date'])+timedelta(days=1)
+                start_time = min(self.storm.dict['time']) - timedelta(days=1)
+            end_time = max(self.storm.dict['time']) + timedelta(days=1)
+
+            timeboundstrs = [f'{t:%Y%m%d%H%M}' for t in (start_time, end_time)]
 
-            timeboundstrs = [f'{t:%Y%m%d%H%M}' for t in (start_time,end_time)]
-            
-            #Retrieve list of files in URL and filter by storm dates
+            # Retrieve list of files in URL and filter by storm dates
             page = requests.get(archive_url).text
             content = page.split("\n")
             files = []
             if self.format == 1:
                 for line in content:
-                    if ".txt" in line: files.append(((line.split('txt">')[1]).split("</a>")[0]).split("."))
+                    if ".txt" in line:
+                        files.append(
+                            ((line.split('txt">')[1]).split("</a>")[0]).split("."))
                 del content
-                files = sorted([i for i in files if i[1]>=min(timeboundstrs) and i[1]<=max(timeboundstrs)],key=lambda x: x[1])
-                linksub = [archive_url+'.'.join(l) for l in files]
+                files = sorted([i for i in files if i[1] >= min(
+                    timeboundstrs) and i[1] <= max(timeboundstrs)], key=lambda x: x[1])
+                linksub = [archive_url + '.'.join(l) for l in files]
             elif self.format == 2:
                 for line in content:
-                    if ".txt" in line and 'DROPS' in line: files.append(((line.split('txt">')[1]).split("</a>")[0]))
+                    if ".txt" in line and 'DROPS' in line:
+                        files.append(
+                            ((line.split('txt">')[1]).split("</a>")[0]))
                 del content
-                linksub = [archive_url+l for l in files]
+                linksub = [archive_url + l for l in files]
             elif self.format == 3:
                 for line in content:
-                    if ".txt" in line: files.append(((line.split('txt">')[1]).split("</a>")[0]))
+                    if ".txt" in line:
+                        files.append(
+                            ((line.split('txt">')[1]).split("</a>")[0]))
                 del content
-                linksub = [archive_url+l for l in files if l[0] in ['D','d']]
-            
+                linksub = [archive_url +
+                           l for l in files if l[0] in ['D', 'd']]
+
             urllib3.disable_warnings()
             http = urllib3.PoolManager()
-            
+
             timer_start = dt.now()
-            print(f'Searching through recon dropsonde files between {timeboundstrs[0]} and {timeboundstrs[-1]} ...')
+            print(
+                f'Searching through recon dropsonde files between {timeboundstrs[0]} and {timeboundstrs[-1]} ...')
             filecount = 0
             for link in linksub:
-                response = http.request('GET',link)
+                response = http.request('GET', link)
                 content = response.data.decode('utf-8')
-                
-                #Post-2006 format
+
+                # Post-2006 format
                 if self.format == 1:
-                    datestamp = dt.strptime(link.split('.')[-2],'%Y%m%d%H%M')
+                    datestamp = dt.strptime(link.split('.')[-2], '%Y%m%d%H%M')
                     try:
-                        missionname,tmp = decode_dropsonde(content,date=datestamp)
+                        missionname, tmp = decode_dropsonde(
+                            content, date=datestamp)
                     except:
                         continue
-                    
-                    testkeys = ('TOPtime','lat','lon')
-                    if missionname[2:5] == self.storm.operational_id[2:4]+self.storm.operational_id[0]:
+
+                    testkeys = ('TOPtime', 'lat', 'lon')
+                    if missionname[2:5] == self.storm.operational_id[2:4] + self.storm.operational_id[0]:
                         filecount += 1
                         if self.data is None:
                             self.data = [copy.copy(tmp)]
                         elif [tmp[k] for k in testkeys] not in [[d[k] for k in testkeys] for d in self.data]:
                             self.data.append(tmp)
                         else:
                             pass
-                
-                #Pre-2002 format
+
+                # Pre-2002 format
                 elif self.format == 3:
-                    
-                    #Check for date
+
+                    # Check for date
                     try:
                         day = int(content.split("\n")[0].split()[2][:2])
-                        for iter_date in storm.dict['date']:
+                        for iter_date in storm.dict['time']:
                             found_date = False
                             if iter_date.day == day:
-                                date = dt(iter_date.year,iter_date.month,iter_date.day)
+                                date = dt(iter_date.year,
+                                          iter_date.month, iter_date.day)
                                 found_date = True
                                 break
-                        if found_date == False: continue
-                        missionname,tmp = decode_dropsonde(content.replace(";",""),date=date)
-                        
-                        #Add date to mission
+                        if not found_date:
+                            continue
+                        missionname, tmp = decode_dropsonde(
+                            content.replace(";", ""), date=date)
+
+                        # Add date to mission
                         hh = int(content.split("\n")[0].split()[2][2:4])
                         mm = int(content.split("\n")[0].split()[2][4:6])
-                        tmp['TOPtime'] = dt(iter_date.year,iter_date.month,iter_date.day,hh,mm)
-                        if np.isnan(tmp['TOPlat']): tmp['TOPlat'] = tmp['lat']
-                        if np.isnan(tmp['TOPlon']): tmp['TOPlon'] = tmp['lon']
-                        
-                        testkeys = ('TOPtime','lat','lon')
+                        tmp['TOPtime'] = dt(
+                            iter_date.year, iter_date.month, iter_date.day, hh, mm)
+                        if np.isnan(tmp['TOPlat']):
+                            tmp['TOPlat'] = tmp['lat']
+                        if np.isnan(tmp['TOPlon']):
+                            tmp['TOPlon'] = tmp['lon']
+
+                        testkeys = ('TOPtime', 'lat', 'lon')
                         filecount += 1
                         if self.data is None:
                             self.data = [copy.copy(tmp)]
                         elif [tmp[k] for k in testkeys] not in [[d[k] for k in testkeys] for d in self.data]:
                             self.data.append(tmp)
                         else:
                             pass
                     except:
                         pass
-                
-                #Pre-2006 format
+
+                # Pre-2006 format
                 elif self.format == 2:
                     strdate = (link.split('.')[-3]).split("_")[-1]
                     content_split = content.split("NNNN")
-                    
+
                     for iter_content in content_split:
-                        
+
                         iter_split = iter_content.split("\n")
-                        if len(iter_split) < 6: continue
-                        
-                        #Format date
+                        if len(iter_split) < 6:
+                            continue
+
+                        # Format date
                         found_date = False
                         for line in iter_split:
                             if 'UZNT13' in line:
                                 date_string = line.split()[2]
                                 found_date = True
-                                datestamp = dt.strptime(strdate,'%Y%m%d')
-                                datestamp = datestamp.replace(day=int(date_string[:2]),hour=int(date_string[2:4]),minute=int(date_string[4:6]))
-                        
-                        #Decode dropsondes
-                        if found_date == False: continue
+                                datestamp = dt.strptime(strdate, '%Y%m%d')
+                                datestamp = datestamp.replace(day=int(date_string[:2]), hour=int(
+                                    date_string[2:4]), minute=int(date_string[4:6]))
+
+                        # Decode dropsondes
+                        if not found_date:
+                            continue
                         try:
-                            missionname,tmp = decode_dropsonde(iter_content,date=datestamp)
+                            missionname, tmp = decode_dropsonde(
+                                iter_content, date=datestamp)
                         except:
                             continue
 
-                        testkeys = ('lat','lon')
+                        testkeys = ('lat', 'lon')
                         filecount += 1
                         if self.data is None:
                             self.data = [copy.copy(tmp)]
                         elif [tmp[k] for k in testkeys] not in [[d[k] for k in testkeys] for d in self.data]:
                             self.data.append(tmp)
                         else:
                             pass
-                
-            print(f'--> Completed reading in recon dropsonde files ({(dt.now()-timer_start).total_seconds():.1f} seconds)'+\
+
+            print(f'--> Completed reading in recon dropsonde files ({(dt.now()-timer_start).total_seconds():.1f} seconds)' +
                   f'\nRead {filecount} files')
-        
+
         try:
             self._recenter()
-            self.keys = sorted(list(set([k for d in self.data for k in d.keys()])))
+            self.keys = sorted(
+                list(set([k for d in self.data for k in d.keys()])))
         except:
             self.keys = []
 
     def update(self):
         r"""
         Update with the latest data for an ongoing storm.
-        
+
         Notes
         -----
         This function has no return value, but simply updates the internal dropsonde data with new observations since the object was created.
         """
-        
-        newobj = dropsondes(storm=self.storm,data=self.data,update=True)
+
+        newobj = dropsondes(storm=self.storm, data=self.data, update=True)
         return newobj
 
     def _recenter(self):
         data = copy.copy(self.data)
-        
-        #Get x,y distance of each ob from coinciding interped center position
-        for stage in ('TOP','BOTTOM'):
-            #Interpolate center position to time of each ob
-            interp_clon,interp_clat = self.storm.recon.get_track([d[f'{stage}time'] for d in data])
-
-            #Get x,y distance of each ob from coinciding interped center position
-            for i,d in enumerate(data):
-                d.update({f'{stage}xdist':great_circle( (interp_clat[i],interp_clon[i]), \
-                    (interp_clat[i],d[f'{stage}lon']) ).kilometers* \
-                    [1,-1][int(d[f'{stage}lon'] < interp_clon[i])]})
-                d.update({f'{stage}ydist':great_circle( (interp_clat[i],interp_clon[i]), \
-                    (d[f'{stage}lat'],interp_clon[i]) ).kilometers* \
-                    [1,-1][int(d[f'{stage}lat'] < interp_clat[i])]})
-                d.update({f'{stage}distance':(d[f'{stage}xdist']**2+d[f'{stage}ydist']**2)**.5})
 
-        #print('Completed dropsonde center-relative coordinates')
+        # Get x,y distance of each ob from coinciding interped center position
+        for stage in ('TOP', 'BOTTOM'):
+            # Interpolate center position to time of each ob
+            interp_clon, interp_clat = self.storm.recon.get_track(
+                [d[f'{stage}time'] for d in data])
+
+            # Get x,y distance of each ob from coinciding interped center position
+            for i, d in enumerate(data):
+                d.update({f'{stage}xdist': great_circle((interp_clat[i], interp_clon[i]),
+                                                        (interp_clat[i], d[f'{stage}lon'])).kilometers *
+                          [1, -1][int(d[f'{stage}lon'] < interp_clon[i])]})
+                d.update({f'{stage}ydist': great_circle((interp_clat[i], interp_clon[i]),
+                                                        (d[f'{stage}lat'], interp_clon[i])).kilometers *
+                          [1, -1][int(d[f'{stage}lat'] < interp_clat[i])]})
+                d.update({f'{stage}distance': (
+                    d[f'{stage}xdist']**2 + d[f'{stage}ydist']**2)**.5})
+
+        # print('Completed dropsonde center-relative coordinates')
         self.data = data
-    
-    def isel(self,index):
+
+    def isel(self, index):
         r"""
         Select a single dropsonde by index of the list.
-        
+
         Parameters
         ----------
         index : int
             Integer containing the index of the dropsonde.
-        
+
         Returns
         -------
         dropsondes
             Instance of Dropsondes for the single requested dropsonde.
         """
-        
+
         NEW_DATA = copy.copy(self.data)
         NEW_DATA = [NEW_DATA[index]]
-        NEW_OBJ = dropsondes(storm = self.storm, data = NEW_DATA)
-        
+        NEW_OBJ = dropsondes(storm=self.storm, data=NEW_DATA)
+
         return NEW_OBJ
-        
-    
-    def sel(self,mission=None,time=None,domain=None,location=None,top=None,\
-            slp=None,MBLspd=None,WL150spd=None,DLMspd=None):
+
+    def sel(self, mission=None, time=None, domain=None, location=None, top=None,
+            slp=None, MBLspd=None, WL150spd=None, DLMspd=None):
         r"""
         Select a subset of dropsondes by any of its parameters and return a new dropsondes object.
-        
+
         Parameters
         ----------
         mission : str
             Mission name (number + storm id), e.g. mission 7 for AL05 is '0705L'
         time : list/tuple of datetimes
             list/tuple of start time and end time datetime objects.
             Default is None, which returns all points
@@ -2142,331 +2327,354 @@
         Returns
         -------
         dropsondes
             A new dropsondes object that satisfies the intersection of all subsetting.
         """
 
         NEW_DATA = copy.copy(pd.DataFrame(self.data))
-        
-        #Apply mission filter
+
+        # Apply mission filter
         if mission is not None:
             mission = str(mission)
-            NEW_DATA = NEW_DATA.loc[NEW_DATA['mission']==mission]
+            NEW_DATA = NEW_DATA.loc[NEW_DATA['mission'] == mission]
 
-        #Apply time filter
+        # Apply time filter
         if time is not None:
             try:
-                if isinstance(time,(tuple,list)):
-                    bounds = get_bounds(NEW_DATA['TOPtime'],time)
-                    NEW_DATA = NEW_DATA.loc[(NEW_DATA['TOPtime']>=bounds[0]) & (NEW_DATA['TOPtime']<=bounds[1])]
+                if isinstance(time, (tuple, list)):
+                    bounds = get_bounds(NEW_DATA['TOPtime'], time)
+                    NEW_DATA = NEW_DATA.loc[(NEW_DATA['TOPtime'] >= bounds[0]) & (
+                        NEW_DATA['TOPtime'] <= bounds[1])]
                 else:
-                    i = np.argmin(abs(time-NEW_DATA['TOPtime']))
+                    i = np.argmin(abs(time - NEW_DATA['TOPtime']))
                     return self.isel(i)
             except:
-                if isinstance(time,(tuple,list)):
-                    bounds = get_bounds(NEW_DATA['BOTTOMtime'],time)
-                    NEW_DATA = NEW_DATA.loc[(NEW_DATA['BOTTOMtime']>=bounds[0]) & (NEW_DATA['BOTTOMtime']<=bounds[1])]
+                if isinstance(time, (tuple, list)):
+                    bounds = get_bounds(NEW_DATA['BOTTOMtime'], time)
+                    NEW_DATA = NEW_DATA.loc[(NEW_DATA['BOTTOMtime'] >= bounds[0]) & (
+                        NEW_DATA['BOTTOMtime'] <= bounds[1])]
                 else:
-                    i = np.argmin(abs(time-NEW_DATA['BOTTOMtime']))
+                    i = np.argmin(abs(time - NEW_DATA['BOTTOMtime']))
                     return self.isel(i)
-        
-        #Apply domain filter
+
+        # Apply domain filter
         if domain is not None:
-            tmp = {k[0].lower():v for k,v in domain.items()}
-            domain = {'n':90,'s':-90,'e':359.99,'w':0}
+            tmp = {k[0].lower(): v for k, v in domain.items()}
+            domain = {'n': 90, 's': -90, 'e': 359.99, 'w': 0}
             domain.update(tmp)
-            bounds = get_bounds(NEW_DATA['lon']%360,(domain['w']%360,domain['e']%360))
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lon']%360>=bounds[0]) & (NEW_DATA['lon']%360<=bounds[1])]
-            bounds = get_bounds(NEW_DATA['lat'],(domain['s'],domain['n']))
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lat']>=bounds[0]) & (NEW_DATA['lat']<=bounds[1])]
-        
-        #Apply location filter
+            bounds = get_bounds(
+                NEW_DATA['lon'] % 360, (domain['w'] % 360, domain['e'] % 360))
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lon'] % 360 >= bounds[0]) & (
+                NEW_DATA['lon'] % 360 <= bounds[1])]
+            bounds = get_bounds(NEW_DATA['lat'], (domain['s'], domain['n']))
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lat'] >= bounds[0]) & (
+                NEW_DATA['lat'] <= bounds[1])]
+
+        # Apply location filter
         if location is not None:
-            NEW_DATA = NEW_DATA.loc[NEW_DATA['location']==location.upper()]
-        
-        #Apply top standard level filter
+            NEW_DATA = NEW_DATA.loc[NEW_DATA['location'] == location.upper()]
+
+        # Apply top standard level filter
         if top is not None:
-            bounds = get_bounds(NEW_DATA['top'],top)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['top']>=bounds[0]) & (NEW_DATA['top']<=bounds[1])]
-        
-        #Apply surface pressure filter
+            bounds = get_bounds(NEW_DATA['top'], top)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['top'] >= bounds[0]) & (
+                NEW_DATA['top'] <= bounds[1])]
+
+        # Apply surface pressure filter
         if slp is not None:
-            bounds = get_bounds(NEW_DATA['slp'],slp)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['slp']>=bounds[0]) & (NEW_DATA['slp']<=bounds[1])]
-            
-        #Apply MBL wind speed filter
+            bounds = get_bounds(NEW_DATA['slp'], slp)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['slp'] >= bounds[0]) & (
+                NEW_DATA['slp'] <= bounds[1])]
+
+        # Apply MBL wind speed filter
         if MBLspd is not None:
-            bounds = get_bounds(NEW_DATA['MBLspd'],MBLspd)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['MBLspd']>=bounds[0]) & (NEW_DATA['MBLspd']<=bounds[1])]
-            
-        #Apply DLM wind speed filter
+            bounds = get_bounds(NEW_DATA['MBLspd'], MBLspd)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['MBLspd'] >= bounds[0]) & (
+                NEW_DATA['MBLspd'] <= bounds[1])]
+
+        # Apply DLM wind speed filter
         if DLMspd is not None:
-            bounds = get_bounds(NEW_DATA['DLMspd'],DLMspd)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['DLMspd']>=bounds[0]) & (NEW_DATA['DLMspd']<=bounds[1])]
-            
-        #Apply WL150 wind speed filter
+            bounds = get_bounds(NEW_DATA['DLMspd'], DLMspd)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['DLMspd'] >= bounds[0]) & (
+                NEW_DATA['DLMspd'] <= bounds[1])]
+
+        # Apply WL150 wind speed filter
         if WL150spd is not None:
-            bounds = get_bounds(NEW_DATA['WL150spd'],WL150spd)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['WL150spd']>=bounds[0]) & (NEW_DATA['WL150spd']<=bounds[1])]
-            
-        NEW_OBJ = dropsondes(storm=self.storm,data=list(NEW_DATA.T.to_dict().values()))
-        
+            bounds = get_bounds(NEW_DATA['WL150spd'], WL150spd)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['WL150spd'] >= bounds[0]) & (
+                NEW_DATA['WL150spd'] <= bounds[1])]
+
+        NEW_OBJ = dropsondes(storm=self.storm, data=list(
+            NEW_DATA.T.to_dict().values()))
+
         return NEW_OBJ
-        
-    def to_pickle(self,filename):
+
+    def to_pickle(self, filename):
         r"""
         Save dropsonde data (list of dictionaries) to a pickle file
-        
+
         Parameters
         ----------
         filename : str
             name of file to save pickle file to.
-        
+
         Notes
         -----
         This method saves the dropsondes data as a pickle within the current working directory, given a filename as an argument.
-        
+
         For example, assume ``dropsondes`` was retrieved from a Storm object (using the first method described in the ``dropsondes`` class documentation). The dropsondes data would be saved to a pickle file as follows:
-        
+
         >>> storm.recon.dropsondes.to_pickle(data="mystorm_dropsondes.pickle")
-        
+
         Now the dropsondes data is saved locally, and next time recon data for this storm needs to be analyzed, this allows to bypass re-reading the dropsondes data from the NHC server by providing the pickle file as an argument:
-        
+
         >>> storm.recon.get_dropsondes(data="mystorm_dropsondes.pickle")
-        
+
         """
-        
-        with open(filename,'wb') as f:
-            pickle.dump(self.data,f)
-            
-    def plot_points(self,varname='slp',level=None,domain="dynamic",ax=None,cartopy_proj=None,**kwargs):
-        
+
+        with open(filename, 'wb') as f:
+            pickle.dump(self.data, f)
+
+    def plot_points(self, varname='slp', level=None, domain="dynamic", ax=None, cartopy_proj=None, **kwargs):
         r"""
         Creates a plot of dropsonde data points.
-        
+
         Parameters
         ----------
         varname : str
             Variable to plot. Can be one of the keys in the dropsonde dictionary (retrieved using ``recon.dropsondes.data``).
         level : int, optional
             Pressure level (in hPa) to plot varname for. Only valid if varname is in "pres", "hgt", "temp", "dwpt", "wdir", "wspd".
         domain : str/dict
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of recon plot. Please refer to :ref:`options-prop-recon-plot` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         To retrieve all possible varnames, check the ``data`` attribute of this Dropsonde object. For example, if ReconDataset was retrieved through a Storm object as in the example below, the possible varnames would be retrieved as follows:
-        
+
         .. code-block:: python
-    
+
             from tropycal import tracks
-            
+
             #Get dataset object
             basin = tracks.TrackDataset()
-            
+
             #Get storm object
             storm = basin.get_storm(('michael',2018))
-            
+
             #Get dropsondes for this storm
             storm.recon.get_dropsondes()
-            
+
             #Retrieve list of all possible varnames
             print(storm.recon.dropsondes.data)
         """
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-                
-        #Get plot data
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Get plot data
         if level is not None:
-            plotdata = [m['levels'].loc[m['levels']['pres']==level][varname].to_numpy()[0] \
-            if 'levels' in m.keys() and level in m['levels']['pres'].to_numpy() else np.nan \
-            for m in self.data]
+            plotdata = [m['levels'].loc[m['levels']['pres'] == level][varname].to_numpy()[0]
+                        if 'levels' in m.keys() and level in m['levels']['pres'].to_numpy() else np.nan
+                        for m in self.data]
         else:
-            plotdata = [m[varname] if varname in m.keys() else np.nan for m in self.data]
-        
-        #Make sure data doesn't have NaNs
-        check_data = [m['BOTTOMlat'] for m in self.data if np.isnan(m['BOTTOMlat']) == False]
+            plotdata = [m[varname] if varname in m.keys()
+                        else np.nan for m in self.data]
+
+        # Make sure data doesn't have NaNs
+        check_data = [m['BOTTOMlat']
+                      for m in self.data if not np.isnan(m['BOTTOMlat'])]
         if len(check_data) == 0:
-            dfRecon = pd.DataFrame.from_dict({'time':[m['TOPtime'] for m in self.data],
-                                              'lat':[m['TOPlat'] for m in self.data],
-                                              'lon':[m['TOPlon'] for m in self.data],
-                                              varname:plotdata})
+            dfRecon = pd.DataFrame.from_dict({'time': [m['TOPtime'] for m in self.data],
+                                              'lat': [m['TOPlat'] for m in self.data],
+                                              'lon': [m['TOPlon'] for m in self.data],
+                                              varname: plotdata})
         else:
-            dfRecon = pd.DataFrame.from_dict({'time':[m['BOTTOMtime'] for m in self.data],
-                                              'lat':[m['BOTTOMlat'] for m in self.data],
-                                              'lon':[m['BOTTOMlon'] for m in self.data],
-                                              varname:plotdata})
-        
-        #Create instance of plot object
+            dfRecon = pd.DataFrame.from_dict({'time': [m['BOTTOMtime'] for m in self.data],
+                                              'lat': [m['BOTTOMlat'] for m in self.data],
+                                              'lon': [m['BOTTOMlon'] for m in self.data],
+                                              varname: plotdata})
+
+        # Create instance of plot object
         self.plot_obj = ReconPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
             cartopy_proj = self.plot_obj.proj
-        
-        #Plot recon
-        plot_ax = self.plot_obj.plot_points(self.storm,dfRecon,domain,varname=(varname,level),ax=ax,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+
+        # Plot recon
+        plot_ax = self.plot_obj.plot_points(self.storm, dfRecon, domain, varname=(
+            varname, level), ax=ax, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
 
-    def plot_skewt(self,time=None):
+    def plot_skewt(self, time=None):
         r"""
         Plot a Skew-T chart for selected dropsondes.
-        
+
         Parameters
         ----------
         time : datetime.datetime, optional
             Time closest to requested Skew-T. If none, all dropsondes will plot.
-        
+
         Returns
         -------
         list
             Returns a list of figures, or a single figure for a single plot.
         """
         storm_data = self.storm.dict
 
         if time is None:
             dict_list = self.data
         else:
             dict_list = self.sel(time=time).data
-    
-        #Format storm name
+
+        # Format storm name
         storm_data = self.storm.dict
         type_array = np.array(storm_data['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
-        if ('invest' in storm_data.keys() and storm_data['invest'] == False) or len(idx[0]) > 0:
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
+        if ('invest' in storm_data.keys() and not storm_data['invest']) or len(idx[0]) > 0:
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             add_ptc_flag = False
             if len(tropical_vmax) == 0:
                 add_ptc_flag = True
                 idx = np.where((type_array == 'LO') | (type_array == 'DB'))
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             subtrop = classify_subtropical(np.array(storm_data['type']))
             peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
             peak_basin = storm_data['wmo_basin'][peak_idx]
-            storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-            if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
+            storm_type = get_storm_classification(
+                np.nanmax(tropical_vmax), subtrop, peak_basin)
+            if add_ptc_flag:
+                storm_type = "Potential Tropical Cyclone"
         title_string = f'{storm_type} {storm_data["name"]}\nDropsonde DDD, Mission MMM'
-        
-        #Plot Skew-T
-        return plot_skewt(dict_list,title_string)
-    
+
+        # Plot Skew-T
+        return plot_skewt(dict_list, title_string)
+
+
 class vdms:
-    
+
     r"""
     Creates an instance of a VDMs object containing all Vortex Data Message (VDM) data for a single storm.
-    
+
     Parameters
     ----------
     storm : tropycal.tracks.Storm
         Requested storm.
     data : str, optional
         Filepath of pickle file containing VDM data retrieved from ``vdms.to_pickle()``. If provided, data will be retrieved from the local pickle file instead of the NHC server.
     update : bool
         True = search for new data, following existing data in the dropsonde object, and concatenate.
 
     Returns
     -------
     Dataset
         An instance of VDMs.
-    
+
     Notes
     -----
     .. warning::
-    
+
         Recon data is currently only available from 1989 onwards.
-    
+
     VDM data is currently retrieved from the National Hurricane Center from 2006 onwards, and UCAR from 1989 through 2005.
-    
+
     There are two recommended ways of retrieving a vdms object. Since the ``ReconDataset``, ``hdobs``, ``dropsondes`` and ``vdms`` classes are **storm-centric**, a Storm object is required for both methods.
-    
+
     .. code-block:: python
-    
+
         #Retrieve Hurricane Michael (2018) from TrackDataset
         basin = tracks.TrackDataset()
         storm = basin.get_storm(('michael',2018))
-    
+
     The first method is to use the empty instance of ReconDataset already initialized in the Storm object, which has a ``get_vdms()`` method thus allowing all of the vdms attributes and methods to be accessed from the Storm object. As a result, a Storm object does not need to be provided as an argument.
-    
+
     .. code-block:: python
-    
+
         #Retrieve all VDMs for this storm
         storm.recon.get_vdms()
-        
+
         #Retrieve the raw VDM data
         storm.recon.vdms.data
-        
+
         #Use the plot_points() method of hdobs
         storm.recon.vdms.plot_points()
-    
+
     The second method is to use the vdms class independently of the other recon classes:
-    
+
     .. code-block:: python
-    
+
         from tropycal.recon import vdms
-        
+
         #Retrieve all VDMs for this storm, passing the Storm object as an argument
         vdms_obj = vdms(storm)
-        
+
         #Retrieve the raw VDM data
         vdms_obj.data
-        
+
         #Use the plot_points() method of vdms
         vdms_obj.plot_points()
     """
-    
+
     def __repr__(self):
         summary = ["<tropycal.recon.vdms>"]
 
-        #Find maximum wind and minimum pressure
-        time_range = (np.nanmin([i['time'] for i in self.data]),np.nanmax([i['time'] for i in self.data]))
+        # Find maximum wind and minimum pressure
+        time_range = (np.nanmin([i['time'] for i in self.data]), np.nanmax(
+            [i['time'] for i in self.data]))
         time_range = list(set(time_range))
-        min_slp = np.nanmin([i['Minimum Sea Level Pressure (hPa)'] for i in self.data])
+        min_slp = np.nanmin([i['Minimum Sea Level Pressure (hPa)']
+                            for i in self.data])
         min_slp = 'N/A' if np.isnan(min_slp) else min_slp
         missions = set([i['mission'] for i in self.data])
-        
-        #Add general summary
+
+        # Add general summary
         emdash = '\u2014'
-        summary_keys = {'Storm':f'{self.storm.name} {self.storm.year}',\
-                        'Missions':len(missions),
-                        'VDMs':len(self.data),
-                        'Min sea level pressure':f"{min_slp} hPa",
-                        'Source':self.source}
+        summary_keys = {
+            'Storm': f'{self.storm.name} {self.storm.year}',
+            'Missions': len(missions),
+            'VDMs': len(self.data),
+            'Min sea level pressure': f"{min_slp} hPa",
+            'Source': self.source
+        }
 
-        #Add dataset summary
+        # Add dataset summary
         summary.append("Dataset Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
         return "\n".join(summary)
-            
+
     def __init__(self, storm, data=None, update=False):
 
         self.storm = storm
         self.source = 'National Hurricane Center (NHC)'
         if storm.year >= 2006:
             self.format = 1
             if storm.basin == 'north_atlantic':
@@ -2475,153 +2683,160 @@
                 archive_url = f'https://www.nhc.noaa.gov/archive/recon/{self.storm.year}/REPPN2/'
         elif storm.year >= 1989:
             self.format = 2
             self.source = "UCAR's Tropical Cyclone Guidance Project (TCGP)"
             archive_url = f'http://hurricanes.ral.ucar.edu/structure/vortex/vdm_data/{self.storm.year}/'
         else:
             raise RuntimeError("Recon data is not available prior to 1989.")
-        
-        timestr = [f'{t:%Y%m%d}' for t in self.storm.dict['date']]
 
-        #Retrieve list of files in URL and filter by storm dates
+        timestr = [f'{t:%Y%m%d}' for t in self.storm.dict['time']]
+
+        # Retrieve list of files in URL and filter by storm dates
         page = requests.get(archive_url).text
         content = page.split("\n")
         files = []
         for line in content:
-            if ".txt" in line: files.append(((line.split('txt">')[1]).split("</a>")[0]).split("."))
+            if ".txt" in line:
+                files.append(
+                    ((line.split('txt">')[1]).split("</a>")[0]).split("."))
         del content
         if self.format == 1:
-            files = sorted([i for i in files if i[1][:8] in timestr],key=lambda x: x[1])
-            linksub = [archive_url+'.'.join(l) for l in files]
+            files = sorted([i for i in files if i[1][:8]
+                           in timestr], key=lambda x: x[1])
+            linksub = [archive_url + '.'.join(l) for l in files]
         elif self.format == 2:
             files = [f[0] for f in files]
-            linksub = [archive_url+l for l in files if storm.name.upper() in l]
-        
+            linksub = [archive_url +
+                       l for l in files if storm.name.upper() in l]
+
         self.data = []
 
         if data is None:
-            
+
             urllib3.disable_warnings()
             http = urllib3.PoolManager()
-            
+
             filecount = 0
             timer_start = dt.now()
-            print(f'Searching through recon VDM files between {timestr[0]} and {timestr[-1]} ...')
+            print(
+                f'Searching through recon VDM files between {timestr[0]} and {timestr[-1]} ...')
             for link in linksub:
-                response = http.request('GET',link)
+                response = http.request('GET', link)
                 content = response.data.decode('utf-8')
-                
-                #Parse with NHC format
+
+                # Parse with NHC format
                 if self.format == 1:
                     try:
                         date = link.split('.')[-2]
-                        date = dt(int(date[:4]),int(date[4:6]),int(date[6:8]))
-                        missionname,tmp = decode_vdm(content,date)
+                        date = dt(int(date[:4]), int(
+                            date[4:6]), int(date[6:8]))
+                        missionname, tmp = decode_vdm(content, date)
                     except:
                         continue
-                    
-                    testkeys = ('time','lat','lon')
-                    if missionname[2:5] == self.storm.operational_id[2:4]+self.storm.operational_id[0]:
+
+                    testkeys = ('time', 'lat', 'lon')
+                    if missionname[2:5] == self.storm.operational_id[2:4] + self.storm.operational_id[0]:
                         if self.data is None:
                             self.data = [copy.copy(tmp)]
-                            filecount+=1
+                            filecount += 1
                         elif [tmp[k] for k in testkeys] not in [[d[k] for k in testkeys] for d in self.data]:
                             self.data.append(tmp)
-                            filecount+=1
+                            filecount += 1
                         else:
                             pass
-                
-                #Parse with UCAR format
+
+                # Parse with UCAR format
                 elif self.format == 2:
                     content_split = content.split("URNT12")
                     content_split = ['URNT12' + i for i in content_split]
                     for iter_content in content_split:
-                        
+
                         try:
-                            #Check for line length
+                            # Check for line length
                             iter_split = iter_content.split("\n")
-                            if len(iter_split) < 10: continue
+                            if len(iter_split) < 10:
+                                continue
 
-                            #Check for date
+                            # Check for date
                             for line in iter_split:
-                                if line[:2] == 'A.': day = int((line[3:].split('/'))[0])
-                            for iter_date in storm.dict['date']:
+                                if line[:2] == 'A.':
+                                    day = int((line[3:].split('/'))[0])
+                            for iter_date in storm.dict['time']:
                                 found_date = False
                                 if iter_date.day == day:
-                                    date = dt(iter_date.year,iter_date.month,iter_date.day)
+                                    date = dt(iter_date.year,
+                                              iter_date.month, iter_date.day)
                                     found_date = True
                                     break
-                            if found_date == False: continue
+                            if not found_date:
+                                continue
 
-                            #Decode VDMs
-                            missionname,tmp = decode_vdm(iter_content,date)
+                            # Decode VDMs
+                            missionname, tmp = decode_vdm(iter_content, date)
 
-                            testkeys = ('time','lat','lon')
+                            testkeys = ('time', 'lat', 'lon')
                             if self.data is None:
                                 self.data = [copy.copy(tmp)]
-                                filecount+=1
+                                filecount += 1
                             elif [tmp[k] for k in testkeys] not in [[d[k] for k in testkeys] for d in self.data]:
                                 self.data.append(tmp)
-                                filecount+=1
+                                filecount += 1
                             else:
                                 pass
-                        
+
                         except:
                             continue
 
-                
-            print(f'--> Completed reading in recon VDM files ({(dt.now()-timer_start).total_seconds():.1f} seconds)'+\
+            print(f'--> Completed reading in recon VDM files ({(dt.now()-timer_start).total_seconds():.1f} seconds)' +
                   f'\nRead {filecount} files')
-            
-        elif isinstance(data,str):
+
+        elif isinstance(data, str):
             with open(data, 'rb') as f:
                 self.data = pickle.load(f)
         else:
             self.data = data
         self.keys = sorted(list(set([k for d in self.data for k in d.keys()])))
 
     def update(self):
         r"""
         Update with the latest data for an ongoing storm.
-        
+
         Notes
         -----
         This function has no return value, but simply updates the internal VDM data with new observations since the object was created.
         """
-        
-        newobj = vdms(storm=self.storm,data=self.data,update=True)
+
+        newobj = vdms(storm=self.storm, data=self.data, update=True)
         return newobj
-        
-    def isel(self,index):
-        
+
+    def isel(self, index):
         r"""
         Select a single VDM by index of the list.
-        
+
         Parameters
         ----------
         index : int
             Integer containing the index of the dropsonde.
-        
+
         Returns
         -------
         vdms
             Instance of VDMs for the single requested VDM.
         """
-        
+
         NEW_DATA = copy.copy(self.data)
         NEW_DATA = [NEW_DATA[index]]
-        NEW_OBJ = vdms(storm = self.storm, data = NEW_DATA)
-        
+        NEW_OBJ = vdms(storm=self.storm, data=NEW_DATA)
+
         return NEW_OBJ
 
-    def sel(self,mission=None,time=None,domain=None):
-        
+    def sel(self, mission=None, time=None, domain=None):
         r"""
         Select a subset of VDMs by any of its parameters and return a new vdms object.
-        
+
         Parameters
         ----------
         mission : str
             Mission name (number + storm id), e.g. mission 7 for AL05 is '0705L'
         time : list/tuple of datetimes
             list/tuple of start time and end time datetime objects.
             Default is None, which returns all points
@@ -2631,205 +2846,225 @@
         Returns
         -------
         vdms object
             A new vdms object that satisfies the intersection of all subsetting.
         """
 
         NEW_DATA = copy.copy(pd.DataFrame(self.data))
-        
-        #Apply mission filter
+
+        # Apply mission filter
         if mission is not None:
             mission = str(mission)
-            NEW_DATA = NEW_DATA.loc[NEW_DATA['mission']==mission]
+            NEW_DATA = NEW_DATA.loc[NEW_DATA['mission'] == mission]
 
-        #Apply time filter
+        # Apply time filter
         if time is not None:
-            bounds = get_bounds(NEW_DATA['time'],time)
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['time']>=bounds[0]) & (NEW_DATA['time']<=bounds[1])]
-        
-        #Apply domain filter
+            bounds = get_bounds(NEW_DATA['time'], time)
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['time'] >= bounds[0]) & (
+                NEW_DATA['time'] <= bounds[1])]
+
+        # Apply domain filter
         if domain is not None:
-            tmp = {k[0].lower():v for k,v in domain.items()}
-            domain = {'n':90,'s':-90,'e':359.99,'w':0}
+            tmp = {k[0].lower(): v for k, v in domain.items()}
+            domain = {'n': 90, 's': -90, 'e': 359.99, 'w': 0}
             domain.update(tmp)
-            bounds = get_bounds(NEW_DATA['lon']%360,(domain['w']%360,domain['e']%360))
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lon']%360>=bounds[0]) & (NEW_DATA['lon']%360<=bounds[1])]
-            bounds = get_bounds(NEW_DATA['lat'],(domain['s'],domain['n']))
-            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lat']>=bounds[0]) & (NEW_DATA['lat']<=bounds[1])]
-        
-        NEW_OBJ = vdms(storm=self.storm,data=list(NEW_DATA.T.to_dict().values()))
-        
+            bounds = get_bounds(
+                NEW_DATA['lon'] % 360, (domain['w'] % 360, domain['e'] % 360))
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lon'] % 360 >= bounds[0]) & (
+                NEW_DATA['lon'] % 360 <= bounds[1])]
+            bounds = get_bounds(NEW_DATA['lat'], (domain['s'], domain['n']))
+            NEW_DATA = NEW_DATA.loc[(NEW_DATA['lat'] >= bounds[0]) & (
+                NEW_DATA['lat'] <= bounds[1])]
+
+        NEW_OBJ = vdms(storm=self.storm, data=list(
+            NEW_DATA.T.to_dict().values()))
+
         return NEW_OBJ
-    
-    def to_pickle(self,filename):
-        
+
+    def to_pickle(self, filename):
         r"""
         Save VDM data (list of dictionaries) to a pickle file
-        
+
         Parameters
         ----------
         filename : str
             name of file to save pickle file to.
-        
+
         Notes
         -----
         This method saves the VDMs data as a pickle within the current working directory, given a filename as an argument.
-        
+
         For example, assume ``vdms`` was retrieved from a Storm object (using the first method described in the ``vdms`` class documentation). The VDMs data would be saved to a pickle file as follows:
-        
+
         >>> storm.recon.vdms.to_pickle(data="mystorm_vdms.pickle")
-        
+
         Now the VDMs data is saved locally, and next time recon data for this storm needs to be analyzed, this allows to bypass re-reading the VDMs data from the NHC server by providing the pickle file as an argument:
-        
+
         >>> storm.recon.get_vdms("mystorm_vdms.pickle")
-        
+
         """
-        
-        with open(filename,'wb') as f:
-            pickle.dump(self.data,f)
-     
-    def plot_time_series(self,time=None,best_track=False,dots=True):
-        
+
+        with open(filename, 'wb') as f:
+            pickle.dump(self.data, f)
+
+    def plot_time_series(self, time=None, best_track=False, dots=True):
         r"""
         Creates a time series of MSLP VDM data.
-        
+
         Parameters
         ----------
         time : tuple, optional
             Tuple of start and end datetime.datetime objects for plot. If None, all times will be plotted.
         best_track : bool, optional
             If True, Best Track MSLP will be plotted alongside VDM MSLP. Default is False.
         dots : bool, optional
             If True, dots will be plotted for each VDM point. Default is True.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve data
+
+        # Retrieve data
         storm_data = self.storm.dict
         data = self.data
-        
-        #Retrive data and subset by time
+
+        # Retrive data and subset by time
         if time is not None:
-            times = [i['time'] for i in data if i['time'] >= time[0] and i['time'] <= time[1]]
-            mslp = [i['Minimum Sea Level Pressure (hPa)'] for i in data if i['time'] >= time[0] and i['time'] <= time[1]]
+            times = [i['time'] for i in data if i['time']
+                     >= time[0] and i['time'] <= time[1]]
+            mslp = [i['Minimum Sea Level Pressure (hPa)']
+                    for i in data if i['time'] >= time[0] and i['time'] <= time[1]]
         else:
             times = [i['time'] for i in data]
             mslp = [i['Minimum Sea Level Pressure (hPa)'] for i in data]
 
-        #Create figure
-        fig,ax = plt.subplots(figsize=(9,6),dpi=200)
+        # Create figure
+        fig, ax = plt.subplots(figsize=(9, 6), dpi=200)
         ax.grid()
 
-        #Plot VDM MSLP
-        ax.plot(times,mslp,color='b',alpha=0.5,label='VDM MSLP (hPa)')
-        if dots: ax.plot(times,mslp,'o',color='b')
-        
-        #Retrieve & plot Best Track data
+        # Plot VDM MSLP
+        ax.plot(times, mslp, color='b', alpha=0.5, label='VDM MSLP (hPa)')
+        if dots:
+            ax.plot(times, mslp, 'o', color='b')
+
+        # Retrieve & plot Best Track data
         if best_track:
             if time is not None:
-                times_btk = [i for i in storm_data['date'] if i >= time[0] and i <= time[1]]
-                mslp_btk = [storm_data['mslp'][i] for i in range(len(storm_data['mslp'])) if storm_data['date'][i] >= time[0] and storm_data['date'][i] <= time[1]]
+                times_btk = [i for i in storm_data['time']
+                             if i >= time[0] and i <= time[1]]
+                mslp_btk = [storm_data['mslp'][i] for i in range(len(
+                    storm_data['mslp'])) if storm_data['time'][i] >= time[0] and storm_data['time'][i] <= time[1]]
             else:
-                times_btk = [i for i in storm_data['date']]
+                times_btk = [i for i in storm_data['time']]
                 mslp_btk = [i for i in storm_data['mslp']]
-            ax.plot(times_btk,mslp_btk,color='r',alpha=0.25,label='Best Track MSLP (hPa)')
-            if dots: ax.plot(times_btk,mslp_btk,'o',color='r',alpha=0.5)
+            ax.plot(times_btk, mslp_btk, color='r',
+                    alpha=0.25, label='Best Track MSLP (hPa)')
+            if dots:
+                ax.plot(times_btk, mslp_btk, 'o', color='r', alpha=0.5)
 
-        #Add labels
+        # Add labels
         ax.set_ylabel("MSLP (hPa)")
         ax.set_xlabel("Vortex Data Message time (UTC)")
 
-        #Add time labels
+        # Add time labels
         times_use = []
-        start_date = times[0].replace(hour=0)
-        total_days = (times[-1] - start_date).total_seconds() / 86400
+        start_time = times[0].replace(hour=0)
+        total_days = (times[-1] - start_time).total_seconds() / 86400
         increment_hour = 6
-        if total_days > 3: increment_hour = 12
-        if total_days > 6: increment_hour = 24
-        while start_date <= (times[-1] + timedelta(hours=increment_hour)):
-            times_use.append(start_date)
-            start_date += timedelta(hours=increment_hour)
+        if total_days > 3:
+            increment_hour = 12
+        if total_days > 6:
+            increment_hour = 24
+        while start_time <= (times[-1] + timedelta(hours=increment_hour)):
+            times_use.append(start_time)
+            start_time += timedelta(hours=increment_hour)
         ax.set_xticks(times_use)
-        ax.set_xlim(times[0]-timedelta(hours=6),times[-1]+timedelta(hours=6))
+        ax.set_xlim(times[0] - timedelta(hours=6),
+                    times[-1] + timedelta(hours=6))
         ax.xaxis.set_major_formatter(mdates.DateFormatter('%H UTC\n%b %d'))
 
-        #Add titles
+        # Add titles
         type_array = np.array(storm_data['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
-        if ('invest' in storm_data.keys() and storm_data['invest'] == False) or len(idx[0]) > 0:
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
+        if ('invest' in storm_data.keys() and not storm_data['invest']) or len(idx[0]) > 0:
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             add_ptc_flag = False
             if len(tropical_vmax) == 0:
                 add_ptc_flag = True
                 idx = np.where((type_array == 'LO') | (type_array == 'DB'))
             tropical_vmax = np.array(storm_data['vmax'])[idx]
 
             subtrop = classify_subtropical(np.array(storm_data['type']))
             peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
             peak_basin = storm_data['wmo_basin'][peak_idx]
-            storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-            if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
+            storm_type = get_storm_classification(
+                np.nanmax(tropical_vmax), subtrop, peak_basin)
+            if add_ptc_flag:
+                storm_type = "Potential Tropical Cyclone"
 
-        if best_track: ax.legend()
-        ax.set_title(f'{storm_type} {storm_data["name"]}\nVDM Minimum Sea Level Pressure (hPa)',loc='left',fontweight='bold')
+        if best_track:
+            ax.legend()
+        ax.set_title(
+            f'{storm_type} {storm_data["name"]}\nVDM Minimum Sea Level Pressure (hPa)', loc='left', fontweight='bold')
 
         return ax
-    
-    def plot_points(self,varname='Minimum Sea Level Pressure (hPa)',domain="dynamic",ax=None,cartopy_proj=None,**kwargs):
-        
+
+    def plot_points(self, varname='Minimum Sea Level Pressure (hPa)', domain="dynamic", ax=None, cartopy_proj=None, **kwargs):
         r"""
         Creates a plot of recon data points.
-        
+
         Parameters
         ----------
         varname : str
             Variable to plot. Currently the best option is "Minimum Sea Level Pressure (hPa)".
         domain : str
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of recon plot. Please refer to :ref:`options-prop-recon-plot` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-                
-        #Get plot data
-        plotdata = [m[varname] if varname in m.keys() else np.nan for m in self.data]
-            
-        dfRecon = pd.DataFrame.from_dict({'time':[m['time'] for m in self.data],\
-                                          'lat':[m['lat'] for m in self.data],\
-                                          'lon':[m['lon'] for m in self.data],\
-                                          varname:plotdata})
-        
-        #Create instance of plot object
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Get plot data
+        plotdata = [m[varname] if varname in m.keys()
+                    else np.nan for m in self.data]
+
+        dfRecon = pd.DataFrame.from_dict({'time': [m['time'] for m in self.data],
+                                          'lat': [m['lat'] for m in self.data],
+                                          'lon': [m['lon'] for m in self.data],
+                                          varname: plotdata})
+
+        # Create instance of plot object
         self.plot_obj = ReconPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
             cartopy_proj = self.plot_obj.proj
-        
-        #Plot recon
-        plot_ax = self.plot_obj.plot_points(self.storm,dfRecon,domain,varname=varname,ax=ax,prop=prop,map_prop=map_prop)        
-        
-        #Return axis
+
+        # Plot recon
+        plot_ax = self.plot_obj.plot_points(
+            self.storm, dfRecon, domain, varname=varname, ax=ax, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
```

### Comparing `tropycal-0.6.1/src/tropycal/recon/plot.py` & `tropycal-1.0/src/tropycal/recon/plot.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,54 +1,49 @@
-import calendar
 import numpy as np
 import pandas as pd
-import re
-import scipy.interpolate as interp
-import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt
 from scipy.ndimage import gaussian_filter as gfilt
 import copy
 
 from ..plot import Plot
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 
 try:
-    import cartopy.feature as cfeature
     from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
-except:
-    warnings.warn("Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
 
 try:
     import matplotlib as mlib
     import matplotlib.lines as mlines
     import matplotlib.patheffects as patheffects
     import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
     import matplotlib.patches as mpatches
     import matplotlib.gridspec as gridspec
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+
 
 class ReconPlot(Plot):
-    
+
     def __init__(self):
-        
+
         self.use_credit = True
-                 
-    def plot_points(self,storm,recon_data,domain="dynamic",varname='wspd',radlim=None,barbs=False,scatter=False,\
-                    ax=None,return_domain=False,prop={},map_prop={},mission=False,vdms=[],mission_id=''):
-        
+
+    def plot_points(self, storm, recon_data, domain="dynamic", varname='wspd', radlim=None, barbs=False, scatter=False,
+                    ax=None, return_domain=False, prop={}, map_prop={}, mission=False, vdms=[], mission_id=''):
         r"""
         Creates a plot of recon data points
-        
+
         Parameters
         ----------
         recon_data : dataframe
             Recon data, must be dataframe
         domain : str
             Domain for the plot. Can be one of the following:
             "dynamic" - default. Dynamically focuses the domain using the tornado track(s) plotted.
@@ -58,832 +53,920 @@
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
+
         if not barbs and not scatter:
             scatter = True
-        
-        if isinstance(varname,tuple):
+
+        if isinstance(varname, tuple):
             titleinput = copy.copy(varname)
             varname = varname[0]
         else:
-            titleinput = (varname,None)
-        
-        #Set default properties
-        default_prop={'cmap':'category','levels':(np.nanmin(recon_data[varname]),np.nanmax(recon_data[varname])),\
-                      'sortby':varname,'ascending':(varname!='p_sfc'),'linewidth':1.5,'ms':7.5,'marker':'o','zorder':None}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k',
-                          'figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #set default properties
+            titleinput = (varname, None)
+
+        # Set default properties
+        default_prop = {'cmap': 'category', 'levels': (np.nanmin(recon_data[varname]), np.nanmax(recon_data[varname])),
+                        'sortby': varname, 'ascending': (varname != 'p_sfc'), 'linewidth': 1.5, 'ms': 7.5, 'marker': 'o', 'zorder': None}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF', 'linewidth': 0.5, 'linecolor': 'k',
+                            'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # set default properties
         input_prop = prop
         input_map_prop = map_prop
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # --------------------------------------------------------------------------------------
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = None
         min_lat = None
         max_lon = None
         min_lon = None
 
-        #Retrieve storm data
+        # Retrieve storm data
         storm_data = storm.dict
 
-        #Check recon_data type
-        if isinstance(recon_data,pd.core.frame.DataFrame):
+        # Check recon_data type
+        if isinstance(recon_data, pd.core.frame.DataFrame):
             pass
         else:
             raise RuntimeError("Error: recon_data must be dataframe")
-        
-        #Retrieve storm data
-        if radlim == None:
+
+        # Retrieve storm data
+        if radlim is None:
             lats = recon_data['lat']
             lons = recon_data['lon']
         else:
-            temp_df = recon_data.loc[recon_data['distance']<=radlim]
+            temp_df = recon_data.loc[recon_data['distance'] <= radlim]
             lats = temp_df['lat']
             lons = temp_df['lon']
 
-        #Add to coordinate extrema
+        # Add to coordinate extrema
         if max_lat is None:
             max_lat = max(lats)
         else:
-            if max(lats) > max_lat: max_lat = max(lats)
+            if max(lats) > max_lat:
+                max_lat = max(lats)
         if min_lat is None:
             min_lat = min(lats)
         else:
-            if min(lats) < min_lat: min_lat = min(lats)
+            if min(lats) < min_lat:
+                min_lat = min(lats)
         if max_lon is None:
             max_lon = max(lons)
         else:
-            if max(lons) > max_lon: max_lon = max(lons)
+            if max(lons) > max_lon:
+                max_lon = max(lons)
         if min_lon is None:
             min_lon = min(lons)
         else:
-            if min(lons) < min_lon: min_lon = min(lons)
+            if min(lons) < min_lon:
+                min_lon = min(lons)
 
-        #Get colormap and level extrema
-        cmap,clevs = get_cmap_levels(varname,prop['cmap'],prop['levels'])
-        if varname in ['vmax','sfmr','wspd','fl_to_sfc'] and prop['cmap'] in ['category','category_recon']:
-            vmin = min(clevs); vmax = max(clevs)
-        else:
-            vmin = min(prop['levels']); vmax = max(prop['levels'])
-        
-        #Plot recon data as specified
+        # Get colormap and level extrema
+        cmap, clevs = get_cmap_levels(varname, prop['cmap'], prop['levels'])
+        if varname in ['vmax', 'sfmr', 'wspd', 'fl_to_sfc'] and prop['cmap'] in ['category', 'category_recon']:
+            vmin = min(clevs)
+            vmax = max(clevs)
+        else:
+            vmin = min(prop['levels'])
+            vmax = max(prop['levels'])
+
+        # Plot recon data as specified
         if barbs:
-            dataSort = recon_data.sort_values(by=prop['sortby']).reset_index(drop=True)
-            if radlim is not None: dataSort = dataSort.loc[dataSort['distance']<=radlim]
+            dataSort = recon_data.sort_values(
+                by=prop['sortby']).reset_index(drop=True)
+            if radlim is not None:
+                dataSort = dataSort.loc[dataSort['distance'] <= radlim]
             norm = mlib.colors.Normalize(vmin=vmin, vmax=vmax)
             colors = cmap(norm(dataSort[prop['sortby']].values))
             colors = [tuple(i) for i in colors]
-            qv = plt.barbs(dataSort['lon'],dataSort['lat'],
-                           *uv_from_wdir(dataSort[prop['sortby']],dataSort['wdir']),color=colors,length=5,linewidth=0.5)
+            qv = plt.barbs(dataSort['lon'], dataSort['lat'],
+                           *uv_from_wdir(dataSort[prop['sortby']], dataSort['wdir']), color=colors, length=5, linewidth=0.5)
             cbmap = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
             cbmap.set_array([])
 
         if scatter:
-            dataSort = recon_data.sort_values(by=prop['sortby'],ascending=prop['ascending']).reset_index(drop=True)
-            if radlim is not None: dataSort = dataSort.loc[dataSort['distance']<=radlim]
-            cbmap = plt.scatter(dataSort['lon'],dataSort['lat'],c=dataSort[varname],
-                                cmap=cmap,vmin=vmin,vmax=vmax, s=prop['ms'], marker=prop['marker'],zorder=prop['zorder'])
-        
-        #Plot latest point if from a Mission object
-        if mission: plt.plot(recon_data['lon'].values[-1],recon_data['lat'].values[-1],'o',mfc='none',mec='k',mew=1.5,ms=10)
+            dataSort = recon_data.sort_values(
+                by=prop['sortby'], ascending=prop['ascending']).reset_index(drop=True)
+            if radlim is not None:
+                dataSort = dataSort.loc[dataSort['distance'] <= radlim]
+            cbmap = plt.scatter(dataSort['lon'], dataSort['lat'], c=dataSort[varname],
+                                cmap=cmap, vmin=vmin, vmax=vmax, s=prop['ms'], marker=prop['marker'], zorder=prop['zorder'])
+
+        # Plot latest point if from a Mission object
+        if mission:
+            plt.plot(recon_data['lon'].values[-1], recon_data['lat'].values[-1],
+                     'o', mfc='none', mec='k', mew=1.5, ms=10)
 
-        #Plot VDMs
+        # Plot VDMs
         if len(vdms) > 0:
             for vdm_idx in range(len(vdms)):
                 vdm = vdms[vdm_idx]
 
-                #Transform coordinates for label
+                # Transform coordinates for label
                 try:
-                    a = self.ax.text(vdm['lon'],vdm['lat'],str(int(np.round(vdm['Minimum Sea Level Pressure (hPa)']))),zorder=30,clip_on=True,fontsize=12,fontweight='bold',ha='center',va='center')
-                    a.set_path_effects([patheffects.Stroke(linewidth=0.5,foreground='w'),patheffects.Normal()])
+                    a = self.ax.text(vdm['lon'], vdm['lat'], str(int(np.round(vdm['Minimum Sea Level Pressure (hPa)']))),
+                                     zorder=30, clip_on=True, fontsize=12, fontweight='bold', ha='center', va='center')
+                    a.set_path_effects(
+                        [patheffects.Stroke(linewidth=0.5, foreground='w'), patheffects.Normal()])
                 except:
                     pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Storm-centered plot domain
+
+        # --------------------------------------------------------------------------------------
+
+        # Storm-centered plot domain
         if domain == "dynamic":
-            
-            bound_w,bound_e,bound_s,bound_n = dynamic_map_extent(min_lon,max_lon,min_lat,max_lat,recon=True)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
-        else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Determine number of lat/lon lines to use for parallels & meridians
+
+            bound_w, bound_e, bound_s, bound_n = dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat, recon=True)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
+        else:
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Determine number of lat/lon lines to use for parallels & meridians
         if map_prop['plot_gridlines']:
-            self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
+            self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
+
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
         type_array = np.array(storm_data['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
         tropical_vmax = np.array(storm_data['vmax'])[idx]
-        
-        #Coerce to include non-TC points if storm hasn't been designated yet
+
+        # Coerce to include non-TC points if storm hasn't been designated yet
         add_ptc_flag = False
         if len(tropical_vmax) == 0:
             add_ptc_flag = True
             idx = np.where((type_array == 'LO') | (type_array == 'DB'))
         tropical_vmax = np.array(storm_data['vmax'])[idx]
-            
+
         subtrop = classify_subtropical(np.array(storm_data['type']))
         peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
         peak_basin = storm_data['wmo_basin'][peak_idx]
-        storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-        if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
-        
+        storm_type = get_storm_classification(
+            np.nanmax(tropical_vmax), subtrop, peak_basin)
+        if add_ptc_flag == True:
+            storm_type = "Potential Tropical Cyclone"
+
         dot = u"\u2022"
         try:
             vartitle = get_recon_title(*titleinput)
         except:
             vartitle = [varname]
-        self.ax.set_title(f"{storm_type} {storm_data['name']}\n" + 'Recon: '+' '.join(vartitle),loc='left',fontsize=17,fontweight='bold')
+        self.ax.set_title(f"{storm_type} {storm_data['name']}\n" + 'Recon: ' + ' '.join(
+            vartitle), loc='left', fontsize=17, fontweight='bold')
         if mission_id != '':
-            self.ax.set_title(f"Mission ID: {mission_id}\nRecon: " + ' '.join(vartitle),loc='left',fontsize=17,fontweight='bold')
+            self.ax.set_title(f"Mission ID: {mission_id}\nRecon: " + ' '.join(
+                vartitle), loc='left', fontsize=17, fontweight='bold')
+
+        # Add right title
+        start_time = dt.strftime(min(recon_data['time']), '%H:%M UTC %d %b %Y')
+        end_time = dt.strftime(max(recon_data['time']), '%H:%M UTC %d %b %Y')
+        self.ax.set_title(
+            f'Start ... {start_time}\nEnd ... {end_time}', loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
 
-        #Add right title
-        start_date = dt.strftime(min(recon_data['time']),'%H:%M UTC %d %b %Y')
-        end_date = dt.strftime(max(recon_data['time']),'%H:%M UTC %d %b %Y')
-        self.ax.set_title(f'Start ... {start_date}\nEnd ... {end_date}',loc='right',fontsize=13)
-
-        #--------------------------------------------------------------------------------------
-        
-        #Add legend
-        
-        #Phantom legend
-        handles=[]
+        # Add legend
+
+        # Phantom legend
+        handles = []
         for _ in range(10):
-            handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-        l = self.ax.legend(handles=handles,loc='upper left',fancybox=True,framealpha=0,fontsize=11.5)
+            handles.append(mlines.Line2D(
+                [], [], linestyle='-', label='', lw=0))
+        l = self.ax.legend(handles=handles, loc='upper left',
+                           fancybox=True, framealpha=0, fontsize=11.5)
         plt.draw()
 
-        #Get the bbox
+        # Get the bbox
         try:
             bb = l.legendPatch.get_bbox().inverse_transformed(self.fig.transFigure)
         except:
             bb = l.legendPatch.get_bbox().transformed(self.fig.transFigure.inverted())
         bb_ax = self.ax.get_position()
 
-        #Define colorbar axis
-        cax = self.fig.add_axes([bb.x0+bb.width, bb.y0-.05*bb.height, 0.015, bb.height])
+        # Define colorbar axis
+        cax = self.fig.add_axes(
+            [bb.x0 + bb.width, bb.y0 - .05 * bb.height, 0.015, bb.height])
 #        cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-        cbar = self.fig.colorbar(cbmap,cax=cax,orientation='vertical',\
+        cbar = self.fig.colorbar(cbmap, cax=cax, orientation='vertical',
                                  ticks=clevs)
-            
-        if len(prop['levels'])>2:
-            cax.yaxis.set_ticks(np.linspace(min(clevs),max(clevs),len(clevs)))
+
+        if len(prop['levels']) > 2:
+            cax.yaxis.set_ticks(np.linspace(
+                min(clevs), max(clevs), len(clevs)))
             cax.yaxis.set_ticklabels(clevs)
         else:
             cax.yaxis.set_ticks(clevs)
         cax.tick_params(labelsize=11.5)
         cax.yaxis.set_ticks_position('left')
-    
+
         rect_offset = 0.0
-        if prop['cmap'] in ['category','category_recon'] and varname in ['sfmr','wspd']:
-            cax.yaxis.set_ticks(np.linspace(min(clevs),max(clevs),len(clevs)))
+        if prop['cmap'] in ['category', 'category_recon'] and varname in ['sfmr', 'wspd']:
+            cax.yaxis.set_ticks(np.linspace(
+                min(clevs), max(clevs), len(clevs)))
             cax.yaxis.set_ticklabels(clevs)
             cax2 = cax.twinx()
             cax2.yaxis.set_ticks_position('right')
-            cax2.yaxis.set_ticks((np.linspace(0,1,len(clevs))[:-1]+np.linspace(0,1,len(clevs))[1:])*.5)
+            cax2.yaxis.set_ticks((np.linspace(0, 1, len(clevs))[
+                                 :-1] + np.linspace(0, 1, len(clevs))[1:]) * .5)
             if prop['cmap'] == 'category':
-                cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+                cax2.set_yticklabels(
+                    ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
             else:
-                cax2.set_yticklabels(['TD','TS','>50 kt','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+                cax2.set_yticklabels(
+                    ['TD', 'TS', '>50 kt', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
             cax2.tick_params('both', length=0, width=0, which='major')
             cax.yaxis.set_ticks_position('left')
-            
+
             rect_offset = 0.7
-            
-        rectangle = mpatches.Rectangle((bb.x0,bb.y0-0.1*bb.height),(1.8+rect_offset)*bb.width,1.1*bb.height,\
-                                       fc = 'w',edgecolor = '0.8',alpha = 0.8,\
+
+        rectangle = mpatches.Rectangle((bb.x0, bb.y0 - 0.1 * bb.height), (1.8 + rect_offset) * bb.width, 1.1 * bb.height,
+                                       fc='w', edgecolor='0.8', alpha=0.8,
                                        transform=self.fig.transFigure, zorder=2)
         self.ax.add_patch(rectangle)
-        
-        #Add plot credit
+
+        # Add plot credit
         text = self.plot_credit()
         self.add_credit(text)
-        
-        #Return axis and domain
+
+        # Return axis and domain
         if return_domain:
-            return self.ax,'/'.join([str(b) for b in [bound_w,bound_e,bound_s,bound_n]])
+            return self.ax, '/'.join([str(b) for b in [bound_w, bound_e, bound_s, bound_n]])
         else:
             return self.ax
 
-    
-    def plot_swath(self,storm,Maps,varname,swathfunc,track_dict,\
-                   domain="dynamic",ax=None,prop={},map_prop={}):
-
-        #Set default properties
-        default_prop={'cmap':'category','levels':None,'left_title':'','right_title':'All storms','pcolor':True}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-                          
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-                
-        #Keep record of lat/lon coordinate extrema
+    def plot_swath(self, storm, Maps, varname, swathfunc, track_dict,
+                   domain="dynamic", ax=None, prop={}, map_prop={}):
+
+        # Set default properties
+        default_prop = {'cmap': 'category', 'levels': None,
+                        'left_title': '', 'right_title': 'All storms', 'pcolor': True}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = None
         min_lat = None
         max_lon = None
         min_lon = None
 
-        #Retrieve recon data
+        # Retrieve recon data
         lats = Maps['center_lat']
         lons = Maps['center_lon']
-        
-        #Retrieve storm data
+
+        # Retrieve storm data
         storm_data = storm.dict
 
-        #Add to coordinate extrema
+        # Add to coordinate extrema
         if max_lat is None:
-            max_lat = max(lats)+2.5
+            max_lat = max(lats) + 2.5
         else:
-            if max(lats) > max_lat: max_lat = max(lats)
+            if max(lats) > max_lat:
+                max_lat = max(lats)
         if min_lat is None:
-            min_lat = min(lats)-2.5
+            min_lat = min(lats) - 2.5
         else:
-            if min(lats) < min_lat: min_lat = min(lats)
+            if min(lats) < min_lat:
+                min_lat = min(lats)
         if max_lon is None:
-            max_lon = max(lons)+2.5
+            max_lon = max(lons) + 2.5
         else:
-            if max(lons) > max_lon: max_lon = max(lons)
+            if max(lons) > max_lon:
+                max_lon = max(lons)
         if min_lon is None:
-            min_lon = min(lons)-2.5
+            min_lon = min(lons) - 2.5
         else:
-            if min(lons) < min_lon: min_lon = min(lons)      
-        
-        bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-                
+            if min(lons) < min_lon:
+                min_lon = min(lons)
+
+        bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+            min_lon, max_lon, min_lat, max_lat)
+
         distproj = ccrs.LambertConformal()
-        out = distproj.transform_points(ccrs.PlateCarree(),np.array([bound_w,bound_w,bound_e,bound_e]),\
-                                        np.array([bound_s,bound_n,bound_s,bound_n]))
-        grid_res = 1*1e3 #m
-        xi = np.arange(int(min(out[:,0])/grid_res)*grid_res,int(max(out[:,0])/grid_res)*grid_res+grid_res,grid_res)
-        yi = np.arange(int(min(out[:,1])/grid_res)*grid_res,int(max(out[:,1])/grid_res)*grid_res+grid_res,grid_res)
-        xmgrid,ymgrid = np.meshgrid(xi,yi)
-        
-        out = distproj.transform_points(ccrs.PlateCarree(),Maps['center_lon'],Maps['center_lat'])
-        
-        cx = np.rint(gfilt(out[:,0],1)/grid_res)*grid_res
-        cy = np.rint(gfilt(out[:,1],1)/grid_res)*grid_res
-        aggregate_grid=np.ones(xmgrid.shape)*np.nan
+        out = distproj.transform_points(ccrs.PlateCarree(), np.array([bound_w, bound_w, bound_e, bound_e]),
+                                        np.array([bound_s, bound_n, bound_s, bound_n]))
+        grid_res = 1 * 1e3  # m
+        xi = np.arange(int(min(out[:, 0]) / grid_res) * grid_res,
+                       int(max(out[:, 0]) / grid_res) * grid_res + grid_res, grid_res)
+        yi = np.arange(int(min(out[:, 1]) / grid_res) * grid_res,
+                       int(max(out[:, 1]) / grid_res) * grid_res + grid_res, grid_res)
+        xmgrid, ymgrid = np.meshgrid(xi, yi)
+
+        out = distproj.transform_points(
+            ccrs.PlateCarree(), Maps['center_lon'], Maps['center_lat'])
+
+        cx = np.rint(gfilt(out[:, 0], 1) / grid_res) * grid_res
+        cy = np.rint(gfilt(out[:, 1], 1) / grid_res) * grid_res
+        aggregate_grid = np.ones(xmgrid.shape) * np.nan
 
-        def nanfunc(func,a,b):
-            c = np.concatenate([a[None],b[None]])
+        def nanfunc(func, a, b):
+            c = np.concatenate([a[None], b[None]])
             c = np.ma.array(c, mask=np.isnan(c))
-            d = func(c,axis=0)
+            d = func(c, axis=0)
             e = d.data
             e[d.mask] = np.nan
             return e
 
-        for t,(x_center,y_center,var) in enumerate(zip(cx,cy,Maps['maps'])):
-            x_fromc = x_center+Maps['grid_x']*1e3
-            y_fromc = y_center+Maps['grid_y']*1e3
-            inrecon = np.where((xmgrid>=np.min(x_fromc)) & (xmgrid<=np.max(x_fromc)) & \
-                           (ymgrid>=np.min(y_fromc)) & (ymgrid<=np.max(y_fromc)))
-            inmap = np.where((x_fromc>=np.min(xmgrid)) & (x_fromc<=np.max(xmgrid)) & \
-                           (y_fromc>=np.min(ymgrid)) & (y_fromc<=np.max(ymgrid)))
-            aggregate_grid[inrecon] = nanfunc(swathfunc,aggregate_grid[inrecon],var[inmap])
-        
-    
+        for t, (x_center, y_center, var) in enumerate(zip(cx, cy, Maps['maps'])):
+            x_fromc = x_center + Maps['grid_x'] * 1e3
+            y_fromc = y_center + Maps['grid_y'] * 1e3
+            inrecon = np.where((xmgrid >= np.min(x_fromc)) & (xmgrid <= np.max(x_fromc)) &
+                               (ymgrid >= np.min(y_fromc)) & (ymgrid <= np.max(y_fromc)))
+            inmap = np.where((x_fromc >= np.min(xmgrid)) & (x_fromc <= np.max(xmgrid)) &
+                             (y_fromc >= np.min(ymgrid)) & (y_fromc <= np.max(ymgrid)))
+            aggregate_grid[inrecon] = nanfunc(
+                swathfunc, aggregate_grid[inrecon], var[inmap])
+
         if prop['levels'] is None:
-            prop['levels'] = (np.nanmin(aggregate_grid),np.nanmax(aggregate_grid))
-        cmap,clevs = get_cmap_levels(varname,prop['cmap'],prop['levels'])
-                        
-        out = self.proj.transform_points(distproj,xmgrid,ymgrid)
-        lons = out[:,:,0]
-        lats = out[:,:,1]
-        
+            prop['levels'] = (np.nanmin(aggregate_grid),
+                              np.nanmax(aggregate_grid))
+        cmap, clevs = get_cmap_levels(varname, prop['cmap'], prop['levels'])
+
+        out = self.proj.transform_points(distproj, xmgrid, ymgrid)
+        lons = out[:, :, 0]
+        lats = out[:, :, 1]
+
         norm = mlib.colors.BoundaryNorm(clevs, cmap.N)
-        cbmap = self.ax.contourf(lons,lats,aggregate_grid,cmap=cmap,norm=norm,levels=clevs,transform=ccrs.PlateCarree())
-        
-        #Storm-centered plot domain
+        cbmap = self.ax.contourf(lons, lats, aggregate_grid, cmap=cmap,
+                                 norm=norm, levels=clevs, transform=ccrs.PlateCarree())
+
+        # Storm-centered plot domain
         if domain == "dynamic":
-            
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
-        else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Determine number of lat/lon lines to use for parallels & meridians
+
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
+        else:
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Determine number of lat/lon lines to use for parallels & meridians
         if map_prop['plot_gridlines']:
-            self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+            self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
+
+        # --------------------------------------------------------------------------------------
 
-        #--------------------------------------------------------------------------------------
-                
-        #Add left title
+        # Add left title
         type_array = np.array(storm_data['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
         tropical_vmax = np.array(storm_data['vmax'])[idx]
-            
+
         subtrop = classify_subtropical(np.array(storm_data['type']))
         peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
         peak_basin = storm_data['wmo_basin'][peak_idx]
-        storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-        
+        storm_type = get_storm_classification(
+            np.nanmax(tropical_vmax), subtrop, peak_basin)
+
         dot = u"\u2022"
         vartitle = get_recon_title(varname)
-        self.ax.set_title(f"{storm_type} {storm_data['name']}\n" + 'Recon: '+' '.join(vartitle),loc='left',fontsize=17,fontweight='bold')
+        self.ax.set_title(f"{storm_type} {storm_data['name']}\n" + 'Recon: ' + ' '.join(
+            vartitle), loc='left', fontsize=17, fontweight='bold')
+
+        # Add right title
+        # max_ppf = max(PPF)
+        start_time = dt.strftime(min(Maps['time']), '%H:%M UTC %d %b %Y')
+        end_time = dt.strftime(max(Maps['time']), '%H:%M UTC %d %b %Y')
+        self.ax.set_title(
+            f'Start ... {start_time}\nEnd ... {end_time}', loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
 
-        #Add right title
-        #max_ppf = max(PPF)
-        start_date = dt.strftime(min(Maps['time']),'%H:%M UTC %d %b %Y')
-        end_date = dt.strftime(max(Maps['time']),'%H:%M UTC %d %b %Y')
-        self.ax.set_title(f'Start ... {start_date}\nEnd ... {end_date}',loc='right',fontsize=13)
-
-        #--------------------------------------------------------------------------------------
-        
-        #Add legend
-        
-        #Phantom legend
-        handles=[]
+        # Add legend
+
+        # Phantom legend
+        handles = []
         for _ in range(10):
-            handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-        l = self.ax.legend(handles=handles,loc='upper left',fancybox=True,framealpha=0,fontsize=11.5)
+            handles.append(mlines.Line2D(
+                [], [], linestyle='-', label='', lw=0))
+        l = self.ax.legend(handles=handles, loc='upper left',
+                           fancybox=True, framealpha=0, fontsize=11.5)
         plt.draw()
 
-        #Get the bbox
+        # Get the bbox
         try:
             bb = l.legendPatch.get_bbox().inverse_transformed(self.fig.transFigure)
         except:
             bb = l.legendPatch.get_bbox().transformed(self.fig.transFigure.inverted())
         bb_ax = self.ax.get_position()
 
-        #Define colorbar axis
-        cax = self.fig.add_axes([bb.x0+bb.width, bb.y0-.05*bb.height, 0.015, bb.height])
+        # Define colorbar axis
+        cax = self.fig.add_axes(
+            [bb.x0 + bb.width, bb.y0 - .05 * bb.height, 0.015, bb.height])
 #        cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-        cbar = self.fig.colorbar(cbmap,cax=cax,orientation='vertical',\
+        cbar = self.fig.colorbar(cbmap, cax=cax, orientation='vertical',
                                  ticks=clevs)
-                
+
         cax.tick_params(labelsize=11.5)
         cax.yaxis.set_ticks_position('left')
-    
+
         rect_offset = 0.0
-        if prop['cmap'] == 'category' and varname in ['sfmr','wspd']:
-            cax.yaxis.set_ticks(np.linspace(0,1,len(clevs)))
+        if prop['cmap'] == 'category' and varname in ['sfmr', 'wspd']:
+            cax.yaxis.set_ticks(np.linspace(0, 1, len(clevs)))
             cax.yaxis.set_ticklabels(clevs)
             cax2 = cax.twinx()
             cax2.yaxis.set_ticks_position('right')
-            cax2.yaxis.set_ticks((np.linspace(0,1,len(clevs))[:-1]+np.linspace(0,1,len(clevs))[1:])*.5)
-            cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+            cax2.yaxis.set_ticks((np.linspace(0, 1, len(clevs))[
+                                 :-1] + np.linspace(0, 1, len(clevs))[1:]) * .5)
+            cax2.set_yticklabels(
+                ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
             cax2.tick_params('both', length=0, width=0, which='major')
             cax.yaxis.set_ticks_position('left')
-            
+
             rect_offset = 0.7
-            
-        rectangle = mpatches.Rectangle((bb.x0,bb.y0-0.1*bb.height),(1.8+rect_offset)*bb.width,1.1*bb.height,\
-                                       fc = 'w',edgecolor = '0.8',alpha = 0.8,\
+
+        rectangle = mpatches.Rectangle((bb.x0, bb.y0 - 0.1 * bb.height), (1.8 + rect_offset) * bb.width, 1.1 * bb.height,
+                                       fc='w', edgecolor='0.8', alpha=0.8,
                                        transform=self.fig.transFigure, zorder=2)
         self.ax.add_patch(rectangle)
-        
- 
-        #Add plot credit
+
+        # Add plot credit
         text = self.plot_credit()
         self.add_credit(text)
 
-
-    
-    def plot_polar(self,dfRecon,track_dict,time=None,reconInterp=None,radlim=150,ax=None,prop={}):
-
+    def plot_polar(self, dfRecon, track_dict, time=None, reconInterp=None, radlim=150, ax=None, prop={}):
         r"""
         Creates a plot of storm-centered recon data interpolated to a grid
-        
+
         Parameters
         ----------
         recon_data : dataframe
         radlim : int
             Radius (km) from the center of the storm that interpolation is calculated,
             and field plotted ... axis limits will be [-radlim,radlim,-radlim,radlim]
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Properties of plot
         """
 
-        #Set default properties
-        default_prop={'colors':'category','levels':[34,64,83,96,113,137,200]}
- 
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        
-        #mlib.rcParams.update({'font.size': 16})
+        # Set default properties
+        default_prop = {'colors': 'category',
+                        'levels': [34, 64, 83, 96, 113, 137, 200]}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+
+        # mlib.rcParams.update({'font.size': 16})
 
         fig = plt.figure(figsize=prop['figsize'])
         if ax is None:
             self.ax = plt.subplot()
         else:
             self.ax = ax
-        
-        cmap,clevs = get_cmap_levels(varname,prop['cmap'],prop['clevs'])
+
+        cmap, clevs = get_cmap_levels(varname, prop['cmap'], prop['clevs'])
         norm = mlib.colors.BoundaryNorm(clevs, cmap.N)
-        cbmap = self.ax.contourf(Maps_dict['grid_x'],Maps_dict['grid_y'],Maps_dict['maps'],\
-                                 cmap=cmap,norm=norm,levels=clevs,transform=ccrs.PlateCarree())
-        
+        cbmap = self.ax.contourf(Maps_dict['grid_x'], Maps_dict['grid_y'], Maps_dict['maps'],
+                                 cmap=cmap, norm=norm, levels=clevs, transform=ccrs.PlateCarree())
+
         rightarrow = u"\u2192"
         plt.xlabel(f'W {rightarrow} E Distance (km)')
         plt.ylabel(f'S {rightarrow} N Distance (km)')
-        plt.axis([-radlim,radlim,-radlim,radlim])
+        plt.axis([-radlim, radlim, -radlim, radlim])
         plt.axis('equal')
-        
-        cbar=plt.colorbar()
+
+        cbar = plt.colorbar()
         cbar.set_label('wind (kt)')
-                
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
-        self.ax.set_title('Recon interpolated',loc='left',fontsize=17,fontweight='bold')
-
-        #Add right title
-        #max_ppf = max(PPF)
-        start_date = dt.strftime(min(dfRecon['time']),'%H:%M UTC %d %b %Y')
-        end_date = dt.strftime(max(dfRecon['time']),'%H:%M UTC %d %b %Y')
-        self.ax.set_title(f'Start ... {start_date}\nEnd ... {end_date}',loc='right',fontsize=13)
 
-        #Add plot credit
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
+        self.ax.set_title('Recon interpolated', loc='left',
+                          fontsize=17, fontweight='bold')
+
+        # Add right title
+        # max_ppf = max(PPF)
+        start_time = dt.strftime(min(dfRecon['time']), '%H:%M UTC %d %b %Y')
+        end_time = dt.strftime(max(dfRecon['time']), '%H:%M UTC %d %b %Y')
+        self.ax.set_title(
+            f'Start ... {start_time}\nEnd ... {end_time}', loc='right', fontsize=13)
+
+        # Add plot credit
         text = self.plot_credit()
         self.add_credit(text)
 
-        #--------------------------------------------------------------------------------------
-           
-        #Display figure, return axis
+        # --------------------------------------------------------------------------------------
+
+        # Display figure, return axis
         plt.show()
         return self.ax
         plt.close()
-     
-    def plot_maps(self,storm,Maps_dict,varname,recon_stats=None,\
-                  domain='dynamic',ax=None,return_domain=False,prop={},map_prop={}):
-        
+
+    def plot_maps(self, storm, Maps_dict, varname, recon_stats=None,
+                  domain='dynamic', ax=None, return_domain=False, prop={}, map_prop={}):
         r"""
         Creates a plot of storm-centered recon data interpolated to a grid
-        
+
         Parameters
         ----------
         recon_data : dataframe
         radlim : int
             Radius (km) from the center of the storm that interpolation is calculated,
             and field plotted ... axis limits will be [-radlim,radlim,-radlim,radlim]
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Properties of plot
         """
 
-        #Set default properties
-        default_prop={'cmap':'category','levels':None,'left_title':'','right_title':'','pcolor':True}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(12.5,8.5),'dpi':120,'plot_gridlines':True}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
+        # Set default properties
+        default_prop = {'cmap': 'category', 'levels': None,
+                        'left_title': '', 'right_title': '', 'pcolor': True}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (12.5, 8.5), 'dpi': 120, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
 
-        MULTIVAR=False
-        if isinstance(varname,(tuple,list)):
+        MULTIVAR = False
+        if isinstance(varname, (tuple, list)):
             varname2 = varname[1]
             varname = varname[0]
             Maps_dict2 = Maps_dict[1]
             Maps_dict = Maps_dict[0]
-            MULTIVAR=True
-        
-        grid_res = 1*1e3 #m
+            MULTIVAR = True
+
+        grid_res = 1 * 1e3  # m
         clon = Maps_dict['center_lon']
         clat = Maps_dict['center_lat']
         distproj = ccrs.LambertConformal()
-        out = distproj.transform_points(ccrs.PlateCarree(),np.array([clon]),np.array([clat]))        
-        cx = np.rint(out[:,0]/grid_res)*grid_res
-        cy = np.rint(out[:,1]/grid_res)*grid_res
-        xmgrid = cx+Maps_dict['grid_x']*grid_res
-        ymgrid = cy+Maps_dict['grid_y']*grid_res
-        out = self.proj.transform_points(distproj,xmgrid,ymgrid)
-        lons = out[:,:,0]
-        lats = out[:,:,1]
-        
-        #mlib.rcParams.update({'font.size': 16})
-        
-        cmap,clevs = get_cmap_levels(varname,prop['cmap'],prop['levels'])
+        out = distproj.transform_points(
+            ccrs.PlateCarree(), np.array([clon]), np.array([clat]))
+        cx = np.rint(out[:, 0] / grid_res) * grid_res
+        cy = np.rint(out[:, 1] / grid_res) * grid_res
+        xmgrid = cx + Maps_dict['grid_x'] * grid_res
+        ymgrid = cy + Maps_dict['grid_y'] * grid_res
+        out = self.proj.transform_points(distproj, xmgrid, ymgrid)
+        lons = out[:, :, 0]
+        lats = out[:, :, 1]
+
+        # mlib.rcParams.update({'font.size': 16})
+
+        cmap, clevs = get_cmap_levels(varname, prop['cmap'], prop['levels'])
 
         norm = mlib.colors.BoundaryNorm(clevs, cmap.N)
-        cbmap = self.ax.contourf(lons,lats,Maps_dict['maps'],\
-                                 cmap=cmap,norm=norm,levels=clevs,transform=ccrs.PlateCarree())
-    
+        cbmap = self.ax.contourf(lons, lats, Maps_dict['maps'],
+                                 cmap=cmap, norm=norm, levels=clevs, transform=ccrs.PlateCarree())
+
         if MULTIVAR:
-            CS = self.ax.contour(lons,lats,Maps_dict2['maps'],levels = np.arange(0,2000,4),colors='k',linewidths=0.5)
+            CS = self.ax.contour(lons, lats, Maps_dict2['maps'], levels=np.arange(
+                0, 2000, 4), colors='k', linewidths=0.5)
             # Recast levels to new class
             CS.levels = [int(val) for val in CS.levels]
             self.ax.clabel(CS, CS.levels, fmt='%i', inline=True, fontsize=10)
-        
-        
-        #Storm-centered plot domain
+
+        # Storm-centered plot domain
         if domain == "dynamic":
-            
-            bound_w,bound_e,bound_s,bound_n = np.amin(lons)-.1,np.amax(lons)+.1,np.amin(lats)-.1,np.amax(lats)+.1
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
+
+            bound_w, bound_e, bound_s, bound_n = np.amin(
+                lons) - .1, np.amax(lons) + .1, np.amin(lats) - .1, np.amax(lats) + .1
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
 
-        #Determine number of lat/lon lines to use for parallels & meridians
+        # Determine number of lat/lon lines to use for parallels & meridians
         if map_prop['plot_gridlines']:
-            self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
-        
+            self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
+
 #        rightarrow = u"\u2192"
 #        plt.xlabel(f'W {rightarrow} E Distance (km)')
 #        plt.ylabel(f'S {rightarrow} N Distance (km)')
 #        plt.axis([-radlim,radlim,-radlim,radlim])
 #        plt.axis('equal')
-        
-        cbar = self.fig.colorbar(cbmap,orientation='vertical',\
+
+        cbar = self.fig.colorbar(cbmap, orientation='vertical',
                                  ticks=clevs)
 #        cbar.set_label('wind (kt)')
-                
-        #--------------------------------------------------------------------------------------
+
+        # --------------------------------------------------------------------------------------
 
         storm_data = storm.dict
-        #Add left title
+        # Add left title
         type_array = np.array(storm_data['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
         tropical_vmax = np.array(storm_data['vmax'])[idx]
-            
+
         subtrop = classify_subtropical(np.array(storm_data['type']))
         peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
         peak_basin = storm_data['wmo_basin'][peak_idx]
-        storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-        
-        vartitle = get_recon_title(varname)
-        title_left = f"{storm_type} {storm_data['name']}\n" + 'Recon: '+' '.join(vartitle)
-        self.ax.set_title(title_left,loc='left',fontsize=17,fontweight='bold')
+        storm_type = get_storm_classification(
+            np.nanmax(tropical_vmax), subtrop, peak_basin)
 
-        #Add right title
-        self.ax.set_title(Maps_dict['time'].strftime('%H:%M UTC %d %b %Y'),loc='right',fontsize=13)
+        vartitle = get_recon_title(varname)
+        title_left = f"{storm_type} {storm_data['name']}\n" + \
+            'Recon: ' + ' '.join(vartitle)
+        self.ax.set_title(title_left, loc='left',
+                          fontsize=17, fontweight='bold')
+
+        # Add right title
+        self.ax.set_title(Maps_dict['time'].strftime(
+            '%H:%M UTC %d %b %Y'), loc='right', fontsize=13)
 
-        #Add stats
+        # Add stats
         if recon_stats is not None:
-            a = self.ax.text(0.8,0.97,f"Max FL Wind: {int(recon_stats['pkwnd_max'])} kt\n"+\
-                                       f"Max SFMR: {int(recon_stats['sfmr_max'])} kt\n"+\
-                                       f"Min SLP: {int(recon_stats['p_min'])} hPa",fontsize=9.5,color='k',\
-                                       bbox=dict(facecolor='0.9', edgecolor='black', boxstyle='round,pad=1'),\
-                                       transform=self.ax.transAxes,ha='left',va='top',zorder=10)
+            a = self.ax.text(0.8, 0.97, f"Max FL Wind: {int(recon_stats['pkwnd_max'])} kt\n" +
+                             f"Max SFMR: {int(recon_stats['sfmr_max'])} kt\n" +
+                             f"Min SLP: {int(recon_stats['p_min'])} hPa", fontsize=9.5, color='k',
+                             bbox=dict(
+                                 facecolor='0.9', edgecolor='black', boxstyle='round,pad=1'),
+                             transform=self.ax.transAxes, ha='left', va='top', zorder=10)
 
-        #Add plot credit
+        # Add plot credit
         text = self.plot_credit()
         self.add_credit(text)
 
         if return_domain:
-            return self.ax,{'n':bound_n,'e':bound_e,'s':bound_s,'w':bound_w}
+            return self.ax, {'n': bound_n, 'e': bound_e, 's': bound_s, 'w': bound_w}
         else:
             return self.ax
 
-def plot_skewt(dict_list,storm_name_title):
+
+def plot_skewt(dict_list, storm_name_title):
 
     def time2text(time):
         try:
             return f'{time:%H:%M UTC %d %b %Y}'
         except:
             return 'N/A'
-    
+
     def location_text(indict):
         try:
             loc = indict['location'].lower()
         except:
             return ''
         if loc == 'eyewall':
-            return r"$\bf{"+loc.capitalize()+'}$, '
+            return r"$\bf{" + loc.capitalize() + '}$, '
         else:
-            return r"$\bf{"+loc.capitalize()+'}$, '
-    
+            return r"$\bf{" + loc.capitalize() + '}$, '
+
     degsym = u"\u00B0"
-    def latlon2text(lat,lon):
+
+    def latlon2text(lat, lon):
         NA = False
-        if lat<0:
+        if lat < 0:
             lattx = f'{abs(lat)}{degsym}S'
-        elif lat>=0:
+        elif lat >= 0:
             lattx = f'{lat}{degsym}N'
         else:
             NA = True
-        if lon<0:
+        if lon < 0:
             lontx = f'{abs(lon)}{degsym}W'
-        elif lon>=0:
+        elif lon >= 0:
             lontx = f'{lon}{degsym}E'
         else:
             NA = True
         if NA:
             return 'N/A'
         else:
-            return lattx+' '+lontx
+            return lattx + ' ' + lontx
 
     def mission2text(x):
         try:
             return int(x[:2])
         except:
             return x[:2]
-    
-    def wind_components(speed,direction):
-        u = -speed * np.sin(direction*np.pi/180)
-        v = -speed * np.cos(direction*np.pi/180)
-        return u,v
-    
+
+    def wind_components(speed, direction):
+        u = -speed * np.sin(direction * np.pi / 180)
+        v = -speed * np.cos(direction * np.pi / 180)
+        return u, v
+
     def deg2dir(x):
-        dirs = ['N','NNE','NE','ENE','E','ESE','SE','SSE','S','SSW','SW','WSW','W','WNW','NW','NNW']
+        dirs = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE',
+                'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']
         try:
-            idx = int(round(x*16/360,0)%16)
+            idx = int(round(x * 16 / 360, 0) % 16)
             return dirs[idx]
         except:
             return 'N/A'
-    
-    def rh_from_dp(t,td):
-        rh = np.exp(17.67 * (td) / (td+273.15 - 29.65)) / np.exp(17.67 * (t) / (t+273.15 - 29.65))
-        return rh*100
-    
-    def cellcolor(color,value):
+
+    def rh_from_dp(t, td):
+        rh = np.exp(17.67 * (td) / (td + 273.15 - 29.65)) / \
+            np.exp(17.67 * (t) / (t + 273.15 - 29.65))
+        return rh * 100
+
+    def cellcolor(color, value):
         if np.isnan(value):
             return 'w'
         else:
-            return list(color[:3])+[.5]
-    
-    def skew_t(t,p):
-        t0 = np.log(p/1050)*80/np.log(100/1050)
-        return t0+t,p
+            return list(color[:3]) + [.5]
+
+    def skew_t(t, p):
+        t0 = np.log(p / 1050) * 80 / np.log(100 / 1050)
+        return t0 + t, p
 
     figs = []
     for data in dict_list:
-        
-        #Retrieve dropsondes data
-        df = data['levels'].sort_values('pres',ascending=True)
+
+        # Retrieve dropsondes data
+        df = data['levels'].sort_values('pres', ascending=True)
         Pres = df['pres']
         Temp = df['temp']
         Dwpt = df['dwpt']
         wind_speed = df['wspd']
         wind_dir = df['wdir']
-        U,V = wind_components(wind_speed, wind_dir)
+        U, V = wind_components(wind_speed, wind_dir)
 
-        ytop = int(np.nanmin(Pres)-50)
-        yticks = np.arange(1000,ytop,-100)
-        xticks = np.arange(-30,51,10)
+        ytop = int(np.nanmin(Pres) - 50)
+        yticks = np.arange(1000, ytop, -100)
+        xticks = np.arange(-30, 51, 10)
 
         # Get mandatory and significant wind sub-dataframes
-        dfmand = df.loc[df['pres'].isin((1000,925,850,700,500,400,300,250,200,150,100))]
-        sfc = df.loc[df['hgt']==0]
-        if len(sfc)>0:
+        dfmand = df.loc[df['pres'].isin(
+            (1000, 925, 850, 700, 500, 400, 300, 250, 200, 150, 100))]
+        sfc = df.loc[df['hgt'] == 0]
+        if len(sfc) > 0:
             SLP = sfc['pres'].values[0]
-            dfmand = pd.concat([dfmand,sfc])
-            dfmand = dfmand.loc[dfmand['pres']<=SLP]
+            dfmand = pd.concat([dfmand, sfc])
+            dfmand = dfmand.loc[dfmand['pres'] <= SLP]
         else:
             SLP = None
-        dfwind = df.loc[df['pres']>=700]
+        dfwind = df.loc[df['pres'] >= 700]
 
         # Start figure
-        fig = plt.figure(figsize=(17,11),facecolor='w')
-        gs = gridspec.GridSpec(2,3,width_ratios=(2,.2,1.1),height_ratios=(len(dfmand)+3,len(dfwind)+3), wspace=0.0)
+        fig = plt.figure(figsize=(17, 11), facecolor='w')
+        gs = gridspec.GridSpec(2, 3, width_ratios=(2, .2, 1.1), height_ratios=(
+            len(dfmand) + 3, len(dfwind) + 3), wspace=0.0)
 
-        ax1 = fig.add_subplot(gs[:,0])
+        ax1 = fig.add_subplot(gs[:, 0])
 
-        #Determine dropsonde time
+        # Determine dropsonde time
         try:
             if np.isnan(data["TOPtime"]):
                 use_time = data["BOTTOMtime"]
             else:
                 use_time = data["TOPtime"]
         except:
             use_time = data["TOPtime"]
-        
-        #Add title for main axis
-        storm_name_title = storm_name_title.replace("DDD",str(data["obsnum"]))
-        storm_name_title = storm_name_title.replace("MMM",str(mission2text(data["mission"])))
-        ax1.set_title(storm_name_title,loc='left',fontsize=17,fontweight='bold')
-        ax1.set_title(f'Drop time: {time2text(use_time)}'+\
-                      f'\nDrop location: {location_text(data)}{latlon2text(data["lat"],data["lon"])}',loc='right',fontsize=13)
+
+        # Add title for main axis
+        storm_name_title = storm_name_title.replace("DDD", str(data["obsnum"]))
+        storm_name_title = storm_name_title.replace(
+            "MMM", str(mission2text(data["mission"])))
+        ax1.set_title(storm_name_title, loc='left',
+                      fontsize=17, fontweight='bold')
+        ax1.set_title(f'Drop time: {time2text(use_time)}' +
+                      f'\nDrop location: {location_text(data)}{latlon2text(data["lat"],data["lon"])}', loc='right', fontsize=13)
         plt.yscale('log')
-        plt.yticks(yticks,[f'{i:d}' for i in yticks],fontsize=12)
-        plt.xticks(xticks,[f'{i:d}' for i in xticks],fontsize=12)
-        for y in range(1000,ytop,-50):plt.plot([-30,50],[y]*2,color='0.5',lw=0.5)
-        for x in range(-30-80,50,10):plt.plot([x,x+80],[1050,100],color='0.5',linestyle='--',lw=0.5)
-
-        plt.plot(*skew_t(Temp.loc[~np.isnan(Temp)],Pres.loc[~np.isnan(Temp)]),'o-',color='r')
-        plt.plot(*skew_t(Dwpt.loc[~np.isnan(Dwpt)],Pres.loc[~np.isnan(Dwpt)]),'o-',color='g')
-        plt.xlabel(f'Temperature ({degsym}C)',fontsize=13)
-        plt.ylabel('Pressure (hPa)',fontsize=13)
-        plt.axis([-30,50,1050,ytop])
-        
-        #Try plotting dropsonde location with respect to storm center (unavailable for realtime)
+        plt.yticks(yticks, [f'{i:d}' for i in yticks], fontsize=12)
+        plt.xticks(xticks, [f'{i:d}' for i in xticks], fontsize=12)
+        for y in range(1000, ytop, -50):
+            plt.plot([-30, 50], [y] * 2, color='0.5', lw=0.5)
+        for x in range(-30 - 80, 50, 10):
+            plt.plot([x, x + 80], [1050, 100],
+                     color='0.5', linestyle='--', lw=0.5)
+
+        plt.plot(*skew_t(Temp.loc[~np.isnan(Temp)],
+                 Pres.loc[~np.isnan(Temp)]), 'o-', color='r')
+        plt.plot(*skew_t(Dwpt.loc[~np.isnan(Dwpt)],
+                 Pres.loc[~np.isnan(Dwpt)]), 'o-', color='g')
+        plt.xlabel(f'Temperature ({degsym}C)', fontsize=13)
+        plt.ylabel('Pressure (hPa)', fontsize=13)
+        plt.axis([-30, 50, 1050, ytop])
+
+        # Try plotting dropsonde location with respect to storm center (unavailable for realtime)
         try:
-            lim = max([i for stage in ('TOP','BOTTOM') for i in [1.5*abs(data[f'{stage}xdist'])+.1,1.5*abs(data[f'{stage}ydist'])+.1]])
+            lim = max([i for stage in ('TOP', 'BOTTOM') for i in [
+                      1.5 * abs(data[f'{stage}xdist']) + .1, 1.5 * abs(data[f'{stage}ydist']) + .1]])
             iscoords = np.isnan(lim)
             if iscoords:
                 lim = 1
-            for stage,ycoord in zip(('TOP','BOTTOM'),(.8,.05)):
+            for stage, ycoord in zip(('TOP', 'BOTTOM'), (.8, .05)):
                 ax1in1 = ax1.inset_axes([0.05, ycoord, 0.15, 0.15])
                 if iscoords:
                     ax1in1.set_title('distance N/A')
                 else:
-                    ax1in1.scatter(0,0,c='k')
-                    ax1in1.scatter(data[f'{stage}xdist'],data[f'{stage}ydist'],c='w',marker='v',edgecolor='k')
-                    ax1in1.set_title(f'{data[f"{stage}distance"]:0.0f} km {deg2dir(90-math.atan2(data[f"{stage}ydist"],data[f"{stage}xdist"])*180/np.pi)}')                    
-                ax1in1.axis([-lim,lim,-lim,lim])
+                    ax1in1.scatter(0, 0, c='k')
+                    ax1in1.scatter(
+                        data[f'{stage}xdist'], data[f'{stage}ydist'], c='w', marker='v', edgecolor='k')
+                    ax1in1.set_title(
+                        f'{data[f"{stage}distance"]:0.0f} km {deg2dir(90-math.atan2(data[f"{stage}ydist"],data[f"{stage}xdist"])*180/np.pi)}')
+                ax1in1.axis([-lim, lim, -lim, lim])
                 ax1in1.xaxis.set_major_locator(plt.NullLocator())
                 ax1in1.yaxis.set_major_locator(plt.NullLocator())
         except:
             pass
 
-        ax4 = fig.add_subplot(gs[:,1],sharey=ax1)
-        barbs = {k:[v.values[-1]] for k,v in zip(('p','u','v'),(Pres,U,V))}
-        for p,u,v in zip(Pres.values[::-1],U.values[::-1],V.values[::-1]):
-            if abs(p-barbs['p'][-1])>10 and not np.isnan(u):
-                for k,v in zip(('p','u','v'),(p,u,v)):
+        ax4 = fig.add_subplot(gs[:, 1], sharey=ax1)
+        barbs = {k: [v.values[-1]]
+                 for k, v in zip(('p', 'u', 'v'), (Pres, U, V))}
+        for p, u, v in zip(Pres.values[::-1], U.values[::-1], V.values[::-1]):
+            if abs(p - barbs['p'][-1]) > 10 and not np.isnan(u):
+                for k, v in zip(('p', 'u', 'v'), (p, u, v)):
                     barbs[k].append(v)
-        plt.barbs([.4]*len(barbs['p']),barbs['p'],barbs['u'],barbs['v'], pivot='middle')
-        ax4.set_xlim(0,1)
+        plt.barbs([.4] * len(barbs['p']), barbs['p'],
+                  barbs['u'], barbs['v'], pivot='middle')
+        ax4.set_xlim(0, 1)
         ax4.axis('off')
 
-        RH = [rh_from_dp(i,j) for i,j in zip(dfmand['temp'],dfmand['dwpt'])]
-        cellText = np.array([['' if np.isnan(i) else f'{int(i)} hPa' for i in dfmand['pres']],\
-                    ['' if np.isnan(i) else f'{int(i)} m' for i in dfmand['hgt']],\
-                    ['' if np.isnan(i) else f'{i:.1f} {degsym}C' for i in dfmand['temp']],\
-                    ['' if np.isnan(i) else f'{int(i)} %' for i in RH],\
-                    ['' if np.isnan(i) else f'{deg2dir(j)} at {int(i)} kt' for i,j in zip(dfmand['wspd'],dfmand['wdir'])]]).T
-        colLabels = ['Pressure','Height','Temp','RH','Wind']
+        RH = [rh_from_dp(i, j) for i, j in zip(dfmand['temp'], dfmand['dwpt'])]
+        cellText = np.array([['' if np.isnan(i) else f'{int(i)} hPa' for i in dfmand['pres']],
+                             ['' if np.isnan(
+                                 i) else f'{int(i)} m' for i in dfmand['hgt']],
+                             ['' if np.isnan(
+                                 i) else f'{i:.1f} {degsym}C' for i in dfmand['temp']],
+                             ['' if np.isnan(
+                                 i) else f'{int(i)} %' for i in RH],
+                             ['' if np.isnan(i) else f'{deg2dir(j)} at {int(i)} kt' for i, j in zip(dfmand['wspd'], dfmand['wdir'])]]).T
+        colLabels = ['Pressure', 'Height', 'Temp', 'RH', 'Wind']
 
         cmap_rh = mlib.cm.get_cmap('BrBG')
         cmap_temp = mlib.cm.get_cmap('RdBu_r')
         cmap_wind = mlib.cm.get_cmap('Purples')
 
-        colors = [['w','w',cellcolor(cmap_temp(t/120+.5),t),\
-                            cellcolor(cmap_rh(r/100),r),\
-                            cellcolor(cmap_wind(w/200),w)] for t,r,w in zip(dfmand['temp'],RH,dfmand['wspd'])]
+        colors = [['w', 'w', cellcolor(cmap_temp(t / 120 + .5), t),
+                   cellcolor(cmap_rh(r / 100), r),
+                   cellcolor(cmap_wind(w / 200), w)] for t, r, w in zip(dfmand['temp'], RH, dfmand['wspd'])]
 
-        ax2 = fig.add_subplot(gs[0,2])
+        ax2 = fig.add_subplot(gs[0, 2])
         ax2.xaxis.set_visible(False)  # hide the x axis
         ax2.yaxis.set_visible(False)  # hide the y axis
-        TB = ax2.table(cellText=cellText,colLabels=colLabels,cellColours=colors,cellLoc='center',bbox = [0, .05, 1, .95])
+        TB = ax2.table(cellText=cellText, colLabels=colLabels,
+                       cellColours=colors, cellLoc='center', bbox=[0, .05, 1, .95])
         if SLP is not None:
             TB[(len(cellText), 0)].get_text().set_weight('bold')
         ax2.axis('off')
         TB.auto_set_font_size(False)
         TB.set_fontsize(9)
-        #TB.scale(3,1.2)
+        # TB.scale(3,1.2)
         try:
-            ax2.text(0,.05,f'\nDeep Layer Mean Wind: {deg2dir(data["DLMdir"])} at {int(data["DLMspd"])} kt',va='top',fontsize=12)
+            ax2.text(
+                0, .05, f'\nDeep Layer Mean Wind: {deg2dir(data["DLMdir"])} at {int(data["DLMspd"])} kt', va='top', fontsize=12)
         except:
-            ax2.text(0,.05,f'\nDeep Layer Mean Wind: N/A',va='top',fontsize=12)
+            ax2.text(0, .05, f'\nDeep Layer Mean Wind: N/A',
+                     va='top', fontsize=12)
 
-        ax2.set_title('Generated using Tropycal \n',fontsize=12,fontweight='bold',color='0.7',loc='right')
+        ax2.set_title('Generated using Tropycal \n', fontsize=12,
+                      fontweight='bold', color='0.7', loc='right')
 
-        cellText = np.array([[f'{int(i)} hPa' for i,j in zip(dfwind['pres'],dfwind['wspd']) if not np.isnan(j)],\
-                    [f'{deg2dir(j)} at {int(i)} kt' for i,j in zip(dfwind['wspd'],dfwind['wdir']) if not np.isnan(i)]]).T
-        colLabels = ['Pressure','Wind']
-        colors = [['w',cellcolor(cmap_wind(i/200),i)] for i in dfwind['wspd'] if not np.isnan(i)]
+        cellText = np.array([[f'{int(i)} hPa' for i, j in zip(dfwind['pres'], dfwind['wspd']) if not np.isnan(j)],
+                             [f'{deg2dir(j)} at {int(i)} kt' for i, j in zip(dfwind['wspd'], dfwind['wdir']) if not np.isnan(i)]]).T
+        colLabels = ['Pressure', 'Wind']
+        colors = [['w', cellcolor(cmap_wind(i / 200), i)]
+                  for i in dfwind['wspd'] if not np.isnan(i)]
 
-        ax3 = fig.add_subplot(gs[1,2])
+        ax3 = fig.add_subplot(gs[1, 2])
 
         try:
-            TB = ax3.table(cellText=cellText,colLabels=colLabels,cellColours=colors,cellLoc='center',bbox = [0, .1, 1, .9])
+            TB = ax3.table(cellText=cellText, colLabels=colLabels,
+                           cellColours=colors, cellLoc='center', bbox=[0, .1, 1, .9])
             TB.auto_set_font_size(False)
             TB.set_fontsize(9)
             meanwindoffset = 0
         except:
             meanwindoffset = 0.9
-        #TB.scale(2,1.2)
+        # TB.scale(2,1.2)
         ax3.xaxis.set_visible(False)  # hide the x axis
         ax3.yaxis.set_visible(False)  # hide the y axis
         ax3.axis('off')
 
         try:
-            ax3.text(0,.1+meanwindoffset,\
-                     f'\nMean Wind in Lowest 500 m: {deg2dir(data["MBLdir"])} at {int(data["MBLspd"])} kt',va='top',fontsize=12)
+            ax3.text(0, .1 + meanwindoffset,
+                     f'\nMean Wind in Lowest 500 m: {deg2dir(data["MBLdir"])} at {int(data["MBLspd"])} kt', va='top', fontsize=12)
         except:
-            ax3.text(0,.1+meanwindoffset,\
-                     f'\nMean Wind in Lowest 500 m: N/A',va='top',fontsize=12)
+            ax3.text(0, .1 + meanwindoffset,
+                     f'\nMean Wind in Lowest 500 m: N/A', va='top', fontsize=12)
         try:
-            ax3.text(0,.1+meanwindoffset,\
-                     f'\n\nMean Wind in Lowest 150 m: {deg2dir(data["WL150dir"])} at {int(data["WL150spd"])} kt',va='top',fontsize=12)
+            ax3.text(0, .1 + meanwindoffset,
+                     f'\n\nMean Wind in Lowest 150 m: {deg2dir(data["WL150dir"])} at {int(data["WL150spd"])} kt', va='top', fontsize=12)
         except:
-            ax3.text(0,.1+meanwindoffset,\
-                     f'\n\nMean Wind in Lowest 150 m: N/A',va='top',fontsize=12)
+            ax3.text(0, .1 + meanwindoffset,
+                     f'\n\nMean Wind in Lowest 150 m: N/A', va='top', fontsize=12)
 
         figs.append(fig)
         plt.close()
 
-    if len(figs)>1:
+    if len(figs) > 1:
         return figs
-    elif len(figs)==1:
+    elif len(figs) == 1:
         return fig
     else:
         print("No dropsondes in selection")
```

### Comparing `tropycal-0.6.1/src/tropycal/recon/realtime.py` & `tropycal-1.0/src/tropycal/recon/realtime.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,373 +1,413 @@
 import urllib3
 import requests
 import pandas as pd
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 import matplotlib.pyplot as plt
 
 from .plot import *
 from .tools import *
 from ..utils import *
 
+
 class RealtimeRecon():
-    
+
     r"""
     Creates an instance of a RealtimeRecon object.
-    
+
     Parameters
     ----------
     hours : int
         Number of hours to search back for recon missions. Default is 6 hours. Max allowed is 48 hours.
-    
+
     Returns
     -------
     RealtimeRecon
         Instance of a RealtimeRecon object.
-    
+
     Notes
     -----
     .. warning::
 
         RealtimeRecon data is retrieved directly through NHC's archive, though this source can lag by up to 40 minutes compared to the very latest recon data available directly on their website.
-    
+
     The ``RealtimeRecon`` Class and accompanying ``Mission`` class make up the realtime part of the recon module. Unlike the ``ReconDataset`` Class and its accompanying ``hdobs``, ``dropsondes`` and ``vdms`` classes which are **storm-centric**, the realtime recon functionality is **mission-centric**.
-    
+
     This mission-centric functionality means realtime recon missions lack the storm-centering functionality that the more comprehensive full recon functionality has, but is also much faster at reading recon data for realtime purposes and includes non-tropical cyclone recon missions.
-    
+
     The following example shows how to use realtime recon functionality. First, create an instance of ``RealtimeRecon``, which retrieves all active missions within the specified hour window (default is the most recent 6 hours):
-    
+
     .. code-block:: python
-    
+
         from tropycal import recon
         realtime_obj = recon.RealtimeRecon()
-        
+
     We can now quickly retrieve a list of all active mission IDs:
-    
+
     >>> realtime_obj.get_mission_ids()
     ['AF306-0801A-ALEX', 'AF303-0901A-ALEX']
-    
+
     Now use the ``get_mission()`` method to retrieve a Mission object containing all available data (HDOBs, VDMs & dropsondes) for this mission. The resulting object can access all of the methods and attributes of the ``Mission`` class.
-    
+
     .. code-block:: python
-        
+
         mission = realtime_obj.get_mission('AF303-0901A-ALEX')
-    
+
     """
-    
+
     def __repr__(self):
         summary = ["<tropycal.recon.RealtimeRecon>"]
 
-        #Add dataset summary
+        # Add dataset summary
         summary.append("Dataset Summary:")
-        summary.append(f'{" "*4}Numbers of active missions: {len(self.missions)}')
-        
+        summary.append(
+            f'{" "*4}Numbers of active missions: {len(self.missions)}')
+
         return "\n".join(summary)
-    
-    def __init__(self,hours=6):
 
-        #Error check
+    def __init__(self, hours=6):
+
+        # Error check
         self.hours = hours
         if hours > 48 or hours <= 0:
             raise ValueError("Maximum allowed search is 48 hours back.")
-        
-        #Start timing
+
+        # Start timing
         print("--> Searching for active missions")
         timer_start = dt.now()
 
-        #Set URLs for reading data
+        # Set URLs for reading data
         self.urls = {
-            'hdobs':f'https://www.nhc.noaa.gov/archive/recon/{dt.utcnow().year}/AHONT1/',
-            'dropsondes':f'https://www.nhc.noaa.gov/archive/recon/{dt.utcnow().year}/REPNT3/',
-            'vdms':f'https://www.nhc.noaa.gov/archive/recon/{dt.utcnow().year}/REPNT2/'
+            'hdobs': f'https://www.nhc.noaa.gov/archive/recon/{dt.utcnow().year}/AHONT1/',
+            'dropsondes': f'https://www.nhc.noaa.gov/archive/recon/{dt.utcnow().year}/REPNT3/',
+            'vdms': f'https://www.nhc.noaa.gov/archive/recon/{dt.utcnow().year}/REPNT2/'
         }
 
-        #Start time set by hour window
+        # Start time set by hour window
         start_time_request = dt.utcnow() - timedelta(hours=hours)
-        start_time = dt.utcnow() - timedelta(hours=hours+12)
+        start_time = dt.utcnow() - timedelta(hours=hours + 12)
         self.start_time_request = start_time_request
 
-        #Retrieve list of files in URL and filter by storm dates
-        files = {'hdobs':[],'dropsondes':[],'vdms':[]}
+        # Retrieve list of files in URL and filter by storm dates
+        files = {
+            'hdobs': [],
+            'dropsondes': [],
+            'vdms': []
+        }
         for key in files.keys():
             page = requests.get(self.urls[key]).text
             content = page.split("\n")
             file_list = []
             for line in content:
-                if ".txt" in line: file_list.append(((line.split('txt">')[1]).split("</a>")[0]).split("."))
+                if ".txt" in line:
+                    file_list.append(
+                        ((line.split('txt">')[1]).split("</a>")[0]).split("."))
             del content
-            file_list = sorted([i for i in file_list if dt.strptime(i[1][:10],'%Y%m%d%H') >= start_time],key=lambda x: x[1])
-            files[key] = [self.urls[key]+'.'.join(l) for l in file_list]
+            file_list = sorted([i for i in file_list if dt.strptime(
+                i[1][:10], '%Y%m%d%H') >= start_time], key=lambda x: x[1])
+            files[key] = [self.urls[key] + '.'.join(l) for l in file_list]
         self.files = files
 
-        #Retrieve all active missions & read HDOBs
+        # Retrieve all active missions & read HDOBs
         urllib3.disable_warnings()
         http = urllib3.PoolManager()
         self.missions = {}
         for file in files['hdobs']:
 
-            #Retrieve content
-            response = http.request('GET',file)
+            # Retrieve content
+            response = http.request('GET', file)
             content = response.data.decode('utf-8')
             content_split = content.split("\n")
 
-            #Construct mission ID
+            # Construct mission ID
             try:
-                mission_id = '-'.join((content_split[3].replace("  "," ")).split(" ")[:3])
+                mission_id = '-'.join(
+                    (content_split[3].replace("  ", " ")).split(" ")[:3])
                 if mission_id not in self.missions:
-                    self.missions[mission_id] = {'hdobs':decode_hdob(content),
-                                            'vdms':[],
-                                            'dropsondes':[],
-                                            'aircraft':mission_id.split("-")[0],
-                                            'storm_name':mission_id.split("-")[2]
-                                           }
+                    self.missions[mission_id] = {
+                        'hdobs': decode_hdob(content),
+                        'vdms': [],
+                        'dropsondes': [],
+                        'aircraft': mission_id.split("-")[0],
+                        'storm_name': mission_id.split("-")[2]
+                    }
                 else:
-                    self.missions[mission_id]['hdobs'] = pd.concat([self.missions[mission_id]['hdobs'],decode_hdob(content)])
+                    self.missions[mission_id]['hdobs'] = pd.concat(
+                        [self.missions[mission_id]['hdobs'], decode_hdob(content)])
             except:
                 pass
 
-        #Retrieve VDMs
+        # Retrieve VDMs
         for file in files['vdms']:
 
-            #Retrieve content
-            response = http.request('GET',file)
+            # Retrieve content
+            response = http.request('GET', file)
             content = response.data.decode('utf-8')
             content_split = content.split("\n")
 
-            #Construct mission ID
-            mission_id = ['-'.join(i.split("U. ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:2] == "U."][0]
-            date = dt.strptime((file.split('.')[-2])[:8],'%Y%m%d')
+            # Construct mission ID
+            mission_id = ['-'.join(i.split("U. ")[1].replace("  ", " ").split(" ")[:3])
+                          for i in content_split if i[:2] == "U."][0]
+            time = dt.strptime((file.split('.')[-2])[:8], '%Y%m%d')
             try:
-                blank, data = decode_vdm(content,date)
+                blank, data = decode_vdm(content, time)
             except:
                 continue
-            if mission_id in self.missions.keys(): self.missions[mission_id]['vdms'].append(data)
+            if mission_id in self.missions.keys():
+                self.missions[mission_id]['vdms'].append(data)
 
-        #Retrieve dropsondes
+        # Retrieve dropsondes
         for file in files['dropsondes']:
 
-            #Retrieve content
-            response = http.request('GET',file)
+            # Retrieve content
+            response = http.request('GET', file)
             content = response.data.decode('utf-8')
             content_split = content.split("\n")
 
-            #Construct mission ID
-            mission_id = ['-'.join(i.split("61616 ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:5] == "61616"][0]
-            date = dt.strptime((file.split('.')[-2])[:8],'%Y%m%d')
+            # Construct mission ID
+            mission_id = ['-'.join(i.split("61616 ")[1].replace("  ", " ").split(" ")[:3])
+                          for i in content_split if i[:5] == "61616"][0]
+            time = dt.strptime((file.split('.')[-2])[:8], '%Y%m%d')
             try:
-                blank, data = decode_dropsonde(content,date)
+                blank, data = decode_dropsonde(content, time)
             except:
                 continue
-            if mission_id in self.missions.keys(): self.missions[mission_id]['dropsondes'].append(data)
+            if mission_id in self.missions.keys():
+                self.missions[mission_id]['dropsondes'].append(data)
 
-        #Temporally filter missions
+        # Temporally filter missions
         keys = [k for k in self.missions.keys()]
         for key in keys:
-            end_date = pd.to_datetime(self.missions[key]['hdobs']['time'].values[-1])
-            if end_date < start_time_request: del self.missions[key]
-        
-        #Sort each mission by date
+            end_time = pd.to_datetime(
+                self.missions[key]['hdobs']['time'].values[-1])
+            if end_time < start_time_request:
+                del self.missions[key]
+
+        # Sort each mission by time
         for key in self.missions.keys():
-            self.missions[key]['hdobs'].sort_values(['time'],inplace=True)
-        
-        #Save current time
+            self.missions[key]['hdobs'].sort_values(['time'], inplace=True)
+
+        # Save current time
         self.time = dt.now()
-        
-        #Add attributes
+
+        # Add attributes
         self.attrs = {
-            'hours':self.hours,
-            'time':self.time,
+            'hours': self.hours,
+            'time': self.time,
         }
-        
-        print(f"--> Completed retrieving active missions ({(dt.now()-timer_start).total_seconds():.1f} seconds)")
-        
+
+        print(
+            f"--> Completed retrieving active missions ({(dt.now()-timer_start).total_seconds():.1f} seconds)")
+
     def update(self):
-        
         r"""
         Updates RealtimeRecon with the latest available data.
-        
+
         Notes
         -----
         This function has no return value, but simply updates RealtimeRecon with the latest available recon data.
         """
-        
-        #Start timing
+
+        # Start timing
         timer_start = dt.now()
-        
-        #Start time set by hour window
+
+        # Start time set by hour window
         start_time = dt.utcnow() - timedelta(hours=24)
-        if start_time < self.start_time_request: start_time = self.start_time_request
+        if start_time < self.start_time_request:
+            start_time = self.start_time_request
 
-        #Retrieve list of files in URL and filter by storm dates
-        files = {'hdobs':[],'dropsondes':[],'vdms':[]}
+        # Retrieve list of files in URL and filter by storm dates
+        files = {
+            'hdobs': [],
+            'dropsondes': [],
+            'vdms': []
+        }
         for key in files.keys():
             page = requests.get(self.urls[key]).text
             content = page.split("\n")
             file_list = []
             for line in content:
-                if ".txt" in line: file_list.append(((line.split('txt">')[1]).split("</a>")[0]).split("."))
+                if ".txt" in line:
+                    file_list.append(
+                        ((line.split('txt">')[1]).split("</a>")[0]).split("."))
             del content
-            file_list = sorted([i for i in file_list if dt.strptime(i[1][:10],'%Y%m%d%H') >= start_time],key=lambda x: x[1])
-            files[key] = [self.urls[key]+'.'.join(l) for l in file_list if self.urls[key]+'.'.join(l) not in self.files[key]]
+            file_list = sorted([i for i in file_list if dt.strptime(
+                i[1][:10], '%Y%m%d%H') >= start_time], key=lambda x: x[1])
+            files[key] = [self.urls[key] + '.'.join(
+                l) for l in file_list if self.urls[key] + '.'.join(l) not in self.files[key]]
             self.files[key] += files[key]
-        
-        #Retrieve all active missions & read HDOBs
+
+        # Retrieve all active missions & read HDOBs
         urllib3.disable_warnings()
         http = urllib3.PoolManager()
         for file in files['hdobs']:
 
-            #Retrieve content
-            response = http.request('GET',file)
+            # Retrieve content
+            response = http.request('GET', file)
             content = response.data.decode('utf-8')
             content_split = content.split("\n")
 
-            #Construct mission ID
-            mission_id = '-'.join((content_split[3].replace("  "," ")).split(" ")[:3])
+            # Construct mission ID
+            mission_id = '-'.join(
+                (content_split[3].replace("  ", " ")).split(" ")[:3])
             if mission_id not in self.missions:
-                self.missions[mission_id] = {'hdobs':decode_hdob(content),
-                                        'vdms':[],
-                                        'dropsondes':[],
-                                        'aircraft':mission_id.split("-")[0],
-                                        'storm_name':mission_id.split("-")[2]
-                                       }
+                self.missions[mission_id] = {
+                    'hdobs': decode_hdob(content),
+                    'vdms': [],
+                    'dropsondes': [],
+                    'aircraft': mission_id.split("-")[0],
+                    'storm_name': mission_id.split("-")[2]
+                }
             else:
-                self.missions[mission_id]['hdobs'] = pd.concat([self.missions[mission_id]['hdobs'],decode_hdob(content)])
+                self.missions[mission_id]['hdobs'] = pd.concat(
+                    [self.missions[mission_id]['hdobs'], decode_hdob(content)])
 
-        #Retrieve VDMs
+        # Retrieve VDMs
         for file in files['vdms']:
 
-            #Retrieve content
-            response = http.request('GET',file)
+            # Retrieve content
+            response = http.request('GET', file)
             content = response.data.decode('utf-8')
             content_split = content.split("\n")
 
-            #Construct mission ID
-            mission_id = ['-'.join(i.split("U. ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:2] == "U."][0]
-            date = dt.strptime((file.split('.')[-2])[:8],'%Y%m%d')
-            blank, data = decode_vdm(content,date)
-            if mission_id in self.missions.keys(): self.missions[mission_id]['vdms'].append(data)
+            # Construct mission ID
+            mission_id = ['-'.join(i.split("U. ")[1].replace("  ", " ").split(" ")[:3])
+                          for i in content_split if i[:2] == "U."][0]
+            time = dt.strptime((file.split('.')[-2])[:8], '%Y%m%d')
+            blank, data = decode_vdm(content, time)
+            if mission_id in self.missions.keys():
+                self.missions[mission_id]['vdms'].append(data)
 
-        #Retrieve dropsondes
+        # Retrieve dropsondes
         for file in files['dropsondes']:
 
-            #Retrieve content
-            response = http.request('GET',file)
+            # Retrieve content
+            response = http.request('GET', file)
             content = response.data.decode('utf-8')
             content_split = content.split("\n")
 
-            #Construct mission ID
-            mission_id = ['-'.join(i.split("61616 ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:5] == "61616"][0]
-            date = dt.strptime((file.split('.')[-2])[:8],'%Y%m%d')
-            blank, data = decode_dropsonde(content,date)
-            if mission_id in self.missions.keys(): self.missions[mission_id]['dropsondes'].append(data)
-        
-        #Temporally filter missions
+            # Construct mission ID
+            mission_id = ['-'.join(i.split("61616 ")[1].replace("  ", " ").split(" ")[:3])
+                          for i in content_split if i[:5] == "61616"][0]
+            time = dt.strptime((file.split('.')[-2])[:8], '%Y%m%d')
+            blank, data = decode_dropsonde(content, time)
+            if mission_id in self.missions.keys():
+                self.missions[mission_id]['dropsondes'].append(data)
+
+        # Temporally filter missions
         keys = [k for k in self.missions.keys()]
         for key in keys:
-            end_date = pd.to_datetime(self.missions[key]['hdobs']['time'].values[-1])
+            end_time = pd.to_datetime(
+                self.missions[key]['hdobs']['time'].values[-1])
             start_time_request = dt.utcnow() - timedelta(hours=self.hours)
-            if end_date < start_time_request: del self.missions[key]
-        
-        #Sort each mission by date
+            if end_time < start_time_request:
+                del self.missions[key]
+
+        # Sort each mission by time
         for key in self.missions.keys():
-            self.missions[key]['hdobs'].sort_values(['time'],inplace=True)
-        
-        print(f"--> Completed updating mission data ({(dt.now()-timer_start).total_seconds():.1f} seconds)")
-    
-    def get_mission(self,mission_id):
-        
+            self.missions[key]['hdobs'].sort_values(['time'], inplace=True)
+
+        print(
+            f"--> Completed updating mission data ({(dt.now()-timer_start).total_seconds():.1f} seconds)")
+
+    def get_mission(self, mission_id):
         r"""
         Retrieve a Mission object given the mission ID.
-        
+
         Parameters
         ----------
         mission_id : str
             String denoting requested mission ID. All active mission IDs can be retrieved using ``get_mission_ids()``.
-        
+
         Returns
         -------
         tropycal.recon.Mission
             An instance of a Mission object for this mission.
         """
-        
-        return Mission(self.missions[mission_id],mission_id)
-    
-    def get_mission_ids(self,storm_name=None):
-        
+
+        return Mission(self.missions[mission_id], mission_id)
+
+    def get_mission_ids(self, storm_name=None):
         r"""
         Retrieve a list of all active mission IDs.
-        
+
         Parameters
         ----------
         storm_name : str, optional
             Storm name (case-insensitive) to filter the search by. If None, all active missions are searched.
-        
+
         Returns
         -------
         list
             List containing all active mission IDs.
         """
-        
+
         mission_ids = [key for key in self.missions.keys()]
         if storm_name is not None:
-            mission_ids = [i for i in mission_ids if i.split("-")[2].lower() == storm_name.lower()]
-        
+            mission_ids = [i for i in mission_ids if i.split(
+                "-")[2].lower() == storm_name.lower()]
+
         return mission_ids
-    
+
     def get_hdobs_summary(self):
-        
         r"""
         Retrieve the latest 10-minute HDOBs for all missions. Only valid for missions that were active within the last hour.
 
         Returns
         -------
         dict
             Dictionary with the parsed HDOB data highlights.
-        
+
         Notes
         -----
         .. note::
-            
+
             The decoded HDOBs summary automatically filters out all flagged observations for each variable. If all observations are flagged or missing for a variable, a value of NaN is returned.
         """
-        
+
         data = {}
         for mission_id in self.missions.keys():
             sub_df = self.missions[mission_id]['hdobs'].tail(20)
-            if pd.to_datetime(sub_df['time'].values[-1]) < dt.utcnow()-timedelta(hours=1): continue
-            
-            #Parse data
-            array = [val for i,val in enumerate(sub_df['sfmr']) if 'sfmr' not in sub_df['flag'].values[i]]
-            max_sfmr = np.nanmax(array) if all_nan(array) == False else np.nan
-
-            array = [val for i,val in enumerate(sub_df['p_sfc']) if 'p_sfc' not in sub_df['flag'].values[i]]
-            min_p_sfc = np.nanmin(array) if all_nan(array) == False else np.nan
-
-            array = [val for i,val in enumerate(sub_df['wspd']) if 'wspd' not in sub_df['flag'].values[i]]
-            max_wspd = np.nanmax(array) if all_nan(array) == False else np.nan
-
-            array = [val for i,val in enumerate(sub_df['temp']) if 'temp' not in sub_df['flag'].values[i]]
-            max_temp = np.nanmax(array) if all_nan(array) == False else np.nan
-
-            array = [val for i,val in enumerate(sub_df['dwpt']) if 'dwpt' not in sub_df['flag'].values[i]]
-            max_dwpt = np.nanmax(array) if all_nan(array) == False else np.nan
-            
+            if pd.to_datetime(sub_df['time'].values[-1]) < dt.utcnow() - timedelta(hours=1):
+                continue
+
+            # Parse data
+            array = [val for i, val in enumerate(
+                sub_df['sfmr']) if 'sfmr' not in sub_df['flag'].values[i]]
+            max_sfmr = np.nanmax(array) if not all_nan(array) else np.nan
+
+            array = [val for i, val in enumerate(
+                sub_df['p_sfc']) if 'p_sfc' not in sub_df['flag'].values[i]]
+            min_p_sfc = np.nanmin(array) if not all_nan(array) else np.nan
+
+            array = [val for i, val in enumerate(
+                sub_df['wspd']) if 'wspd' not in sub_df['flag'].values[i]]
+            max_wspd = np.nanmax(array) if not all_nan(array) else np.nan
+
+            array = [val for i, val in enumerate(
+                sub_df['temp']) if 'temp' not in sub_df['flag'].values[i]]
+            max_temp = np.nanmax(array) if not all_nan(array) else np.nan
+
+            array = [val for i, val in enumerate(
+                sub_df['dwpt']) if 'dwpt' not in sub_df['flag'].values[i]]
+            max_dwpt = np.nanmax(array) if not all_nan(array) else np.nan
+
             data[mission_id] = {
                 'min_mslp': min_p_sfc,
                 'max_sfmr': max_sfmr,
                 'max_wspd': max_wspd,
                 'max_temp': max_temp,
                 'max_dwpt': max_dwpt,
                 'start_time': pd.to_datetime(sub_df['time'].values[0]),
                 'end_time': pd.to_datetime(sub_df['time'].values[-1]),
             }
-        
+
         return data
-    
-    def get_hdobs_realtime(self,basin,aircraft,decoded=True):
-        
+
+    def get_hdobs_realtime(self, basin, aircraft, decoded=True):
         r"""
         Retrieve the latest 10-minute HDOBs directly from NHC.
 
         Parameters
         ----------
         basin : str
             Basin for which to retrieve the latest HDOBs for. Available options are ``"north_atlantic"`` or ``"east_pacific"``.
@@ -380,286 +420,305 @@
         -------
         pandas.DataFrame or dict
             Depending on the value of ``decoded``, returns either a Pandas DataFrame or a dictionary with the parsed HDOB data highlights.
 
         Notes
         -----
         .. note::
-            
+
             If using ``decoded=True``, the decoded HDOBs summary automatically filters out all flagged observations for each variable. If all observations are flagged or missing for a variable, a value of NaN is returned.
-        
+
         The National Hurricane Center (NHC) website has 4 links for the latest HDOBs available:
 
         1. NOAA aircraft in the North Atlantic basin
         2. US Air Force (USAF) aircraft in the North Atlantic basin
         3. NOAA aircraft in the East Pacific basin
         4. US Air Force (USAF) aircraft in the East Pacific basin
 
         Each of these links store the most recent 10-minute HDOB observations from the most recent mission for each aircraft type and basin, regardless of how long ago that mission was.
-        
+
         Note that this function is different from ``get_hdobs_summary()``, as the archived NHC recon observations can lag by 10 to as much as 40 minutes. This function fetches the recon observations directly from NHC's realtime recon webpage with the very latest data as soon as it comes in.
         """
 
-        #Determine URL
+        # Determine URL
         if basin == 'north_atlantic':
             if aircraft == 'noaa':
                 url = 'https://www.nhc.noaa.gov/text/URNT15-NOAA.shtml'
             elif aircraft == 'usaf':
                 url = 'https://www.nhc.noaa.gov/text/URNT15-USAF.shtml'
         elif basin == 'east_pacific':
             if aircraft == 'noaa':
                 url = 'https://www.nhc.noaa.gov/text/URPN15-NOAA.shtml'
             elif aircraft == 'usaf':
                 url = 'https://www.nhc.noaa.gov/text/URPN15-USAF.shtml'
 
-        #Read URL content and get DataFrame
-        content = read_url(url,subsplit=False)
+        # Read URL content and get DataFrame
+        content = read_url(url, subsplit=False)
         hdob_content = []
         found = False
         for line in content:
             if line == '<pre>':
                 found = True
                 continue
             if found:
-                if line == '': continue
+                if line == '':
+                    continue
                 hdob_content.append(line)
-            if line == '</pre>': break
-        df = decode_hdob('\n'.join(hdob_content),mission_row=2)
-        
-        if decoded == False: return df
-        
-        #Parse data
-        array = [val for i,val in enumerate(df['sfmr']) if 'sfmr' not in df['flag'].values[i]]
-        max_sfmr = np.nanmax(array) if all_nan(array) == False else np.nan
-
-        array = [val for i,val in enumerate(df['p_sfc']) if 'p_sfc' not in df['flag'].values[i]]
-        min_p_sfc = np.nanmin(array) if all_nan(array) == False else np.nan
-
-        array = [val for i,val in enumerate(df['wspd']) if 'wspd' not in df['flag'].values[i]]
-        max_wspd = np.nanmax(array) if all_nan(array) == False else np.nan
-
-        array = [val for i,val in enumerate(df['temp']) if 'temp' not in df['flag'].values[i]]
-        max_temp = np.nanmax(array) if all_nan(array) == False else np.nan
-
-        array = [val for i,val in enumerate(df['dwpt']) if 'dwpt' not in df['flag'].values[i]]
-        max_dwpt = np.nanmax(array) if all_nan(array) == False else np.nan
+            if line == '</pre>':
+                break
+        df = decode_hdob('\n'.join(hdob_content), mission_row=2)
+
+        if not decoded:
+            return df
+
+        # Parse data
+        array = [val for i, val in enumerate(
+            df['sfmr']) if 'sfmr' not in df['flag'].values[i]]
+        max_sfmr = np.nanmax(array) if not all_nan(array) else np.nan
+
+        array = [val for i, val in enumerate(
+            df['p_sfc']) if 'p_sfc' not in df['flag'].values[i]]
+        min_p_sfc = np.nanmin(array) if not all_nan(array) else np.nan
+
+        array = [val for i, val in enumerate(
+            df['wspd']) if 'wspd' not in df['flag'].values[i]]
+        max_wspd = np.nanmax(array) if not all_nan(array) else np.nan
+
+        array = [val for i, val in enumerate(
+            df['temp']) if 'temp' not in df['flag'].values[i]]
+        max_temp = np.nanmax(array) if not all_nan(array) else np.nan
+
+        array = [val for i, val in enumerate(
+            df['dwpt']) if 'dwpt' not in df['flag'].values[i]]
+        max_dwpt = np.nanmax(array) if not all_nan(array) else np.nan
 
         data = {
             'mission_id': df['mission_id'].values[0],
             'min_mslp': min_p_sfc,
             'max_sfmr': max_sfmr,
             'max_wspd': max_wspd,
             'max_temp': max_temp,
             'max_dwpt': max_dwpt,
             'start_time': pd.to_datetime(df['time'].values[0]),
             'end_time': pd.to_datetime(df['time'].values[-1]),
         }
 
         return data
-    
+
 class PseudoStorm():
-    
+
     r"""
     Creates a dummy Storm object for functions that require a Storm object.
     """
-    
+
     def __init__(self):
-        
-        self.dict = {'type':['TS'],'vmax':[50],'wmo_basin':'north_atlantic','name':'Test'}
+
+        self.dict = {
+            'type': ['TS'],
+            'vmax': [50],
+            'wmo_basin': 'north_atlantic',
+            'name': 'Test'
+        }
 
 class Mission():
-    
+
     r"""
     Creates an instance of a Mission object.
-    
+
     Parameters
     ----------
     data : dict
         Dictionary containing mission data. This is passed automatically from ``RealtimeRecon.get_mission()`` or ``ReconDataset.get_mission()``.
     mission_id : str
         String representing the full Mission ID. This is passed automatically from ``RealtimeRecon.get_mission()`` or ``ReconDataset.get_mission()``.
-    
+
     Returns
     -------
     tropycal.recon.Mission
         An instance of a Mission object.
-    
+
     Notes
     -----
     Mission objects can be retrieved via two methods. The first method is for realtime Recon missions, retrieved directly from ``RealtimeRecon`` objects provided a mission ID string. For this example below, we'll retrieve the latest mission available:
-    
+
     .. code-block:: python
-    
+
         #Read in all recent mission data
         from tropycal import recon
         realtime_obj = recon.RealtimeRecon()
-        
+
         #Get latest mission ID string (or set this to any of the available mission IDs)
         latest_mission_id = realtime_obj.get_mission_ids()[-1]
-        
+
         #Retrieve an instance of Mission
         mission = realtime_obj.get_mission(latest_mission_id)
-    
+
     The second method to retrieve a Mission object is for archived recon missions, via ReconDataset:
-    
+
     .. code-block:: python
-    
+
         #Read in HURDATv2 dataset
         from tropycal import tracks
         basin = tracks.TrackDataset()
         storm = basin.get_storm(('michael',2018))
-        
+
         #Get mission #11
         mission = storm.recon.get_mission(11)
-        
+
     This instance of Mission can now access all of the attributes and methods of the Mission class.
-    
+
     Note that since the Mission object is mission-centric, not storm-centric, much of the functionality available in the storm-centric recon classes is not available here, though some of the plotting functionality (plotting HDOB points, time series, or dropsonde Skew-T) is available.
     """
-    
+
     def __repr__(self):
         summary = ["<tropycal.recon.Mission>"]
-        
-        #Find maximum wind and minimum pressure
-        array = [val for i,val in enumerate(self.hdobs['sfmr']) if 'sfmr' not in self.hdobs['flag'].values[i]]
-        max_sfmr = np.nanmax(array) if all_nan(array) == False else np.nan
-        array = [val for i,val in enumerate(self.hdobs['p_sfc']) if 'p_sfc' not in self.hdobs['flag'].values[i]]
-        min_psfc = np.nanmin(array) if all_nan(array) == False else np.nan
-        array = [val for i,val in enumerate(self.hdobs['wspd']) if 'wspd' not in self.hdobs['flag'].values[i]]
-        max_wspd = np.nanmax(array) if all_nan(array) == False else np.nan
-        array = [val for i,val in enumerate(self.hdobs['pkwnd']) if 'pkwnd' not in self.hdobs['flag'].values[i]]
-        max_pkwnd = np.nanmax(array) if all_nan(array) == False else np.nan
-        time_range = [pd.to_datetime(t) for t in (np.nanmin(self.hdobs['time']),np.nanmax(self.hdobs['time']))]
 
-        #Add summary text
+        # Find maximum wind and minimum pressure
+        array = [val for i, val in enumerate(
+            self.hdobs['sfmr']) if 'sfmr' not in self.hdobs['flag'].values[i]]
+        max_sfmr = np.nanmax(array) if not all_nan(array) else np.nan
+        array = [val for i, val in enumerate(
+            self.hdobs['p_sfc']) if 'p_sfc' not in self.hdobs['flag'].values[i]]
+        min_psfc = np.nanmin(array) if not all_nan(array) else np.nan
+        array = [val for i, val in enumerate(
+            self.hdobs['wspd']) if 'wspd' not in self.hdobs['flag'].values[i]]
+        max_wspd = np.nanmax(array) if not all_nan(array) else np.nan
+        array = [val for i, val in enumerate(
+            self.hdobs['pkwnd']) if 'pkwnd' not in self.hdobs['flag'].values[i]]
+        max_pkwnd = np.nanmax(array) if not all_nan(array) else np.nan
+        time_range = [pd.to_datetime(t) for t in (
+            np.nanmin(self.hdobs['time']), np.nanmax(self.hdobs['time']))]
+
+        # Add summary text
         emdash = '\u2014'
-        summary_keys = {'Dropsondes':len(self.dropsondes),
-                        'VDMs':len(self.vdms),
-                        'Max 30sec flight level wind':f"{max_wspd} knots",
-                        'Max 10sec flight level wind':f"{max_pkwnd} knots",
-                        'Max SFMR wind':f"{max_sfmr} knots",
-                        'Min surface pressure':f"{min_psfc} hPa"}
-        
-        #Add text to output
+        summary_keys = {
+            'Dropsondes': len(self.dropsondes),
+            'VDMs': len(self.vdms),
+            'Max 30sec flight level wind': f"{max_wspd} knots",
+            'Max 10sec flight level wind': f"{max_pkwnd} knots",
+            'Max SFMR wind': f"{max_sfmr} knots",
+            'Min surface pressure': f"{min_psfc} hPa"
+        }
+
+        # Add text to output
         summary.append("Mission Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
-        #Add attributes
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
+        # Add attributes
         summary.append("\nAttributes:")
-        add_space = np.max([len(key) for key in self.attrs.keys()])+3
+        add_space = np.max([len(key) for key in self.attrs.keys()]) + 3
         for key in self.attrs.keys():
-            key_name = key+":"
+            key_name = key + ":"
             summary.append(f'{" "*4}{key_name:<{add_space}}{self.attrs[key]}')
 
         return "\n".join(summary)
-    
-    def __init__(self,data,mission_id):
-        
-        #Retrieve variables
+
+    def __init__(self, data, mission_id):
+
+        # Retrieve variables
         self.vdms = data['vdms']
         self.hdobs = data['hdobs']
         self.dropsondes = data['dropsondes']
-        
-        #Retrieve attributes
+
+        # Retrieve attributes
         self.mission_id = mission_id
         self.aircraft = data['aircraft']
         self.storm_name = data['storm_name']
         self.mission_id = mission_id
         self.start_time = pd.to_datetime(np.nanmin(self.hdobs['time']))
         self.end_time = pd.to_datetime(np.nanmax(self.hdobs['time']))
-        
-        #Get sources
+
+        # Get sources
         if 'source' in data.keys():
             self.source = data['source']
         else:
             if self.start_time.year <= 2005 and self.start_time.year >= 1989:
-                self.source = ['National Hurricane Center (NHC)',"UCAR's Tropical Cyclone Guidance Project (TCGP)"]
+                self.source = [
+                    'National Hurricane Center (NHC)', "UCAR's Tropical Cyclone Guidance Project (TCGP)"]
             else:
                 self.source = 'National Hurricane Center (NHC)'
-        
-        #Retrieve attributes
+
+        # Retrieve attributes
         self.attrs = {
-            'aircraft':data['aircraft'],
-            'storm_name':data['storm_name'],
-            'mission_id':mission_id,
+            'aircraft': data['aircraft'],
+            'storm_name': data['storm_name'],
+            'mission_id': mission_id,
             'start_time': pd.to_datetime(np.nanmin(self.hdobs['time'])),
             'end_time': pd.to_datetime(np.nanmax(self.hdobs['time'])),
             'source': self.source,
         }
-        
-        #Add status for mission
+
+        # Add status for mission
         if self.aircraft.upper() == 'NOAA9':
             values = ['Upper Air' for i in self.hdobs['plane_p'].values]
         else:
             if self.attrs['start_time'].year >= 2007:
                 values = get_status(self.hdobs['plane_p'].values)
             else:
-                values = get_status(self.hdobs['plane_z'].values,use_z=True)
+                values = get_status(self.hdobs['plane_z'].values, use_z=True)
         try:
             del self.hdobs['status']
         except:
             pass
-        self.hdobs.insert(loc=len(self.hdobs.columns),column='status',value=values)
-    
+        self.hdobs.insert(loc=len(self.hdobs.columns),
+                          column='status', value=values)
+
     def get_hdobs(self):
-        
         r"""
         Returns High Density Observations (HDOBs) for this mission.
-        
+
         Returns
         -------
         Pandas.DataFrame
             DataFrame containing HDOBs entries for this mission.
         """
-        
+
         return self.hdobs
-        
+
     def get_dropsondes(self):
-        
         r"""
         Returns dropsondes for this mission.
-        
+
         Returns
         -------
         dict
             Dictionary containing dropsonde data.
         """
-        
+
         return self.dropsondes
-    
+
     def get_vdms(self):
-        
         r"""
         Returns Vortex Data Messages (VDMs) for this mission.
-        
+
         Returns
         -------
         dict
             Dictionary containing VDM data.
         """
-        
+
         return self.vdms
-    
+
     def status(self):
-        
         r"""
         Returns the current mission status.
-        
+
         Returns
         -------
         str
             String corresponding to the current mission status.
-        
+
         Notes
         -----
         This function returns four possible values:
-        
+
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Value
              - Description
            * - "En Route"
@@ -667,27 +726,26 @@
            * - "In Progress"
              - Mission has descended below 650 hPa, or is constant below 650 hPa
            * - "Finished"
              - Mission has re-ascended above 650 hPa for a sufficient duration
            * - "Upper Air"
              - Mission is a NOAA9 aircraft, suggesting it is an upper-level mission.
         """
-        
+
         return self.hdobs['status'].values[-1]
-    
-    def plot_points(self,varname='wspd',in_storm=False,barbs=False,plot_vdms=False,domain="dynamic",ax=None,cartopy_proj=None,**kwargs):
-        
+
+    def plot_points(self, varname='wspd', in_storm=False, barbs=False, plot_vdms=False, domain="dynamic", ax=None, cartopy_proj=None, **kwargs):
         r"""
         Creates a plot of High Density Observations (HDOBs) data points.
-        
+
         Parameters
         ----------
         varname : str
             Variable to plot. Can be one of the following keys in dataframe:
-            
+
             * **"sfmr"** = SFMR surface wind
             * **"wspd"** = 30-second flight level wind (default)
             * **"pkwnd"** = 10-second flight level wind
             * **"p_sfc"** = extrapolated surface pressure
         in_storm : bool, optional
             If True, only plots points considered within the storm, or all points if en route. Default is False (plot all points).
         barbs : bool, optional
@@ -696,141 +754,145 @@
             If True, plots a dot with the minimum MSLP (hPa) from all mission VDMs. Default is False.
         domain : str, optional
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs, optional
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-            
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of recon plot. Please refer to :ref:`options-prop-recon-plot` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         1. Plotting wind barbs only works for wind related variables. ``barbs`` will be automatically set to False for non-wind variables.
-        
+
         2. The special colormap **category_recon** can be used in the prop dict (``prop={'cmap':'category_recon'}``). This uses the standard SSHWS colormap, but with a new color for wind between 50 and 64 knots.
-        
+
         .. warning::
 
             The ``in_storm`` flag uses an internal algorithm to determine if the recon mission is near the storm, without knowledge of the actual coordinates of the storm. This algorithm works well for most standard recon missions, but can return unexpected results for the occasional outlier missions or incomplete/aborted missions.
-        
+
         """
-        
-        #Change barbs
-        if varname == 'p_sfc': barbs = False
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-                
-        #Get plot data
+
+        # Change barbs
+        if varname == 'p_sfc':
+            barbs = False
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Get plot data
         if in_storm and self.aircraft.upper() != 'NOAA9':
             try:
                 dfRecon = self.hdobs.loc[self.hdobs['status'] == 'In Storm']
-                if len(dfRecon) == 0: dfRecon = self.hdobs
+                if len(dfRecon) == 0:
+                    dfRecon = self.hdobs
             except:
                 dfRecon = self.hdobs
         else:
             dfRecon = self.hdobs
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         self.plot_obj = ReconPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
             cartopy_proj = self.plot_obj.proj
-        
-        #Get VDMs
+
+        # Get VDMs
         if plot_vdms:
             vdms = self.vdms
         else:
             vdms = []
-        
-        #Plot recon
-        plot_ax = self.plot_obj.plot_points(PseudoStorm(),dfRecon,domain,barbs=barbs,varname=varname,radlim=None,ax=ax,prop=prop,map_prop=map_prop,mission=True,vdms=vdms,mission_id=self.mission_id)
-        
-        #Return axis
+
+        # Plot recon
+        plot_ax = self.plot_obj.plot_points(PseudoStorm(), dfRecon, domain, barbs=barbs, varname=varname,
+                                            radlim=None, ax=ax, prop=prop, map_prop=map_prop, mission=True, vdms=vdms, mission_id=self.mission_id)
+
+        # Return axis
         return plot_ax
-        
-    def plot_skewt(self,number=None):
-        
+
+    def plot_skewt(self, number=None):
         r"""
         Plot a single dropsonde Skew-T for a given dropsonde number.
-        
+
         Parameters
         ----------
         number : int
             Number of dropsonde during mission. If None (default), plots the latest available dropsonde.
-        
+
         Returns
         -------
         fig
             Figure instance containing the plot.
         """
-        
-        #Retrieve data for dropsonde number
+
+        # Retrieve data for dropsonde number
         try:
-            if number == -1: number = len(self.dropsondes)
-            if number is None: number = len(self.dropsondes)
-            data = [self.dropsondes[number-1]]
+            if number == -1:
+                number = len(self.dropsondes)
+            if number is None:
+                number = len(self.dropsondes)
+            data = [self.dropsondes[number - 1]]
         except:
             raise ValueError("Requested dropsonde number not available.")
-        
-        #Format title string
+
+        # Format title string
         title_string = f'Mission ID: {self.mission_id}\nDropsonde #{number}'
-        
-        return plot_skewt(data,title_string)
-    
-    def plot_time_series(self,varname=('p_sfc','wspd'),time=None,realtime=False,**kwargs):
-        
+
+        return plot_skewt(data, title_string)
+
+    def plot_time_series(self, varname=('p_sfc', 'wspd'), time=None, realtime=False, **kwargs):
         r"""
         Plots a time series of one or two variables on an axis.
-        
+
         Parameters
         ----------
         varname : str or tuple
             If one variable to plot, varname is a string of the variable name. If two variables to plot, varname is a tuple of the left and right variable names, respectively. Available varnames are:
-            
+
             * **p_sfc** - Mean Sea Level Pressure (hPa)
             * **temp** - Flight Level Temperature (C)
             * **dwpt** - Flight Level Dewpoint (C)
             * **wspd** - Flight Level Wind (kt)
             * **sfmr** - Surface Wind (kt)
             * **pkwnd** - Peak Wind Gust (kt)
             * **rain** - Rain Rate (mm/hr)
             * **plane_z** - Geopotential Height (m)
             * **plane_p** - Pressure (hPa)
         time : tuple
             Tuple of start and end times (datetime.datetime) to plot. If None, all times available are plotted.
         realtime : bool
             If True, the most recent 2 hours of the mission will plot, overriding the time argument. Default is False.
-        
+
         Other Parameters
         ----------------
         left_prop : dict
             Dictionary of properties for the left line. Scroll down for more information.
         right_prop : dict
             Dictionary of properties for the right line. Scroll down for more information.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         The following properties are available for customizing the plot, via ``left_prop`` and ``right_prop``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
@@ -840,109 +902,135 @@
            * - ms
              - Marker size. If zero, none will be plotted. Default is zero.
            * - color
              - Color of lines (and markers if used). Default varies per varname.
            * - linewidth
              - Line width. Default is 1.0.
         """
-        
-        #Pop kwargs
-        left_prop = kwargs.pop('left_prop',{})
-        right_prop = kwargs.pop('right_prop',{})
-        
-        #Retrieve variables
+
+        # Pop kwargs
+        left_prop = kwargs.pop('left_prop', {})
+        right_prop = kwargs.pop('right_prop', {})
+
+        # Retrieve variables
         twin_ax = False
-        if isinstance(varname,tuple):
+        if isinstance(varname, tuple):
             varname_right = varname[1]
             varname = varname[0]
             twin_ax = True
             varname_right_info = time_series_plot(varname_right)
         varname_info = time_series_plot(varname)
-        
-        #Get data
+
+        # Get data
         df = self.hdobs
-        
-        #Filter by time or realtime flag
+
+        # Filter by time or realtime flag
         if realtime:
             end_time = pd.to_datetime(df['time'].values[-1])
-            df = df.loc[(df['time'] >= end_time-timedelta(hours=2)) & (df['time'] <= end_time)]
+            df = df.loc[(df['time'] >= end_time - timedelta(hours=2))
+                        & (df['time'] <= end_time)]
         elif time is not None:
             df = df.loc[(df['time'] >= time[0]) & (df['time'] <= time[1])]
-        if len(df) == 0: raise ValueError("Time range provided is invalid.")
-        
-        #Filter by default kwargs
-        left_prop_default = {'ms':0,'color':varname_info['color'],'linewidth':1}
-        for key in left_prop.keys(): left_prop_default[key] = left_prop[key]
+        if len(df) == 0:
+            raise ValueError("Time range provided is invalid.")
+
+        # Filter by default kwargs
+        left_prop_default = {
+            'ms': 0,
+            'color': varname_info['color'],
+            'linewidth': 1
+        }
+        for key in left_prop.keys():
+            left_prop_default[key] = left_prop[key]
         left_prop = left_prop_default
         if twin_ax:
-            right_prop_default = {'ms':0,'color':varname_right_info['color'],'linewidth':1}
-            for key in right_prop.keys(): right_prop_default[key] = right_prop[key]
+            right_prop_default = {
+                'ms': 0,
+                'color': varname_right_info['color'],
+                'linewidth': 1
+            }
+            for key in right_prop.keys():
+                right_prop_default[key] = right_prop[key]
             right_prop = right_prop_default
-        
-        #----------------------------------------------------------------------------------
-        
-        #Create figure
-        fig,ax = plt.subplots(figsize=(9,6),dpi=200)
+
+        # ----------------------------------------------------------------------------------
+
+        # Create figure
+        fig, ax = plt.subplots(figsize=(9, 6), dpi=200)
         if twin_ax:
             ax.grid(axis='x')
         else:
             ax.grid()
-        
-        #Plot line
-        line1 = ax.plot(df['time'],df[varname],color=left_prop['color'],linewidth=left_prop['linewidth'],label=varname_info['name'])
+
+        # Plot line
+        line1 = ax.plot(df['time'], df[varname], color=left_prop['color'],
+                        linewidth=left_prop['linewidth'], label=varname_info['name'])
         ax.set_ylabel(varname_info['full_name'])
-        
-        #Plot dots
+
+        # Plot dots
         if left_prop['ms'] >= 1:
             plot_times = df['time'].values
             plot_var = df[varname].values
-            plot_times = [plot_times[i] for i in range(len(plot_times)) if varname not in df['flag'].values[i]]
-            plot_var = [plot_var[i] for i in range(len(plot_var)) if varname not in df['flag'].values[i]]
-            ax.plot(plot_times,plot_var,'o',color=left_prop['color'],ms=left_prop['ms'])
-        
-        #Format x-axis dates
+            plot_times = [plot_times[i] for i in range(
+                len(plot_times)) if varname not in df['flag'].values[i]]
+            plot_var = [plot_var[i] for i in range(
+                len(plot_var)) if varname not in df['flag'].values[i]]
+            ax.plot(plot_times, plot_var, 'o',
+                    color=left_prop['color'], ms=left_prop['ms'])
+
+        # Format x-axis dates
         ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%Mz\n%m/%d'))
-        
-        #Add twin axis
+
+        # Add twin axis
         if twin_ax:
             ax2 = ax.twinx()
-            
-            #Plot line
-            line2 = ax2.plot(df['time'],df[varname_right],color=right_prop['color'],linewidth=right_prop['linewidth'],label=varname_right_info['name'])
+
+            # Plot line
+            line2 = ax2.plot(df['time'], df[varname_right], color=right_prop['color'],
+                             linewidth=right_prop['linewidth'], label=varname_right_info['name'])
             ax2.set_ylabel(varname_right_info['full_name'])
-            
-            #Plot dots
+
+            # Plot dots
             if right_prop['ms'] >= 1:
                 plot_times = df['time'].values
                 plot_var = df[varname_right].values
-                plot_times = [plot_times[i] for i in range(len(plot_times)) if varname_right not in df['flag'].values[i]]
-                plot_var = [plot_var[i] for i in range(len(plot_var)) if varname_right not in df['flag'].values[i]]
-                ax2.plot(plot_times,plot_var,'o',color=right_prop['color'],ms=right_prop['ms'])
+                plot_times = [plot_times[i] for i in range(
+                    len(plot_times)) if varname_right not in df['flag'].values[i]]
+                plot_var = [plot_var[i] for i in range(
+                    len(plot_var)) if varname_right not in df['flag'].values[i]]
+                ax2.plot(plot_times, plot_var, 'o',
+                         color=right_prop['color'], ms=right_prop['ms'])
 
-            #Add legend
+            # Add legend
             lines = line1 + line2
             labels = [l.get_label() for l in lines]
-            ax.legend(lines,labels)
-        
-            #Special handling if both are in units of Celsius
+            ax.legend(lines, labels)
+
+            # Special handling if both are in units of Celsius
             same_unit = False
-            if varname in ['temp','dwpt'] and varname_right in ['temp','dwpt']: same_unit = True
-            if varname in ['sfmr','wspd','pkwnd'] and varname_right in ['sfmr','wspd','pkwnd']: same_unit = True
+            if varname in ['temp', 'dwpt'] and varname_right in ['temp', 'dwpt']:
+                same_unit = True
+            if varname in ['sfmr', 'wspd', 'pkwnd'] and varname_right in ['sfmr', 'wspd', 'pkwnd']:
+                same_unit = True
             if same_unit:
-                min_val = np.nanmin([np.nanmin(df[varname]),np.nanmin(df[varname_right])])
-                max_val = np.nanmax([np.nanmax(df[varname]),np.nanmax(df[varname_right])])*1.05
+                min_val = np.nanmin(
+                    [np.nanmin(df[varname]), np.nanmin(df[varname_right])])
+                max_val = np.nanmax(
+                    [np.nanmax(df[varname]), np.nanmax(df[varname_right])]) * 1.05
                 min_val = min_val * 1.05 if min_val < 0 else min_val * 0.95
-                if np.isnan(min_val): min_val = 0
-                if np.isnan(max_val): max_val = 0
+                if np.isnan(min_val):
+                    min_val = 0
+                if np.isnan(max_val):
+                    max_val = 0
                 if min_val == max_val:
                     min_val = 0
                     max_val = 10
-                ax.set_ylim(min_val,max_val)
-                ax2.set_ylim(min_val,max_val)
-        
-        #Add titles
+                ax.set_ylim(min_val, max_val)
+                ax2.set_ylim(min_val, max_val)
+
+        # Add titles
         title_string = f"\nRecon Aircraft HDOBs\nMission ID: {self.mission_id}"
-        ax.set_title(title_string,loc='left',fontweight='bold')
-        ax.set_title("Plot generated using Tropycal",fontsize=8,loc='right')
-        
-        #Return plot
-        return ax
+        ax.set_title(title_string, loc='left', fontweight='bold')
+        ax.set_title("Plot generated using Tropycal", fontsize=8, loc='right')
+
+        # Return plot
+        return ax
```

### Comparing `tropycal-0.6.1/src/tropycal/recon/tools.py` & `tropycal-1.0/src/tropycal/recon/tools.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,407 +1,423 @@
-import os, sys
 import numpy as np
 import pandas as pd
-from datetime import datetime as dt,timedelta
-from scipy.ndimage import gaussian_filter as gfilt,gaussian_filter1d as gfilt1d
-from scipy.interpolate import griddata,interp2d,interp1d,SmoothBivariateSpline
-import warnings
-import matplotlib as mlib
-import matplotlib.colors as mcolors
+from datetime import datetime as dt, timedelta
+from scipy.ndimage import gaussian_filter as gfilt
+from scipy.interpolate import griddata, interp1d
 import matplotlib.dates as mdates
 import copy
 
 from ..utils import classify_subtropical, get_storm_classification
 
-def uv_from_wdir(wspd,wdir):
-    d2r = np.pi/180.
+
+def uv_from_wdir(wspd, wdir):
+    d2r = np.pi / 180.
     theta = (270 - wdir) * d2r
     u = wspd * np.cos(theta)
     v = wspd * np.sin(theta)
-    return u,v
+    return u, v
 
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
 # TOOLS FOR RECON INTERPOLATION
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
+
 
 class interpRecon:
-    
+
     """
     Interpolates storm-centered data by time and space.
     """
-    
-    def __init__(self,dfRecon,varname,radlim=None,window=6,align='center'):
-        
-        #Retrieve dataframe containing recon data, and variable to be interpolated
+
+    def __init__(self, dfRecon, varname, radlim=None, window=6, align='center'):
+
+        # Retrieve dataframe containing recon data, and variable to be interpolated
         self.dfRecon = dfRecon
         self.varname = varname
         self.window = window
         self.align = align
-        
-        #Specify outer radius cutoff in kilometer
+
+        # Specify outer radius cutoff in kilometer
         if radlim is None:
-            self.radlim = 200 #km
+            self.radlim = 200  # km
         else:
             self.radlim = radlim
-    
-    
+
     def interpPol(self):
         r"""
         Interpolates storm-centered recon data into a polar grid, and outputs the radius grid, azimuth grid and interpolated variable.
         """
-        
-        #Read in storm-centered data and storm-relative coordinates for all times
-        data = [k for i,j,k in zip(self.dfRecon['xdist'],self.dfRecon['ydist'],self.dfRecon[self.varname]) if not np.isnan([i,j,k]).any()]
-        path = [(i,j) for i,j,k in zip(self.dfRecon['xdist'],self.dfRecon['ydist'],self.dfRecon[self.varname]) if not np.isnan([i,j,k]).any()]
 
-        #Function for interpolating cartesian to polar coordinates
+        # Read in storm-centered data and storm-relative coordinates for all times
+        data = [k for i, j, k in zip(self.dfRecon['xdist'], self.dfRecon['ydist'],
+                                     self.dfRecon[self.varname]) if not np.isnan([i, j, k]).any()]
+        path = [(i, j) for i, j, k in zip(self.dfRecon['xdist'], self.dfRecon['ydist'],
+                                          self.dfRecon[self.varname]) if not np.isnan([i, j, k]).any()]
+
+        # Function for interpolating cartesian to polar coordinates
         def cart2pol(x, y, offset=0):
             rho = np.sqrt(x**2 + y**2)
             phi = np.arctan2(y, x)
-            return(rho, phi+offset)
-        
-        #Interpolate every storm-centered coordinate pair into polar coordinates
+            return (rho, phi + offset)
+
+        # Interpolate every storm-centered coordinate pair into polar coordinates
         pol_path = [cart2pol(*p) for p in path]
 
-        #Wraps around the data to ensure no cutoff around 0 degrees
-        pol_path_wrap = [cart2pol(*p,offset=-2*np.pi) for p in path]+pol_path+\
-                    [cart2pol(*p,offset=2*np.pi) for p in path]
-        data_wrap = np.concatenate([data]*3)
-        
-        #Creates a grid of rho (radius) and phi (azimuth)
-        grid_rho, grid_phi = np.meshgrid(np.arange(0,self.radlim+.1,.5),np.linspace(-np.pi,np.pi,181))
-    
-        #Interpolates storm-centered point data in polar coordinates onto a gridded polar coordinate field
-        grid_z_pol = griddata(pol_path_wrap,data_wrap,(grid_rho,grid_phi),method='linear')
-        
+        # Wraps around the data to ensure no cutoff around 0 degrees
+        pol_path_wrap = [cart2pol(*p, offset=-2 * np.pi) for p in path] + pol_path +\
+            [cart2pol(*p, offset=2 * np.pi) for p in path]
+        data_wrap = np.concatenate([data] * 3)
+
+        # Creates a grid of rho (radius) and phi (azimuth)
+        grid_rho, grid_phi = np.meshgrid(
+            np.arange(0, self.radlim + .1, .5), np.linspace(-np.pi, np.pi, 181))
+
+        # Interpolates storm-centered point data in polar coordinates onto a gridded polar coordinate field
+        grid_z_pol = griddata(pol_path_wrap, data_wrap,
+                              (grid_rho, grid_phi), method='linear')
+
         try:
-            #Calculate radius of maximum wind (RMW)
-            rmw = grid_rho[0,np.nanargmax(np.mean(grid_z_pol,axis=0))]
+            # Calculate radius of maximum wind (RMW)
+            rmw = grid_rho[0, np.nanargmax(np.mean(grid_z_pol, axis=0))]
 
-            #Within the RMW, replace NaNs with minimum value within the RMW
-            filleye = np.where((grid_rho<rmw) & (np.isnan(grid_z_pol)))
+            # Within the RMW, replace NaNs with minimum value within the RMW
+            filleye = np.where((grid_rho < rmw) & (np.isnan(grid_z_pol)))
 
-            grid_z_pol[filleye]=np.nanmin(grid_z_pol[np.where(grid_rho<rmw)])
+            grid_z_pol[filleye] = np.nanmin(
+                grid_z_pol[np.where(grid_rho < rmw)])
         except:
             pass
-    
-        #Return fields
-        return grid_rho, grid_phi, grid_z_pol      
-        
-    
+
+        # Return fields
+        return grid_rho, grid_phi, grid_z_pol
+
     def interpCart(self):
         r"""
         Interpolates polar storm-centered gridded fields into cartesian coordinates
         """
-        
-        #Interpolate storm-centered recon data into gridded polar grid (rho, phi and gridded data)
+
+        # Interpolate storm-centered recon data into gridded polar grid (rho, phi and gridded data)
         grid_rho, grid_phi, grid_z_pol = self.interpPol()
-        
-        #Calculate RMW
-        rmw = grid_rho[0,np.nanargmax(np.mean(grid_z_pol,axis=0))]
-        
-        #Wraps around the data to ensure no cutoff around 0 degrees
-        grid_z_pol_wrap = np.concatenate([grid_z_pol]*3)
-        
-        #Radially smooth based on RMW - more smoothing farther out from RMW
-        grid_z_pol_final = np.array([gfilt(grid_z_pol_wrap,(6,3+abs(r-rmw)/10))[:,i] \
-                                     for i,r in enumerate(grid_rho[0,:])]).T[len(grid_phi):2*len(grid_phi)]
-        
-        #Function for interpolating polar cartesian to coordinates
+
+        # Calculate RMW
+        rmw = grid_rho[0, np.nanargmax(np.mean(grid_z_pol, axis=0))]
+
+        # Wraps around the data to ensure no cutoff around 0 degrees
+        grid_z_pol_wrap = np.concatenate([grid_z_pol] * 3)
+
+        # Radially smooth based on RMW - more smoothing farther out from RMW
+        grid_z_pol_final = np.array([gfilt(grid_z_pol_wrap, (6, 3 + abs(r - rmw) / 10))[:, i]
+                                     for i, r in enumerate(grid_rho[0, :])]).T[len(grid_phi):2 * len(grid_phi)]
+
+        # Function for interpolating polar cartesian to coordinates
         def pol2cart(rho, phi):
             x = rho * np.cos(phi)
             y = rho * np.sin(phi)
-            return(x, y)
-        
-        #Interpolate the rho and phi gridded fields to a 1D cartesian list
-        pinterp_grid = [pol2cart(i,j) for i,j in zip(grid_rho.flatten(),grid_phi.flatten())]
-        
-        #Flatten the radially smoothed variable grid to match with the shape of pinterp_grid
+            return (x, y)
+
+        # Interpolate the rho and phi gridded fields to a 1D cartesian list
+        pinterp_grid = [pol2cart(i, j) for i, j in zip(
+            grid_rho.flatten(), grid_phi.flatten())]
+
+        # Flatten the radially smoothed variable grid to match with the shape of pinterp_grid
         pinterp_z = grid_z_pol_final.flatten()
-        
-        #Setting up the grid in cartesian coordinate space, based on previously specified radial limit
-        #Grid resolution = 1 kilometer
-        grid_x, grid_y = np.meshgrid(np.linspace(-self.radlim,self.radlim,self.radlim*2+1),\
-                                     np.linspace(-self.radlim,self.radlim,self.radlim*2+1))
-        grid_z = griddata(pinterp_grid,pinterp_z,(grid_x,grid_y),method='linear')
-    
-        #Return output grid
+
+        # Setting up the grid in cartesian coordinate space, based on previously specified radial limit
+        # Grid resolution = 1 kilometer
+        grid_x, grid_y = np.meshgrid(np.linspace(-self.radlim, self.radlim, self.radlim * 2 + 1),
+                                     np.linspace(-self.radlim, self.radlim, self.radlim * 2 + 1))
+        grid_z = griddata(pinterp_grid, pinterp_z,
+                          (grid_x, grid_y), method='linear')
+
+        # Return output grid
         return grid_x, grid_y, grid_z
-    
 
-    def interpHovmoller(self,target_track):
+    def interpHovmoller(self, target_track):
         r"""
         Creates storm-centered interpolated data in polar coordinates for each timestep, and averages azimuthally to create a hovmoller.
-        
+
         target_track = dict
             dict of either archer or hurdat data (contains lat, lon, time/date)
         window = hours
             sets window in hours relative to the time of center pass for interpolation use.
         """
-    
+
         window = self.window
         align = self.align
-    
-        #Store the dataframe containing recon data
+
+        # Store the dataframe containing recon data
         tmpRecon = self.dfRecon.copy()
-        #Sets window as a timedelta object
-        window = timedelta(seconds=int(window*3600))
-        
-        #Error check for time dimension name
+        # Sets window as a timedelta object
+        window = timedelta(seconds=int(window * 3600))
+
+        # Error check for time dimension name
         if 'time' not in target_track.keys():
-            target_track['time']=target_track['date']
-        
-        #Find times of all center passes
-        centerTimes = tmpRecon[tmpRecon['iscenter']==1]['time']
-        
-        #Data is already centered on center time, so shift centerTimes to the end of the window
-        spaceInterpTimes = [t+window/2 for t in centerTimes]
-        
-        #Takes all times within track dictionary that fall between spaceInterpTimes
-        trackTimes = [t for t in target_track['time'] if min(spaceInterpTimes)<t<max(spaceInterpTimes)]
-        
-        #Iterate through all data surrounding a center pass given the window previously specified, and create a polar
-        #grid for each
+            target_track['time'] = target_track['date']
+
+        # Find times of all center passes
+        centerTimes = tmpRecon[tmpRecon['iscenter'] == 1]['time']
+
+        # Data is already centered on center time, so shift centerTimes to the end of the window
+        spaceInterpTimes = [t + window / 2 for t in centerTimes]
+
+        # Takes all times within track dictionary that fall between spaceInterpTimes
+        trackTimes = [t for t in target_track['time'] if min(
+            spaceInterpTimes) < t < max(spaceInterpTimes)]
+
+        # Iterate through all data surrounding a center pass given the window previously specified, and create a polar
+        # grid for each
         start_time = dt.now()
         print("--> Starting interpolation")
-        
-        spaceInterpData={}
+
+        spaceInterpData = {}
         for time in spaceInterpTimes:
-            #Temporarily set dfRecon to this centered subset window
-            self.dfRecon = tmpRecon[(tmpRecon['time']>time-window) & (tmpRecon['time']<=time)]
-            #print(time) #temporarily disabling this
-            grid_rho, grid_phi, grid_z_pol = self.interpPol() #Create polar centered grid
-            grid_azim_mean = np.mean(grid_z_pol,axis=0) #Average azimuthally
-            spaceInterpData[time] = grid_azim_mean #Append data for this time step to dictionary
-        
-        #Sets dfRecon back to original full data
+            # Temporarily set dfRecon to this centered subset window
+            self.dfRecon = tmpRecon[(
+                tmpRecon['time'] > time - window) & (tmpRecon['time'] <= time)]
+            # print(time) #temporarily disabling this
+            grid_rho, grid_phi, grid_z_pol = self.interpPol()  # Create polar centered grid
+            grid_azim_mean = np.mean(grid_z_pol, axis=0)  # Average azimuthally
+            # Append data for this time step to dictionary
+            spaceInterpData[time] = grid_azim_mean
+
+        # Sets dfRecon back to original full data
         self.dfRecon = tmpRecon
         reconArray = np.array([i for i in spaceInterpData.values()])
 
-        #Interpolate over every half hour
-        newTimes = np.arange(mdates.date2num(trackTimes[0]),mdates.date2num(trackTimes[-1])+1e-3,1/48)    
+        # Interpolate over every half hour
+        newTimes = np.arange(mdates.date2num(
+            trackTimes[0]), mdates.date2num(trackTimes[-1]) + 1e-3, 1 / 48)
         oldTimes = mdates.date2num(np.array(list(spaceInterpData.keys())))
-        #print(len(oldTimes),reconArray.shape)
-        reconTimeInterp=np.apply_along_axis(lambda x: np.interp(newTimes,oldTimes,x),
-                                 axis=0,arr=reconArray)
+        # print(len(oldTimes),reconArray.shape)
+        reconTimeInterp = np.apply_along_axis(lambda x: np.interp(newTimes, oldTimes, x),
+                                              axis=0, arr=reconArray)
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
+        tsec = str(round(time_elapsed.total_seconds(), 2))
         print(f"--> Completed interpolation ({tsec} seconds)")
-        
-        #Output RMW and hovmoller data and store as an attribute in the object
-        self.rmw = grid_rho[0,np.nanargmax(reconTimeInterp,axis=1)]
-        self.Hovmoller = {'time':mdates.num2date(newTimes),'radius':grid_rho[0,:],'hovmoller':reconTimeInterp}
+
+        # Output RMW and hovmoller data and store as an attribute in the object
+        self.rmw = grid_rho[0, np.nanargmax(reconTimeInterp, axis=1)]
+        self.Hovmoller = {'time': mdates.num2date(
+            newTimes), 'radius': grid_rho[0, :], 'hovmoller': reconTimeInterp}
         return self.Hovmoller
 
-    def interpMaps(self,target_track,interval=0.5,stat_vars=None):
+    def interpMaps(self, target_track, interval=0.5, stat_vars=None):
         r"""
         1. Can just output a single map (interpolated to lat/lon grid and projected onto the Cartopy map
         2. If target_track is longer than 1, outputs multiple maps to a directory
         """
-        
+
         window = self.window
         align = self.align
-        
-        #Store the dataframe containing recon data
+
+        # Store the dataframe containing recon data
         tmpRecon = self.dfRecon.copy()
-        #Sets window as a timedelta object
-        window = timedelta(seconds=int(window*3600))
- 
-        #Error check for time dimension name
+        # Sets window as a timedelta object
+        window = timedelta(seconds=int(window * 3600))
+
+        # Error check for time dimension name
         if 'time' not in target_track.keys():
-            target_track['time']=target_track['date']
-       
-        #If target_track > 1 (tuple or list of times), then retrieve multiple center pass times and center around the window
-        if isinstance(target_track['time'],(tuple,list,np.ndarray)):
-            centerTimes=tmpRecon[tmpRecon['iscenter']==1]['time']
-            spaceInterpTimes=[t for t in centerTimes]
-            trackTimes=[t for t in target_track['time'] if min(spaceInterpTimes)-window/2<t<max(spaceInterpTimes)+window/2]
-        #Otherwise, just use a single time
+            target_track['time'] = target_track['date']
+
+        # If target_track > 1 (tuple or list of times), then retrieve multiple center pass times and center around the window
+        if isinstance(target_track['time'], (tuple, list, np.ndarray)):
+            centerTimes = tmpRecon[tmpRecon['iscenter'] == 1]['time']
+            spaceInterpTimes = [t for t in centerTimes]
+            trackTimes = [t for t in target_track['time'] if min(
+                spaceInterpTimes) - window / 2 < t < max(spaceInterpTimes) + window / 2]
+        # Otherwise, just use a single time
         else:
-            spaceInterpTimes=list([target_track['time']])
-            trackTimes=spaceInterpTimes.copy()
-        
-        #Experimental - add recon statistics (e.g., wind, MSLP) to plot
+            spaceInterpTimes = list([target_track['time']])
+            trackTimes = spaceInterpTimes.copy()
+
+        # Experimental - add recon statistics (e.g., wind, MSLP) to plot
         # **** CHECK BACK ON THIS ****
-        spaceInterpData={}
-        recon_stats=None
+        spaceInterpData = {}
+        recon_stats = None
         if stat_vars is not None:
-            recon_stats={name:[] for name in stat_vars.keys()}
-        #Iterate through all data surrounding a center pass given the window previously specified, and create a polar
-        #grid for each
+            recon_stats = {name: [] for name in stat_vars.keys()}
+        # Iterate through all data surrounding a center pass given the window previously specified, and create a polar
+        # grid for each
         start_time = dt.now()
         print("--> Starting interpolation")
-        
+
         for time in spaceInterpTimes:
             print(time)
-            self.dfRecon = tmpRecon[(tmpRecon['time']>time-window/2) & (tmpRecon['time']<=time+window/2)]
-            grid_x,grid_y,grid_z = self.interpCart()
+            self.dfRecon = tmpRecon[(
+                tmpRecon['time'] > time - window / 2) & (tmpRecon['time'] <= time + window / 2)]
+            grid_x, grid_y, grid_z = self.interpCart()
             spaceInterpData[time] = grid_z
             if stat_vars is not None:
                 for name in stat_vars.keys():
-                    recon_stats[name].append(stat_vars[name](self.dfRecon[name]))
-        
-        #Sets dfRecon back to original full data
-        self.dfRecon = tmpRecon        
+                    recon_stats[name].append(
+                        stat_vars[name](self.dfRecon[name]))
+
+        # Sets dfRecon back to original full data
+        self.dfRecon = tmpRecon
         reconArray = np.array([i for i in spaceInterpData.values()])
 
-        #If multiple times, create a lat & lon grid for half hour intervals
-        if len(trackTimes)>1:
-            newTimes = np.arange(mdates.date2num(trackTimes[0]),mdates.date2num(trackTimes[-1])+interval/24,interval/24)    
+        # If multiple times, create a lat & lon grid for half hour intervals
+        if len(trackTimes) > 1:
+            newTimes = np.arange(mdates.date2num(trackTimes[0]), mdates.date2num(
+                trackTimes[-1]) + interval / 24, interval / 24)
             oldTimes = mdates.date2num(np.array(list(spaceInterpData.keys())))
-            reconTimeInterp=np.apply_along_axis(lambda x: np.interp(newTimes,oldTimes,x),
-                                 axis=0,arr=reconArray)
-            #Get centered lat and lon by interpolating from target_track dictionary (whether archer or HURDAT)
-            clon = np.interp(newTimes,mdates.date2num(target_track['time']),target_track['lon'])
-            clat = np.interp(newTimes,mdates.date2num(target_track['time']),target_track['lat'])
+            reconTimeInterp = np.apply_along_axis(lambda x: np.interp(newTimes, oldTimes, x),
+                                                  axis=0, arr=reconArray)
+            # Get centered lat and lon by interpolating from target_track dictionary (whether archer or HURDAT)
+            clon = np.interp(newTimes, mdates.date2num(
+                target_track['time']), target_track['lon'])
+            clat = np.interp(newTimes, mdates.date2num(
+                target_track['time']), target_track['lat'])
         else:
             newTimes = mdates.date2num(trackTimes)[0]
             reconTimeInterp = reconArray[0]
             clon = target_track['lon']
             clat = target_track['lat']
 
-        #Interpolate storm stats to corresponding times
+        # Interpolate storm stats to corresponding times
         if stat_vars is not None:
             for varname in recon_stats.keys():
-                recon_stats[varname] = np.interp(newTimes,oldTimes,recon_stats[varname])
-            
-        #Determine time elapsed
+                recon_stats[varname] = np.interp(
+                    newTimes, oldTimes, recon_stats[varname])
+
+        # Determine time elapsed
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
-        print(f"--> Completed interpolation ({tsec} seconds)")            
-            
-        #Create dict of map data (interpolation time, x & y grids, 'maps' (3D grid of field, time/x/y), and return
-        self.Maps = {'time':mdates.num2date(newTimes),'grid_x':grid_x,'grid_y':grid_y,'maps':reconTimeInterp,
-                           'center_lon':clon,'center_lat':clat,'stats':recon_stats}
+        tsec = str(round(time_elapsed.total_seconds(), 2))
+        print(f"--> Completed interpolation ({tsec} seconds)")
+
+        # Create dict of map data (interpolation time, x & y grids, 'maps' (3D grid of field, time/x/y), and return
+        self.Maps = {'time': mdates.num2date(newTimes), 'grid_x': grid_x, 'grid_y': grid_y, 'maps': reconTimeInterp,
+                     'center_lon': clon, 'center_lat': clat, 'stats': recon_stats}
         return self.Maps
 
     @staticmethod
     def _interpFunc(data1, times1, times2):
-        #Interpolate data
-        f = interp1d(mdates.date2num(times1),data1)
+        # Interpolate data
+        f = interp1d(mdates.date2num(times1), data1)
         data2 = f(mdates.date2num(times2))
         return data2
 
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
 # TOOLS FOR PLOTTING
-#------------------------------------------------------------------------------
+# ------------------------------------------------------------------------------
 
-def find_var(request,thresh):
-    
+
+def find_var(request, thresh):
     r"""
     Given a variable and threshold, returns the variable for plotting. Referenced from ``TrackDataset.gridded_stats()`` and ``TrackPlot.plot_gridded()``. Internal function.
-    
+
     Parameters
     ----------
     request : str
         Descriptor of the requested plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
     thresh : dict
         Dictionary containing thresholds for the plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
-    
+
     Returns
     -------
     thresh : dict
         Returns the thresh dictionary, modified depending on the request.
     varname : str
         String denoting the variable for plotting.
     """
-    
-    #Convert command to lowercase
+
+    # Convert command to lowercase
     request = request.lower()
-    
-    #Count of number of storms
+
+    # Count of number of storms
     if request.find('count') >= 0 or request.find('num') >= 0:
         return thresh, 'time'
 
     if request.find('time') >= 0 or request.find('day') >= 0:
         return thresh, 'time'
-    
-    #Sustained wind, or change in wind speed
+
+    # Sustained wind, or change in wind speed
     if request.find('wind') >= 0 or request.find('vmax') >= 0:
-        return thresh,'wspd'
+        return thresh, 'wspd'
     if request.find('30s wind') >= 0 or request.find('vmax') >= 0:
-        return thresh,'wspd'
+        return thresh, 'wspd'
     if request.find('10s wind') >= 0 or request.find('vmax') >= 0:
-        return thresh,'pkwnd'
-    
-    #Minimum MSLP
+        return thresh, 'pkwnd'
+
+    # Minimum MSLP
     elif request.find('pressure') >= 0 or request.find('slp') >= 0:
-        return thresh,'p_sfc'
+        return thresh, 'p_sfc'
 
-    #Otherwise, error
+    # Otherwise, error
     else:
         msg = "Error: Could not decipher variable. Please refer to documentation for examples on how to phrase the \"request\" string."
         raise RuntimeError(msg)
-        
-def find_func(request,thresh):
-    
+
+
+def find_func(request, thresh):
     r"""
     Given a request and threshold, returns the requested function. Referenced from ``TrackDataset.gridded_stats()``. Internal function.
-    
+
     Parameters
     ----------
     request : str
         Descriptor of the requested plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
     thresh : dict
         Dictionary containing thresholds for the plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
-    
+
     Returns
     -------
     thresh : dict
         Returns the thresh dictionary, modified depending on the request.
     func : lambda
         Returns a function to apply to the data.
     """
-    
+
     print(request)
-    
-    #Convert command to lowercase
+
+    # Convert command to lowercase
     request = request.lower()
-    
-    #Numpy maximum function
+
+    # Numpy maximum function
     if request.find('max') == 0 or request.find('latest') == 0:
         return thresh, lambda x: np.nanmax(x)
-    
-    #Numpy minimum function
+
+    # Numpy minimum function
     if request.find('min') == 0 or request.find('earliest') == 0:
         return thresh, lambda x: np.nanmin(x)
-    
-    #Numpy average function
+
+    # Numpy average function
     elif request.find('mean') >= 0 or request.find('average') >= 0 or request.find('avg') >= 0:
-        thresh['sample_min'] = max([5,thresh['sample_min']]) #Ensure sample minimum is at least 5 per gridpoint
+        # Ensure sample minimum is at least 5 per gridpoint
+        thresh['sample_min'] = max([5, thresh['sample_min']])
         return thresh, lambda x: np.nanmean(x)
-    
-    #Numpy percentile function
+
+    # Numpy percentile function
     elif request.find('percentile') >= 0:
-        ptile = int(''.join([c for i,c in enumerate(request) if c.isdigit() and i < request.find('percentile')]))
-        thresh['sample_min'] = max([5,thresh['sample_min']]) #Ensure sample minimum is at least 5 per gridpoint
-        return thresh, lambda x: np.nanpercentile(x,ptile)
-    
-    #Count function
+        ptile = int(''.join([c for i, c in enumerate(
+            request) if c.isdigit() and i < request.find('percentile')]))
+        # Ensure sample minimum is at least 5 per gridpoint
+        thresh['sample_min'] = max([5, thresh['sample_min']])
+        return thresh, lambda x: np.nanpercentile(x, ptile)
+
+    # Count function
     elif request.find('count') >= 0 or request.find('num') >= 0:
         return thresh, lambda x: len(x)
-    
-    #ACE - cumulative function
-    elif request.find('ace') >=0:
+
+    # ACE - cumulative function
+    elif request.find('ace') >= 0:
         return thresh, lambda x: np.nansum(x)
-    elif request.find('acie') >=0:
+    elif request.find('acie') >= 0:
         return thresh, lambda x: np.nansum(x)
-    
-    #Otherwise, function cannot be identified
+
+    # Otherwise, function cannot be identified
     else:
         msg = "Cannot decipher the function. Please refer to documentation for examples on how to phrase the \"request\" string."
         raise RuntimeError(msg)
 
 
-def get_recon_title(varname,level=None):
-    
+def get_recon_title(varname, level=None):
     r"""
     Generate title descriptor for plots.
     """
-    
+
     if level is None:
         if varname.lower() == 'top':
             titlename = 'Top mandatory level'
             unitname = r'(hPa)'
         if varname.lower() == 'slp':
             titlename = 'Sea level pressure'
             unitname = r'(hPa)'
@@ -451,419 +467,468 @@
         if varname.lower() == 'wdir':
             titlename = f'{level}hPa wind direction'
             unitname = r'(deg)'
         if varname.lower() == 'wspd':
             titlename = f'{level}hPa wind speed'
             unitname = r'(kt)'
 
-    return titlename,unitname
+    return titlename, unitname
+
 
-def hovmoller_plot_title(storm_obj,Hov,varname):
-    
+def hovmoller_plot_title(storm_obj, Hov, varname):
     r"""
     Generate plot title for hovmoller.
     """
-    
-    #Retrieve storm dictionary from Storm object
+
+    # Retrieve storm dictionary from Storm object
     storm_data = storm_obj.dict
-    
-    #------- construct left title ---------
-    
-    #Subset sustained wind array to when the storm was tropical
+
+    # ------- construct left title ---------
+
+    # Subset sustained wind array to when the storm was tropical
     type_array = np.array(storm_data['type'])
-    idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+    idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+        type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
     tropical_vmax = np.array(storm_data['vmax'])[idx]
-    
-    #Coerce to include non-TC points if storm hasn't been designated yet
+
+    # Coerce to include non-TC points if storm hasn't been designated yet
     add_ptc_flag = False
     if len(tropical_vmax) == 0:
         add_ptc_flag = True
         idx = np.where((type_array == 'LO') | (type_array == 'DB'))
     tropical_vmax = np.array(storm_data['vmax'])[idx]
-    
-    #Determine storm classification based on subtropical status & basin
+
+    # Determine storm classification based on subtropical status & basin
     subtrop = classify_subtropical(np.array(storm_data['type']))
     peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
     peak_basin = storm_data['wmo_basin'][peak_idx]
-    storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-    if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
-    
-    #Get title descriptor based on variable
+    storm_type = get_storm_classification(
+        np.nanmax(tropical_vmax), subtrop, peak_basin)
+    if add_ptc_flag:
+        storm_type = "Potential Tropical Cyclone"
+
+    # Get title descriptor based on variable
     vartitle = get_recon_title(varname)
-    
-    #Add left title
+
+    # Add left title
     dot = u"\u2022"
-    title_left = f"{storm_type} {storm_data['name']}\n" + 'Recon: '+' '.join(vartitle)
+    title_left = f"{storm_type} {storm_data['name']}\n" + \
+        'Recon: ' + ' '.join(vartitle)
 
-    #------- construct right title ---------
-    
-    #Determine start and end dates of hovmoller
-    start_date = dt.strftime(min(Hov['time']),'%H:%M UTC %d %b %Y')
-    end_date = dt.strftime(max(Hov['time']),'%H:%M UTC %d %b %Y')
-    title_right = f'Start ... {start_date}\nEnd ... {end_date}'
-    
-    #Return both titles
-    return title_left,title_right
+    # ------- construct right title ---------
 
+    # Determine start and end dates of hovmoller
+    start_time = dt.strftime(min(Hov['time']), '%H:%M UTC %d %b %Y')
+    end_time = dt.strftime(max(Hov['time']), '%H:%M UTC %d %b %Y')
+    title_right = f'Start ... {start_time}\nEnd ... {end_time}'
 
-def get_bounds(data,bounds):
+    # Return both titles
+    return title_left, title_right
+
+
+def get_bounds(data, bounds):
     try:
-        datalims = (np.nanmin(data),np.nanmax(data))
+        datalims = (np.nanmin(data), np.nanmax(data))
     except:
         datalims = bounds
-    bounds = [l if b is None else b for b,l in zip(bounds,datalims)]
+    bounds = [l if b is None else b for b, l in zip(bounds, datalims)]
     bounds = [b for b in bounds if b is not None]
-    return (min(bounds),max(bounds))
+    return (min(bounds), max(bounds))
+
 
 def time_series_plot(varname):
-    
     r"""
     Returns a default color and name associated with each varname for hdob time series.
-    
+
     Parameters
     ----------
     varname : str
         Requested variable name.
-    
+
     Returns
     -------
     dict
         Dictionary with color, name and full name for variable to plot.
     """
 
     colors = {
-        'p_sfc':'red',
-        'temp':'red',
-        'dwpt':'green',
-        'wspd':'blue',
-        'sfmr':'#282893',
-        'pkwnd':'#5A9AF0',
-        'rain':'#C551DC',
-        'plane_z':'#909090',
-        'plane_p':'#4D4D4D',
+        'p_sfc': 'red',
+        'temp': 'red',
+        'dwpt': 'green',
+        'wspd': 'blue',
+        'sfmr': '#282893',
+        'pkwnd': '#5A9AF0',
+        'rain': '#C551DC',
+        'plane_z': '#909090',
+        'plane_p': '#4D4D4D',
     }
-    
+
     names = {
-        'p_sfc':'MSLP',
-        'temp':'Temperature',
-        'dwpt':'Dewpoint',
-        'wspd':'Flight Level Wind',
-        'sfmr':'Surface Wind',
-        'pkwnd':'Peak Wind Gust',
-        'rain':'Rain Rate',
-        'plane_z':'Altitude',
-        'plane_p':'Pressure',
+        'p_sfc': 'MSLP',
+        'temp': 'Temperature',
+        'dwpt': 'Dewpoint',
+        'wspd': 'Flight Level Wind',
+        'sfmr': 'Surface Wind',
+        'pkwnd': 'Peak Wind Gust',
+        'rain': 'Rain Rate',
+        'plane_z': 'Altitude',
+        'plane_p': 'Pressure',
     }
-    
+
     full_names = {
-        'p_sfc':'Mean Sea Level Pressure (hPa)',
-        'temp':'Temperature (C)',
-        'dwpt':'Dewpoint (C)',
-        'wspd':'Flight Level Wind (kt)',
-        'sfmr':'Surface Wind (kt)',
-        'pkwnd':'Peak Wind Gust (kt)',
-        'rain':'Rain Rate (mm/hr)',
-        'plane_z':'Geopotential Height (m)',
-        'plane_p':'Pressure (hPa)',
+        'p_sfc': 'Mean Sea Level Pressure (hPa)',
+        'temp': 'Temperature (C)',
+        'dwpt': 'Dewpoint (C)',
+        'wspd': 'Flight Level Wind (kt)',
+        'sfmr': 'Surface Wind (kt)',
+        'pkwnd': 'Peak Wind Gust (kt)',
+        'rain': 'Rain Rate (mm/hr)',
+        'plane_z': 'Geopotential Height (m)',
+        'plane_p': 'Pressure (hPa)',
     }
-    
-    color = colors.get(varname,'')
-    name = names.get(varname,'')
-    full_name = full_names.get(varname,'')
-    
-    return {'color':color,'name':name,'full_name':full_name}
 
-#=======================================================================================================
+    color = colors.get(varname, '')
+    name = names.get(varname, '')
+    full_name = full_names.get(varname, '')
+
+    return {'color': color, 'name': name, 'full_name': full_name}
+
+# =======================================================================================================
 # Decoding HDOBs
-#=======================================================================================================
+# =======================================================================================================
 
-def decode_hdob_2005_noaa(content,strdate,mission_row=0):
-    
+
+def decode_hdob_2005_noaa(content, strdate, mission_row=0):
     r"""
     Function for decoding HDOBs in the format between 2002 and 2005, for NOAA aircraft.
     """
-    
+
     def check_error(string):
-        if '/' in string: return True
-        if ';' in string: return True
-        if 'off' in string: return True
+        if '/' in string:
+            return True
+        if ';' in string:
+            return True
+        if 'off' in string:
+            return True
         return False
-    
-    #Split items by lines
+
+    # Split items by lines
     temp = [i.split() for i in content.split('\n')]
-    temp = [i for j,i in enumerate(temp) if len(i)>0]
+    temp = [i for j, i in enumerate(temp) if len(i) > 0]
     items = []
     found_sxxx = False
-    for j,i in enumerate(temp):
-        if j>mission_row and len(i[0]) >= 2 and i[0][:2] == "$$": break
-        if j>mission_row+12 and len(i[0]) >= 3 and i[0][:3] == "000": break #avoid reading in extra set of HDOBs
-        if j<=mission_row:
-            if len(i[0]) >= 4 and i[0][:3] in ["URN","URP"]:
-                if found_sxxx == False:
+    for j, i in enumerate(temp):
+        if j > mission_row and len(i[0]) >= 2 and i[0][:2] == "$$":
+            break
+        if j > mission_row + 12 and len(i[0]) >= 3 and i[0][:3] == "000":
+            break  # avoid reading in extra set of HDOBs
+        if j <= mission_row:
+            if len(i[0]) >= 4 and i[0][:3] in ["URN", "URP"]:
+                if not found_sxxx:
                     found_sxxx = True
                     items.append(i)
             else:
                 items.append(i)
-        if j>mission_row and i[0][0].isdigit() and len(i) > 8:
+        if j > mission_row and i[0][0].isdigit() and len(i) > 8:
             items.append(i)
-    
-    #Parse dates
+
+    # Parse dates
     data = {}
-    data['time'] = [dt.strptime(strdate+i[0],'%Y%m%d%H%M%S') for i in items[3:]]
-    if data['time'][0].hour>12 and data['time'][-1].hour<12:
-        data['time'] = [t+timedelta(days=[0,1][t.hour<12]) for t in data['time']]
+    data['time'] = [dt.strptime(strdate + i[0], '%Y%m%d%H%M%S')
+                    for i in items[3:]]
+    if data['time'][0].hour > 12 and data['time'][-1].hour < 12:
+        data['time'] = [t + timedelta(days=[0, 1][t.hour < 12])
+                        for t in data['time']]
 
-    #Derive plane altitude using D-value
+    # Derive plane altitude using D-value
     altitude = []
     for i in items[3:]:
         if '/' in i[3] or '/' in i[4]:
             altitude.append(np.nan)
         else:
             pres_altitude = int(i[3])
-            d_value = int(i[4][1:])*-1 if i[4][0] == '-' else int(i[4][1:])
-            altitude.append(pres_altitude+d_value)
+            d_value = int(i[4][1:]) * -1 if i[4][0] == '-' else int(i[4][1:])
+            altitude.append(pres_altitude + d_value)
     data['plane_z'] = altitude
-    
-    #Parse full data
-    data['lat'] = [np.nan if check_error(i[1]) else round((float(i[1][:-2])+float(i[1][-2:])/60),2) \
+
+    # Parse full data
+    data['lat'] = [np.nan if check_error(i[1]) else round((float(i[1][:-2]) + float(i[1][-2:]) / 60), 2)
                    for i in items[3:]]
-    data['lon'] = [np.nan if check_error(i[2]) else round((float(i[2][:-2])+float(i[2][-2:])/60),2)*-1 \
+    data['lon'] = [np.nan if check_error(i[2]) else round((float(i[2][:-2]) + float(i[2][-2:]) / 60), 2) * -1
                    for i in items[3:]]
-    data['temp'] = [np.nan if check_error(i[6]) else round(float(i[7])*0.1,1) for i in items[3:]]
-    data['dwpt'] = [np.nan if check_error(i[7]) else round(float(i[8])*0.1,1) for i in items[3:]]
-    data['wdir'] = [np.nan if check_error(i[5][:3]) else round(float(i[5][:3]),0) for i in items[3:]]
-    data['wspd'] = [np.nan if check_error(i[5][3:]) else round(float(i[5][3:]),0) for i in items[3:]]
-    data['pkwnd'] = [np.nan if check_error(i[8]) else round(float(i[8][3:]),0) for i in items[3:]]
-    
-    #Fix erroneous data
+    data['temp'] = [np.nan if check_error(i[6]) else round(
+        float(i[7]) * 0.1, 1) for i in items[3:]]
+    data['dwpt'] = [np.nan if check_error(i[7]) else round(
+        float(i[8]) * 0.1, 1) for i in items[3:]]
+    data['wdir'] = [np.nan if check_error(i[5][:3]) else round(
+        float(i[5][:3]), 0) for i in items[3:]]
+    data['wspd'] = [np.nan if check_error(i[5][3:]) else round(
+        float(i[5][3:]), 0) for i in items[3:]]
+    data['pkwnd'] = [np.nan if check_error(i[8]) else round(
+        float(i[8][3:]), 0) for i in items[3:]]
+
+    # Fix erroneous data
     data['wspd'] = [np.nan if i > 300 else i for i in data['wspd']]
     data['pkwnd'] = [np.nan if i > 300 else i for i in data['pkwnd']]
-    
-    #Data not available prior to 2007
+
+    # Data not available prior to 2007
     data['plane_p'] = [np.nan for i in items[3:]]
     data['p_sfc'] = [np.nan for i in items[3:]]
     data['sfmr'] = [np.nan for i in items[3:]]
     data['rain'] = [np.nan for i in items[3:]]
     data['flag'] = [[] for i in items[3:]]
 
-    #Ignore entries with lat/lon of 0
+    # Ignore entries with lat/lon of 0
     orig_lat = np.copy(data['lat'])
     orig_lon = np.copy(data['lon'])
     for key in data.keys():
-        data[key] = [data[key][i] for i in range(len(orig_lat)) if orig_lat[i] != 0 and orig_lon[i] != 0 and np.isnan(orig_lat[i]) == False and np.isnan(orig_lat[i]) == False]
+        data[key] = [data[key][i] for i in range(len(orig_lat)) if orig_lat[i] != 0 and orig_lon[i] != 0 and not np.isnan(
+            orig_lat[i]) and not np.isnan(orig_lat[i])]
 
-    #Identify mission number and ID
+    # Identify mission number and ID
     content_split = content.split("\n")
-    mission_id = '-'.join((content_split[mission_row].replace("  "," ")).split(" ")[:3])
+    mission_id = '-'.join(
+        (content_split[mission_row].replace("  ", " ")).split(" ")[:3])
     missionname = (mission_id.split("-")[1])[:2]
-    data['mission'] = [missionname[:2]]*len(data['time'])
-    data['mission_id'] = [mission_id]*len(data['time'])
+    data['mission'] = [missionname[:2]] * len(data['time'])
+    data['mission_id'] = [mission_id] * len(data['time'])
 
-    #remove nan's for lat/lon coordinates
+    # remove nan's for lat/lon coordinates
     return_data = pd.DataFrame.from_dict(data).reset_index()
     return_data = return_data.dropna(subset=['lat', 'lon'])
 
     return return_data
 
-def decode_hdob_2006(content,strdate,mission_row=3):
-    
+
+def decode_hdob_2006(content, strdate, mission_row=3):
     r"""
     Function for decoding HDOBs in the format between 2006 and early 2007. This also serves as the USAF decoder between 2002 and 2005.
     """
-    
+
     def check_error(string):
-        if '/' in string: return True
-        if ';' in string: return True
-        if 'off' in string: return True
+        if '/' in string:
+            return True
+        if ';' in string:
+            return True
+        if 'off' in string:
+            return True
         return False
-    
-    #Split items by lines
+
+    # Split items by lines
     temp = [i.split() for i in content.split('\n')]
-    temp = [i for j,i in enumerate(temp) if len(i)>0]
+    temp = [i for j, i in enumerate(temp) if len(i) > 0]
     items = []
     found_sxxx = False
-    for j,i in enumerate(temp):
-        if j>mission_row and len(i[0]) >= 2 and i[0][:2] == "$$": break
-        if j>mission_row+12 and len(i[0]) >= 3 and i[0][:3] == "000": break #avoid reading in extra set of HDOBs
-        if j<=mission_row:
+    for j, i in enumerate(temp):
+        if j > mission_row and len(i[0]) >= 2 and i[0][:2] == "$$":
+            break
+        if j > mission_row + 12 and len(i[0]) >= 3 and i[0][:3] == "000":
+            break  # avoid reading in extra set of HDOBs
+        if j <= mission_row:
             if len(i[0]) >= 4 and i[0][:4] == "SXXX":
-                if found_sxxx == False:
+                if not found_sxxx:
                     found_sxxx = True
                     items.append(i)
             else:
                 items.append(i)
-        if j>mission_row and i[0][0].isdigit() and len(i) > 8:
+        if j > mission_row and i[0][0].isdigit() and len(i) > 8:
             if int(strdate[0:4]) >= 2006:
                 items.append(i)
             else:
                 if '.' in i[0] and i[0][-1] != '.' and i[0][-2] != '.':
-                    new_i = [i[0].split(".")[0]+'.',i[0].split(".")[1]] + i[1:]
+                    new_i = [i[0].split(".")[0] + '.',
+                             i[0].split(".")[1]] + i[1:]
                 else:
                     new_i = i
                 items.append(new_i)
-    
-    #Parse dates
+
+    # Parse dates
     missionname = items[2][1]
     data = {}
-    data['time'] = [dt.strptime(strdate+(i[0].split('.')[0])+'30','%Y%m%d%H%M%S') if '.' in i[0] else dt.strptime(strdate+i[0]+'00','%Y%m%d%H%M%S') for i in items[3:]]
-    if data['time'][0].hour>12 and data['time'][-1].hour<12:
-        data['time'] = [t+timedelta(days=[0,1][t.hour<12]) for t in data['time']]
+    data['time'] = [dt.strptime(strdate + (i[0].split('.')[0]) + '30', '%Y%m%d%H%M%S') if '.' in i[0]
+                    else dt.strptime(strdate + i[0] + '00', '%Y%m%d%H%M%S') for i in items[3:]]
+    if data['time'][0].hour > 12 and data['time'][-1].hour < 12:
+        data['time'] = [t + timedelta(days=[0, 1][t.hour < 12])
+                        for t in data['time']]
 
-    #Derive plane altitude using D-value
+    # Derive plane altitude using D-value
     altitude = []
     for i in items[3:]:
         if '/' in i[3] or '/' in i[4]:
             altitude.append(np.nan)
         else:
             pres_altitude = int(i[3])
-            d_value = int(i[4][1:])*-1 if i[4][0] == '5' else int(i[4][1:])
-            altitude.append(pres_altitude+d_value)
+            d_value = int(i[4][1:]) * -1 if i[4][0] == '5' else int(i[4][1:])
+            altitude.append(pres_altitude + d_value)
     data['plane_z'] = altitude
-    
-    #Parse full data
-    data['lat'] = [np.nan if check_error(i[1]) else round((float(i[1][:-3])+float(i[1][-3:-1])/60)*[-1,1][i[1][-1]=='N'],2) \
+
+    # Parse full data
+    data['lat'] = [np.nan if check_error(i[1]) else round((float(i[1][:-3]) + float(i[1][-3:-1]) / 60) * [-1, 1][i[1][-1] == 'N'], 2)
                    for i in items[3:]]
-    data['lon'] = [np.nan if check_error(i[2]) else round((float(i[2][:-3])+float(i[2][-3:-1])/60)*[-1,1][i[2][-1]=='E'],2) \
+    data['lon'] = [np.nan if check_error(i[2]) else round((float(i[2][:-3]) + float(i[2][-3:-1]) / 60) * [-1, 1][i[2][-1] == 'E'], 2)
                    for i in items[3:]]
-    data['temp'] = [np.nan if check_error(i[6]) else round(float(i[7])*0.1,1) for i in items[3:]]
-    data['dwpt'] = [np.nan if check_error(i[7]) else round(float(i[8])*0.1,1) for i in items[3:]]
-    data['wdir'] = [np.nan if check_error(i[5]) else round(float(i[5]),0) for i in items[3:]]
-    data['wspd'] = [np.nan if check_error(i[6]) else round(float(i[6]),0) for i in items[3:]]
-    data['pkwnd'] = [np.nan if check_error(i[9]) else round(float(i[9]),0) for i in items[3:]]
-    
-    #Data not available prior to 2007
+    data['temp'] = [np.nan if check_error(i[6]) else round(
+        float(i[7]) * 0.1, 1) for i in items[3:]]
+    data['dwpt'] = [np.nan if check_error(i[7]) else round(
+        float(i[8]) * 0.1, 1) for i in items[3:]]
+    data['wdir'] = [np.nan if check_error(i[5]) else round(
+        float(i[5]), 0) for i in items[3:]]
+    data['wspd'] = [np.nan if check_error(i[6]) else round(
+        float(i[6]), 0) for i in items[3:]]
+    data['pkwnd'] = [np.nan if check_error(
+        i[9]) else round(float(i[9]), 0) for i in items[3:]]
+
+    # Data not available prior to 2007
     data['plane_p'] = [np.nan for i in items[3:]]
     data['p_sfc'] = [np.nan for i in items[3:]]
     data['sfmr'] = [np.nan for i in items[3:]]
     data['rain'] = [np.nan for i in items[3:]]
     data['flag'] = [[] for i in items[3:]]
-    
-    #Fix erroneous data
+
+    # Fix erroneous data
     data['wspd'] = [np.nan if i > 300 else i for i in data['wspd']]
     data['pkwnd'] = [np.nan if i > 300 else i for i in data['pkwnd']]
 
-    #Ignore entries with lat/lon of 0
+    # Ignore entries with lat/lon of 0
     orig_lat = np.copy(data['lat'])
     orig_lon = np.copy(data['lon'])
     for key in data.keys():
-        data[key] = [data[key][i] for i in range(len(orig_lat)) if orig_lat[i] != 0 and orig_lon[i] != 0 and np.isnan(orig_lat[i]) == False and np.isnan(orig_lat[i]) == False]
+        data[key] = [data[key][i] for i in range(len(orig_lat)) if orig_lat[i] != 0 and orig_lon[i] != 0 and not np.isnan(
+            orig_lat[i]) and not np.isnan(orig_lat[i])]
 
-    #Identify mission number and ID
+    # Identify mission number and ID
     content_split = content.split("\n")
-    mission_id = '-'.join((content_split[mission_row].replace("  "," ")).split(" ")[:3])
-    if int(strdate[0:4]) < 2006: missionname = (mission_id.split("-")[1])[:2]
-    data['mission'] = [missionname[:2]]*len(data['time'])
-    data['mission_id'] = [mission_id]*len(data['time'])
+    mission_id = '-'.join(
+        (content_split[mission_row].replace("  ", " ")).split(" ")[:3])
+    if int(strdate[0:4]) < 2006:
+        missionname = (mission_id.split("-")[1])[:2]
+    data['mission'] = [missionname[:2]] * len(data['time'])
+    data['mission_id'] = [mission_id] * len(data['time'])
 
-    #remove nan's for lat/lon coordinates
+    # remove nan's for lat/lon coordinates
     return_data = pd.DataFrame.from_dict(data).reset_index()
     return_data = return_data.dropna(subset=['lat', 'lon'])
-    if np.nanmax(data['lat']) < 0: return_data = {}
+    if np.nanmax(data['lat']) < 0:
+        return_data = {}
+
+    return return_data
 
-    return return_data 
-    
 
-def decode_hdob(content,mission_row=3):
-    
+def decode_hdob(content, mission_row=3):
     r"""
     Function for decoding HDOBs in the present format (2007-present).
     """
-    
-    #Split items by lines
+
+    # Split items by lines
     tmp = [i.split() for i in content.split('\n')]
-    tmp = [i for j,i in enumerate(tmp) if len(i)>0]
+    tmp = [i for j, i in enumerate(tmp) if len(i) > 0]
     items = []
-    for j,i in enumerate(tmp):
-        if j>mission_row and len(i[0]) >= 2 and i[0][:2] == "$$": break
-        if j>mission_row+12 and len(i[0]) >= 3 and i[0][:3] == "000": break #avoid reading in extra set of HDOBs
-        if j<=mission_row:
+    for j, i in enumerate(tmp):
+        if j > mission_row and len(i[0]) >= 2 and i[0][:2] == "$$":
+            break
+        if j > mission_row + 12 and len(i[0]) >= 3 and i[0][:3] == "000":
+            break  # avoid reading in extra set of HDOBs
+        if j <= mission_row:
             items.append(i)
-        if j>mission_row and i[0][0].isdigit() and len(i) > 3:
+        if j > mission_row and i[0][0].isdigit() and len(i) > 3:
             items.append(i)
-    
-    #Parse dates
+
+    # Parse dates
     missionname = items[2][1]
     data = {}
-    data['time'] = [dt.strptime(items[2][-1]+i[0],'%Y%m%d%H%M%S') for i in items[3:]]
-    if data['time'][0].hour>12 and data['time'][-1].hour<12:
-        data['time'] = [t+timedelta(days=[0,1][t.hour<12]) for t in data['time']]
+    data['time'] = [dt.strptime(
+        items[2][-1] + i[0], '%Y%m%d%H%M%S') for i in items[3:]]
+    if data['time'][0].hour > 12 and data['time'][-1].hour < 12:
+        data['time'] = [t + timedelta(days=[0, 1][t.hour < 12])
+                        for t in data['time']]
 
-    #Parse full data
-    data['lat'] = [np.nan if '/' in i[1] else round((float(i[1][:-3])+float(i[1][-3:-1])/60)*[-1,1][i[1][-1]=='N'],2) \
+    # Parse full data
+    data['lat'] = [np.nan if '/' in i[1] else round((float(i[1][:-3]) + float(i[1][-3:-1]) / 60) * [-1, 1][i[1][-1] == 'N'], 2)
                    for i in items[3:]]
-    data['lon'] = [np.nan if '/' in i[2] else round((float(i[2][:-3])+float(i[2][-3:-1])/60)*[-1,1][i[2][-1]=='E'],2) \
+    data['lon'] = [np.nan if '/' in i[2] else round((float(i[2][:-3]) + float(i[2][-3:-1]) / 60) * [-1, 1][i[2][-1] == 'E'], 2)
                    for i in items[3:]]
-    data['plane_p'] = [np.nan if '/' in i[3] else round(float(i[3])*0.1+[0,1000][float(i[3])<1000],1) for i in items[3:]]
-    data['plane_z'] = [np.nan if '/' in i[4] else round(float(i[4]),0) for i in items[3:]]
-    data['p_sfc'] = [np.nan if (('/' in i[5]) | (p<550)) \
-                     else round(float(i[5])*0.1+[0,1000][float(i[5])<1000],1) for i,p in zip(items[3:],data['plane_p'])]
-    data['temp'] = [np.nan if '/' in i[6] else round(float(i[6])*0.1,1) for i in items[3:]]
-    data['dwpt'] = [np.nan if '/' in i[7] else round(float(i[7])*0.1,1) for i in items[3:]]
-    data['wdir'] = [np.nan if '/' in i[8][:3] else round(float(i[8][:3]),0) for i in items[3:]]
-    data['wspd'] = [np.nan if '/' in i[8][3:] else round(float(i[8][3:]),0) for i in items[3:]]
-    data['pkwnd'] = [np.nan if '/' in i[9] else round(float(i[9]),0) for i in items[3:]]
-    data['sfmr'] = [np.nan if '/' in i[10] else round(float(i[10]),0) for i in items[3:]]
-    data['rain'] = [np.nan if '/' in i[11] else round(float(i[11]),0) for i in items[3:]]
-    
-    #Fix erroneous SFMR
+    data['plane_p'] = [np.nan if '/' in i[3]
+                       else round(float(i[3]) * 0.1 + [0, 1000][float(i[3]) < 1000], 1) for i in items[3:]]
+    data['plane_z'] = [np.nan if '/' in i[4]
+                       else round(float(i[4]), 0) for i in items[3:]]
+    data['p_sfc'] = [np.nan if (('/' in i[5]) | (p < 550))
+                     else round(float(i[5]) * 0.1 + [0, 1000][float(i[5]) < 1000], 1) for i, p in zip(items[3:], data['plane_p'])]
+    data['temp'] = [np.nan if '/' in i[6]
+                    else round(float(i[6]) * 0.1, 1) for i in items[3:]]
+    data['dwpt'] = [np.nan if '/' in i[7]
+                    else round(float(i[7]) * 0.1, 1) for i in items[3:]]
+    data['wdir'] = [np.nan if '/' in i[8][:3]
+                    else round(float(i[8][:3]), 0) for i in items[3:]]
+    data['wspd'] = [np.nan if '/' in i[8][3:]
+                    else round(float(i[8][3:]), 0) for i in items[3:]]
+    data['pkwnd'] = [np.nan if '/' in i[9]
+                     else round(float(i[9]), 0) for i in items[3:]]
+    data['sfmr'] = [np.nan if '/' in i[10]
+                    else round(float(i[10]), 0) for i in items[3:]]
+    data['rain'] = [np.nan if '/' in i[11]
+                    else round(float(i[11]), 0) for i in items[3:]]
+
+    # Fix erroneous SFMR
     data['sfmr'] = [np.nan if i > 300 else i for i in data['sfmr']]
     data['wspd'] = [np.nan if i > 300 else i for i in data['wspd']]
     data['pkwnd'] = [np.nan if i > 300 else i for i in data['pkwnd']]
 
-    #Ignore entries with lat/lon of 0
+    # Ignore entries with lat/lon of 0
     orig_lat = np.copy(data['lat'])
     orig_lon = np.copy(data['lon'])
     for key in data.keys():
-        data[key] = [data[key][i] for i in range(len(orig_lat)) if orig_lat[i] != 0 and orig_lon[i] != 0]
+        data[key] = [data[key][i] for i in range(
+            len(orig_lat)) if orig_lat[i] != 0 and orig_lon[i] != 0]
 
-    #Add flag
-    data['flag']=[]
+    # Add flag
+    data['flag'] = []
     for i in items[3:]:
         flag = []
-        if int(i[12][0]) in [1,3]:
-            flag.extend(['lat','lon'])
-        if int(i[12][0]) in [2,3]:
-            flag.extend(['plane_p','plane_z'])
-        if int(i[12][1]) in [1,4,5,9]:
-            flag.extend(['temp','dwpt'])
-        if int(i[12][1]) in [2,4,6,9]:
-            flag.extend(['wdir','wspd','pkwnd'])
-        if int(i[12][1]) in [3,5,6,9]:
-            flag.extend(['sfmr','rain'])
+        if int(i[12][0]) in [1, 3]:
+            flag.extend(['lat', 'lon'])
+        if int(i[12][0]) in [2, 3]:
+            flag.extend(['plane_p', 'plane_z'])
+        if int(i[12][1]) in [1, 4, 5, 9]:
+            flag.extend(['temp', 'dwpt'])
+        if int(i[12][1]) in [2, 4, 6, 9]:
+            flag.extend(['wdir', 'wspd', 'pkwnd'])
+        if int(i[12][1]) in [3, 5, 6, 9]:
+            flag.extend(['sfmr', 'rain'])
         data['flag'].append(flag)
 
-    #QC p_sfc
-    if any(abs(np.gradient(data['p_sfc'],np.array(data['time']).astype('datetime64[s]').astype(float)))>1):
-        data['p_sfc']=[np.nan]*len(data['p_sfc'])
+    # QC p_sfc
+    if any(abs(np.gradient(data['p_sfc'], np.array(data['time']).astype('datetime64[s]').astype(float))) > 1):
+        data['p_sfc'] = [np.nan] * len(data['p_sfc'])
         data['flag'] = [d.append('p_sfc') for d in data['flag']]
 
-    #Identify mission number and ID
+    # Identify mission number and ID
     content_split = content.split("\n")
-    mission_id = '-'.join((content_split[mission_row].replace("  "," ")).split(" ")[:3])
-    data['mission'] = [missionname[:2]]*len(data['time'])
-    data['mission_id'] = [mission_id]*len(data['time'])
+    mission_id = '-'.join(
+        (content_split[mission_row].replace("  ", " ")).split(" ")[:3])
+    data['mission'] = [missionname[:2]] * len(data['time'])
+    data['mission_id'] = [mission_id] * len(data['time'])
 
-    #remove nan's for lat/lon coordinates
+    # remove nan's for lat/lon coordinates
     return_data = pd.DataFrame.from_dict(data).reset_index()
     return_data = return_data.dropna(subset=['lat', 'lon'])
 
-    return return_data 
+    return return_data
 
-#=======================================================================================================
+# =======================================================================================================
 # Decoding VDMs
-#=======================================================================================================
+# =======================================================================================================
+
 
-def decode_vdm(content,date):
+def decode_vdm(content, date):
     data = {}
     lines = content.split('\n')
     RemarksNext = False
     LonNext = False
     missionname = ''
-    
-    
+
     if date.year >= 2018:
         FORMAT = 1
     elif date.year >= 1999:
         FORMAT = 2
     elif date.year == 1998:
         FORMAT = 3
     else:
@@ -875,555 +940,631 @@
         else:
             try:
                 return float(x)
             except:
                 return x.lower()
 
     for line in lines:
-        
+
         if RemarksNext:
-            data['Remarks'] += (' '+line)
+            data['Remarks'] += (' ' + line)
         if LonNext:
             info = line.split()
-            data['lon'] = np.round((float(info[0])+float(info[2])/60)*[-1,1][info[4]=='E'],2)
+            data['lon'] = np.round(
+                (float(info[0]) + float(info[2]) / 60) * [-1, 1][info[4] == 'E'], 2)
             LonNext = False
 
         if 'VORTEX DATA MESSAGE' in line:
             stormid = line.split()[-1]
         if line[:2] == 'A.':
             if ':' in line[3:]:
                 info = line[3:].split('/')
                 day = int(info[0])
-                month = (date.month-int(day-date.day>15)-1)%12+1
-                year = date.year-int(date.month-int(day-date.day>15)==0)
-                hour,minute,second = [int(i) for i in (info[1].split("Z")[0]).split(':')]
-                data['time'] = dt(year,month,day,hour,minute,second)
+                month = (date.month - int(day - date.day > 15) - 1) % 12 + 1
+                year = date.year - \
+                    int(date.month - int(day - date.day > 15) == 0)
+                hour, minute, second = [int(i) for i in (
+                    info[1].split("Z")[0]).split(':')]
+                data['time'] = dt(year, month, day, hour, minute, second)
             else:
                 info = line[3:].split('/')
                 day = int(info[0])
-                month = (date.month-int(day-date.day>15)-1)%12+1
-                year = date.year-int(date.month-int(day-date.day>15)==0)
+                month = (date.month - int(day - date.day > 15) - 1) % 12 + 1
+                year = date.year - \
+                    int(date.month - int(day - date.day > 15) == 0)
                 hour = int(info[1].split("Z")[0][:2])
                 minute = int(info[1].split("Z")[0][2:4])
-                data['time'] = dt(year,month,day,hour,minute)
+                data['time'] = dt(year, month, day, hour, minute)
 
         if line[:2] == 'B.':
             info = line[3:].split()
             if FORMAT >= 2:
-                data['lat'] = np.round((float(info[0])+float(info[2])/60)*[-1,1][info[4]=='N'],2)
+                data['lat'] = np.round(
+                    (float(info[0]) + float(info[2]) / 60) * [-1, 1][info[4] == 'N'], 2)
                 LonNext = True
             if FORMAT == 1:
-                data['lat'] = float(info[0])*[-1,1][info[2]=='N']
-                data['lon'] = float(info[3])*[-1,1][info[5]=='E']
+                data['lat'] = float(info[0]) * [-1, 1][info[2] == 'N']
+                data['lon'] = float(info[3]) * [-1, 1][info[5] == 'E']
 
         if line[:2] == 'C.':
-            info = line[3:].split()*5
-            data[f'Standard Level (hPa)']=isNA(info[0])
-            data[f'Minimum Height at Standard Level (m)']=isNA(info[2])
+            info = line[3:].split() * 5
+            data[f'Standard Level (hPa)'] = isNA(info[0])
+            data[f'Minimum Height at Standard Level (m)'] = isNA(info[2])
 
         if line[:2] == 'D.':
-            info = line[3:].split()*5
+            info = line[3:].split() * 5
             if FORMAT >= 2:
-                data['Estimated Maximum Surface Wind Inbound (kt)'] = isNA(info[0])
+                data['Estimated Maximum Surface Wind Inbound (kt)'] = isNA(
+                    info[0])
             if FORMAT == 1:
-                data['Minimum Sea Level Pressure (hPa)']=isNA(info[-2])                    
+                data['Minimum Sea Level Pressure (hPa)'] = isNA(info[-2])
 
         if line[:2] == 'E.':
-            info = line[3:].split()*5
+            info = line[3:].split() * 5
             if FORMAT >= 2:
-                data['Dropsonde Surface Wind Speed at Center (kt)']=isNA(info[2])
-                data['Dropsonde Surface Wind Direction at Center (deg)']=isNA(info[0])
+                data['Dropsonde Surface Wind Speed at Center (kt)'] = isNA(
+                    info[2])
+                data['Dropsonde Surface Wind Direction at Center (deg)'] = isNA(
+                    info[0])
             if FORMAT == 1:
-                data['Location of Estimated Maximum Surface Wind Inbound']=isNA(line[3:])
+                data['Location of Estimated Maximum Surface Wind Inbound'] = isNA(
+                    line[3:])
 
         if line[:2] == 'F.':
             info = line[3:]
             if FORMAT >= 2:
-                data['Maximum Flight Level Wind Inbound']=isNA(info)
+                data['Maximum Flight Level Wind Inbound'] = isNA(info)
             if FORMAT == 1:
-                data['Eye character']=isNA(info)
+                data['Eye character'] = isNA(info)
 
         if line[:2] == 'G.':
             info = line[3:]
             if FORMAT >= 2:
-                data['Location of the Maximum Flight Level Wind Inbound']=isNA(info)
+                data['Location of the Maximum Flight Level Wind Inbound'] = isNA(
+                    info)
             if FORMAT == 1:
                 if isNA(info) == np.nan:
-                    data.update({'Eye Shape':np.nan,'Eye Diameter (nmi)':np.nan})
+                    data.update(
+                        {'Eye Shape': np.nan, 'Eye Diameter (nmi)': np.nan})
                 else:
                     shape = ''.join([i for i in info[:2] if not i.isdigit()])
                     size = info[len(shape):]
-                    if shape=='C':
+                    if shape == 'C':
                         if '-' in size:
-                            data.update({'Eye Shape':'circular','Eye Diameter (nmi)':float(size.split('-')[0])})
+                            data.update(
+                                {'Eye Shape': 'circular', 'Eye Diameter (nmi)': float(size.split('-')[0])})
                         else:
-                            data.update({'Eye Shape':'circular','Eye Diameter (nmi)':float(size)})
-                    elif shape=='CO':
-                        data['Eye Shape']='concentric'
+                            data.update({'Eye Shape': 'circular',
+                                        'Eye Diameter (nmi)': float(size)})
+                    elif shape == 'CO':
+                        data['Eye Shape'] = 'concentric'
                         if '-' in size:
-                            data.update({f'Eye Diameter {i+1} (nmi)':float(s) for i,s in enumerate(size.split('-'))})
+                            data.update({f'Eye Diameter {i+1} (nmi)': float(s)
+                                        for i, s in enumerate(size.split('-'))})
                         else:
-                            data.update({f'Eye Diameter {i+1} (nmi)':float(s) for i,s in enumerate(size.split(' ')[1:])})
-                    elif shape=='E':
+                            data.update({f'Eye Diameter {i+1} (nmi)': float(s)
+                                        for i, s in enumerate(size.split(' ')[1:])})
+                    elif shape == 'E':
                         einfo = size.split('/')
-                        data.update({'Eye Shape':'elliptical','Orientation':float(einfo[0])*10,\
-                                     'Eye Major Axis (nmi)':float(einfo[1]),'Eye Minor Axis (nmi)':float(einfo[1])})
+                        data.update({'Eye Shape': 'elliptical', 'Orientation': float(einfo[0]) * 10,
+                                     'Eye Major Axis (nmi)': float(einfo[1]), 'Eye Minor Axis (nmi)': float(einfo[1])})
                     else:
-                        data.update({'Eye Shape':np.nan,'Eye Diameter (nmi)':np.nan})
+                        data.update(
+                            {'Eye Shape': np.nan, 'Eye Diameter (nmi)': np.nan})
 
         if line[:2] == 'H.':
-            info = line[3:].split()*5
+            info = line[3:].split() * 5
             if FORMAT >= 3:
                 info = (line[3:].split("MB")[0]).split()
                 if info[0] == '/':
-                    data['Minimum Sea Level Pressure (hPa)']=np.nan
+                    data['Minimum Sea Level Pressure (hPa)'] = np.nan
                 else:
                     parsed_mslp = info[-1]
-                    if '/' in parsed_mslp: parsed_mslp = parsed_mslp.split("/")[1]
-                    data['Minimum Sea Level Pressure (hPa)']=isNA(parsed_mslp)
+                    if '/' in parsed_mslp:
+                        parsed_mslp = parsed_mslp.split("/")[1]
+                    data['Minimum Sea Level Pressure (hPa)'] = isNA(
+                        parsed_mslp)
             elif FORMAT == 2:
-                data['Minimum Sea Level Pressure (hPa)']=isNA(info[-2])
+                data['Minimum Sea Level Pressure (hPa)'] = isNA(info[-2])
             elif FORMAT == 1:
-                data['Estimated Maximum Surface Wind Inbound (kt)']=isNA(info[0])
+                data['Estimated Maximum Surface Wind Inbound (kt)'] = isNA(
+                    info[0])
 
         if line[:2] == 'I.':
             info = line[3:]
             if FORMAT >= 2:
-                data['Maximum Flight Level Temp Outside Eye (C)']=isNA(info.split()[0])
+                data['Maximum Flight Level Temp Outside Eye (C)'] = isNA(
+                    info.split()[0])
             if FORMAT == 1:
-                data['Location & Time of the Estimated Maximum Surface Wind Inbound']=isNA(info)
+                data['Location & Time of the Estimated Maximum Surface Wind Inbound'] = isNA(
+                    info)
 
         if line[:2] == 'J.':
             info = line[3:]
             if FORMAT >= 2:
-                data['Maximum Flight Level Temp Inside Eye (C)']=isNA(info.split()[0])
+                data['Maximum Flight Level Temp Inside Eye (C)'] = isNA(
+                    info.split()[0])
             if FORMAT == 1:
-                data['Maximum Flight Level Wind Inbound (kt)']=isNA(info)
+                data['Maximum Flight Level Wind Inbound (kt)'] = isNA(info)
 
         if line[:2] == 'K.':
             info = line[3:]
             if FORMAT >= 2:
-                data['Dew Point Inside Eye (C)']=isNA(info.split()[0])
+                data['Dew Point Inside Eye (C)'] = isNA(info.split()[0])
             if FORMAT == 1:
-                data['Location & Time of the Maximum Flight Level Wind Inbound']=isNA(info)
+                data['Location & Time of the Maximum Flight Level Wind Inbound'] = isNA(
+                    info)
 
         if line[:2] == 'L.':
             info = line[3:]
             if FORMAT >= 2:
-                data['Eye character']=isNA(info)
+                data['Eye character'] = isNA(info)
             if FORMAT == 1:
-                data['Estimated Maximum Surface Wind Outbound (kt)']=isNA(info)
+                data['Estimated Maximum Surface Wind Outbound (kt)'] = isNA(
+                    info)
 
         if line[:2] == 'M.':
             info = line[3:]
             if FORMAT >= 2:
                 if isNA(info) == np.nan:
-                    data.update({'Eye Shape':np.nan,'Eye Diameter (nmi)':np.nan})
+                    data.update(
+                        {'Eye Shape': np.nan, 'Eye Diameter (nmi)': np.nan})
                 else:
                     shape = ''.join([i for i in info[:2] if not i.isdigit()])
                     size = info[len(shape):]
-                    if shape=='C':
+                    if shape == 'C':
                         if '-' in size:
-                            data.update({'Eye Shape':'circular','Eye Diameter (nmi)':float(size.split('-')[0])})
+                            data.update(
+                                {'Eye Shape': 'circular', 'Eye Diameter (nmi)': float(size.split('-')[0])})
                         else:
-                            data.update({'Eye Shape':'circular','Eye Diameter (nmi)':float(size)})
-                    elif shape=='CO':
-                        data['Eye Shape']='concentric'
+                            data.update({'Eye Shape': 'circular',
+                                        'Eye Diameter (nmi)': float(size)})
+                    elif shape == 'CO':
+                        data['Eye Shape'] = 'concentric'
                         if '-' in size:
-                            data.update({f'Eye Diameter {i+1} (nmi)':float(s) for i,s in enumerate(size.split('-'))})
+                            data.update({f'Eye Diameter {i+1} (nmi)': float(s)
+                                        for i, s in enumerate(size.split('-'))})
                         else:
-                            data.update({f'Eye Diameter {i+1} (nmi)':float(s) for i,s in enumerate(size.split(' ')[1:])})
-                    elif shape=='E':
+                            data.update({f'Eye Diameter {i+1} (nmi)': float(s)
+                                        for i, s in enumerate(size.split(' ')[1:])})
+                    elif shape == 'E':
                         einfo = size.split('/')
                         try:
-                            data.update({'Eye Shape':'elliptical','Orientation':float(einfo[0])*10,
-                                         'Eye Major Axis (nmi)':float(einfo[1]),'Eye Minor Axis (nmi)':float(einfo[1])})
+                            data.update({'Eye Shape': 'elliptical', 'Orientation': float(einfo[0]) * 10,
+                                         'Eye Major Axis (nmi)': float(einfo[1]), 'Eye Minor Axis (nmi)': float(einfo[1])})
                         except:
-                            data.update({'Eye Shape':'elliptical','Orientation':np.nan,
-                                         'Eye Major Axis (nmi)':np.nan,'Eye Minor Axis (nmi)':np.nan})
+                            data.update({'Eye Shape': 'elliptical', 'Orientation': np.nan,
+                                         'Eye Major Axis (nmi)': np.nan, 'Eye Minor Axis (nmi)': np.nan})
                     else:
-                        data.update({'Eye Shape':np.nan,'Eye Diameter (nmi)':np.nan})
+                        data.update(
+                            {'Eye Shape': np.nan, 'Eye Diameter (nmi)': np.nan})
             if FORMAT == 1:
-                data['Location & Time of the Estimated Maximum Surface Wind Outbound']=isNA(info)
+                data['Location & Time of the Estimated Maximum Surface Wind Outbound'] = isNA(
+                    info)
 
         if line[:2] == 'N.':
             info = line[3:]
             if FORMAT == 1:
-                data['Maximum Flight Level Wind Outbound (kt)']=isNA(info)
+                data['Maximum Flight Level Wind Outbound (kt)'] = isNA(info)
 
         if line[:2] == 'O.':
             info = line[3:]
             if FORMAT == 1:
-                data['Location & Time of the Maximum Flight Level Wind Outbound']=isNA(info)
+                data['Location & Time of the Maximum Flight Level Wind Outbound'] = isNA(
+                    info)
 
         if line[:2] == 'P.':
             info = line[3:]
             if FORMAT >= 2:
                 data['Aircraft'] = info.split()[0]
                 missionname = info.split()[1]
                 data['mission'] = missionname[:2]
                 data['Remarks'] = ''
                 RemarksNext = True
             if FORMAT == 1:
-                data['Maximum Flight Level Temp & Pressure Altitude Outside Eye']=isNA(info)
+                data['Maximum Flight Level Temp & Pressure Altitude Outside Eye'] = isNA(
+                    info)
 
         if line[:2] == 'Q.':
             info = line[3:]
             if FORMAT == 1:
-                data['Maximum Flight Level Temp & Pressure Altitude Inside Eye']=isNA(info)
+                data['Maximum Flight Level Temp & Pressure Altitude Inside Eye'] = isNA(
+                    info)
             if FORMAT == 3:
                 data['Aircraft'] = info.split()[0]
                 missionname = info.split()[1]
                 data['mission'] = missionname[:2]
                 data['Remarks'] = ''
 
         if line[:2] == 'R.':
             info = line[3:]
             if FORMAT == 1:
-                data['Dewpoint Temp (collected at same location as temp inside eye)']=isNA(info)
+                data['Dewpoint Temp (collected at same location as temp inside eye)'] = isNA(
+                    info)
 
         if line[:2] == 'S.':
             info = line[3:]
             if FORMAT == 1:
-                data['Fix']=isNA(info)
+                data['Fix'] = isNA(info)
 
         if line[:2] == 'T.':
             info = line[3:]
             if FORMAT == 1:
-                data['Accuracy']=isNA(info)
+                data['Accuracy'] = isNA(info)
 
         if line[:2] == 'U.':
             info = line[3:]
             if FORMAT == 1:
                 data['Aircraft'] = info.split()[0]
                 missionname = info.split()[1]
                 data['mission'] = missionname[:2]
                 data['Remarks'] = ''
                 RemarksNext = True
 
     content_split = content.split("\n")
     if FORMAT == 4:
-        mission_id = '-'.join(content_split[1].replace("  "," ").split(" ")[:3])
+        mission_id = '-'.join(content_split[1].replace("  ",
+                              " ").split(" ")[:3])
         data['Aircraft'] = mission_id.split("-")[0]
         missionname = mission_id.split("-")[1]
         data['mission'] = missionname[:2]
         data['Remarks'] = ''
     elif FORMAT == 3:
-        mission_id = ['-'.join(i.split("Q. ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:2] == "Q."][0]
+        mission_id = ['-'.join(i.split("Q. ")[1].replace("  ", " ").split(" ")[:3])
+                      for i in content_split if i[:2] == "Q."][0]
     elif FORMAT == 2:
-        mission_id = ['-'.join(i.split("P. ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:2] == "P."][0]
+        mission_id = ['-'.join(i.split("P. ")[1].replace("  ", " ").split(" ")[:3])
+                      for i in content_split if i[:2] == "P."][0]
     elif FORMAT == 1:
-        mission_id = ['-'.join(i.split("U. ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:2] == "U."][0]
+        mission_id = ['-'.join(i.split("U. ")[1].replace("  ", " ").split(" ")[:3])
+                      for i in content_split if i[:2] == "U."][0]
     data['mission_id'] = mission_id
-    
+
     return missionname, data
 
-#=======================================================================================================
+# =======================================================================================================
 # Decoding dropsondes
-#=======================================================================================================
+# =======================================================================================================
+
 
-def decode_dropsonde(content,date):
+def decode_dropsonde(content, date):
 
     NOLOCFLAG = False
     missionname = '_____'
 
-    delimiters = ['XXAA','31313','51515','61616','62626','XXBB','21212','_____']
+    delimiters = ['XXAA', '31313', '51515',
+                  '61616', '62626', 'XXBB', '21212', '_____']
     sections = {}
-    for i,d in enumerate(delimiters[:-1]):
-        a = content.split('\n'+d)
-        if len(a)>1:
-            a = ('\n'+d).join(a[1:]) if len(a)>2 else a[1] 
-            b = a.split('\n'+delimiters[i+1])[0]
+    for i, d in enumerate(delimiters[:-1]):
+        a = content.split('\n' + d)
+        if len(a) > 1:
+            a = ('\n' + d).join(a[1:]) if len(a) > 2 else a[1]
+            b = a.split('\n' + delimiters[i + 1])[0]
             sections[d] = b
 
-    for k,v in sections.items():
+    for k, v in sections.items():
         tmp = copy.copy(v)
         for d in delimiters:
-            tmp=tmp.split('\n'+d)[0]
-        tmp = [i for i in tmp.split(' ') if len(i)>0]
-        tmp = [j.replace('\n','') if '\n' in j and (len(j)<(7+j.count('\n')) or len(j)==(11+j.count('\n'))) else j for j in tmp]
-        tmp = [i for j in tmp for i in j.split('\n') if len(i)>0]
+            tmp = tmp.split('\n' + d)[0]
+        tmp = [i for i in tmp.split(' ') if len(i) > 0]
+        tmp = [j.replace('\n', '') if '\n' in j and (len(j) < (
+            7 + j.count('\n')) or len(j) == (11 + j.count('\n'))) else j for j in tmp]
+        tmp = [i for j in tmp for i in j.split('\n') if len(i) > 0]
         sections[k] = tmp
 
     def _time(timestr):
         try:
             if timestr < f'{date:%H%M}':
-                return date.replace(hour=int(timestr[:2]),minute=int(timestr[2:4]))
+                return date.replace(hour=int(timestr[:2]), minute=int(timestr[2:4]))
             else:
-                return date.replace(hour=int(timestr[:2]),minute=int(timestr[2:4]))-timedelta(days=1)
+                return date.replace(hour=int(timestr[:2]), minute=int(timestr[2:4])) - timedelta(days=1)
         except:
             return None
 
     def _tempdwpt(item):
         if '/' in item[:3]:
             temp = np.nan
             dwpt = np.nan
         elif '/' in item[4:]:
-            z = round(float(item[:3]),0)
-            temp = round(z*0.1,1) if z%2==0 else round(z*-0.1,1)
+            z = round(float(item[:3]), 0)
+            temp = round(z * 0.1, 1) if z % 2 == 0 else round(z * -0.1, 1)
             dwpt = np.nan
         else:
-            z = round(float(item[:3]),0)
-            temp = round(z*0.1,1) if z%2==0 else round(z*-0.1,1)
-            z = round(float(item[3:]),0)
-            dwpt = temp-(round(z*0.1,1) if z<=50 else z-50)
-        return temp,dwpt
+            z = round(float(item[:3]), 0)
+            temp = round(z * 0.1, 1) if z % 2 == 0 else round(z * -0.1, 1)
+            z = round(float(item[3:]), 0)
+            dwpt = temp - (round(z * 0.1, 1) if z <= 50 else z - 50)
+        return temp, dwpt
 
     def _wdirwspd(item):
         try:
-            wdir = round(np.floor(float(item[:3])/5)*5,0) if '/' not in item else np.nan
-            wspd = round(float(item[3:])+100*(float(item[2])%5),0) if '/' not in item else np.nan
+            wdir = round(
+                np.floor(float(item[:3]) / 5) * 5, 0) if '/' not in item else np.nan
+            wspd = round(float(item[3:]) + 100 * (float(item[2]) %
+                         5), 0) if '/' not in item else np.nan
         except:
             wdir = np.nan
             wspd = np.nan
-        return wdir,wspd
+        return wdir, wspd
 
     def _standard(I3):
         levkey = I3[0][:2]
-        levdict = {'99':-1,'00':1000,'92':925,'85':850,'70':700,'50':500,'40':400,'30':300,'25':250,'20':200,'15':150,'10':100,'__':None}
+        levdict = {'99': -1, '00': 1000, '92': 925, '85': 850, '70': 700, '50': 500,
+                   '40': 400, '30': 300, '25': 250, '20': 200, '15': 150, '10': 100, '__': None}
         pres = float(levdict[levkey])
         output = {}
         output['pres'] = pres
-        if pres==-1:
-            output['pres'] = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:])+[0,1000][float(I3[0][2:])<100],1)
+        if pres == -1:
+            output['pres'] = np.nan if '/' in I3[0][2:] else round(
+                float(I3[0][2:]) + [0, 1000][float(I3[0][2:]) < 100], 1)
             output['hgt'] = 0.0
-        elif pres==1000:
-            z = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:]),0)
-            output['hgt'] = round(500-z,0) if z>=500 else z
-        elif pres==925:
-            output['hgt'] = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:]),0)
-        elif pres==850:
-            output['hgt'] = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:])+1000,0)
-        elif pres==700:
-            z = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:]),0)
-            output['hgt'] = round(z+3000,0) if z<500 else round(z+2000,0)       
-        elif pres in (500,400,300):
-            output['hgt'] = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:])*10,0)
+        elif pres == 1000:
+            z = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:]), 0)
+            output['hgt'] = round(500 - z, 0) if z >= 500 else z
+        elif pres == 925:
+            output['hgt'] = np.nan if '/' in I3[0][2:
+                                                   ] else round(float(I3[0][2:]), 0)
+        elif pres == 850:
+            output['hgt'] = np.nan if '/' in I3[0][2:
+                                                   ] else round(float(I3[0][2:]) + 1000, 0)
+        elif pres == 700:
+            z = np.nan if '/' in I3[0][2:] else round(float(I3[0][2:]), 0)
+            output['hgt'] = round(
+                z + 3000, 0) if z < 500 else round(z + 2000, 0)
+        elif pres in (500, 400, 300):
+            output['hgt'] = np.nan if '/' in I3[0][2:
+                                                   ] else round(float(I3[0][2:]) * 10, 0)
         else:
-            output['hgt'] = np.nan if '/' in I3[0][2:] else round(1e4+float(I3[0][2:])*10,0)
-        output['temp'],output['dwpt'] = _tempdwpt(I3[1])
-        if I3[2][:2]=='88':
-            output['wdir'],output['wspd'] = np.nan,np.nan
+            output['hgt'] = np.nan if '/' in I3[0][2:] else round(
+                1e4 + float(I3[0][2:]) * 10, 0)
+        output['temp'], output['dwpt'] = _tempdwpt(I3[1])
+        if I3[2][:2] == '88':
+            output['wdir'], output['wspd'] = np.nan, np.nan
             skipflag = 0
-        elif I3[2][:2]==list(levdict.keys())[list(levdict.keys()).index(levkey)+1] and \
-        I3[3][:2]!=list(levdict.keys())[list(levdict.keys()).index(levkey)+1]:
-            output['wdir'],output['wspd'] = np.nan,np.nan
+        elif I3[2][:2] == list(levdict.keys())[list(levdict.keys()).index(levkey) + 1] and \
+                I3[3][:2] != list(levdict.keys())[list(levdict.keys()).index(levkey) + 1]:
+            output['wdir'], output['wspd'] = np.nan, np.nan
             skipflag = 1
         else:
-            output['wdir'],output['wspd'] = _wdirwspd(I3[2])  
-            skipflag = 0 
+            output['wdir'], output['wspd'] = _wdirwspd(I3[2])
+            skipflag = 0
         endflag = True if '88' in [i[:2] for i in I3] else False
-        return output,skipflag,endflag
+        return output, skipflag, endflag
 
-    data = {k:np.nan for k in ('lat','lon','slp',\
-                               'TOPlat','TOPlon','TOPtime',\
-                               'BOTTOMlat','BOTTOMlon','BOTTOMtime',\
-                               'MBLdir','MBLspd','DLMdir','DLMspd',\
-                               'WL150dir','WL150spd','top','LSThgt','software','levels')}
+    data = {k: np.nan for k in ('lat', 'lon', 'slp',
+                                'TOPlat', 'TOPlon', 'TOPtime',
+                                'BOTTOMlat', 'BOTTOMlon', 'BOTTOMtime',
+                                'MBLdir', 'MBLspd', 'DLMdir', 'DLMspd',
+                                'WL150dir', 'WL150spd', 'top', 'LSThgt', 'software', 'levels')}
     data['TOPtime'] = None
     data['BOTTOMtime'] = None
 
-    for sec,items in sections.items():
+    for sec, items in sections.items():
 
-        if sec == '61616' and len(items)>0:
+        if sec == '61616' and len(items) > 0:
             missionname = items[1]
             data['mission'] = items[1][:2]
             data['stormname'] = items[2]
             try:
                 data['obsnum'] = int(items[-1])
             except:
                 data['obsnum'] = items[-1]
 
-        if sec == 'XXAA' and len(items)>0 and not NOLOCFLAG:
-            if '/' in items[1]+items[2]:
+        if sec == 'XXAA' and len(items) > 0 and not NOLOCFLAG:
+            if '/' in items[1] + items[2]:
                 NOLOCFLAG = True
             else:
                 octant = int(items[2][0])
-                data['lat'] = round(float(items[1][2:])*0.1*[-1,1][octant in (2,3,7,8)],1)
-                data['lon'] = round(float(items[2][1:])*0.1*[-1,1][octant in (0,1,2,3)],1)
-                data['slp'] = np.nan if '/' in items[4][2:] else round(float(items[4][2:])+[0,1000][float(items[4][2:])<100],1)
+                data['lat'] = round(float(items[1][2:]) *
+                                    0.1 * [-1, 1][octant in (2, 3, 7, 8)], 1)
+                data['lon'] = round(float(items[2][1:]) *
+                                    0.1 * [-1, 1][octant in (0, 1, 2, 3)], 1)
+                data['slp'] = np.nan if '/' in items[4][2:] else round(
+                    float(items[4][2:]) + [0, 1000][float(items[4][2:]) < 100], 1)
 
-                standard = {k:[] for k in ['pres','hgt','temp','dwpt','wdir','wspd']}
+                standard = {k: []
+                            for k in ['pres', 'hgt', 'temp', 'dwpt', 'wdir', 'wspd']}
                 skips = 0
-                for jj,item in enumerate(items[4::3]):
-                    if items[4+jj*3-skips][:2]=='88':
+                for jj, item in enumerate(items[4::3]):
+                    if items[4 + jj * 3 - skips][:2] == '88':
                         break
-                    output,skipflag,endflag = _standard(items[4+jj*3-skips:8+jj*3-skips])
+                    output, skipflag, endflag = _standard(
+                        items[4 + jj * 3 - skips:8 + jj * 3 - skips])
                     skips += skipflag
                     for k in standard.keys():
                         standard[k].append(output[k])
                     if endflag:
                         break
-                standard = pd.DataFrame.from_dict(standard).sort_values('pres',ascending=False)
+                standard = pd.DataFrame.from_dict(
+                    standard).sort_values('pres', ascending=False)
 
-        if sec == '62626' and len(items)>0 and not NOLOCFLAG:
-            if items[0] in ['CENTER','MXWNDBND','RAINBAND','EYEWALL']:
+        if sec == '62626' and len(items) > 0 and not NOLOCFLAG:
+            if items[0] in ['CENTER', 'MXWNDBND', 'RAINBAND', 'EYEWALL']:
                 data['location'] = items[0]
-                if items[0]=='EYEWALL':
-                    data['octant'] = {'000':'N','045':'NE','090':'E','135':'SE',\
-                              '180':'S','225':'SW','270':'W','315':'NW'}[items[1]]
+                if items[0] == 'EYEWALL':
+                    data['octant'] = {'000': 'N', '045': 'NE', '090': 'E', '135': 'SE',
+                                      '180': 'S', '225': 'SW', '270': 'W', '315': 'NW'}[items[1]]
             if 'REL' in items:
-                tmp = items[items.index('REL')+1]
-                data['TOPlat'] = round(float(tmp[:4])*.01*[-1,1][tmp[4]=='N'],2)
-                data['TOPlon'] = round(float(tmp[5:10])*.01*[-1,1][tmp[10]=='E'],2)
-                tmp = items[items.index('REL')+2]
-                if data['TOPtime'] == None: data['TOPtime'] = _time(tmp)
+                tmp = items[items.index('REL') + 1]
+                data['TOPlat'] = round(
+                    float(tmp[:4]) * .01 * [-1, 1][tmp[4] == 'N'], 2)
+                data['TOPlon'] = round(
+                    float(tmp[5:10]) * .01 * [-1, 1][tmp[10] == 'E'], 2)
+                tmp = items[items.index('REL') + 2]
+                if data['TOPtime'] is None:
+                    data['TOPtime'] = _time(tmp)
             if 'SPG' in items:
-                tmp = items[items.index('SPG')+1]
-                data['BOTTOMlat'] = round(float(tmp[:4])*.01*[-1,1][tmp[4]=='N'],2)
-                data['BOTTOMlon'] = round(float(tmp[5:10])*.01*[-1,1][tmp[10]=='E'],2)
-                tmp = items[items.index('SPG')+2]
-                if data['BOTTOMtime'] == None: data['BOTTOMtime'] = _time(tmp)
+                tmp = items[items.index('SPG') + 1]
+                data['BOTTOMlat'] = round(
+                    float(tmp[:4]) * .01 * [-1, 1][tmp[4] == 'N'], 2)
+                data['BOTTOMlon'] = round(
+                    float(tmp[5:10]) * .01 * [-1, 1][tmp[10] == 'E'], 2)
+                tmp = items[items.index('SPG') + 2]
+                if data['BOTTOMtime'] is None:
+                    data['BOTTOMtime'] = _time(tmp)
             elif 'SPL' in items:
-                tmp = items[items.index('SPL')+1]
-                data['BOTTOMlat'] = round(float(tmp[:4])*.01*[-1,1][tmp[4]=='N'],2)
-                data['BOTTOMlon'] = round(float(tmp[5:10])*.01*[-1,1][tmp[10]=='E'],2)
-                tmp = items[items.index('SPL')+2]
-                if data['BOTTOMtime'] == None: data['BOTTOMtime'] = _time(tmp)
+                tmp = items[items.index('SPL') + 1]
+                data['BOTTOMlat'] = round(
+                    float(tmp[:4]) * .01 * [-1, 1][tmp[4] == 'N'], 2)
+                data['BOTTOMlon'] = round(
+                    float(tmp[5:10]) * .01 * [-1, 1][tmp[10] == 'E'], 2)
+                tmp = items[items.index('SPL') + 2]
+                if data['BOTTOMtime'] is None:
+                    data['BOTTOMtime'] = _time(tmp)
             if 'MBL' in items:
-                tmp = items[items.index('MBL')+2]
-                wdir,wspd = _wdirwspd(tmp)
+                tmp = items[items.index('MBL') + 2]
+                wdir, wspd = _wdirwspd(tmp)
                 data['MBLdir'] = wdir
                 data['MBLspd'] = wspd
             if 'DLM' in items:
-                tmp = items[items.index('DLM')+2]
-                wdir,wspd = _wdirwspd(tmp)
+                tmp = items[items.index('DLM') + 2]
+                wdir, wspd = _wdirwspd(tmp)
                 data['DLMdir'] = wdir
-                data['DLMspd'] = wspd                   
+                data['DLMspd'] = wspd
             if 'WL150' in items:
-                tmp = items[items.index('WL150')+1]
-                wdir,wspd = _wdirwspd(tmp)
+                tmp = items[items.index('WL150') + 1]
+                wdir, wspd = _wdirwspd(tmp)
                 data['WL150dir'] = wdir
                 data['WL150spd'] = wspd
             if 'LST' in items:
-                tmp = items[items.index('LST')+2]
-                tmp = tmp.replace('=','')
-                data['LSThgt'] = round(float(tmp),0)
+                tmp = items[items.index('LST') + 2]
+                tmp = tmp.replace('=', '')
+                data['LSThgt'] = round(float(tmp), 0)
             if 'AEV' in items:
-                tmp = items[items.index('AEV')+1]
-                data['software'] = 'AEV '+tmp
+                tmp = items[items.index('AEV') + 1]
+                data['software'] = 'AEV ' + tmp
 
-        if sec == 'XXBB' and len(items)>0 and not NOLOCFLAG:
-            sigtemp = {k:[] for k in ['pres','temp','dwpt']}
-            for jj,item in enumerate(items[6::2]):
-                z = np.nan if '/' in items[6+jj*2][2:] else round(float(items[6+jj*2][2:]),0)
-                sigtemp['pres'].append(round(z+1000,0) if z<100 else z)
-                temp,dwpt = _tempdwpt(items[7+jj*2])
+        if sec == 'XXBB' and len(items) > 0 and not NOLOCFLAG:
+            sigtemp = {k: [] for k in ['pres', 'temp', 'dwpt']}
+            for jj, item in enumerate(items[6::2]):
+                z = np.nan if '/' in items[6 + jj *
+                                           2][2:] else round(float(items[6 + jj * 2][2:]), 0)
+                sigtemp['pres'].append(round(z + 1000, 0) if z < 100 else z)
+                temp, dwpt = _tempdwpt(items[7 + jj * 2])
                 sigtemp['temp'].append(temp)
                 sigtemp['dwpt'].append(dwpt)
-            sigtemp = pd.DataFrame.from_dict(sigtemp).sort_values('pres',ascending=False)
+            sigtemp = pd.DataFrame.from_dict(
+                sigtemp).sort_values('pres', ascending=False)
 
-        if sec == '21212' and len(items)>0 and not NOLOCFLAG:
-            sigwind = {k:[] for k in ['pres','wdir','wspd']}
-            for jj,item in enumerate(items[2::2]):
-                z = np.nan if '/' in items[2+jj*2][2:] else round(float(items[2+jj*2][2:]),0)
-                sigwind['pres'].append(round(z+1000,0) if z<100 else z)
-                wdir,wspd = _wdirwspd(items[3+jj*2])
+        if sec == '21212' and len(items) > 0 and not NOLOCFLAG:
+            sigwind = {k: [] for k in ['pres', 'wdir', 'wspd']}
+            for jj, item in enumerate(items[2::2]):
+                z = np.nan if '/' in items[2 + jj *
+                                           2][2:] else round(float(items[2 + jj * 2][2:]), 0)
+                sigwind['pres'].append(round(z + 1000, 0) if z < 100 else z)
+                wdir, wspd = _wdirwspd(items[3 + jj * 2])
                 sigwind['wdir'].append(wdir)
                 sigwind['wspd'].append(wspd)
-            sigwind = pd.DataFrame.from_dict(sigwind).sort_values('pres',ascending=False)
-        
-        if sec == '31313' and len(items)>0 and not NOLOCFLAG:
+            sigwind = pd.DataFrame.from_dict(
+                sigwind).sort_values('pres', ascending=False)
+
+        if sec == '31313' and len(items) > 0 and not NOLOCFLAG:
             tmp = [i for i in items if i[0] == '8'][0]
             data['TOPtime'] = _time(tmp[1:])
 
     if not NOLOCFLAG:
-        def _justify(a, axis=0):    
+        def _justify(a, axis=0):
             mask = pd.notnull(a)
-            arg_justified = np.argsort(mask,axis=0)[-1]
-            anew = [col[i] for i,col in zip(arg_justified,a.T)]
+            arg_justified = np.argsort(mask, axis=0)[-1]
+            anew = [col[i] for i, col in zip(arg_justified, a.T)]
             return anew
-        df = pd.concat([standard,sigtemp,sigwind],ignore_index=True, sort=False).sort_values('pres',ascending=False)
+        df = pd.concat([standard, sigtemp, sigwind], ignore_index=True,
+                       sort=False).sort_values('pres', ascending=False)
         data['levels'] = pd.DataFrame(np.vstack(df.groupby('pres', sort=False)
-                          .apply(lambda gp: _justify(gp.to_numpy()))), columns=df.columns)
+                                                .apply(lambda gp: _justify(gp.to_numpy()))), columns=df.columns)
 
         data['top'] = np.nanmin(data['levels']['pres'])
 
     content_split = content.split("\n")
     try:
-        mission_id = ['-'.join(i.split("61616 ")[1].replace("  "," ").split(" ")[:3]) for i in content_split if i[:5] == "61616"][0]
+        mission_id = ['-'.join(i.split("61616 ")[1].replace("  ", " ").split(" ")[:3])
+                      for i in content_split if i[:5] == "61616"][0]
     except:
-        mission_id = '-'.join(content.split("\n")[1].replace("  "," ").split(" ")[:3])
+        mission_id = '-'.join(content.split("\n")
+                              [1].replace("  ", " ").split(" ")[:3])
     data['mission_id'] = mission_id
-    
-    #Fix NaNs
-    if data['BOTTOMtime'] == None: data['BOTTOMtime'] = np.nan
-    if data['TOPtime'] == None: data['TOPtime'] = np.nan
-    
+
+    # Fix NaNs
+    if data['BOTTOMtime'] is None:
+        data['BOTTOMtime'] = np.nan
+    if data['TOPtime'] is None:
+        data['TOPtime'] = np.nan
+
     return missionname, data
 
-def get_status(plane_p,use_z=False):
+
+def get_status(plane_p, use_z=False):
 
     status = []
     in_storm = False
     finished = False
-    for idx,pres in enumerate(plane_p):
+    for idx, pres in enumerate(plane_p):
         if idx < 8:
             status.append('En Route')
             continue
         if np.isnan(pres):
             status.append(status[-1])
             continue
-        
-        #Use default pressure method
-        if use_z == False:
-            if np.nanmin(plane_p[:idx+1]) >= 850:
+
+        # Use default pressure method
+        if not use_z:
+            if np.nanmin(plane_p[:idx + 1]) >= 850:
                 status.append('En Route')
                 continue
-            if np.abs(plane_p[idx] - plane_p[idx-8]) < 10:
+            if np.abs(plane_p[idx] - plane_p[idx - 8]) < 10:
                 if finished:
-                    if idx > 40 and pres < 800 and pres > 650 and np.nanmax(np.abs(plane_p[idx-40:idx] - plane_p[idx-39:idx+1])) < 20:
+                    if idx > 40 and pres < 800 and pres > 650 and np.nanmax(np.abs(plane_p[idx - 40:idx] - plane_p[idx - 39:idx + 1])) < 20:
                         if idx > 100 and 'In Storm' in status[-100:]:
                             finished = False
                         else:
                             status.append('Finished')
                     else:
                         status.append('Finished')
-                if finished == False:
+                if not finished:
                     if pres < 650:
                         status.append('En Route')
                     else:
                         in_storm = True
                         status.append('In Storm')
             else:
-                if in_storm == False:
+                if not in_storm:
                     status.append('En Route')
                 else:
                     if pres < 650:
                         finished = True
-                    if finished == True:
+                    if finished:
                         status.append('Finished')
                     else:
                         status.append('In Storm')
-        
-        #Use height method
+
+        # Use height method
         else:
-            if np.nanmax(plane_p[:idx+1]) <= 1515:
+            if np.nanmax(plane_p[:idx + 1]) <= 1515:
                 status.append('En Route')
                 continue
-            if np.abs(plane_p[idx] - plane_p[idx-8]) < 60:
+            if np.abs(plane_p[idx] - plane_p[idx - 8]) < 60:
                 if finished:
-                    if idx > 40 and pres > 2050 and pres < 3820 and np.nanmin(np.abs(plane_p[idx-40:idx] - plane_p[idx-39:idx+1])) < 120:
+                    if idx > 40 and pres > 2050 and pres < 3820 and np.nanmin(np.abs(plane_p[idx - 40:idx] - plane_p[idx - 39:idx + 1])) < 120:
                         if idx > 100 and 'In Storm' in status[-100:]:
                             finished = False
                         else:
                             status.append('Finished')
                     else:
                         status.append('Finished')
-                if finished == False:
+                if not finished:
                     if pres > 3820:
                         status.append('En Route')
                     else:
                         in_storm = True
                         status.append('In Storm')
             else:
-                if in_storm == False:
+                if not in_storm:
                     status.append('En Route')
                 else:
                     if pres > 3820:
                         finished = True
-                    if finished == True:
+                    if finished:
                         status.append('Finished')
                     else:
                         status.append('In Storm')
-        
+
     return status
```

### Comparing `tropycal-0.6.1/src/tropycal/tornado/dataset.py` & `tropycal-1.0/src/tropycal/tornado/dataset.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,38 +1,29 @@
 r"""Functionality for reading and analyzing SPC tornado dataset."""
 
 import requests
 import numpy as np
 import pandas as pd
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 from scipy.interpolate import interp1d
 import matplotlib.dates as mdates
 import warnings
 
 import matplotlib.pyplot as plt
 import matplotlib.lines as mlines
-import matplotlib.colors as mcolors
-import matplotlib.patheffects as patheffects
-
-try:
-    import cartopy.feature as cfeature
-    from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
-except:
-    warn_message = "Warning: Cartopy is not installed in your python environment. Plotting functions will not work."
-    warnings.warn(warn_message)
 
 from .plot import TornadoPlot
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 
+
 class TornadoDataset():
-    
+
     r"""
     Creates an instance of a TornadoDataset object containing tornado data.
 
     Parameters
     ----------
     mag_thresh : int
         Minimum threshold for tornado rating.
@@ -42,332 +33,338 @@
     Returns
     -------
     TornadoDataset
         An instance of TornadoDataset.
     """
 
     def __init__(self, mag_thresh=0, tornado_path='spc'):
-        
-        #Error check
-        if isinstance(mag_thresh,int) == False:
+
+        # Error check
+        if isinstance(mag_thresh, int) == False:
             raise TypeError("mag_thresh must be of type int.")
-        elif mag_thresh not in [0,1,2,3,4,5]:
+        elif mag_thresh not in [0, 1, 2, 3, 4, 5]:
             raise ValueError("mag_thresh must be between 0 and 5.")
-        
-        #Read in tornado dataset
+
+        # Read in tornado dataset
         timer_start = dt.now()
         yrnow = timer_start.year
         print(f'--> Starting to read in tornado track data')
         if tornado_path == 'spc':
-            #Find most recent year
+            # Find most recent year
             year_found = False
-            for year_diff in [0,1,2,3,4,5]:
+            for year_diff in [0, 1, 2, 3, 4, 5]:
                 yrlast = yrnow - year_diff
                 url = f"https://www.spc.noaa.gov/wcm/data/1950-{yrlast}_actual_tornadoes.csv"
                 if requests.get(url).status_code == 200:
                     year_found = True
-                    Tors = pd.read_csv(f'https://www.spc.noaa.gov/wcm/data/1950-{yrlast}_actual_tornadoes.csv',\
-                                       error_bad_lines=False,parse_dates=[['mo','dy','yr','time']])
-                    print(f'--> Completed reading in tornado data for 1950-{yrlast} (%.2f seconds)' % (dt.now()-timer_start).total_seconds())
+                    Tors = pd.read_csv(f'https://www.spc.noaa.gov/wcm/data/1950-{yrlast}_actual_tornadoes.csv',
+                                       error_bad_lines=False, parse_dates=[['mo', 'dy', 'yr', 'time']])
+                    print(f'--> Completed reading in tornado data for 1950-{yrlast} (%.2f seconds)' % (
+                        dt.now()-timer_start).total_seconds())
                     break
             if year_found == False:
-                raise RuntimeError("Error: No SPC tornado dataset available within the last 5 years.")
+                raise RuntimeError(
+                    "Error: No SPC tornado dataset available within the last 5 years.")
         else:
-            Tors = pd.read_csv(tornado_path,\
-                               error_bad_lines=False,parse_dates=[['mo','dy','yr','time']])
-            print(f'--> Completed reading in tornado data from local file (%.2f seconds)' % (dt.now()-timer_start).total_seconds())
-        
-        #Get UTC from timezone (most are 3 = CST, but some 0 and 9 = GMT)
+            Tors = pd.read_csv(tornado_path,
+                               error_bad_lines=False, parse_dates=[['mo', 'dy', 'yr', 'time']])
+            print(f'--> Completed reading in tornado data from local file (%.2f seconds)' %
+                  (dt.now()-timer_start).total_seconds())
+
+        # Get UTC from timezone (most are 3 = CST, but some 0 and 9 = GMT)
         tz = np.array([timedelta(hours=9-int(i)) for i in Tors['tz']])
         tors_dt = [pd.to_datetime(i) for i in Tors['mo_dy_yr_time']]
-        Tors = Tors.assign(UTC_time = tors_dt+tz)
-        
-        #Filter for only those tors at least F/EF scale mag_thresh.
-        Tors = Tors[Tors['mag']>=mag_thresh]
-
-        #Clean up lat/lons       
-        Tors = Tors[(Tors['slat']!=0)|(Tors['slon']!=0)]
-        Tors = Tors[(Tors['slat']>=20) & (Tors['slat']<=50)]
-        Tors = Tors[(Tors['slon']>=-130) & (Tors['slon']<=-65)]
-        Tors = Tors[(Tors['elat']>=20)|(Tors['elat']==0)]
-        Tors = Tors[(Tors['elat']<=50)|(Tors['elat']==0)]
-        Tors = Tors[(Tors['elon']>=-130)|(Tors['elat']==0)]
-        Tors = Tors[(Tors['elon']<=-65)|(Tors['elat']==0)]
-        Tors = Tors.assign(elat = [Tors['slat'].values[u] if i==0 else i for u, i in enumerate(Tors['elat'].values)])
-        self.Tors = Tors.assign(elon = [Tors['slon'].values[u] if i==0 else i for u, i in enumerate(Tors['elon'].values)])
+        Tors = Tors.assign(UTC_time=tors_dt+tz)
+
+        # Filter for only those tors at least F/EF scale mag_thresh.
+        Tors = Tors[Tors['mag'] >= mag_thresh]
 
-    def get_storm_tornadoes(self,storm,dist_thresh):
-        
+        # Clean up lat/lons
+        Tors = Tors[(Tors['slat'] != 0) | (Tors['slon'] != 0)]
+        Tors = Tors[(Tors['slat'] >= 20) & (Tors['slat'] <= 50)]
+        Tors = Tors[(Tors['slon'] >= -130) & (Tors['slon'] <= -65)]
+        Tors = Tors[(Tors['elat'] >= 20) | (Tors['elat'] == 0)]
+        Tors = Tors[(Tors['elat'] <= 50) | (Tors['elat'] == 0)]
+        Tors = Tors[(Tors['elon'] >= -130) | (Tors['elat'] == 0)]
+        Tors = Tors[(Tors['elon'] <= -65) | (Tors['elat'] == 0)]
+        Tors = Tors.assign(elat=[Tors['slat'].values[u] if i ==
+                           0 else i for u, i in enumerate(Tors['elat'].values)])
+        self.Tors = Tors.assign(
+            elon=[Tors['slon'].values[u] if i == 0 else i for u, i in enumerate(Tors['elon'].values)])
+
+    def get_storm_tornadoes(self, storm, dist_thresh):
         r"""
         Retrieves all tornado tracks that occur along the track of a tropical cyclone.
-        
+
         Parameters
         ----------
         storm : tropycal.tracks.Storm
             Instance of a Storm object.
         dist_thresh : int
             Distance threshold (in kilometers) from the tropical cyclone track over which to attribute tornadoes to the TC.
-        
+
         Returns
         -------
         pandas.DataFrame
             Pandas DataFrame object containing data about the tornadoes associated with this tropical cyclone.
         """
-        
-        #Get storm dict from object
+
+        # Get storm dict from object
         stormdict = storm.to_dict()
-    
-        stormTors = self.Tors[(self.Tors['UTC_time']>=min(stormdict['date'])) & \
-                         (self.Tors['UTC_time']<=max(stormdict['date']))]
-        
-        #Interpolate storm track time to the time of each tornado
-        f = interp1d(mdates.date2num(stormdict['date']),stormdict['lon'])
+
+        stormTors = self.Tors[(self.Tors['UTC_time'] >= min(stormdict['time'])) &
+                              (self.Tors['UTC_time'] <= max(stormdict['time']))]
+
+        # Interpolate storm track time to the time of each tornado
+        f = interp1d(mdates.date2num(stormdict['time']), stormdict['lon'])
         interp_clon = f(mdates.date2num(stormTors['UTC_time']))
-        f = interp1d(mdates.date2num(stormdict['date']),stormdict['lat'])
+        f = interp1d(mdates.date2num(stormdict['time']), stormdict['lat'])
         interp_clat = f(mdates.date2num(stormTors['UTC_time']))
-        
-        #Retrieve x&y distance of each tornado from TC center
-        stormTors = stormTors.assign(xdist_s = [great_circle((.5*(lat1+lat2),lon1),(.5*(lat1+lat2),lon2)).kilometers \
-                 for lat1,lon1,lat2,lon2 in zip(interp_clat,interp_clon,stormTors['slat'],stormTors['slon'])])
-        stormTors = stormTors.assign(ydist_s = [great_circle((lat1,.5*(lon1+lon2)),(lat2,.5*(lon1+lon2))).kilometers \
-                 for lat1,lon1,lat2,lon2 in zip(interp_clat,interp_clon,stormTors['slat'],stormTors['slon'])])
-
-        stormTors = stormTors.assign(xdist_e = [great_circle((.5*(lat1+lat2),lon1),(.5*(lat1+lat2),lon2)).kilometers \
-                 for lat1,lon1,lat2,lon2 in zip(interp_clat,interp_clon,stormTors['elat'],stormTors['elon'])])
-        stormTors = stormTors.assign(ydist_e = [great_circle((lat1,.5*(lon1+lon2)),(lat2,.5*(lon1+lon2))).kilometers \
-                 for lat1,lon1,lat2,lon2 in zip(interp_clat,interp_clon,stormTors['elat'],stormTors['elon'])])
-        
-        #Assign tornado within specified distance threshold to this storm
-        stormTors = stormTors[stormTors['xdist_s']**2 + stormTors['ydist_s']**2 < dist_thresh**2]
-        
-        #Return DataFrame
+
+        # Retrieve x&y distance of each tornado from TC center
+        stormTors = stormTors.assign(xdist_s=[great_circle((.5*(lat1+lat2), lon1), (.5*(lat1+lat2), lon2)).kilometers
+                                              for lat1, lon1, lat2, lon2 in zip(interp_clat, interp_clon, stormTors['slat'], stormTors['slon'])])
+        stormTors = stormTors.assign(ydist_s=[great_circle((lat1, .5*(lon1+lon2)), (lat2, .5*(lon1+lon2))).kilometers
+                                              for lat1, lon1, lat2, lon2 in zip(interp_clat, interp_clon, stormTors['slat'], stormTors['slon'])])
+
+        stormTors = stormTors.assign(xdist_e=[great_circle((.5*(lat1+lat2), lon1), (.5*(lat1+lat2), lon2)).kilometers
+                                              for lat1, lon1, lat2, lon2 in zip(interp_clat, interp_clon, stormTors['elat'], stormTors['elon'])])
+        stormTors = stormTors.assign(ydist_e=[great_circle((lat1, .5*(lon1+lon2)), (lat2, .5*(lon1+lon2))).kilometers
+                                              for lat1, lon1, lat2, lon2 in zip(interp_clat, interp_clon, stormTors['elat'], stormTors['elon'])])
+
+        # Assign tornado within specified distance threshold to this storm
+        stormTors = stormTors[stormTors['xdist_s'] **
+                              2 + stormTors['ydist_s']**2 < dist_thresh**2]
+
+        # Return DataFrame
         return stormTors
 
-    def rotateToHeading(self,storm,stormTors):
-        
+    def rotateToHeading(self, storm, stormTors):
         r"""
         Rotate tornado tracks to their position relative to the heading of the TC at the time.
-        
+
         Parameters
         ----------
         stormTors : pandas.DataFrame
             Pandas DataFrame containing tornado tracks.
-        
+
         Returns
         -------
         pandas.DataFrame
             StormTors modified to include motion relative coordinates.
         """
-        
-        #Check to make sure there's enough tornadoes
+
+        # Check to make sure there's enough tornadoes
         if len(stormTors) == 0:
             stormTors['rot_xdist_s'] = []
             stormTors['rot_xdist_e'] = []
             stormTors['rot_ydist_s'] = []
             stormTors['rot_ydist_e'] = []
             return stormTors
-        
-        #Get storm dict from object
+
+        # Get storm dict from object
         stormdict = storm.to_dict()
-        
-        #Temporal interpolation of storm track
+
+        # Temporal interpolation of storm track
         dx = np.gradient(stormdict['lon'])
         dy = np.gradient(stormdict['lat'])
-        
-        f = interp1d(mdates.date2num(stormdict['date']),dx)
+
+        f = interp1d(mdates.date2num(stormdict['time']), dx)
         interp_dx = f(mdates.date2num(stormTors['UTC_time']))
-        f = interp1d(mdates.date2num(stormdict['date']),dy)
+        f = interp1d(mdates.date2num(stormdict['time']), dy)
         interp_dy = f(mdates.date2num(stormTors['UTC_time']))
-        
-        ds = np.hypot(interp_dx,interp_dy)
-        
+
+        ds = np.hypot(interp_dx, interp_dy)
+
         # Rotation matrix for +x pointing 90deg right of storm heading
-        ds[ds == 0.0] = ds[ds == 0.0] + 0.01 #avoid warnings for divide by zero
-        rot = np.array([[interp_dy,-interp_dx],[interp_dx,interp_dy]])/ds
-        
-        oldvec_s = np.array([stormTors['xdist_s'].values,stormTors['ydist_s'].values])
-        newvec_s = [np.dot(rot[:,:,i],v) for i,v in enumerate(oldvec_s.T)]
-        
-        oldvec_e = np.array([stormTors['xdist_e'].values,stormTors['ydist_e'].values])
-        newvec_e = [np.dot(rot[:,:,i],v) for i,v in enumerate(oldvec_e.T)]
-        
-        #Enter motion relative coordinates into stormTors dict
+        # avoid warnings for divide by zero
+        ds[ds == 0.0] = ds[ds == 0.0] + 0.01
+        rot = np.array([[interp_dy, -interp_dx], [interp_dx, interp_dy]])/ds
+
+        oldvec_s = np.array([stormTors['xdist_s'].values,
+                            stormTors['ydist_s'].values])
+        newvec_s = [np.dot(rot[:, :, i], v) for i, v in enumerate(oldvec_s.T)]
+
+        oldvec_e = np.array([stormTors['xdist_e'].values,
+                            stormTors['ydist_e'].values])
+        newvec_e = [np.dot(rot[:, :, i], v) for i, v in enumerate(oldvec_e.T)]
+
+        # Enter motion relative coordinates into stormTors dict
         stormTors['rot_xdist_s'] = [v[0] for v in newvec_s]
         stormTors['rot_xdist_e'] = [v[0] for v in newvec_e]
         stormTors['rot_ydist_s'] = [v[1] for v in newvec_s]
         stormTors['rot_ydist_e'] = [v[1] for v in newvec_e]
-        
-        #return modified stormtors
+
+        # return modified stormtors
         return stormTors
-        
 
-    def plot_TCtors_rotated(self,storm,dist_thresh=1000,return_ax=False):
-        
+    def plot_TCtors_rotated(self, storm, dist_thresh=1000, return_ax=False):
         r"""
         Plot tracks of tornadoes relative to the storm motion vector of the tropical cyclone.
-        
+
         Parameters
         ----------
         storm : tropycal.tracks.Storm
             Instance of a Storm object.
         dist_thresh : int
             Distance threshold (in kilometers) from the tropical cyclone track over which to attribute tornadoes to the TC.
         return_ax : bool
             Whether to return the axis plotted. Default is False.
-        
+
         Notes
         -----
         The motion vector is oriented upwards (in the +y direction).
         """
-        
-        #Retrieve tornadoes for the requested storm
+
+        # Retrieve tornadoes for the requested storm
         try:
             stormTors = storm.StormTors
         except:
-            stormTors = self.get_storm_tornadoes(storm,dist_thresh)
-        
-        #Add motion vector relative coordinates
-        stormTors = self.rotateToHeading(storm,stormTors)
-        
-        #Create figure for plotting
-        plt.figure(figsize=(9,9),dpi=150)
+            stormTors = self.get_storm_tornadoes(storm, dist_thresh)
+
+        # Add motion vector relative coordinates
+        stormTors = self.rotateToHeading(storm, stormTors)
+
+        # Create figure for plotting
+        plt.figure(figsize=(9, 9), dpi=150)
         ax = plt.subplot()
-        
-        #Default EF color scale
+
+        # Default EF color scale
         EFcolors = get_colors_ef('default')
-        
-        #Plot all tornado tracks in motion relative coords
-        for _,row in stormTors.iterrows():
-            plt.plot([row['rot_xdist_s'],row['rot_xdist_e']+.01],[row['rot_ydist_s'],row['rot_ydist_e']+.01],\
-                     lw=2,c=EFcolors[row['mag']])
-            
-        #Plot dist_thresh radius
+
+        # Plot all tornado tracks in motion relative coords
+        for _, row in stormTors.iterrows():
+            plt.plot([row['rot_xdist_s'], row['rot_xdist_e']+.01], [row['rot_ydist_s'], row['rot_ydist_e']+.01],
+                     lw=2, c=EFcolors[row['mag']])
+
+        # Plot dist_thresh radius
         ax.set_facecolor('#F6F6F6')
-        circle = plt.Circle((0,0), dist_thresh, color='w')
+        circle = plt.Circle((0, 0), dist_thresh, color='w')
         ax.add_artist(circle)
         an = np.linspace(0, 2 * np.pi, 100)
-        ax.plot(dist_thresh * np.cos(an), dist_thresh * np.sin(an),'k')
-        ax.plot([-dist_thresh,dist_thresh],[0,0],'k--',lw=.5)
-        ax.plot([0,0],[-dist_thresh,dist_thresh],'k--',lw=.5)
-        
-        #Plot motion vector
+        ax.plot(dist_thresh * np.cos(an), dist_thresh * np.sin(an), 'k')
+        ax.plot([-dist_thresh, dist_thresh], [0, 0], 'k--', lw=.5)
+        ax.plot([0, 0], [-dist_thresh, dist_thresh], 'k--', lw=.5)
+
+        # Plot motion vector
         plt.arrow(0, -dist_thresh*.1, 0, dist_thresh*.2, length_includes_head=True,
-          head_width=45, head_length=45,fc='k',lw=2)
-        
-        #Labels
+                  head_width=45, head_length=45, fc='k', lw=2)
+
+        # Labels
         ax.set_aspect('equal', 'box')
-        ax.set_xlabel('Left/Right of Storm Heading (km)',fontsize=13)
-        ax.set_ylabel('Behind/Ahead of Storm Heading (km)',fontsize=13)
-        ax.set_title(f'{storm.name} {storm.year} tornadoes relative to heading',fontsize=17)
+        ax.set_xlabel('Left/Right of Storm Heading (km)', fontsize=13)
+        ax.set_ylabel('Behind/Ahead of Storm Heading (km)', fontsize=13)
+        ax.set_title(
+            f'{storm.name} {storm.year} tornadoes relative to heading', fontsize=17)
         ax.tick_params(axis='both', which='major', labelsize=11.5)
-        
-        #Add legend
-        handles=[]
-        for ef,color in enumerate(EFcolors):
-            count = len(stormTors[stormTors['mag']==ef])
-            handles.append(mlines.Line2D([], [], linestyle='-',color=color,label=f'EF-{ef} ({count})'))
-        ax.legend(handles=handles,loc='lower left',fontsize=11.5)
-        
-        #Return axis or show figure
+
+        # Add legend
+        handles = []
+        for ef, color in enumerate(EFcolors):
+            count = len(stormTors[stormTors['mag'] == ef])
+            handles.append(mlines.Line2D([], [], linestyle='-',
+                           color=color, label=f'EF-{ef} ({count})'))
+        ax.legend(handles=handles, loc='lower left', fontsize=11.5)
+
+        # Return axis or show figure
         if return_ax == True:
             return ax
         else:
             plt.show()
             plt.close()
-        
 
-    def plot_tors(self,tor_info,domain="conus",plotPPH=False,ax=None,return_ax=False,return_domain=True,cartopy_proj=None,**kwargs):
-        
+    def plot_tors(self, tor_info, domain="conus", plotPPH=False, ax=None, return_ax=False, return_domain=True, cartopy_proj=None, **kwargs):
         r"""
         Creates a plot of tornado tracks and Practically Perfect Forecast (PPH).
-        
+
         Parameters
         ----------
         tor_info : pandas.DataFrame / dict / datetime.datetime / list
             Requested tornadoes to plot. Can be one of the following:
-            
+
             * **Pandas DataFrame** containing the requested tornadoes to plot.
             * **dict** entry containing the requested tornadoes to plot.
             * **datetime.datetime** object for a single day to plot tornadoes.
-            * **list** with 2 datetime.datetime entries, a start date and end date for plotting over a range of dates.
+            * **list** with 2 datetime.datetime entries, a start time and end time for plotting over a range of dates.
         domain : str
             Domain for the plot. Default is "conus". Please refer to :ref:`options-domain` for available domain options.
         plotPPH : bool or str
             Whether to plot practically perfect forecast (PPH). True defaults to "daily". Default is False.
-        
+
             * **False** - no PPH plot.
             * **True** - defaults to "daily".
             * **"total"** - probability of a tornado within 25mi of a point during the period of time selected.
             * **"daily"** - average probability of a tornado within 25mi of a point during a day starting at 12 UTC.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         prop : dict
             Property of tornado tracks.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        prop = kwargs.pop("prop",{})
-        map_prop = kwargs.pop("map_prop",{})
-        
-        #Get plot data
-        
-        if isinstance(tor_info,pd.core.frame.DataFrame):
+
+        prop = kwargs.pop("prop", {})
+        map_prop = kwargs.pop("map_prop", {})
+
+        # Get plot data
+
+        if isinstance(tor_info, pd.core.frame.DataFrame):
             dfTors = tor_info
-        elif isinstance(tor_info,dict):
+        elif isinstance(tor_info, dict):
             dfTors = pd.DataFrame.from_dict(tor_info)
         else:
             dfTors = self.__getTimeTors(tor_info)
-            if isinstance(tor_info,list):
+            if isinstance(tor_info, list):
                 try:
-                    if prop['PPHcolors']=='SPC':
+                    if prop['PPHcolors'] == 'SPC':
                         warning_message = 'SPC colors only allowed for daily PPH. Defaulting to plasma colormap.'
                         warnings.warn(warning_message)
-                        prop['PPHcolors']='plasma'
+                        prop['PPHcolors'] = 'plasma'
                 except:
                     warning_message = 'SPC colors only allowed for daily PPH. Defaulting to plasma colormap.'
                     warnings.warn(warning_message)
-                    prop['PPHcolors']='plasma'
-                    
-                if plotPPH!='total':
+                    prop['PPHcolors'] = 'plasma'
+
+                if plotPPH != 'total':
                     try:
                         prop['PPHlevels']
                     except:
                         t_int = (max(tor_info)-min(tor_info)).days
-                        if t_int>1:
-                            new_levs=[i*t_int**-.7 \
-                                for i in [2,5,10,15,30,45,60,100]]
-                            for i,_ in enumerate(new_levs[:-1]):
-                                new_levs[i] = max([new_levs[i],0.1])
-                                new_levs[i+1] = new_levs[i]+max([new_levs[i+1]-new_levs[i],.1])
-                            prop['PPHlevels']=new_levs
-    
-        #Create instance of plot object
+                        if t_int > 1:
+                            new_levs = [i*t_int**-.7
+                                        for i in [2, 5, 10, 15, 30, 45, 60, 100]]
+                            for i, _ in enumerate(new_levs[:-1]):
+                                new_levs[i] = max([new_levs[i], 0.1])
+                                new_levs[i+1] = new_levs[i] + \
+                                    max([new_levs[i+1]-new_levs[i], .1])
+                            prop['PPHlevels'] = new_levs
+
+        # Create instance of plot object
         self.plot_obj = TornadoPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
             cartopy_proj = self.plot_obj.proj
-        
-        #Plot tornadoes
-        plot_info = self.plot_obj.plot_tornadoes(dfTors,domain,plotPPH,ax,return_ax,return_domain,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+
+        # Plot tornadoes
+        plot_info = self.plot_obj.plot_tornadoes(
+            dfTors, domain, plotPPH, ax, return_ax, return_domain, prop=prop, map_prop=map_prop)
+
+        # Return axis
         if ax is not None or return_ax or return_domain:
             return plot_info
 
-    def __getTimeTors(self,time):
-        
-        if isinstance(time,list):
-            t1=min(time)
-            t2=max(time)
+    def __getTimeTors(self, time):
+
+        if isinstance(time, list):
+            t1 = min(time)
+            t2 = max(time)
         else:
             t1 = time.replace(hour=12)
             t2 = t1+timedelta(hours=24)
-        subTors = self.Tors.loc[(self.Tors['UTC_time']>=t1) & \
-                               (self.Tors['UTC_time']<t2)]
+        subTors = self.Tors.loc[(self.Tors['UTC_time'] >= t1) &
+                                (self.Tors['UTC_time'] < t2)]
         return subTors
-
-
```

### Comparing `tropycal-0.6.1/src/tropycal/tornado/plot.py` & `tropycal-1.0/src/tropycal/tornado/plot.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,224 +1,233 @@
-import calendar
-import numpy as np
-import pandas as pd
-import re
-import scipy.interpolate as interp
-import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt
 
 from ..plot import Plot
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 
 try:
-    import cartopy.feature as cfeature
     from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
 except:
-    warnings.warn("Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
+    warnings.warn(
+        "Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
 
 try:
-    import matplotlib as mlib
     import matplotlib.lines as mlines
     import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
     import matplotlib.patches as mpatches
 
 except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+
 
 class TornadoPlot(Plot):
-    
+
     def __init__(self):
-        
+
         self.use_credit = True
-                 
-    def plot_tornadoes(self,tornado,domain="east_conus",plotPPH=False,ax=None,return_ax=False,return_domain=False,prop={},map_prop={}):
-        
+
+    def plot_tornadoes(self, tornado, domain="east_conus", plotPPH=False, ax=None, return_ax=False, return_domain=False, prop={}, map_prop={}):
         r"""
         Creates a plot of a single storm track.
-        
+
         Parameters
         ----------
         storm : str, tuple or dict
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), tuple with storm name and year (e.g., ("Matthew",2016)), or a dict entry.
         domain : str
             Domain for the plot. Can be one of the following:
-            
+
             * **dynamic** - default. Dynamically focuses the domain using the tornado track(s) plotted.
             * **north_atlantic** - North Atlantic Ocean basin.
             * **conus** - Contiguous United States.
             * **east_conus** - Eastern Contiguous United States and western Atlantic.
             * **lonW/lonE/latS/latN** - Custom plot domain.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         return_ax : bool
             Whether to return axis at the end of the function. If false, plot will be displayed on the screen. Default is false.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        #Set default properties
-        default_prop = {'plotType':'tracks','PPHcolors':'spc','PPHlevels':[2,5,10,15,30,45,60,100],
-                        'EFcolors':'default','linewidth':1.5,'ms':7.5}
-        default_map_prop = {'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF',
-                            'linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #set default properties
+
+        # Set default properties
+        default_prop = {'plotType': 'tracks', 'PPHcolors': 'spc', 'PPHlevels': [2, 5, 10, 15, 30, 45, 60, 100],
+                        'EFcolors': 'default', 'linewidth': 1.5, 'ms': 7.5}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # set default properties
         input_prop = prop
         input_map_prop = map_prop
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # --------------------------------------------------------------------------------------
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = None
         min_lat = None
         max_lon = None
         min_lon = None
 
-        #Check for storm type, then get data for storm
+        # Check for storm type, then get data for storm
         try:
             tornado_data = tornado
         except:
             raise RuntimeError("Error: tornado must be dataframe")
 
-        #Retrieve storm data
+        # Retrieve storm data
         slat = tornado_data['slat']
         slon = tornado_data['slon']
         elat = tornado_data['elat']
         elon = tornado_data['elon']
         mag = tornado_data['mag']
 
         mnlat = (slat+elat)*.5
         mnlon = (slon+elon)*.5
 
-        #Add to coordinate extrema
+        # Add to coordinate extrema
         if max_lat is None:
             max_lat = max(mnlat)
         else:
-            if max(mnlat) > max_lat: max_lat = max(mnlat)
+            if max(mnlat) > max_lat:
+                max_lat = max(mnlat)
         if min_lat is None:
             min_lat = min(mnlat)
         else:
-            if min(mnlat) < min_lat: min_lat = min(mnlat)
+            if min(mnlat) < min_lat:
+                min_lat = min(mnlat)
         if max_lon is None:
             max_lon = max(mnlon)
         else:
-            if max(mnlon) > max_lon: max_lon = max(mnlon)
+            if max(mnlon) > max_lon:
+                max_lon = max(mnlon)
         if min_lon is None:
             min_lon = min(mnlon)
         else:
-            if min(mnlon) < min_lon: min_lon = min(mnlon)
+            if min(mnlon) < min_lon:
+                min_lon = min(mnlon)
 
-        #Plot PPH
-        if plotPPH in ['total','daily',True]:
-            if plotPPH == True: plotPPH = 'daily'
-            PPH,longrid,latgrid = getPPH(tornado_data,method=plotPPH)
-            
-            colors,clevs = get_colors_pph(plotPPH,prop['PPHcolors'],prop['PPHlevels'])
-                    
-            cbmap = self.ax.contourf(longrid,latgrid,PPH,\
-                             levels=clevs,colors=colors,alpha=0.5)
+        # Plot PPH
+        if plotPPH in ['total', 'daily', True]:
+            if plotPPH == True:
+                plotPPH = 'daily'
+            PPH, longrid, latgrid = getPPH(tornado_data, method=plotPPH)
 
-        #Plot tornadoes as specified
+            colors, clevs = get_colors_pph(
+                plotPPH, prop['PPHcolors'], prop['PPHlevels'])
+
+            cbmap = self.ax.contourf(longrid, latgrid, PPH,
+                                     levels=clevs, colors=colors, alpha=0.5)
+
+        # Plot tornadoes as specified
         EFcolors = get_colors_ef(prop['EFcolors'])
-        
+
         tornado_data = tornado_data.sort_values('mag')
-        for _,row in tornado_data.iterrows():
-            plt.plot([row['slon'],row['elon']+.01],[row['slat'],row['elat']+.01], \
-                lw=prop['linewidth'],color=EFcolors[row['mag']], \
-                path_effects=[path_effects.Stroke(linewidth=prop['linewidth']*1.5, foreground='w'), path_effects.Normal()],\
-                transform=ccrs.PlateCarree())
-
-        #--------------------------------------------------------------------------------------
-        
-        #Storm-centered plot domain
+        for _, row in tornado_data.iterrows():
+            plt.plot([row['slon'], row['elon']+.01], [row['slat'], row['elat']+.01],
+                     lw=prop['linewidth'], color=EFcolors[row['mag']],
+                     path_effects=[path_effects.Stroke(
+                         linewidth=prop['linewidth']*1.5, foreground='w'), path_effects.Normal()],
+                     transform=ccrs.PlateCarree())
+
+        # --------------------------------------------------------------------------------------
+
+        # Storm-centered plot domain
         if domain == "dynamic":
-            
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
-        else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Determine number of lat/lon lines to use for parallels & meridians
-        self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
+
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
+        else:
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Determine number of lat/lon lines to use for parallels & meridians
+        self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
+
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
         PPH_title = ''
-        if plotPPH in ['total',True]:
+        if plotPPH in ['total', True]:
             PPH_title = ' and total PPH (%)'
         if plotPPH == 'daily':
             PPH_title = ' and daily PPH (%)'
-        self.ax.set_title('Tornado tracks'+PPH_title,loc='left',fontsize=17,fontweight='bold')
+        self.ax.set_title('Tornado tracks'+PPH_title,
+                          loc='left', fontsize=17, fontweight='bold')
 
-        #Add right title
-        #max_PPH = max(PPH)
-        start_date = dt.strftime(min(tornado_data['UTC_time']),'%H:%M UTC %d %b %Y')
-        end_date = dt.strftime(max(tornado_data['UTC_time']),'%H:%M UTC %d %b %Y')
-        self.ax.set_title(f'Start ... {start_date}\nEnd ... {end_date}',loc='right',fontsize=13)
-
-        #--------------------------------------------------------------------------------------
-        
-        #Add legend
-        handles=[]
-        for ef,color in enumerate(EFcolors):
-            count = len(tornado_data[tornado_data['mag']==ef])
-            handles.append(mlines.Line2D([], [], linestyle='-',color=color,label=f'EF-{ef} ({count})'))
-        leg_tor = self.ax.legend(handles=handles,loc='lower left',fancybox=True,framealpha=0,fontsize=11.5)
+        # Add right title
+        # max_PPH = max(PPH)
+        start_time = dt.strftime(
+            min(tornado_data['UTC_time']), '%H:%M UTC %d %b %Y')
+        end_time = dt.strftime(
+            max(tornado_data['UTC_time']), '%H:%M UTC %d %b %Y')
+        self.ax.set_title(
+            f'Start ... {start_time}\nEnd ... {end_time}', loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
+
+        # Add legend
+        handles = []
+        for ef, color in enumerate(EFcolors):
+            count = len(tornado_data[tornado_data['mag'] == ef])
+            handles.append(mlines.Line2D([], [], linestyle='-',
+                           color=color, label=f'EF-{ef} ({count})'))
+        leg_tor = self.ax.legend(
+            handles=handles, loc='lower left', fancybox=True, framealpha=0, fontsize=11.5)
         leg_tor.set_zorder(101)
         plt.draw()
 
         # Get the bbox
         try:
             bb = leg_tor.legendPatch.get_bbox().inverse_transformed(self.fig.transFigure)
         except:
             bb = leg_tor.legendPatch.get_bbox().transformed(self.fig.transFigure.inverted())
         bb_ax = self.ax.get_position()
 
-        rectangle = mpatches.Rectangle((bb_ax.x0,bb_ax.y0),bb.width+bb.x0-bb_ax.x0,bb.height+2*bb.y0-2*bb_ax.y0,\
-                                       fc = 'w',edgecolor = '0.8',alpha = 0.8,\
+        rectangle = mpatches.Rectangle((bb_ax.x0, bb_ax.y0), bb.width+bb.x0-bb_ax.x0, bb.height+2*bb.y0-2*bb_ax.y0,
+                                       fc='w', edgecolor='0.8', alpha=0.8,
                                        transform=self.fig.transFigure, zorder=100)
-        
-        #Add PPH colorbar
+
+        # Add PPH colorbar
         if plotPPH != False:
-            
+
             # Define colorbar axis
-            cax = self.fig.add_axes([bb.width + 3*bb.x0 - 2*bb_ax.x0, bb.y0, 0.015, bb.height])
-            cbar = self.fig.colorbar(cbmap,cax=cax,orientation='vertical')
+            cax = self.fig.add_axes(
+                [bb.width + 3*bb.x0 - 2*bb_ax.x0, bb.y0, 0.015, bb.height])
+            cbar = self.fig.colorbar(cbmap, cax=cax, orientation='vertical')
             iticks = round(len(clevs)/len(cbar.ax.get_yticks()))
-            cbar.ax.set_yticklabels([round(clevs[i],1) for i in range(0,len(clevs),iticks)],fontsize=11.5,color='k')
-        
-            rectangle = mpatches.Rectangle((bb_ax.x0,bb_ax.y0),bb.width+bb.x0-bb_ax.x0+.06,bb.height+2*bb.y0-2*bb_ax.y0,\
-                                           fc = 'w',edgecolor = '0.8',alpha = 0.8,\
+            cbar.ax.set_yticklabels([round(clevs[i], 1) for i in range(
+                0, len(clevs), iticks)], fontsize=11.5, color='k')
+
+            rectangle = mpatches.Rectangle((bb_ax.x0, bb_ax.y0), bb.width+bb.x0-bb_ax.x0+.06, bb.height+2*bb.y0-2*bb_ax.y0,
+                                           fc='w', edgecolor='0.8', alpha=0.8,
                                            transform=self.fig.transFigure, zorder=100)
 
         self.ax.add_patch(rectangle)
-        
-        #add credit
+
+        # add credit
         text = self.plot_credit()
         self.add_credit(text)
-        
-        #Return axis if specified, otherwise display figure
+
+        # Return axis if specified, otherwise display figure
         if ax is not None or return_ax:
             if return_domain:
-                return self.ax,leg_tor,{'n':bound_n,'e':bound_e,'s':bound_s,'w':bound_w}
+                return self.ax, leg_tor, {'n': bound_n, 'e': bound_e, 's': bound_s, 'w': bound_w}
             else:
-                return self.ax,leg_tor
+                return self.ax, leg_tor
         else:
             plt.show()
             plt.close()
```

### Comparing `tropycal-0.6.1/src/tropycal/tornado/tools.py` & `tropycal-1.0/src/tropycal/tornado/tools.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,63 +1,64 @@
-import os, sys
 import numpy as np
 import pandas as pd
-from datetime import datetime as dt,timedelta
-from scipy.ndimage import gaussian_filter as gfilt,maximum_filter
-import warnings
+from datetime import timedelta
+from scipy.ndimage import gaussian_filter as gfilt, maximum_filter
+
 
 def circle_filter(d):
     r = int(d/2)
-    if d%2 == 0:
-        y,x = np.ogrid[-r: r, -r: r]
-        x=x+.5;y=y+.5
+    if d % 2 == 0:
+        y, x = np.ogrid[-r: r, -r: r]
+        x = x+.5
+        y = y+.5
     else:
-        y,x = np.ogrid[-r: r+1, -r: r+1]
+        y, x = np.ogrid[-r: r+1, -r: r+1]
     disk = x**2+y**2 <= r**2
     disk = disk.astype(float)
     return disk
 
-def getPPH(dfTors,method='daily',res=10):
-    
+
+def getPPH(dfTors, method='daily', res=10):
     r"""
     Calculate PPH density from tornado dataframe
-    
+
     Parameters
     ----------
     dfTors : dataframe
     method : 'total' or 'daily'
     """
-    
+
     # set up ~80km grid over CONUS
-    latgrid = np.arange(20,55,res/111)
-    longrid = np.arange(-130,-65,res/111/np.cos(35*np.pi/180))
+    latgrid = np.arange(20, 55, res/111)
+    longrid = np.arange(-130, -65, res/111/np.cos(35*np.pi/180))
     interval = int(80/res)
     disk = circle_filter(interval)
-    
+
     dfTors['SPC_time'] = dfTors['UTC_time'] - timedelta(hours=12)
     dfTors = dfTors.set_index(['SPC_time'])
     groups = dfTors.groupby(pd.Grouper(freq="D"))
-    
+
     aggregate_grid = []
-    for group in groups: 
-        slon,slat = group[1]['slon'].values,group[1]['slat'].values
-        elon,elat = group[1]['elon'].values,group[1]['elat'].values
-    
-        torlons = [i for x1,x2 in zip(slon,elon) for i in np.linspace(x1,x2, 10)]
-        torlats = [i for y1,y2 in zip(slat,elat) for i in np.linspace(y1,y2, 10)]
-    
+    for group in groups:
+        slon, slat = group[1]['slon'].values, group[1]['slat'].values
+        elon, elat = group[1]['elon'].values, group[1]['elat'].values
+
+        torlons = [i for x1, x2 in zip(slon, elon)
+                   for i in np.linspace(x1, x2, 10)]
+        torlats = [i for y1, y2 in zip(slat, elat)
+                   for i in np.linspace(y1, y2, 10)]
+
         # get grid count
-        grid, _, _ = np.histogram2d(torlats,torlons, bins=[latgrid,longrid])
-        grid = (grid>0)*1.0
-        grid = maximum_filter(grid,footprint=disk)
+        grid, _, _ = np.histogram2d(torlats, torlons, bins=[latgrid, longrid])
+        grid = (grid > 0)*1.0
+        grid = maximum_filter(grid, footprint=disk)
 
         aggregate_grid.append(grid)
-        
+
     if method == 'daily':
-        grid = np.mean(aggregate_grid,axis=0)
-        PPH = gfilt(grid,sigma=1.5*interval)*100
+        grid = np.mean(aggregate_grid, axis=0)
+        PPH = gfilt(grid, sigma=1.5*interval)*100
     if method == 'total':
-        grid = np.sum(aggregate_grid,axis=0)
-        PPH = gfilt((grid>=1)*1.0,sigma=1.5*interval)*100
-        
-    return PPH,.5*(longrid[:len(longrid)-1]+longrid[1:]),.5*(latgrid[:len(latgrid)-1]+latgrid[1:])
+        grid = np.sum(aggregate_grid, axis=0)
+        PPH = gfilt((grid >= 1)*1.0, sigma=1.5*interval)*100
 
+    return PPH, .5*(longrid[:len(longrid)-1]+longrid[1:]), .5*(latgrid[:len(latgrid)-1]+latgrid[1:])
```

### Comparing `tropycal-0.6.1/src/tropycal/tracks/dataset.py` & `tropycal-1.0/src/tropycal/tracks/dataset.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,53 +1,53 @@
 r"""Functionality for storing and analyzing an entire cyclone dataset."""
 
+import re
 import calendar
 import numpy as np
+import xarray as xr
 import pandas as pd
-import re
-import scipy.interpolate as interp
 import scipy.stats as stats
 import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 from scipy.ndimage import gaussian_filter as gfilt
 from matplotlib import path
 
-#Import internal scripts
+# Import internal scripts
 from ..plot import Plot
 from .plot import TrackPlot
 from .storm import Storm
 from .season import Season
 from ..tornado import *
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 from .. import constants
 
-#Import matplotlib
+# Import matplotlib
 try:
     import matplotlib.lines as mlines
-    import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
     import matplotlib.dates as mdates
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+
 
 class TrackDataset:
-    
+
     r"""
     Creates an instance of a TrackDataset object containing various cyclone data.
 
     Parameters
     ----------
     basin : str
         Ocean basin(s) to load data for. Can be any of the following:
-        
+
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Name
              - Source(s)
            * - "north_atlantic"
@@ -60,1193 +60,1402 @@
              - IBTrACS
            * - "north_indian"
              - IBTrACS
            * - "south_indian"
              - IBTrACS
            * - "australia"
              - IBTrACS* (special case)
+           * - "south_pacific"
+             - IBTrACS
            * - "south_atlantic"
              - IBTrACS
            * - "all"
              - IBTrACS
 
     source : str
         Data source to read in. Default is HURDAT2.
-        
+
         * **hurdat** - HURDAT2 data source for the North Atlantic and East/Central Pacific basins
         * **ibtracs** - ibtracs data source for regional or global data
     include_btk : bool, optional
         If True, the best track data from NHC for the most recent years where it doesn't exist in HURDAT2 will be added into the dataset. Valid for "north_atlantic" and "east_pacific" basins. Default is False.
     interpolate_data : bool, optional
         If True, interpolates all storm data to hourly. Default is False.
-    
+
     Other Parameters
     ----------------
     atlantic_url : str, optional
         URL containing the Atlantic HURDAT2 dataset. Can be changed to a local txt reference file. Default is retrieval from online URL.
     pacific_url : str, optional
         URL containing the Pacific HURDAT2 dataset. Can be changed to a local txt reference file. Default is retrieval from online URL.
     ibtracs_url : str, optional
         URL containing the ibtracs dataset. Can be changed to a local txt reference file. Can be a regional or all ibtracs file. If regional, the basin should match the argument basin provided earlier. Default is retrieval from online URL.
     catarina : bool, optional
         Modify the dataset to include cyclone track data for Cyclone Catarina (2004) from McTaggart-Cowan et al. (2006). Default is False.
     ibtracs_hurdat : bool, optional
         Replace ibtracs data for the North Atlantic and East/Central Pacific basins with HURDAT data. Default is False.
     ibtracs_mode : str, optional
         Mode of reading ibtracs in. Default is "jtwc".
-        
+
         * **wmo** = official World Meteorological Organization data. Caveat is sustained wind methodology is inconsistent between basins.
         * **jtwc** = default. Unofficial data from the Joint Typhoon Warning Center. Caveat is some storms are missing and some storm data is inaccurate.
         * **jtwc_neumann** = JTWC data modified with the Neumann reanalysis for the Southern Hemisphere. Improves upon some storms (e.g., Cyclone Tracy 1974) while degrading others.
 
     Returns
     -------
     Dataset
         An instance of Dataset.
-    
+
     Notes
     -----
     This object contains information about all storms in a basin, as well as methods to analyze the dataset and to retrieve individual storms from the dataset.
-    
+
     The following block of code creates an instance of a TrackDataset() object and stores it in a variable called "basin":
-    
+
     .. code-block:: python
-    
+
         from tropycal import tracks
         basin = tracks.TrackDataset()
-        
+
     With an instance of TrackDataset created, any of the methods listed below can be accessed via the "basin" variable:
-    
+
     .. code-block:: python
-    
+
         storm = basin.get_storm(("katrina",2005))
-    
+
     For IBTrACS datasets, please refer to :ref:`ibtracs-caveats` for pros and cons of each mode of IBTrACS data available.
-    
+
     .. note::
-    
+
         1. If using ``basin="both"``, this combines the North Atlantic and East/Central Pacific HURDATv2 data into a single TrackDataset object. As of Tropycal v0.5, this now merges cross-basin storms (i.e., North Atlantic to East Pacific) which were reclassified with a new East Pacific ID into single Storm objects.
-        
+
         2. If using ``basin="australia", source="ibtracs"``, since IBTrACS doesn't provide an Australia-only basin file by default, this will fetch the full global IBTrACS data and filter storms to only those that existed in the Australia basin.
     """
- 
+
     def __repr__(self):
-         
+
         summary = ["<tropycal.tracks.Dataset>"]
-        
-        #Find maximum wind and minimum pressure
-        max_wind = int(np.nanmax([x for stormid in self.keys for x in self.data[stormid]['vmax']]))
+
+        # Find maximum wind and minimum pressure
+        max_wind = int(
+            np.nanmax([x for stormid in self.keys for x in self.data[stormid]['vmax']]))
         max_wind_name = ""
-        min_mslp = int(np.nanmin([x for stormid in self.keys for x in self.data[stormid]['mslp']]))
+        min_mslp = int(
+            np.nanmin([x for stormid in self.keys for x in self.data[stormid]['mslp']]))
         min_mslp_name = ""
-        
+
         for key in self.keys[::-1]:
             array_vmax = np.array(self.data[key]['vmax'])
             array_mslp = np.array(self.data[key]['mslp'])
             if len(array_vmax[~np.isnan(array_vmax)]) > 0 and np.nanmax(array_vmax) == max_wind:
                 max_wind_name = f"{self.data[key]['name'].title()} {self.data[key]['year']}"
             if len(array_mslp[~np.isnan(array_mslp)]) > 0 and np.nanmin(array_mslp) == min_mslp:
                 min_mslp_name = f"{self.data[key]['name'].title()} {self.data[key]['year']}"
 
-        #Add general summary
+        # Add general summary
         emdash = '\u2014'
-        summary_keys = {'Basin':self.basin,\
-                        'Source':self.source+[', '+self.ibtracs_mode,''][self.source=='hurdat'],\
-                        'Number of storms':len(self.keys),\
-                        'Maximum wind':f"{max_wind} knots ({max_wind_name})",
-                        'Minimum pressure':f"{min_mslp} hPa ({min_mslp_name})",
-                        'Year range':f"{self.data[self.keys[0]]['year']} {emdash} {self.data[self.keys[-1]]['year']}"}
-        
-        #Add dataset summary
+        summary_keys = {
+            'Basin': self.basin,
+            'Source': self.source + [', ' + self.ibtracs_mode, ''][self.source == 'hurdat'],
+            'Number of storms': len(self.keys),
+            'Maximum wind': f"{max_wind} knots ({max_wind_name})",
+            'Minimum pressure': f"{min_mslp} hPa ({min_mslp_name})",
+            'Year range': f"{self.data[self.keys[0]]['year']} {emdash} {self.data[self.keys[-1]]['year']}",
+        }
+
+        # Add dataset summary
         summary.append("Dataset Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
 
         return "\n".join(summary)
-    
-    
-    def __init__(self,basin='north_atlantic',source='hurdat',include_btk=False,interpolate_data=False,**kwargs):
-        
-        #kwargs
-        atlantic_url = kwargs.pop('atlantic_url', 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2.html')
-        pacific_url = kwargs.pop('pacific_url', 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2-nepac.html')
-        ibtracs_url = kwargs.pop('ibtracs_url', 'https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.(basin).list.v04r00.csv')
+
+    def __init__(self, basin='north_atlantic', source='hurdat', include_btk=False, interpolate_data=False, **kwargs):
+
+        # kwargs
+        atlantic_url = kwargs.pop('atlantic_url', 'fetch')
+        pacific_url = kwargs.pop('pacific_url', 'fetch')
+        ibtracs_url = kwargs.pop(
+            'ibtracs_url', 'https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r00/access/csv/ibtracs.(basin).list.v04r00.csv')
         ibtracs_mode = kwargs.pop('ibtracs_mode', 'jtwc')
         catarina = kwargs.pop('catarina', False)
         ibtracs_hurdat = kwargs.pop('ibtracs_hurdat', False)
-        
-        #Error check
-        if ibtracs_mode not in ['wmo','jtwc','jtwc_neumann']:
-            raise ValueError("ibtracs_mode must be either 'wmo', 'jtwc', or 'jtwc_neumann'")
-        
-        #Store input arguments
-        self.proj = None #for plotting
+
+        # Error check
+        if ibtracs_mode not in ['wmo', 'jtwc', 'jtwc_neumann']:
+            raise ValueError(
+                "ibtracs_mode must be either 'wmo', 'jtwc', or 'jtwc_neumann'")
+
+        # Get latest HURDATv2 files if requested
+        if basin in constants.NHC_BASINS and source == 'hurdat':
+            condition_1 = basin == 'north_atlantic' and atlantic_url == 'fetch'
+            condition_2 = basin == 'east_pacific' and pacific_url == 'fetch'
+            condition_3 = basin == 'both' and (
+                atlantic_url == 'fetch' or pacific_url == 'fetch')
+            if condition_1 or condition_2 or condition_3:
+                atlantic_url, pacific_url = find_latest_hurdat_files()
+
+        # Store input arguments
+        self.proj = None  # for plotting
         self.basin = basin.lower()
         self.atlantic_url = str(atlantic_url)
         self.pacific_url = str(pacific_url)
         self.ibtracs_url = str(ibtracs_url)
         self.source = source
-        
-        #Modification flags
+
+        # Modification flags
         self.catarina = catarina
         self.ibtracs_mode = ibtracs_mode
         if ibtracs_mode == 'jtwc_neumann':
             self.neumann = True
         else:
             self.neumann = False
-        
-        #initialize empty dict
+
+        # initialize empty dict
         self.data = {}
-        
-        #Read in from specified data source
+
+        # Read in from specified data source
         if source == 'hurdat':
             self.__read_hurdat()
         elif source == 'ibtracs':
             self.__read_ibtracs()
         else:
-            raise RuntimeError("Accepted values for 'source' are 'hurdat' or 'ibtracs'")
-            
-        #Replace ibtracs with hurdat for atl/pac basins
+            raise RuntimeError(
+                "Accepted values for 'source' are 'hurdat' or 'ibtracs'")
+
+        # Replace ibtracs with hurdat for atl/pac basins
         if source == 'ibtracs' and ibtracs_hurdat:
-            if self.basin in ['north_atlantic','east_pacific']:
+            if self.basin in ['north_atlantic', 'east_pacific']:
                 self.__read_hurdat()
             elif self.basin == 'all':
                 self.basin = 'both'
                 self.__read_hurdat(override_basin=True)
                 self.basin = 'all'
-        
-        #Read in best track data
-        if include_btk == True and basin in ['north_atlantic','east_pacific','both']:
+
+        # Read in best track data
+        if include_btk and basin in ['north_atlantic', 'east_pacific', 'both']:
             self.__read_btk()
-        
-        #Delete duplicate entries
+
+        # Delete duplicate entries
         check = []
         check_ids = []
         keys = [k for k in self.data.keys()]
         for key in keys:
-            if self.data[key]['name'].lower() == 'unnamed': continue
-            check_id = f"{self.data[key]['name']},{self.data[key]['year']},{self.data[key]['date'][0].month}"
+            if self.data[key]['name'].lower() == 'unnamed':
+                continue
+            check_id = f"{self.data[key]['name']},{self.data[key]['year']},{self.data[key]['time'][0].month}"
             if check_id not in check:
                 check.append(check_id)
                 check_ids.append(key)
             else:
                 existing_id = check_ids[check.index(check_id)]
                 if len(self.data[key]['vmax']) > len(self.data[existing_id]['vmax']):
                     del self.data[existing_id]
                     check_ids.pop(check_ids.index(existing_id))
                     check_ids.append(key)
                 else:
                     del self.data[key]
-        
-        #Join storms for atlantic-pacific crossovers
+
+        # Join storms for atlantic-pacific crossovers
         if self.basin == 'both':
-            join_keys = [['AL081993','EP141993'],['AL181971','EP151971'],['AL141974','EP151974'],['AL161978','EP151978'],['AL111988','EP131988'],['AL031996','EP071996']]
+            join_keys = [['AL081993', 'EP141993'], ['AL181971', 'EP151971'], ['AL141974', 'EP151974'], [
+                'AL161978', 'EP151978'], ['AL111988', 'EP131988'], ['AL031996', 'EP071996']]
             for key in join_keys:
 
-                #Append East Pacific data to Atlantic data
-                for idx,i_time in enumerate(self.data[key[1]]['date']):
-                    if i_time in self.data[key[0]]['date']: continue
-                    for var in [i for i in self.data[key[1]].keys() if isinstance(self.data[key[1]][i],list)]:
-                        self.data[key[0]][var].append(self.data[key[1]][var][idx])
+                # Append East Pacific data to Atlantic data
+                for idx, i_time in enumerate(self.data[key[1]]['time']):
+                    if i_time in self.data[key[0]]['time']:
+                        continue
+                    for var in [i for i in self.data[key[1]].keys() if isinstance(self.data[key[1]][i], list)]:
+                        self.data[key[0]][var].append(
+                            self.data[key[1]][var][idx])
                     if i_time.strftime('%H%M') in constants.STANDARD_HOURS and self.data[key[1]]['type'][idx] in constants.NAMED_TROPICAL_STORM_TYPES:
-                        self.data[key[0]]['ace'] += accumulated_cyclone_energy(self.data[key[1]]['vmax'][idx])
+                        self.data[key[0]]['ace'] += accumulated_cyclone_energy(
+                            self.data[key[1]]['vmax'][idx])
 
-                #Rename storm if needed
+                # Rename storm if needed
                 if self.data[key[1]]['name'].lower() == 'unnamed' or np.nanmax(self.data[key[1]]['vmax']) < 35:
                     pass
                 else:
-                    self.data[key[0]]['name'] = f"{self.data[key[0]]['name']}-{self.data[key[1]]['name']}"
+                    self.data[key[0]
+                              ]['name'] = f"{self.data[key[0]]['name']}-{self.data[key[1]]['name']}"
 
-                #Remove Pacific storm from data
+                # Remove Pacific storm from data
                 del self.data[key[1]]
-        
-        #Add keys of all storms to object
+
+        # Add keys of all storms to object
         keys = self.data.keys()
         self.keys = [k for k in keys]
-        
-        #Create array of zero-ones for existence of tornado data for a given storm
+
+        # Create array of zero-ones for existence of tornado data for a given storm
         self.keys_tors = [0 for key in self.keys]
-        
-        #Add dict to store all storm-specific tornado data in
+
+        # Add dict to store all storm-specific tornado data in
         self.data_tors = {}
-        
+
         # If interpolate_data, interpolate each storm and save to dictionary.
         self.data_interp = {}
         if interpolate_data:
             self.__interpolate_storms(self.keys)
-    
-    def __read_hurdat(self,override_basin=False):
-        
+
+        # Round ACE for all entries
+        for key in self.keys:
+            self.data[key]['ace'] = round(self.data[key]['ace'], 4)
+
+        # ---------------------------------------------------------------
+
+        # Find maximum wind and minimum pressure
+        max_wind = int(
+            np.nanmax([x for stormid in self.keys for x in self.data[stormid]['vmax']]))
+        max_wind_name = ""
+        min_mslp = int(
+            np.nanmin([x for stormid in self.keys for x in self.data[stormid]['mslp']]))
+        min_mslp_name = ""
+        for key in self.keys[::-1]:
+            array_vmax = np.array(self.data[key]['vmax'])
+            array_mslp = np.array(self.data[key]['mslp'])
+            if len(array_vmax[~np.isnan(array_vmax)]) > 0 and np.nanmax(array_vmax) == max_wind:
+                max_wind_tuple = (
+                    self.data[key]['name'], self.data[key]['year'])
+            if len(array_mslp[~np.isnan(array_mslp)]) > 0 and np.nanmin(array_mslp) == min_mslp:
+                min_mslp_tuple = (
+                    self.data[key]['name'], self.data[key]['year'])
+
+        # Add attributes
+        self.attrs = {
+            'basin': self.basin,
+            'source': self.source,
+            'ibtracs_mode': self.ibtracs_mode if self.source == 'ibtracs' else '',
+            'start_year': self.data[self.keys[0]]['year'],
+            'end_year': self.data[self.keys[-1]]['year'],
+            'max_wind': max_wind_tuple,
+            'min_mslp': min_mslp_tuple,
+        }
+
+    def __read_hurdat(self, override_basin=False):
         r"""
         Reads in HURDATv2 data into the Dataset object.
         """
-        
-        #Time duration to read in HURDAT
+
+        # Time duration to read in HURDAT
         start_time = dt.now()
         print("--> Starting to read in HURDAT2 data")
-        
-        #Quick error check
+
+        # Quick error check
         atl_online = True
         pac_online = True
         fcheck = "https://www.nhc.noaa.gov/data/hurdat/"
         fcheck2 = "https://www.aoml.noaa.gov/hrd/hurdat/"
         if fcheck not in self.atlantic_url and fcheck2 not in self.atlantic_url:
             if "http" in self.atlantic_url:
                 raise RuntimeError("URL provided is not via NHC or HRD")
             else:
                 atl_online = False
         if fcheck not in self.pacific_url and fcheck2 not in self.pacific_url:
             if "http" in self.pacific_url:
                 raise RuntimeError("URL provided is not via NHC or HRD")
             else:
                 pac_online = False
-        
-        #Check if basin is valid
-        if self.basin.lower() not in ['north_atlantic','east_pacific','both']:
-            raise RuntimeError("Only valid basins are 'north_atlantic', 'east_pacific' or 'both'")
-        
-        def read_hurdat(path,flag):
+
+        # Check if basin is valid
+        if self.basin.lower() not in ['north_atlantic', 'east_pacific', 'both']:
+            raise RuntimeError(
+                "Only valid basins are 'north_atlantic', 'east_pacific' or 'both'")
+
+        def read_hurdat(path, flag):
             if flag:
                 content = read_url(path)
             else:
-                f = open(path,"r")
+                f = open(path, "r")
                 content = f.readlines()
-                content = [(i.replace(" ","")).split(",") for i in content]
+                content = [(i.replace(" ", "")).split(",") for i in content]
                 f.close()
             return content
-        
-        #read in HURDAT2 file from URL
+
+        # read in HURDAT2 file from URL
         if self.basin == 'north_atlantic':
-            content = read_hurdat(self.atlantic_url,atl_online)
+            content = read_hurdat(self.atlantic_url, atl_online)
         elif self.basin == 'east_pacific':
-            content = read_hurdat(self.pacific_url,pac_online)
+            content = read_hurdat(self.pacific_url, pac_online)
         elif self.basin == 'both':
-            content = read_hurdat(self.atlantic_url,atl_online)
-            content += read_hurdat(self.pacific_url,pac_online)
-        
-        #keep current storm ID for iteration
+            content = read_hurdat(self.atlantic_url, atl_online)
+            content += read_hurdat(self.pacific_url, pac_online)
+
+        # keep current storm ID for iteration
         current_id = ""
-        
-        #iterate through every line
+
+        # iterate through every line
         for line in content:
-            
-            #Skip if line is empty
-            if len(line) < 2: continue
-            if line[0][0] == "<": continue
-            
-            #identify if this is a header for a storm or content of storm
-            if line[0][0] in ['A','C','E']:
-                
-                #Determine basin
+
+            # Skip if line is empty
+            if len(line) < 2:
+                continue
+            if line[0][0] == "<":
+                continue
+
+            # identify if this is a header for a storm or content of storm
+            if line[0][0] in ['A', 'C', 'E']:
+
+                # Determine basin
                 add_basin = 'north_atlantic'
                 if line[0][0] == 'C':
                     add_basin = 'east_pacific'
                 elif line[0][0] == 'E':
                     add_basin = 'east_pacific'
                 if override_basin:
                     add_basin = 'all'
-                
-                #add empty entry into dict
-                self.data[line[0]] = {'id':line[0],'operational_id':'','name':line[1],'year':int(line[0][4:]),'season':int(line[0][4:]),'basin':add_basin,'source_info':'NHC Hurricane Database'}
+
+                # add empty entry into dict
+                self.data[line[0]] = {'id': line[0], 'operational_id': '', 'name': line[1], 'year': int(
+                    line[0][4:]), 'season': int(line[0][4:]), 'basin': add_basin, 'source_info': 'NHC Hurricane Database'}
                 self.data[line[0]]['source'] = self.source
                 current_id = line[0]
-                
-                #add empty lists
-                for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
+
+                # add empty lists
+                for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin']:
                     self.data[line[0]][val] = []
                 self.data[line[0]]['ace'] = 0.0
-                
-            #if not a header, enter storm info into its dict entry
+
+            # if not a header, enter storm info into its dict entry
             else:
-                
-                #Retrieve important info about storm
-                yyyymmdd,hhmm,special,storm_type,lat,lon,vmax,mslp = line[0:8]
-                
-                #Check date doesn't already exist in dict
-                date = dt.strptime(yyyymmdd+hhmm,'%Y%m%d%H%M')
-                if date in self.data[current_id]['date']:
-                    #Hard-code fix
+
+                # Retrieve important info about storm
+                yyyymmdd, hhmm, special, storm_type, lat, lon, vmax, mslp = line[0:8]
+
+                # Check time doesn't already exist in dict
+                time = dt.strptime(yyyymmdd + hhmm, '%Y%m%d%H%M')
+                if time in self.data[current_id]['time']:
+                    # Hard-code fix
                     if current_id == "AL151966" and yyyymmdd == "19661004":
-                        date = dt.strptime("19661006"+hhmm,'%Y%m%d%H%M')
+                        time = dt.strptime("19661006" + hhmm, '%Y%m%d%H%M')
                     else:
                         continue
-                
-                #Parse into format to be entered into dict
+
+                # Parse into format to be entered into dict
                 if "N" in lat:
-                    lat = round(float(lat.split("N")[0]),1)
+                    lat = round(float(lat.split("N")[0]), 1)
                 elif "S" in lat:
-                    lat = round(float(lat.split("N")[0]),1) * -1.0
+                    lat = round(float(lat.split("S")[0]), 1) * -1.0
                 if "W" in lon:
-                    lon = round(float(lon.split("W")[0]),1) * -1.0
+                    lon = round(float(lon.split("W")[0]), 1) * -1.0
                 elif "E" in lon:
-                    lon = round(float(lon.split("E")[0]),1)
+                    lon = round(float(lon.split("E")[0]), 1)
                 vmax = int(vmax)
                 mslp = int(mslp)
-                
-                #Fix longitude for Atlantic storms east of the prime meridian
+
+                # Fix longitude for Atlantic storms east of the prime meridian
                 if add_basin == 'north_atlantic' and lon < -180:
                     lon += 360.0
-                
-                #Handle missing data
-                if vmax < 0: vmax = np.nan
-                if mslp < 800: mslp = np.nan
-                    
-                #Handle off-hour obs
+
+                # Handle missing data
+                if vmax < 0:
+                    vmax = np.nan
+                if mslp < 800:
+                    mslp = np.nan
+
+                # Handle off-hour obs
                 if hhmm in constants.STANDARD_HOURS:
                     self.data[current_id]['extra_obs'].append(0)
                 else:
                     self.data[current_id]['extra_obs'].append(1)
-                    
-                #Fix storm type for cross-dateline storms
-                storm_type = storm_type.replace("ST","HU")
-                storm_type = storm_type.replace("TY","HU")
-                
-                #Append into dict
-                self.data[current_id]['date'].append(date)
+
+                # Append into dict
+                self.data[current_id]['time'].append(time)
                 self.data[current_id]['special'].append(special)
                 self.data[current_id]['type'].append(storm_type)
                 self.data[current_id]['lat'].append(lat)
                 self.data[current_id]['lon'].append(lon)
                 self.data[current_id]['vmax'].append(vmax)
                 self.data[current_id]['mslp'].append(mslp)
-                
-                #Add basin
+
+                # Add basin
                 origin_basin = add_basin + ''
                 if add_basin == 'east_pacific':
-                    check_basin = get_basin(self.data[current_id]['lat'][0],self.data[current_id]['lon'][0],add_basin)
-                    if check_basin != add_basin: origin_basin = 'north_atlantic'
-                self.data[current_id]['wmo_basin'].append(get_basin(lat,lon,origin_basin))
-                
-                #Calculate ACE & append to storm total
-                if np.isnan(vmax) == False:
-                    ace = (10**-4) * (vmax**2)
+                    check_basin = get_basin(
+                        self.data[current_id]['lat'][0], self.data[current_id]['lon'][0], add_basin)
+                    if check_basin != add_basin:
+                        origin_basin = 'north_atlantic'
+                self.data[current_id]['wmo_basin'].append(
+                    get_basin(lat, lon, origin_basin))
+
+                # Calculate ACE & append to storm total
+                if not np.isnan(vmax):
+                    ace = accumulated_cyclone_energy(vmax)
                     if hhmm in constants.STANDARD_HOURS and storm_type in constants.NAMED_TROPICAL_STORM_TYPES:
-                        self.data[current_id]['ace'] += np.round(ace,4)
-        
-        #Account for operationally unnamed storms
+                        self.data[current_id]['ace'] += np.round(ace, 4)
+
+        # Account for operationally unnamed storms
         current_year = 0
         current_year_id = 1
         for key in self.data.keys():
-            
+
             storm_data = self.data[key]
             storm_name = storm_data['name']
             storm_year = storm_data['year']
             storm_vmax = storm_data['vmax']
             storm_id = storm_data['id']
-            
-            #Get max wind for storm
+
+            # Get max wind for storm
             np_wnd = np.array(storm_vmax)
             if len(np_wnd[~np.isnan(np_wnd)]) == 0:
                 max_wnd = np.nan
             else:
                 max_wnd = int(np.nanmax(storm_vmax))
-            
-            #Fix current year
+
+            # Fix current year
             if current_year == 0:
                 current_year = storm_year
             else:
                 if storm_year != current_year:
                     current_year = storm_year
                     current_year_id = 1
-                    
-                    #special fix for 1992 in Atlantic
+
+                    # special fix for 1992 in Atlantic
                     if current_year == 1992 and self.data[current_id]['basin'] == 'north_atlantic':
                         current_year_id = 2
-                
-            #Estimate operational storm ID (which sometimes differs from HURDAT2 ID)
+
+            # Estimate operational storm ID (which sometimes differs from HURDAT2 ID)
             blocked_list = ['EP072020']
-            potential_tcs = ['AL102017']
             increment_but_pass = []
-            
+
             if storm_name == 'UNNAMED' and max_wnd != np.nan and max_wnd >= 34 and storm_id not in blocked_list:
-                if storm_id in increment_but_pass: current_year_id += 1
+                if storm_id in increment_but_pass:
+                    current_year_id += 1
                 pass
             elif storm_id[0:2] == 'CP':
                 self.data[key]['operational_id'] = storm_id + ''
             else:
-                #Skip potential TCs
-                if f"{storm_id[0:2]}{num_to_str2(current_year_id)}{storm_year}" in potential_tcs:
+                # Skip potential TCs
+                if f"{storm_id[0:2]}{num_to_str2(current_year_id)}{storm_year}" != storm_id and storm_year >= 2017:
                     current_year_id += 1
-                self.data[key]['operational_id'] = f"{storm_id[0:2]}{num_to_str2(current_year_id)}{storm_year}"
+                self.data[key][
+                    'operational_id'] = f"{storm_id[0:2]}{num_to_str2(current_year_id)}{storm_year}"
                 current_year_id += 1
-                
-            #Swap operational storm IDs, if necessary
-            swap_list = ['EP101994','EP111994']
-            swap_pair = ['EP111994','EP101994']
+
+            # Swap operational storm IDs, if necessary
+            swap_list = ['EP101994', 'EP111994']
+            swap_pair = ['EP111994', 'EP101994']
             if self.data[key]['operational_id'] in swap_list:
                 swap_idx = swap_list.index(self.data[key]['operational_id'])
                 self.data[key]['operational_id'] = swap_pair[swap_idx]
 
-        #Determine time elapsed
+        # Determine time elapsed
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
+        tsec = str(round(time_elapsed.total_seconds(), 2))
         print(f"--> Completed reading in HURDAT2 data ({tsec} seconds)")
-    
-    
+
     def __read_btk(self):
-        
         r"""
         Reads in best track data into the Dataset object.
         """
 
-        #Time duration to read in best track
+        # Time duration to read in best track
         start_time = dt.now()
         print("--> Starting to read in best track data")
 
-        #Get range of years missing
+        # Get range of years missing
         start_year = self.data[([k for k in self.data.keys()])[-1]]['year'] + 1
         end_year = (dt.now()).year
 
-        #Get list of files in online directory
+        # Get list of files in online directory
         use_ftp = False
         try:
-            urlpath = urllib.request.urlopen('https://ftp.nhc.noaa.gov/atcf/btk/')
+            urlpath = urllib.request.urlopen(
+                'https://ftp.nhc.noaa.gov/atcf/btk/')
             string = urlpath.read().decode('utf-8')
         except:
             use_ftp = True
-            urlpath = urllib.request.urlopen('ftp://ftp.nhc.noaa.gov/atcf/btk/')
+            urlpath = urllib.request.urlopen(
+                'ftp://ftp.nhc.noaa.gov/atcf/btk/')
             string = urlpath.read().decode('utf-8')
 
-        #Get relevant filenames from directory
+        # Get relevant filenames from directory
         files = []
         files_years = []
-        for iyear in range(start_year,end_year+1):
+        for iyear in range(start_year, end_year + 1):
             if self.basin == 'north_atlantic':
                 search_pattern = f'bal[01234][0123456789]{iyear}.dat'
             elif self.basin == 'east_pacific':
                 search_pattern = f'b[ec]p[01234][0123456789]{iyear}.dat'
             elif self.basin == 'both':
                 search_pattern = f'b[aec][lp][01234][0123456789]{iyear}.dat'
 
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(string)
             for filename in filelist:
-                if filename not in files: files.append(filename)
-                if iyear not in files_years: files_years.append(iyear)
+                if filename not in files:
+                    files.append(filename)
+                if iyear not in files_years:
+                    files_years.append(iyear)
 
-        #If no files are available, go into archive directory
+        # If no files are available, go into archive directory
         archive_years = []
-        for iyear in range(start_year,end_year):
+        for iyear in range(start_year, end_year):
             if iyear not in files_years:
                 archive_years.append(iyear)
-            
-            #retrieve list of storms for that year from the archive
-            path_season = urllib.request.urlopen(f'http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{iyear}/')
+
+            # retrieve list of storms for that year from the archive
+            path_season = urllib.request.urlopen(
+                f'http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{iyear}/')
             string = path_season.read().decode('utf-8')
             nums = "[0123456789]"
             search_pattern = f'bal[0123]{nums}{iyear}.dat'
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(string)
             for file in filelist:
-                if file not in files: files.append(file)
-        
-        #For each file, read in file content and add to hurdat dict
+                if file not in files:
+                    files.append(file)
+
+        # For each file, read in file content and add to hurdat dict
         for file in files:
 
-            #Get file ID
+            # Get file ID
             stormid = ((file.split(".dat")[0])[1:]).upper()
 
-            #Determine basin
+            # Determine basin
             add_basin = 'north_atlantic'
             if stormid[0] == 'C':
                 add_basin = 'east_pacific'
             elif stormid[0] == 'E':
                 add_basin = 'east_pacific'
 
-            #add empty entry into dict
-            self.data[stormid] = {'id':stormid,'operational_id':stormid,'name':'','year':int(stormid[4:8]),'season':int(stormid[4:8]),'basin':add_basin,'source_info':'NHC Hurricane Database','source_method':"NHC's Automated Tropical Cyclone Forecasting System (ATCF)",'source_url':"https://ftp.nhc.noaa.gov/atcf/btk/"}
+            # add empty entry into dict
+            self.data[stormid] = {'id': stormid, 'operational_id': stormid, 'name': '', 'year': int(stormid[4:8]), 'season': int(
+                stormid[4:8]), 'basin': add_basin, 'source_info': 'NHC Hurricane Database', 'source_method': "NHC's Automated Tropical Cyclone Forecasting System (ATCF)", 'source_url': "https://ftp.nhc.noaa.gov/atcf/btk/"}
             self.data[stormid]['source'] = self.source
 
-            #add empty lists
-            for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
+            # add empty lists
+            for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin']:
                 self.data[stormid][val] = []
             self.data[stormid]['ace'] = 0.0
 
-            #Read in file
+            # Read in file
             if use_ftp:
                 url = f"ftp://ftp.nhc.noaa.gov/atcf/btk/{file}"
             else:
                 url = f"https://ftp.nhc.noaa.gov/atcf/btk/{file}"
             if int(stormid[4:8]) in archive_years:
                 url = f"http://hurricanes.ral.ucar.edu/repository/data/bdecks_open/{int(stormid[4:8])}/b{stormid.lower()}.dat"
             content = read_url(url)
 
-            #iterate through file lines
+            # iterate through file lines
             for line in content:
 
-                if len(line) < 28: continue
+                if len(line) < 28:
+                    continue
 
-                #Get date of obs
-                date = dt.strptime(line[2],'%Y%m%d%H')
-                date_hhmm = date.strftime('%H%M')
-                if date_hhmm not in constants.STANDARD_HOURS: continue
+                # Get time of obs
+                time = dt.strptime(line[2], '%Y%m%d%H')
+                date_hhmm = time.strftime('%H%M')
+                if date_hhmm not in constants.STANDARD_HOURS:
+                    continue
 
-                #Ensure obs aren't being repeated
-                if date in self.data[stormid]['date']: continue
+                # Ensure obs aren't being repeated
+                if time in self.data[stormid]['time']:
+                    continue
 
-                #Get latitude into number
+                # Get latitude into number
                 btk_lat_temp = line[6].split("N")[0]
                 btk_lat = float(btk_lat_temp) * 0.1
 
-                #Get longitude into number
+                # Get longitude into number
                 if "W" in line[7]:
                     btk_lon_temp = line[7].split("W")[0]
                     btk_lon = float(btk_lon_temp) * -0.1
                 elif "E" in line[7]:
                     btk_lon_temp = line[7].split("E")[0]
                     btk_lon = float(btk_lon_temp) * 0.1
 
-                #Get other relevant variables
+                # Get other relevant variables
                 btk_wind = int(line[8])
                 btk_mslp = int(line[9])
                 btk_type = line[10]
                 name = line[27]
 
-                #Replace with NaNs
-                if btk_wind > 250 or btk_wind < 10: btk_wind = np.nan
-                if btk_mslp > 1040 or btk_mslp < 800: btk_mslp = np.nan
+                # Replace with NaNs
+                if btk_wind > 250 or btk_wind < 10:
+                    btk_wind = np.nan
+                if btk_mslp > 1040 or btk_mslp < 800:
+                    btk_mslp = np.nan
 
-                #Add extra obs
+                # Add extra obs
                 self.data[stormid]['extra_obs'].append(0)
 
-                #Append into dict
-                self.data[stormid]['date'].append(date)
+                # Append into dict
+                self.data[stormid]['time'].append(time)
                 self.data[stormid]['special'].append('')
                 self.data[stormid]['type'].append(btk_type)
-                self.data[stormid]['lat'].append(round(btk_lat,1))
-                self.data[stormid]['lon'].append(round(btk_lon,1))
+                self.data[stormid]['lat'].append(round(btk_lat, 1))
+                self.data[stormid]['lon'].append(round(btk_lon, 1))
                 self.data[stormid]['vmax'].append(btk_wind)
                 self.data[stormid]['mslp'].append(btk_mslp)
-                
-                #Add basin
+
+                # Add basin
                 if self.basin == 'both':
                     origin_basin = 'north_atlantic' if stormid[0:2] == 'AL' else 'east_pacific'
                 else:
                     origin_basin = self.basin + ''
                 if self.basin == 'east_pacific':
-                    check_basin = get_basin(self.data[stormid]['lat'][0],self.data[stormid]['lon'][0],self.basin)
-                    if check_basin != self.basin: origin_basin = 'north_atlantic'
-                self.data[stormid]['wmo_basin'].append(get_basin(btk_lat,btk_lon,origin_basin))
-
-                #Calculate ACE & append to storm total
-                if np.isnan(btk_wind) == False:
-                    ace = (10**-4) * (btk_wind**2)
+                    check_basin = get_basin(
+                        self.data[stormid]['lat'][0], self.data[stormid]['lon'][0], self.basin)
+                    if check_basin != self.basin:
+                        origin_basin = 'north_atlantic'
+                self.data[stormid]['wmo_basin'].append(
+                    get_basin(btk_lat, btk_lon, origin_basin))
+
+                # Calculate ACE & append to storm total
+                if not np.isnan(btk_wind):
+                    ace = accumulated_cyclone_energy(btk_wind)
                     if btk_type in constants.NAMED_TROPICAL_STORM_TYPES:
-                        self.data[stormid]['ace'] += np.round(ace,4)
+                        self.data[stormid]['ace'] += np.round(ace, 4)
 
-            #Add storm name
+            # Add storm name
             self.data[stormid]['name'] = name
 
-        #Determine time elapsed
+        # Determine time elapsed
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
+        tsec = str(round(time_elapsed.total_seconds(), 2))
         print(f"--> Completed reading in best track data ({tsec} seconds)")
 
-        
     def __read_ibtracs(self):
-        
         r"""
         Reads in ibtracs data into the Dataset object.
         """
 
-        #Time duration to read in ibtracs
+        # Time duration to read in ibtracs
         start_time = dt.now()
         print("--> Starting to read in ibtracs data")
-        
-        #Quick error check
+
+        # Quick error check
         ibtracs_online = True
         fcheck = "https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/"
         if fcheck not in self.ibtracs_url:
             if "http" in self.ibtracs_url:
                 raise RuntimeError("URL provided is not via NCEI")
             else:
                 ibtracs_online = False
 
-        #convert to ibtracs basin
-        basin_convert = {'all':'ALL',
-                         'east_pacific':'EP',
-                         'north_atlantic':'NA',
-                         'north_indian':'NI',
-                         'south_atlantic':'SA',
-                         'south_indian':'SI',
-                         'south_pacific':'SP',
-                         'west_pacific':'WP'}
-        ibtracs_basin = basin_convert.get(self.basin,'')
-        
-        #read in ibtracs file
+        # convert to ibtracs basin
+        basin_convert = {'all': 'ALL',
+                         'east_pacific': 'EP',
+                         'north_atlantic': 'NA',
+                         'north_indian': 'NI',
+                         'south_atlantic': 'SA',
+                         'south_indian': 'SI',
+                         'south_pacific': 'SP',
+                         'west_pacific': 'WP'}
+        ibtracs_basin = basin_convert.get(self.basin, '')
+
+        # read in ibtracs file
         if ibtracs_online:
-            enter_basin = 'ALL' if self.basin == 'australia' else ibtracs_basin+''
-            path = self.ibtracs_url.replace("(basin)",enter_basin)
+            enter_basin = 'ALL' if self.basin == 'australia' else ibtracs_basin + ''
+            path = self.ibtracs_url.replace("(basin)", enter_basin)
             content = read_url(path)
         else:
-            f = open(self.ibtracs_url,"r")
+            f = open(self.ibtracs_url, "r")
             content = f.readlines()
-            content = [(i.replace(" ","")).split(",") for i in content]
+            content = [(i.replace(" ", "")).split(",") for i in content]
             f.close()
 
-        #Initialize empty dict for neumann data
+        # Initialize empty dict for neumann data
         neumann = {}
-        
-        #ibtracs ID to jtwc ID mapping
+
+        # ibtracs ID to jtwc ID mapping
+        map_duplicate_id = {}
         map_all_id = {}
         map_id = {}
-        
+
+        # Blacklist of storm IDs not NOT merge
+        do_not_merge = ['WP371996', 'WP391996']
+
         for line in content[2:]:
-            
-            if len(line) < 150: continue
-            
-            ibtracs_id, year, adv_number, basin, subbasin, name, time, wmo_type, wmo_lat, wmo_lon, wmo_vmax, wmo_mslp, agency, track_type, dist_land, dist_landfall, iflag, usa_agency, sid, lat, lon, special, stype, vmax, mslp = line[:25]
-            
-            date = dt.strptime(time,'%Y-%m-%d%H:%M:00')
-            
-            #Fix name to be consistent with HURDAT
-            if name == 'NOT_NAMED': name = 'UNNAMED'
-            if name[-1] == '-': name = name[:-1]
 
-            #Add storm to list of keys
+            if len(line) < 150:
+                continue
+
+            ibtracs_id, year, adv_number, basin, subbasin, name, time, wmo_type, wmo_lat, wmo_lon, wmo_vmax, wmo_mslp, agency, track_type, dist_land, dist_landfall, iflag, usa_agency, storm_id, lat, lon, special, storm_type, vmax, mslp = line[
+                :25]
+
+            time = dt.strptime(time, '%Y-%m-%d%H:%M:00')
+
+            # Fix name to be consistent with HURDAT
+            if name == 'NOT_NAMED':
+                name = 'UNNAMED'
+            if name[-1] == '-':
+                name = name[:-1]
+
+            # Hard code fix for faulty IBTrACS data
+            if storm_id == 'EP121989' and name == 'HENRIETTE':
+                storm_id = 'EP111989'
+            if storm_id == 'WP391996':
+                name = 'UNNAMED'
+
+            # Add storm to list of keys
             if self.ibtracs_mode == 'wmo' and ibtracs_id not in self.data.keys():
 
-                #add empty entry into dict
-                self.data[ibtracs_id] = {'id':sid,'operational_id':'','name':name,'year':date.year,'season':int(year),'basin':self.basin}
+                # add empty entry into dict
+                self.data[ibtracs_id] = {'id': storm_id, 'operational_id': '', 'name': name,
+                                         'year': time.year, 'season': int(year), 'basin': self.basin}
                 self.data[ibtracs_id]['source'] = self.source
-                self.data[ibtracs_id]['source_info'] = 'World Meteorological Organization (official)'
+                self.data[ibtracs_id][
+                    'source_info'] = 'World Meteorological Organization (official)'
                 self.data[ibtracs_id]['notes'] = "'vmax' = wind converted to 1-minute using the 0.88 conversion factor. 'vmax_orig' = original vmax as assessed by its respective WMO agency."
 
-                #add empty lists
-                for val in ['date','extra_obs','special','type','lat','lon','vmax','vmax_orig','mslp','wmo_basin']:
+                # add empty lists
+                for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'vmax_orig', 'mslp', 'wmo_basin']:
                     self.data[ibtracs_id][val] = []
                 self.data[ibtracs_id]['ace'] = 0.0
-                
-            elif sid != '' and ibtracs_id not in map_all_id.keys():
-                
-                #ID entry method to use
-                use_id = sid
-                
-                #Add id to list
-                map_all_id[ibtracs_id] = sid
-
-                #add empty entry into dict
-                self.data[use_id] = {'id':sid,'operational_id':'','name':name,'year':date.year,'season':int(year),'basin':self.basin}
-                self.data[use_id]['source'] = self.source
-                self.data[use_id]['source_info'] = 'Joint Typhoon Warning Center (unofficial)'
-                if self.neumann: self.data[use_id]['source_info'] += '& Charles Neumann reanalysis for South Hemisphere storms'
-                current_id = use_id
-
-                #add empty lists
-                for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp',
-                            'wmo_type','wmo_lat','wmo_lon','wmo_vmax','wmo_mslp','wmo_basin']:
-                    self.data[use_id][val] = []
-                self.data[use_id]['ace'] = 0.0
 
-            #Get neumann data for storms containing it
+            elif storm_id != '':
+
+                # ID entry method to use
+                use_id = storm_id
+
+                # Hard code problematic early Atlantic IDs
+                check_ids = ['AL041885', 'AL031870']
+                if use_id in check_ids and use_id in self.data.keys() and ibtracs_id in map_all_id.keys() and map_all_id[ibtracs_id] != use_id:
+                    map_all_id[ibtracs_id] = use_id
+
+                if ibtracs_id not in map_all_id.keys():
+
+                    # Check for East Pacific case (overwrite prev entry if data is from 1982 backwards)
+                    east_pacific_case = False
+                    if use_id in self.data.keys() and use_id[0:2] in ['EP', 'CP']:
+                        if self.data[use_id]['year'] <= 1982:
+                            east_pacific_case = True
+
+                    # Check if this is an extension of an existing storm
+                    if use_id in self.data.keys() and not east_pacific_case:
+
+                        # Add to list of duplicate keys
+                        if use_id not in map_duplicate_id:
+                            flipped_dict = dict([(v, k)
+                                                for k, v in map_all_id.items()])
+                            map_duplicate_id[use_id] = [ibtracs_id]
+                        elif ibtracs_id not in map_duplicate_id[use_id]:
+                            map_duplicate_id[use_id].append(ibtracs_id)
+
+                        # Add name if previous one is unnamed
+                        if self.data[use_id]['name'] == 'UNNAMED' and name != 'UNNAMED':
+                            self.data[use_id]['name'] = name
+
+                    # Otherwise, add storm to entry
+                    else:
+                        map_all_id[ibtracs_id] = use_id
+
+                        # Clear any previous entry
+                        if use_id in self.data.keys():
+                            del self.data[use_id]
+
+                        # Add empty entry into dict
+                        self.data[use_id] = {'id': storm_id, 'operational_id': '', 'name': name,
+                                             'year': time.year, 'season': int(year), 'basin': self.basin}
+                        self.data[use_id]['source'] = self.source
+                        self.data[use_id][
+                            'source_info'] = 'Joint Typhoon Warning Center (unofficial)'
+                        if self.neumann:
+                            self.data[use_id]['source_info'] += ' & Charles Neumann reanalysis for South Hemisphere storms'
+                        # add empty lists
+                        for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp',
+                                    'wmo_type', 'wmo_lat', 'wmo_lon', 'wmo_vmax', 'wmo_mslp', 'wmo_basin']:
+                            self.data[use_id][val] = []
+                        self.data[use_id]['ace'] = 0.0
+
+                # Case if IBTrACS ID already exists but a separate JTWC ID exists for it
+                elif use_id not in self.data.keys():
+
+                    # Check for potential conflict match
+                    old_id = map_all_id[ibtracs_id]
+                    check_id_new = use_id[0:2] + use_id[4:]
+                    check_id_old = old_id[0:2] + old_id[4:]
+                    if check_id_new == check_id_old and use_id != old_id:
+
+                        # Hard code fix for certain storms, and for early Atlantic entries
+                        check_keys = ['IO022018']
+                        if use_id in check_keys or (use_id[0:2] == old_id[0:2] and use_id[0:2] == 'AL'):
+
+                            # Clear any previous entry
+                            if use_id in self.data.keys():
+                                del self.data[use_id]
+
+                            # Add empty entry into dict
+                            map_all_id[ibtracs_id] = use_id
+                            self.data[use_id] = {'id': storm_id, 'operational_id': '', 'name': name,
+                                                 'year': time.year, 'season': int(year), 'basin': self.basin}
+                            self.data[use_id]['source'] = self.source
+                            self.data[use_id][
+                                'source_info'] = 'Joint Typhoon Warning Center (unofficial)'
+                            if self.neumann:
+                                self.data[use_id]['source_info'] += '& Charles Neumann reanalysis for South Hemisphere storms'
+                            # add empty lists
+                            for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp',
+                                        'wmo_type', 'wmo_lat', 'wmo_lon', 'wmo_vmax', 'wmo_mslp', 'wmo_basin']:
+                                self.data[use_id][val] = []
+                            self.data[use_id]['ace'] = 0.0
+
+                        # Special name fix for certain storms
+                        if use_id == 'IO022018':
+                            self.data['IO012018']['name'] = 'SAGAR'
+                            self.data['IO022018']['name'] = 'MEKUNU'
+
+            # Get neumann data for storms containing it
             if self.neumann:
-                neumann_lat, neumann_lon, neumann_type, neumann_vmax, neumann_mslp = line[141:146]
+                neumann_lat, neumann_lon, neumann_type, neumann_vmax, neumann_mslp = line[
+                    141:146]
                 if neumann_lat != "" and neumann_lon != "":
-                    
-                    #Add storm to list of keys
+
+                    # Add storm to list of keys
                     if ibtracs_id not in neumann.keys():
-                        neumann[ibtracs_id] = {'id':sid,'operational_id':'','name':name,'year':date.year,'season':int(year),'basin':self.basin}
+                        neumann[ibtracs_id] = {'id': storm_id, 'operational_id': '', 'name': name,
+                                               'year': time.year, 'season': int(year), 'basin': self.basin}
                         neumann[ibtracs_id]['source'] = self.source
-                        neumann[ibtracs_id]['source_info'] = 'Joint Typhoon Warning Center (unofficial) & Charles Neumann reanalysis for South Hemisphere storms'
-                        for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
+                        neumann[ibtracs_id][
+                            'source_info'] = 'Joint Typhoon Warning Center (unofficial) & Charles Neumann reanalysis for South Hemisphere storms'
+                        for val in ['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin']:
                             neumann[ibtracs_id][val] = []
                         neumann[ibtracs_id]['ace'] = 0.0
-                    
-                    #Retrieve data
-                    neumann_date = dt.strptime(time,'%Y-%m-%d%H:%M:00')
+
+                    # Retrieve data
+                    neumann_time = time + timedelta(hours=0)
                     neumann_lat = float(wmo_lat)
                     neumann_lon = float(wmo_lon)
-                    neumann_vmax = np.nan if neumann_vmax == "" else int(neumann_vmax)
-                    neumann_mslp = np.nan if neumann_mslp == "" else int(neumann_mslp)
-                    if np.isnan(neumann_vmax) == False:
-                        if str(neumann_vmax)[-1] in ['4','9']: neumann_vmax += 1
-                        if str(neumann_vmax)[-1] in ['1','6']: neumann_vmax = neumann_vmax - 1
+                    neumann_vmax = np.nan if neumann_vmax == "" else int(
+                        neumann_vmax)
+                    neumann_mslp = np.nan if neumann_mslp == "" else int(
+                        neumann_mslp)
+                    if not np.isnan(neumann_vmax):
+                        if str(neumann_vmax)[-1] in ['4', '9']:
+                            neumann_vmax += 1
+                        if str(neumann_vmax)[-1] in ['1', '6']:
+                            neumann_vmax = neumann_vmax - 1
+
+                    # Edit basin
+                    basin_reverse = {v: k for k, v in basin_convert.items()}
+                    wmo_basin = basin_reverse.get(basin, '')
+                    if subbasin in ['WA', 'EA']:
+                        wmo_basin = 'australia'
+                    neumann[ibtracs_id]['wmo_basin'].append(wmo_basin)
+
                     if neumann_type == 'TC':
                         if neumann_vmax < 34:
                             neumann_type = 'TD'
                         elif neumann_vmax < 64:
                             neumann_type = 'TS'
-                        else:
+                        elif wmo_basin in constants.NHC_BASINS:
                             neumann_type = 'HU'
+                        elif neumann_vmax < 130:
+                            neumann_type = 'TY'
+                        else:
+                            neumann_type = 'ST'
                     elif neumann_type == 'MM' or neumann_type == '':
                         neumann_type = 'LO'
-                    
-                    neumann[ibtracs_id]['date'].append(neumann_date)
+
+                    neumann[ibtracs_id]['time'].append(neumann_time)
                     neumann[ibtracs_id]['special'].append(special)
 
                     neumann[ibtracs_id]['type'].append(neumann_type)
                     neumann[ibtracs_id]['lat'].append(neumann_lat)
                     neumann[ibtracs_id]['lon'].append(neumann_lon)
                     neumann[ibtracs_id]['vmax'].append(neumann_vmax)
                     neumann[ibtracs_id]['mslp'].append(neumann_mslp)
-                    
-                    hhmm = neumann_date.strftime('%H%M')
+
+                    hhmm = neumann_time.strftime('%H%M')
                     if hhmm in constants.STANDARD_HOURS:
                         neumann[ibtracs_id]['extra_obs'].append(0)
                     else:
                         neumann[ibtracs_id]['extra_obs'].append(1)
-                    
-                    #Edit basin
-                    basin_reverse = {v: k for k, v in basin_convert.items()}
-                    wmo_basin = basin_reverse.get(basin,'')
-                    if subbasin in ['WA','EA']:
-                        wmo_basin = 'australia'
-                    neumann[ibtracs_id]['wmo_basin'].append(wmo_basin)
-                    
-                    #Calculate ACE & append to storm total
-                    if np.isnan(neumann_vmax) == False:
-                        ace = (10**-4) * (neumann_vmax**2)
-                        if hhmm in constants.STANDARD_HOURS and neumann_type in constants.NAMED_TROPICAL_STORM_TYPES and np.isnan(ace) == False:
-                            neumann[ibtracs_id]['ace'] += np.round(ace,4)
-                        
-            #Skip missing entries
+
+                    # Calculate ACE & append to storm total
+                    if not np.isnan(neumann_vmax):
+                        ace = accumulated_cyclone_energy(neumann_vmax)
+                        if hhmm in constants.STANDARD_HOURS and neumann_type in constants.NAMED_TROPICAL_STORM_TYPES and not np.isnan(ace):
+                            neumann[ibtracs_id]['ace'] += np.round(ace, 4)
+
+            # Skip missing entries
             if self.ibtracs_mode == 'wmo':
                 if wmo_lat == "" or wmo_lon == "":
                     continue
-                if agency == "": continue
+                if agency == "":
+                    continue
             else:
                 if lat == "" or lon == "":
                     continue
-                if usa_agency == "" and track_type != "PROVISIONAL": continue
-            
-            
-            #map JTWC to ibtracs ID (for neumann replacement)
+                if usa_agency == "" and track_type != "PROVISIONAL":
+                    continue
+
+            # map JTWC to ibtracs ID (for neumann replacement)
             if self.neumann:
                 if ibtracs_id not in map_id.keys():
-                    map_id[ibtracs_id] = sid
-            
-            #Handle WMO mode
+                    map_id[ibtracs_id] = storm_id
+
+            # Handle WMO mode
             if self.ibtracs_mode == 'wmo':
-                
-                #Retrieve data
-                date = dt.strptime(time,'%Y-%m-%d%H:%M:00')
+
+                # Retrieve data
                 dist_land = int(dist_land)
 
-                #Properly format WMO variables
+                # Properly format WMO variables
                 wmo_lat = float(wmo_lat)
                 wmo_lon = float(wmo_lon)
                 wmo_vmax = np.nan if wmo_vmax == "" else int(wmo_vmax)
                 wmo_mslp = np.nan if wmo_mslp == "" else int(wmo_mslp)
-                
-                #Edit basin
+
+                # Edit basin
                 basin_reverse = {v: k for k, v in basin_convert.items()}
-                wmo_basin = basin_reverse.get(basin,'')
-                if subbasin in ['WA','EA']:
+                wmo_basin = basin_reverse.get(basin, '')
+                if subbasin in ['WA', 'EA']:
                     wmo_basin = 'australia'
                 self.data[ibtracs_id]['wmo_basin'].append(wmo_basin)
-                
-                #Account for wind discrepancy
-                if wmo_basin not in ['north_atlantic','east_pacific'] and np.isnan(wmo_vmax) == False:
+
+                # Account for wind discrepancy
+                if wmo_basin not in ['north_atlantic', 'east_pacific'] and not np.isnan(wmo_vmax):
                     jtwc_vmax = int(wmo_vmax / 0.88)
                 else:
-                    if np.isnan(wmo_vmax) == False:
+                    if not np.isnan(wmo_vmax):
                         jtwc_vmax = int(wmo_vmax + 0.0)
                     else:
                         jtwc_vmax = np.nan
-                if np.isnan(jtwc_vmax) == False:
-                    if str(jtwc_vmax)[-1] in ['4','9']: jtwc_vmax += 1
-                    if str(jtwc_vmax)[-1] in ['1','6']: jtwc_vmax = jtwc_vmax - 1
-                
-                #Convert storm type from ibtracs to hurdat style
+                if not np.isnan(jtwc_vmax):
+                    if str(jtwc_vmax)[-1] in ['4', '9']:
+                        jtwc_vmax += 1
+                    if str(jtwc_vmax)[-1] in ['1', '6']:
+                        jtwc_vmax = jtwc_vmax - 1
+
+                # Convert storm type from ibtracs to hurdat style
                 """
                 DS - Disturbance
                 TS - Tropical
                 ET - Extratropical
                 SS - Subtropical
                 NR - Not reported
                 MX - Mixture (contradicting nature reports from different agencies)
                 """
                 if wmo_type == "DS":
-                    stype = "LO"
+                    storm_type = "LO"
                 elif wmo_type == "TS":
                     if np.isnan(jtwc_vmax):
-                        stype = 'LO'
+                        storm_type = 'LO'
                     elif jtwc_vmax < 34:
-                        stype = 'TD'
+                        storm_type = 'TD'
                     elif jtwc_vmax < 64:
-                        stype = 'TS'
+                        storm_type = 'TS'
+                    elif wmo_basin in constants.NHC_BASINS:
+                        storm_type = 'HU'
+                    elif jtwc_vmax < 130:
+                        storm_type = 'TY'
                     else:
-                        stype = 'HU'
+                        storm_type = 'ST'
                 elif wmo_type == 'SS':
                     if np.isnan(jtwc_vmax):
-                        stype = 'LO'
+                        storm_type = 'LO'
                     elif jtwc_vmax < 34:
-                        stype = 'SD'
+                        storm_type = 'SD'
                     else:
-                        stype = 'SS'
-                elif wmo_type in ['ET','MX']:
+                        storm_type = 'SS'
+                elif wmo_type in ['ET', 'MX']:
                     wmo_type = 'EX'
                 else:
-                    stype = 'LO'
+                    storm_type = 'LO'
 
-                #Handle missing data
-                if wmo_vmax < 0: wmo_vmax = np.nan
-                if wmo_mslp < 800: wmo_mslp = np.nan
+                # Handle missing data
+                if wmo_vmax < 0:
+                    wmo_vmax = np.nan
+                if wmo_mslp < 800:
+                    wmo_mslp = np.nan
 
-                self.data[ibtracs_id]['date'].append(date)
+                self.data[ibtracs_id]['time'].append(time)
                 self.data[ibtracs_id]['special'].append(special)
 
-                self.data[ibtracs_id]['type'].append(stype)
+                self.data[ibtracs_id]['type'].append(storm_type)
                 self.data[ibtracs_id]['lat'].append(wmo_lat)
                 self.data[ibtracs_id]['lon'].append(wmo_lon)
                 self.data[ibtracs_id]['vmax'].append(jtwc_vmax)
                 self.data[ibtracs_id]['vmax_orig'].append(wmo_vmax)
                 self.data[ibtracs_id]['mslp'].append(wmo_mslp)
 
-                hhmm = date.strftime('%H%M')
+                hhmm = time.strftime('%H%M')
                 if hhmm in constants.STANDARD_HOURS:
                     self.data[ibtracs_id]['extra_obs'].append(0)
                 else:
                     self.data[ibtracs_id]['extra_obs'].append(1)
 
-                #Calculate ACE & append to storm total
-                if np.isnan(jtwc_vmax) == False:
-                    ace = (10**-4) * (jtwc_vmax**2)
-                    if hhmm in constants.STANDARD_HOURS and stype in constants.NAMED_TROPICAL_STORM_TYPES and np.isnan(ace) == False:
-                        self.data[ibtracs_id]['ace'] += np.round(ace,4)
-                
-            #Handle non-WMO mode
+                # Calculate ACE & append to storm total
+                if not np.isnan(jtwc_vmax):
+                    ace = accumulated_cyclone_energy(jtwc_vmax)
+                    if hhmm in constants.STANDARD_HOURS and storm_type in constants.NAMED_TROPICAL_STORM_TYPES and not np.isnan(ace):
+                        self.data[ibtracs_id]['ace'] += np.round(ace, 4)
+
+            # Handle non-WMO mode
             else:
-                if sid == '': continue
-                sid = map_all_id.get(ibtracs_id)
+                if storm_id not in map_duplicate_id.keys() and storm_id not in do_not_merge:
+                    orig_storm_id = storm_id + ''
+                    storm_id = map_all_id.get(ibtracs_id, None)
+                    if storm_id == '' or storm_id is None:
+                        continue
 
-                #Retrieve data
-                date = dt.strptime(time,'%Y-%m-%d%H:%M:00')
+                # Retrieve data
                 dist_land = int(dist_land)
 
-                #Properly format WMO variables
+                # Properly format WMO variables
                 wmo_lat = float(wmo_lat)
                 wmo_lon = float(wmo_lon)
                 wmo_vmax = np.nan if wmo_vmax == "" else int(wmo_vmax)
                 wmo_mslp = np.nan if wmo_mslp == "" else int(wmo_mslp)
 
-                #Properly format hurdat-style variables
+                # Properly format hurdat-style variables
                 lat = float(lat)
                 lon = float(lon)
                 vmax = np.nan if vmax == "" else int(vmax)
                 mslp = np.nan if mslp == "" else int(mslp)
-                if np.isnan(vmax) == False:
-                    if str(vmax)[-1] in ['4','9']: vmax += 1
-                    if str(vmax)[-1] in ['1','6']: vmax = vmax - 1
-
-                #Convert storm type from ibtracs to hurdat style
-                if stype == "ST" or stype == "TY":
-                    stype = "HU"
-                elif stype == "":
+                if not np.isnan(vmax):
+                    if str(vmax)[-1] in ['4', '9']:
+                        vmax += 1
+                    if str(vmax)[-1] in ['1', '6']:
+                        vmax = vmax - 1
+
+                # Avoid duplicate entries
+                if time in self.data[storm_id]['time']:
+                    continue
+
+                # Edit basin
+                basin_reverse = {v: k for k, v in basin_convert.items()}
+                wmo_basin = basin_reverse.get(basin, '')
+                if subbasin in ['WA', 'EA']:
+                    wmo_basin = 'australia'
+                if storm_id == 'AL041932':
+                    wmo_basin = 'north_atlantic'
+                self.data[storm_id]['wmo_basin'].append(wmo_basin)
+
+                # Convert storm type from ibtracs to hurdat style
+                if storm_type == "":
                     if wmo_type == 'TS':
                         if vmax < 34:
-                            stype = 'TD'
+                            storm_type = 'TD'
                         elif vmax < 64:
-                            stype = 'TS'
+                            storm_type = 'TS'
+                        elif wmo_basin in constants.NHC_BASINS:
+                            storm_type = 'HU'
+                        elif vmax < 130:
+                            storm_type = 'TY'
                         else:
-                            stype = 'HU'
+                            storm_type = 'ST'
                     elif wmo_type == 'SS':
                         if vmax < 34:
-                            stype = 'SD'
+                            storm_type = 'SD'
                         else:
-                            stype = 'SS'
-                    elif wmo_type in ['ET','MX']:
+                            storm_type = 'SS'
+                    elif wmo_type in ['ET', 'MX']:
                         wmo_type = 'EX'
-                    elif stype == 'DS':
-                        stype = 'LO'
+                    elif storm_type == 'DS':
+                        storm_type = 'LO'
                     else:
                         if np.isnan(vmax):
-                            stype = 'LO'
+                            storm_type = 'LO'
                         elif vmax < 34:
-                            stype = 'TD'
+                            storm_type = 'TD'
                         elif vmax < 64:
-                            stype = 'TS'
+                            storm_type = 'TS'
+                        elif wmo_basin in constants.NHC_BASINS:
+                            storm_type = 'HU'
+                        elif vmax < 130:
+                            storm_type = 'TY'
                         else:
-                            stype = 'HU'
+                            storm_type = 'ST'
 
-                #Handle missing data
-                if vmax < 0: vmax = np.nan
-                if mslp < 800: mslp = np.nan
-
-                self.data[sid]['date'].append(date)
-                self.data[sid]['special'].append(special)
-
-                self.data[sid]['wmo_type'].append(wmo_type)
-                self.data[sid]['wmo_lat'].append(wmo_lat)
-                self.data[sid]['wmo_lon'].append(wmo_lon)
-                self.data[sid]['wmo_vmax'].append(wmo_vmax)
-                self.data[sid]['wmo_mslp'].append(wmo_mslp)
-
-                self.data[sid]['type'].append(stype)
-                self.data[sid]['lat'].append(lat)
-                self.data[sid]['lon'].append(lon)
-                self.data[sid]['vmax'].append(vmax)
-                self.data[sid]['mslp'].append(mslp)
+                # Handle missing data
+                if vmax < 0:
+                    vmax = np.nan
+                if mslp < 800:
+                    mslp = np.nan
+
+                self.data[storm_id]['time'].append(time)
+                self.data[storm_id]['special'].append(special)
+
+                self.data[storm_id]['wmo_type'].append(wmo_type)
+                self.data[storm_id]['wmo_lat'].append(wmo_lat)
+                self.data[storm_id]['wmo_lon'].append(wmo_lon)
+                self.data[storm_id]['wmo_vmax'].append(wmo_vmax)
+                self.data[storm_id]['wmo_mslp'].append(wmo_mslp)
+
+                self.data[storm_id]['type'].append(storm_type)
+                self.data[storm_id]['lat'].append(lat)
+                self.data[storm_id]['lon'].append(lon)
+                self.data[storm_id]['vmax'].append(vmax)
+                self.data[storm_id]['mslp'].append(mslp)
 
-                #Edit basin
-                basin_reverse = {v: k for k, v in basin_convert.items()}
-                wmo_basin = basin_reverse.get(basin,'')
-                if subbasin in ['WA','EA']:
-                    wmo_basin = 'australia'
-                if sid == 'AL041932': wmo_basin = 'north_atlantic'
-                self.data[sid]['wmo_basin'].append(wmo_basin)
-
-                hhmm = date.strftime('%H%M')
+                hhmm = time.strftime('%H%M')
                 if hhmm in constants.STANDARD_HOURS:
-                    self.data[sid]['extra_obs'].append(0)
+                    self.data[storm_id]['extra_obs'].append(0)
                 else:
-                    self.data[sid]['extra_obs'].append(1)
+                    self.data[storm_id]['extra_obs'].append(1)
+
+                # Calculate ACE & append to storm total
+                if not np.isnan(vmax):
+                    ace = accumulated_cyclone_energy(vmax)
+                    if hhmm in constants.STANDARD_HOURS and storm_type in constants.NAMED_TROPICAL_STORM_TYPES and not np.isnan(ace):
+                        self.data[storm_id]['ace'] += np.round(ace, 4)
 
-                #Calculate ACE & append to storm total
-                if np.isnan(vmax) == False:
-                    ace = (10**-4) * (vmax**2)
-                    if hhmm in constants.STANDARD_HOURS and stype in constants.NAMED_TROPICAL_STORM_TYPES and np.isnan(ace) == False:
-                        self.data[sid]['ace'] += np.round(ace,4)
-                    
-        #Remove empty entries
+        # Remove empty entries
         all_keys = [k for k in self.data.keys()]
         for key in all_keys:
             if len(self.data[key]['lat']) == 0:
-                del(self.data[key])
-        
-        #Replace neumann entries
+                del (self.data[key])
+
+        # Replace neumann entries
         if self.neumann:
-            
-            #iterate through every neumann entry
+
+            # iterate through every neumann entry
             for key in neumann.keys():
-                
-                #get corresponding JTWC ID
-                jtwc_id = map_id.get(key,'')
-                if jtwc_id == '': continue
-                
-                #plug dict entry
+
+                # get corresponding JTWC ID
+                jtwc_id = map_id.get(key, '')
+                if jtwc_id == '':
+                    continue
+
+                # plug dict entry
                 old_entry = self.data[jtwc_id]
                 self.data[jtwc_id] = neumann[key]
-                
-                #replace id
+
+                # replace id
                 self.data[jtwc_id]['id'] = jtwc_id
-        
-        #Remove entries where requested basin doesn't appear
-        if self.basin not in ['all','both']:
+
+        # Remove entries where requested basin doesn't appear
+        if self.basin not in ['all', 'both']:
             all_keys = [k for k in self.data.keys()]
             for key in all_keys:
                 if self.basin not in self.data[key]['wmo_basin']:
-                    del(self.data[key])
-        
-        #Fix cyclone Catarina, if specified & requested
+                    del (self.data[key])
+
+        # Fix cyclone Catarina, if specified & requested
         all_keys = [k for k in self.data.keys()]
         if '2004086S29318' in all_keys and self.catarina:
             self.data['2004086S29318'] = cyclone_catarina()
         elif 'AL502004' in all_keys and self.catarina:
             self.data['AL502004'] = cyclone_catarina()
-        
-        #Determine time elapsed
+
+        # Sort data temporally
+        for key in self.data.keys():
+            storm_times = self.data[key]['time']
+            for key2 in self.data[key].keys():
+                if isinstance(self.data[key][key2], list):
+                    self.data[key][key2] = [x for _, x in sorted(
+                        zip(storm_times, self.data[key][key2]))]
+
+        # Determine time elapsed
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
+        tsec = str(round(time_elapsed.total_seconds(), 2))
         print(f"--> Completed reading in ibtracs data ({tsec} seconds)")
-    
-    def __interpolate_storms(self,keys):
-        
+
+    def __interpolate_storms(self, keys):
         r"""
         Interpolates storm data temporally to hourly. This is done for every provided key, and is stored in a separate internal dict.
-        
+
         Parameters
         ----------
         keys : list
             List of keys to be interpolated to hourly data.
         """
-        
-        #Check if operation needs to be performed
+
+        # Check if operation needs to be performed
         count = 0
         for key in keys:
-            if key not in self.data_interp.keys(): count += 1
-        if count == 0: return
-        
+            if key not in self.data_interp.keys():
+                count += 1
+        if count == 0:
+            return
+
         start_time = dt.now()
         print("--> Starting to interpolate storms")
-        
+
         for key in keys:
             if key not in self.data_interp.keys():
-                self.data_interp[key] = interp_storm(self.data[key].copy(),hours=1,dt_window=24,dt_align='middle')
-        
-        #Determine time elapsed
+                self.data_interp[key] = interp_storm(
+                    self.data[key].copy(), hours=1, dt_window=24, dt_align='middle')
+
+        # Determine time elapsed
         time_elapsed = dt.now() - start_time
-        tsec = str(round(time_elapsed.total_seconds(),2))
+        tsec = str(round(time_elapsed.total_seconds(), 2))
         print(f"--> Completed interpolating storms ({tsec} seconds)")
 
-    def get_storm_id(self,storm):
-        
+    def get_storm_id(self, storm):
         r"""
         Returns the storm ID (e.g., "AL012019") given the storm name and year.
-        
+
         Parameters
         ----------
         storm : tuple
             Tuple containing the storm name and year (e.g., ("Matthew",2016)).
-            
+
         Returns
         -------
         str or list
             If a single storm was found, returns a string containing its ID. Otherwise returns a list of matching IDs.
         """
-        
-        #Error check
-        if isinstance(storm,tuple) == False:
+
+        # Error check
+        if not isinstance(storm, tuple):
             raise TypeError("storm must be of type tuple.")
         if len(storm) != 2:
-            raise ValueError("storm must contain 2 elements, name (str) and year (int)")
-        name,year = storm
-        
-        #Search for corresponding entry in keys
+            raise ValueError(
+                "storm must contain 2 elements, name (str) and year (int)")
+        name, year = storm
+
+        # Search for corresponding entry in keys
         keys_use = []
         for key in self.keys:
             temp_year = self.data[key]['year']
             if temp_year == year:
                 temp_name = self.data[key]['name']
                 if temp_name == name.upper():
                     keys_use.append(key)
-                
-        #return key, or list of keys
-        if len(keys_use) == 1: keys_use = keys_use[0]
-        if len(keys_use) == 0: raise RuntimeError("Storm not found")
+
+        # return key, or list of keys
+        if len(keys_use) == 1:
+            keys_use = keys_use[0]
+        if len(keys_use) == 0:
+            raise RuntimeError("Storm not found")
         return keys_use
-    
-    def get_storm_tuple(self,storm):
 
+    def get_storm_tuple(self, storm):
         r"""
         Returns the storm tuple (e.g., ("Dorian",2019)) given the storm id.
-        
+
         Parameters
         ----------
         storm : string
             String containing the storm ID (e.g., "AL052019").
-            
+
         Returns
         -------
         tuple
             Returns a list of matching IDs.
         """
 
-        #Error check
-        if isinstance(storm,str) == False:
+        # Error check
+        if not isinstance(storm, str):
             raise TypeError("storm must be of type string.")
         try:
             name = self.data[storm]['name']
             year = self.data[storm]['year']
         except:
             raise RuntimeError("Storm not found")
-        return (name,year)
-    
-    def get_storm(self,storm):
-        
+        return (name, year)
+
+    def get_storm(self, storm):
         r"""
         Retrieves a Storm object for the requested storm.
-        
+
         Parameters
         ----------
         storm : str or tuple
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), or tuple with storm name and year (e.g., ("Matthew",2016)).
-        
+
         Returns
         -------
         tropycal.tracks.Storm
             Object containing information about the requested storm, and methods for analyzing and plotting the storm.
         """
-        
-        #Check if storm is str or tuple
+
+        # Check if storm is str or tuple
         if isinstance(storm, str):
             key = storm
         elif isinstance(storm, tuple):
-            key = self.get_storm_id((storm[0],storm[1]))
+            key = self.get_storm_id((storm[0], storm[1]))
         else:
-            raise RuntimeError("Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
-        
-        #Retrieve key of given storm
+            raise RuntimeError(
+                "Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
+
+        # Retrieve key of given storm
         if isinstance(key, str):
-            
-            #Check to see if tornado data exists for this storm
+
+            # Check to see if tornado data exists for this storm
             if np.max(self.keys_tors) == 1:
                 if key in self.data_tors.keys():
-                    return Storm(self.data[key],{'data':self.data_tors[key],'dist_thresh':self.tornado_dist_thresh})
+                    return Storm(self.data[key], {'data': self.data_tors[key], 'dist_thresh': self.tornado_dist_thresh})
                 else:
                     return Storm(self.data[key])
             else:
                 return Storm(self.data[key])
         else:
             error_message = ''.join([f"\n{i}" for i in key])
             error_message = f"Multiple IDs were identified for the requested storm. Choose one of the following storm IDs and provide it as the 'storm' argument instead of a tuple:{error_message}"
             raise RuntimeError(error_message)
-    
-    
-    def plot_storm(self,storm,domain="dynamic",plot_all_dots=False,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot_storm(self, storm, domain="dynamic", plot_all_dots=False, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of a single storm.
-        
+
         Parameters
         ----------
         storm : str, tuple or dict
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), tuple with storm name and year (e.g., ("Matthew",2016)), or a dict entry.
         domain : str
             Domain for the plot. Default is "dynamic". "dynamic_tropical" is also available. Please refer to :ref:`options-domain` for available domain options.
         plot_all_dots : bool
             Whether to plot dots for all observations along the track. If false, dots will be plotted every 6 hours. Default is false.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Retrieve requested storm
-        if isinstance(storm,dict) == False:
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Retrieve requested storm
+        if not isinstance(storm, dict):
             storm_dict = self.get_storm(storm).dict
         else:
             storm_dict = storm
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         elif max(storm_dict['lon']) > 150 or min(storm_dict['lon']) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storms([storm_dict],domain,plot_all_dots=plot_all_dots,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storms(
+            [storm_dict], domain, plot_all_dots=plot_all_dots, ax=ax, save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-    
-    
-    def plot_storms(self,storms,domain="dynamic",title="TC Track Composite",plot_all_dots=False,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot_storms(self, storms, domain="dynamic", title="TC Track Composite", plot_all_dots=False, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of multiple storms.
-        
+
         Parameters
         ----------
         storms : list
             List of requested storms. List can contain either strings of storm ID (e.g., "AL052019"), tuples with storm name and year (e.g., ("Matthew",2016)), or dict entries.
         domain : str
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         title : str
@@ -1255,1029 +1464,1207 @@
             Whether to plot dots for all observations along the track. If false, dots will be plotted every 6 hours. Default is false.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Create instance of plot object
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Identify plot domain for all requested storms
+
+        # Identify plot domain for all requested storms
         max_lon = -9999
         min_lon = 9999
         storm_dicts = []
         for storm in storms:
-            
-            #Retrieve requested storm
-            if isinstance(storm,dict) == False:
+
+            # Retrieve requested storm
+            if not isinstance(storm, dict):
                 storm_dict = self.get_storm(storm).dict
             else:
                 storm_dict = storm
             storm_dicts.append(storm_dict)
-            
-            #Add to array of max/min lat/lons
-            if max(storm_dict['lon']) > max_lon: max_lon = max(storm_dict['lon'])
-            if min(storm_dict['lon']) < min_lon: min_lon = min(storm_dict['lon'])
-            
-        #Create cartopy projection
+
+            # Add to array of max/min lat/lons
+            if max(storm_dict['lon']) > max_lon:
+                max_lon = max(storm_dict['lon'])
+            if min(storm_dict['lon']) < min_lon:
+                min_lon = min(storm_dict['lon'])
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
-        elif max(storm_dict['lon']) > 150 or min(storm_dict['lon']) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
-        else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storms(storm_dicts,domain,title,plot_all_dots,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+        elif max_lon > 150 or min_lon < -150:
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
+        else:
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storms(
+            storm_dicts, domain, title, plot_all_dots, ax=ax, save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-        
-        
-    def plot_season(self,year,domain=None,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot_season(self, year, domain=None, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of a single season.
-        
+
         Parameters
         ----------
         year : int
             Year to retrieve season data. If in southern hemisphere, year is the 2nd year of the season (e.g., 1975 for 1974-1975).
         domain : str
             Domain for the plot. Default is basin-wide. Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Retrieve season object
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Retrieve season object
         season = self.get_season(year)
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
-        elif season.basin in ['east_pacific','west_pacific','south_pacific','australia','all']:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
-        else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot season
-        plot_ax = self.plot_obj.plot_season(season,domain,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+        elif season.basin in ['east_pacific', 'west_pacific', 'south_pacific', 'australia', 'all']:
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
+        else:
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Plot season
+        plot_ax = self.plot_obj.plot_season(
+            season, domain, ax=ax, save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-    
-    
-    def search_name(self,name):
-        
+
+    def search_name(self, name):
         r"""
         Searches for hurricane seasons containing a storm of the requested name.
-        
+
         Parameters
         ----------
         name : str
             Name to search through the dataset for.
-        
+
         Returns
         -------
         list
             List containing the hurricane seasons where a storm of the requested name was found.
         """
-        
-        #get keys for all storms in requested year
-        years = [self.data[key]['year'] for key in self.keys if self.data[key]['name'] == name.upper()]
-        
+
+        # get keys for all storms in requested year
+        years = [self.data[key]['year']
+                 for key in self.keys if self.data[key]['name'] == name.upper()]
+
         return years
-    
-    
-    def download_tcr(self,storm,save_path=""):
-        
+
+    def download_tcr(self, storm, save_path=""):
         r"""
         Downloads the NHC offical Tropical Cyclone Report (TCR) for the requested storm to the requested directory. Available only for storms with advisories issued by the National Hurricane Center.
-        
+
         Parameters
         ----------
         storm : str, tuple or dict
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), tuple with storm name and year (e.g., ("Matthew",2016)), or a dict entry.
         save_path : str
             Path of directory to download the TCR into. Default is current working directory.
         """
-        
-        #Retrieve requested storm
-        if isinstance(storm,dict) == False:
+
+        # Retrieve requested storm
+        if not isinstance(storm, dict):
             storm_dict = self.get_storm(storm)
         else:
             storm_dict = self.get_storm(storm.id)
-        
-        #Error check
+
+        # Error check
         if self.source != "hurdat":
             msg = "NHC data can only be accessed when HURDAT is used as the data source."
             raise RuntimeError(msg)
         if self.year < 1995:
             msg = "Tropical Cyclone Reports are unavailable prior to 1995."
             raise RuntimeError(msg)
-        if isinstance(save_path,str) == False:
+        if not isinstance(save_path, str):
             msg = "'save_path' must be of type str."
             raise TypeError(msg)
-        
-        #Format URL
+
+        # Format URL
         storm_id = self.dict['id'].upper()
         storm_name = self.dict['name'].title()
         url = f"https://www.nhc.noaa.gov/data/tcr/{storm_id}_{storm_name}.pdf"
-        
-        #Check to make sure PDF is available
+
+        # Check to make sure PDF is available
         request = requests.get(url)
         if request.status_code != 200:
             msg = "This tropical cyclone does not have a Tropical Cyclone Report (TCR) available."
             raise RuntimeError(msg)
-        
-        #Retrieve PDF
+
+        # Retrieve PDF
         response = requests.get(url)
-        full_path = os.path.join(save_path,f"TCR_{storm_id}_{storm_name}.pdf")
+        full_path = os.path.join(save_path, f"TCR_{storm_id}_{storm_name}.pdf")
         with open(full_path, 'wb') as f:
             f.write(response.content)
-    
-    
-    def __retrieve_season(self,year,basin):
-        
-        #Initialize dict to be populated
+
+    def __retrieve_season(self, year, basin):
+
+        # Initialize dict to be populated
         season_dict = {}
-        
-        #Search for corresponding entry in keys
+
+        # Search for corresponding entry in keys
         basin_list = []
         for key in self.keys:
-            
-            #Get year for 'all' (global data), otherwise get season
+
+            # Get year for 'all' (global data), otherwise get season
             if self.basin == 'all' and basin == 'all':
-                temp_year = int(year) if int(year) in [i.year for i in self.data[key]['date']] else 0
+                temp_year = int(year) if int(year) in [
+                    i.year for i in self.data[key]['time']] else 0
             else:
                 temp_year = self.data[key]['season']
-            
-            #Proceed if year/season is a match
+
+            # Proceed if year/season is a match
             if temp_year == int(year):
                 temp_basin = self.data[key]['basin']
                 temp_wmo_basin = self.data[key]['wmo_basin']
                 if temp_basin == 'all':
                     if basin == 'all':
                         season_dict[key] = self.data[key]
                         basin_list.append('all')
                     elif basin in temp_wmo_basin:
                         season_dict[key] = self.data[key]
                         basin_list.append(self.data[key]['wmo_basin'][0])
                 else:
                     season_dict[key] = self.data[key]
                     basin_list.append(self.data[key]['wmo_basin'][0])
-                
-        #Error check
+
+        # Error check
         if len(season_dict) == 0:
-            raise RuntimeError("No storms were identified for the given year in the given basin.")
-        
-        #Add attributes
+            raise RuntimeError(
+                "No storms were identified for the given year in the given basin.")
+
+        # Add attributes
         first_key = [k for k in season_dict.keys()][0]
         season_info = {}
         season_info['year'] = year
         season_info['basin'] = max(set(basin_list), key=basin_list.count)
         season_info['source_basin'] = season_dict[first_key]['basin']
         season_info['source'] = season_dict[first_key]['source']
         season_info['source_info'] = season_dict[first_key]['source_info']
-        
-        #Fix basin
+
+        # Fix basin
         if self.basin == 'all' and basin == 'all':
             season_info['basin'] = 'all'
         if self.basin == 'both':
             season_info['basin'] = 'both'
-        
-        #Return object
-        return Season(season_dict,season_info)
-                   
-    def get_season(self,year,basin='all'):
-        
+
+        # Return object
+        return Season(season_dict, season_info)
+
+    def get_season(self, year, basin='all'):
         r"""
         Retrieves a Season object for the requested season or seasons.
-        
+
         Parameters
         ----------
         year : int or list
             Year(s) to retrieve season data. If in southern hemisphere, year is the 2nd year of the season (e.g., 1975 for 1974-1975). Use of multiple years is only permissible for hurdat sources.
         basin : str, optional
             If using a global ibtracs dataset, this specifies which basin to load in. Otherwise this argument is ignored.
-        
+
         Returns
         -------
         tropycal.tracks.Season
             Object containing every storm entry for the given season, and methods for analyzing and plotting the season.
         """
-        
-        #Error checks
-        if isinstance(year,(int,np.int,np.integer,float,np.floating)) == False and isinstance(year,list) == False:
+
+        # Error checks
+        if not is_number(year) and not isinstance(year, list):
             msg = "'year' must be of type int or list."
             raise TypeError(msg)
-        if isinstance(year,list):
+        if isinstance(year, list):
             for i in year:
-                if isinstance(i,(int,np.int,np.integer,float,np.floating)) == False:
+                if not is_number(i):
                     msg = "Elements of list 'year' must be of type int."
                     raise TypeError(msg)
-        
-        #Retrieve season object(s)
-        if isinstance(year,(int,np.int,np.integer,float,np.floating)):
-            return self.__retrieve_season(year,basin)
+
+        # Retrieve season object(s)
+        if is_number(year):
+            return self.__retrieve_season(year, basin)
         else:
-            return_season = self.__retrieve_season(year[0],basin)
+            return_season = self.__retrieve_season(year[0], basin)
             for i_year in year[1:]:
-                return_season = return_season + self.__retrieve_season(i_year,basin)
+                return_season = return_season + \
+                    self.__retrieve_season(i_year, basin)
             return return_season
-    
-    def ace_climo(self,plot_year=None,compare_years=None,climo_year_range=None,month_range=None,rolling_sum=0,return_dict=False,plot=True,save_path=None):
-        
+
+    def ace_climo(self, plot_year=None, compare_years=None, climo_bounds=None, month_range=None, rolling_sum=0, return_dict=False, save_path=None):
         r"""
         Creates and plots a climatology of accumulated cyclone energy (ACE).
-        
+
         Parameters
         ----------
         plot_year : int
             Year to highlight. If current year, plot will be drawn through today. If none, no year will be highlighted.
         compare_years : int or list
             Seasons to compare against. Can be either a single season (int), or a range or list of seasons (list).
-        climo_year_range : tuple
+        climo_bounds : tuple
             Start and end years to compute the climatology over. Default is from 1950 to last year.
         month_range : tuple
             Start and end months to plot (e.g., ``(5,10)``). Default is peak hurricane season by basin.
         rolling_sum : int
             Days to calculate a rolling sum over. Default is 0 (annual running sum).
         return_dict : bool
-            Determines whether to return data from this function. Default is False.
-        plot : bool
-            Determines whether to generate a plot or not. If False, function simply returns ace dictionary.
+            If False (default), plot axes will be returned. If True, a dictionary containing the raw data is returned.
         save_path : str
             Determines the file path to save the image to. If blank or none, image will be directly shown.
-        
+
         Returns
         -------
         axes or dict
-            By default, the plot axes is returned. If return_dict is True, a dictionary containing the axes and data about the ACE climatology is returned.
+            By default, the plot axes is returned. If return_dict is True, a dictionary containing the raw ACE climatology data is returned.
+
+        Notes
+        -----
+        If in southern hemisphere, year is the 2nd year of the season (e.g., 1975 for 1974-1975).
         """
-        
-        #Retrieve current year
+
+        # Retrieve current year
         cur_year = dt.now().year
-        
-        if climo_year_range is None:
-            climo_year_range = (1950,dt.now().year-1)
-        
-        if self.basin in ['south_indian','australia','south_pacific']:
-            warnings.warn("This function is not currently configured to work in the Southern Hemisphere.")
-        
-        #Create empty dict
+
+        if climo_bounds is None:
+            climo_bounds = (1950, dt.now().year - 1)
+
+        # Create empty dict
         ace = {}
-        
-        #Iterate over every year of HURDAT available
+
+        # Iterate over every year of HURDAT available
         end_year = self.data[self.keys[-1]]['year']
-        years = [yr for yr in range(1851,dt.now().year+1) if (min(climo_year_range)<=yr<=max(climo_year_range)) or yr==plot_year]
+        years = [yr for yr in range(
+            1851, cur_year + 1) if (min(climo_bounds) <= yr <= max(climo_bounds)) or yr == plot_year]
         for year in years:
-            
-            #Get info for this year
-            try:
-                season = self.get_season(year)
-                year_info = season.summary()
-            except:
-                continue
-            
-            #Generate list of dates for this year
-            year_dates = np.array([dt.strptime(((pd.to_datetime(i)).strftime('%Y%m%d%H')),'%Y%m%d%H') for i in np.arange(dt(year,1,1),dt(year+1,1,1),timedelta(hours=6))])
-            
-            #Remove 2/29 from dates
+
+            # Get info for this year
+            if self.basin == 'all':
+                storm_ids = [key for key in self.data.keys() if year in [
+                    i.year for i in self.data[key]['time']]]
+            else:
+                try:
+                    season = self.get_season(year)
+                    storm_ids = season.summary()['id']
+                except:
+                    continue
+
+            # Generate list of dates for this year
+            if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                year_dates = np.array([dt.strptime(((pd.to_datetime(i)).strftime('%Y%m%d%H')), '%Y%m%d%H')
+                                      for i in np.arange(dt(year - 1, 7, 1), dt(year, 7, 1), timedelta(hours=6))])
+            else:
+                year_dates = np.array([dt.strptime(((pd.to_datetime(i)).strftime('%Y%m%d%H')), '%Y%m%d%H')
+                                      for i in np.arange(dt(year, 1, 1), dt(year + 1, 1, 1), timedelta(hours=6))])
+
+            # Remove 2/29 from dates
             if calendar.isleap(year):
-                year_dates = year_dates[year_dates != dt(year,2,29,0)]
-                year_dates = year_dates[year_dates != dt(year,2,29,3)]
-                year_dates = year_dates[year_dates != dt(year,2,29,6)]
-                year_dates = year_dates[year_dates != dt(year,2,29,9)]
-                year_dates = year_dates[year_dates != dt(year,2,29,12)]
-                year_dates = year_dates[year_dates != dt(year,2,29,15)]
-                year_dates = year_dates[year_dates != dt(year,2,29,18)]
-                year_dates = year_dates[year_dates != dt(year,2,29,21)]
-            
-            #Additional empty arrays
-            year_cumace = np.zeros((year_dates.shape))
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 0)]
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 6)]
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 12)]
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 18)]
+
+            # Additional empty arrays
+            year_timestep_ace = np.zeros((year_dates.shape))
             year_genesis = []
-            
-            #Get list of storms for this year
-            storm_ids = year_info['id']
+
+            # Get list of storms for this year
             for storm in storm_ids:
-                
-                #Get HURDAT data for this storm
+
+                # Get HURDAT data for this storm
                 storm_data = self.data[storm]
-                storm_date_y = np.array([int(i.strftime('%Y')) for i in storm_data['date']])
-                storm_date_h = np.array([i.strftime('%H%M') for i in storm_data['date']])
-                storm_date_m = [i.strftime('%m%d') for i in storm_data['date']]
-                storm_date = np.array(storm_data['date'])
+                storm_date_y = np.array([int(i.strftime('%Y'))
+                                        for i in storm_data['time']])
+                storm_date_h = np.array([i.strftime('%H%M')
+                                        for i in storm_data['time']])
+                storm_date_m = [i.strftime('%m%d') for i in storm_data['time']]
+                storm_date = np.array(storm_data['time'])
                 storm_type = np.array(storm_data['type'])
                 storm_vmax = np.array(storm_data['vmax'])
-                
-                #Subset to remove obs not useful for ace
-                idx1 = ((storm_type == 'SS') | (storm_type == 'TS') | (storm_type == 'HU'))
+                storm_basin = np.array(storm_data['wmo_basin'])
+
+                # Subset to remove obs not useful for ace
+                idx1 = ((storm_type == 'SS') | (storm_type == 'TS') | (
+                    storm_type == 'HU') | (storm_type == 'TY') | (storm_type == 'ST'))
                 idx2 = ~np.isnan(storm_vmax)
-                idx3 = ((storm_date_h == '0000') | (storm_date_h == '0600') | (storm_date_h == '1200') | (storm_date_h == '1800'))
+                idx3 = ((storm_date_h == '0000') | (storm_date_h == '0600') | (
+                    storm_date_h == '1200') | (storm_date_h == '1800'))
                 idx4 = storm_date_y == year
+                if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                    idx4[idx4 == False] = True
+                if self.basin not in ['all', 'both']:
+                    idx4 = (idx4) & (storm_basin == self.basin)
                 storm_date = storm_date[(idx1) & (idx2) & (idx3) & (idx4)]
                 storm_type = storm_type[(idx1) & (idx2) & (idx3) & (idx4)]
                 storm_vmax = storm_vmax[(idx1) & (idx2) & (idx3) & (idx4)]
-                if len(storm_vmax) == 0: continue #Continue if doesn't apply to this storm
-                storm_ace = (10**-4) * (storm_vmax**2)
-                
-                #Account for storms on february 29th by pushing them forward 1 day
+                if len(storm_vmax) == 0:
+                    continue  # Continue if doesn't apply to this storm
+                storm_ace = accumulated_cyclone_energy(storm_vmax)
+
+                # Account for storms on february 29th by pushing them forward 1 day
                 if '0229' in storm_date_m:
                     storm_date_temp = []
                     for idate in storm_date:
                         dt_date = pd.to_datetime(idate)
                         if dt_date.strftime('%m%d') == '0229' or dt_date.strftime('%m') == '03':
                             dt_date += timedelta(hours=24)
                         storm_date_temp.append(dt_date)
                     storm_date = storm_date_temp
-                
-                #Append ACE to cumulative sum
+
+                # Append ACE to cumulative sum
                 idx = np.nonzero(np.in1d(year_dates, storm_date))
-                year_cumace[idx] += storm_ace
-                year_genesis.append(np.where(year_dates == storm_date[0])[0][0])
-                
-            #Calculate cumulative sum of year
+                year_timestep_ace[idx] += storm_ace
+                year_genesis.append(
+                    np.where(year_dates == storm_date[0])[0][0])
+
+            # Calculate cumulative sum of year
             if rolling_sum == 0:
-                year_cum = np.cumsum(year_cumace)
+                year_cumulative_ace = np.cumsum(year_timestep_ace)
                 year_genesis = np.array(year_genesis)
-            
-                #Attach to dict
+
+                # Attach to dict
                 ace[str(year)] = {}
-                ace[str(year)]['date'] = year_dates
-                ace[str(year)]['ace'] = year_cum
+                ace[str(year)]['time'] = year_dates
+                ace[str(year)]['ace'] = year_cumulative_ace
                 ace[str(year)]['genesis_index'] = year_genesis
             else:
-                year_cum = np.sum(rolling_window(year_cumace,rolling_sum*4),axis=1)
-                year_genesis = np.array(year_genesis) - ((rolling_sum*4)-1)
-                
-                #Attach to dict
+                year_cumulative_ace = np.sum(rolling_window(
+                    year_timestep_ace, rolling_sum * 4), axis=1)
+                year_genesis = np.array(year_genesis) - ((rolling_sum * 4) - 1)
+
+                # Attach to dict
                 ace[str(year)] = {}
-                ace[str(year)]['date'] = year_dates[((rolling_sum*4)-1):]
-                ace[str(year)]['ace'] = year_cum
+                ace[str(year)]['time'] = year_dates[((rolling_sum * 4) - 1):]
+                ace[str(year)]['ace'] = year_cumulative_ace
                 ace[str(year)]['genesis_index'] = year_genesis
-                 
-        #------------------------------------------------------------------------------------------
-        
-        #Construct non-leap year julian day array
-        julian = np.arange(365*4.0) / 4.0
+
+        # ------------------------------------------------------------------------------------------
+
+        # Construct non-leap year julian day array
+        julian_x = np.arange(365 * 4.0) / 4.0
+        julian = np.arange(365 * 4.0) / 4.0
+        if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+            julian = list(julian[182 * 4:]) + list(julian[:182 * 4])
         if rolling_sum != 0:
-            julian = julian[((rolling_sum*4)-1):]
-          
-        #Get julian days for a non-leap year
+            julian = julian[((rolling_sum * 4) - 1):]
+            julian_x = julian_x[((rolling_sum * 4) - 1):]
+
+        # Get julian days for a non-leap year
         months_julian = months_in_julian(2019)
         julian_start = months_julian['start']
         julian_midpoint = months_julian['midpoint']
         julian_name = months_julian['name']
-        
-        #Construct percentile arrays
-        all_ace = np.ones((len(years),len(julian)))*np.nan
-        for year in range(min(climo_year_range),max(climo_year_range)+1):
+        julian_months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
+        if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+            julian_start = list(
+                np.array(julian_start[6:]) - 181) + list(np.array(julian_start[:6]) + 183)
+            julian_midpoint = list(np.array(
+                julian_midpoint[6:]) - 181) + list(np.array(julian_midpoint[:6]) + 183)
+            julian_name = julian_name[6:] + julian_name[:6]
+            julian_months = [7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6]
+
+        # Construct percentile arrays
+        all_ace = np.ones((len(years), len(julian))) * np.nan
+        for year in range(min(climo_bounds), max(climo_bounds) + 1):
             all_ace[years.index(year)] = ace[str(year)]['ace']
-        pmin,p10,p25,p40,p60,p75,p90,pmax = np.nanpercentile(all_ace,[0,10,25,40,60,75,90,100],axis=0)
-        
-        #Return if not plotting
-        if plot == False:
-            if return_dict:
-                return ace
-            else:
-                return
-        
-        #------------------------------------------------------------------------------------------
-        
-        #Create figure
-        fig,ax=plt.subplots(figsize=(9,7),dpi=200)
-        
-        #Set up x-axis
-        ax.grid(axis='y',linewidth=0.5,color='k',alpha=0.2,zorder=1,linestyle='--')
+        pmin, p10, p25, p40, p60, p75, p90, pmax = np.nanpercentile(
+            all_ace, [0, 10, 25, 40, 60, 75, 90, 100], axis=0)
+
+        # Return if not plotting
+        if return_dict:
+            return ace
+
+        # ------------------------------------------------------------------------------------------
+
+        # Create figure
+        fig, ax = plt.subplots(figsize=(9, 7), dpi=200)
+
+        # Set up x-axis
+        ax.grid(axis='y', linewidth=0.5, color='k',
+                alpha=0.2, zorder=1, linestyle='--')
         ax.set_xticks(julian_midpoint)
         ax.set_xticklabels(julian_name)
-        for i,(istart,iend) in enumerate(zip(julian_start[:-1][::2],julian_start[1:][::2])):
-            ax.axvspan(istart,iend,color='#e4e4e4',alpha=0.5,zorder=0)
-        
-        #Set x-axis bounds
+        for i, (istart, iend) in enumerate(zip(julian_start[:-1][::2], julian_start[1:][::2])):
+            ax.axvspan(istart, iend, color='#e4e4e4', alpha=0.5, zorder=0)
+
+        # Set x-axis bounds
         if month_range is None:
-            ax.set_xlim(julian_start[4],julian[-1])
+            ax.set_xlim(julian_start[4], julian_x[-1])
         else:
-            end_month = month_range[1]-1
-            end_julian = julian[-1] if end_month == 11 else julian_start[end_month]-1
-            ax.set_xlim(julian_start[month_range[0]-1],end_julian)
-
-        #Add plot title
+            start_month = julian_months.index(int(month_range[0]))
+            end_month = julian_months.index(int(month_range[1]))
+            start_julian = julian_x[0] if start_month == 0 else julian_start[start_month]
+            end_julian = julian_x[-1] if end_month == len(
+                julian_months) - 1 else julian_start[end_month + 1]
+            ax.set_xlim(start_julian, end_julian)
+
+        # Add plot title
+        basin_title = self.basin.title().replace(
+            '_', ' ') if self.basin != 'all' else 'Global'
+        plot_year_title = ''
         if plot_year is None:
-            title_string = f"{self.basin.title().replace('_',' ')} Accumulated Cyclone Energy Climatology"
+            title_string = f"{basin_title} Accumulated Cyclone Energy Climatology"
         else:
+            plot_year_title = str(
+                plot_year) if self.basin not in constants.SOUTH_HEMISPHERE_BASINS else f'{plot_year-1}-{plot_year}'
             cur_year = (dt.now()).year
             if plot_year == cur_year:
                 add_current = f"(through {dt.now().strftime('%m/%d')})"
             else:
                 add_current = ""
-            title_string = f"{plot_year} {self.basin.title().replace('_',' ')} Accumulated Cyclone Energy {add_current}"
+            title_string = f"{plot_year_title} {basin_title} Accumulated Cyclone Energy {add_current}"
         if rolling_sum != 0:
             title_add = f"\n{rolling_sum}-Day Running Sum"
         else:
             title_add = ""
-        ax.set_title(f"{title_string}{title_add}",fontsize=12,fontweight='bold',loc='left')
-        
-        #Plot requested year
+        ax.set_title(f"{title_string}{title_add}", fontsize=12,
+                     fontweight='bold', loc='left')
+
+        # Plot requested year
         if plot_year is not None:
-            
-            year_julian = np.copy(julian)
+
+            year_julian_x = np.copy(julian_x)
             year_ace = ace[str(plot_year)]['ace']
             year_genesis = ace[str(plot_year)]['genesis_index']
-            
-            #Check to see if this is current year
+
+            # Check to see if this is current year
             cur_year = (dt.now()).year
             if plot_year == cur_year:
-                cur_julian = int(convert_to_julian( (dt.now()).replace(year=2019,minute=0,second=0) ))*4 - int(rolling_sum*4)
-                year_julian = year_julian[:cur_julian+1]
-                year_ace = year_ace[:cur_julian+1]
-                year_genesis = year_genesis[:cur_julian+1]
-
-            ax.plot(year_julian[-1],year_ace[-1],'o',color='k',ms=8,mec='w',mew=0.8,zorder=8)
-            ax.plot(year_julian,year_ace,'-',color='w',linewidth=2.8,zorder=6)
-            ax.plot(year_julian,year_ace,'-',color='k',linewidth=2.0,zorder=6,label=f'{plot_year} ACE ({np.max(year_ace):.1f})')
-            ax.plot(year_julian[year_genesis],year_ace[year_genesis],'D',color='k',ms=5,mec='w',mew=0.5,zorder=7,label='TC Genesis')
-            
-        #Plot comparison years
+
+                start_time = dt(2019, 1, 1)
+                if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                    start_time = dt(2018, 7, 1)
+                end_time = (dt.now()).replace(year=2019, minute=0, second=0)
+                temp_julian = ((end_time - start_time).days +
+                               (end_time - start_time).seconds / 86400.0) + 1
+                cur_julian = int(temp_julian) * 4 - int(rolling_sum * 4)
+
+                year_julian_x = year_julian_x[:cur_julian + 1]
+                year_ace = year_ace[:cur_julian + 1]
+                year_genesis = year_genesis[year_genesis <= cur_julian]
+
+            ax.plot(year_julian_x[-1], year_ace[-1], 'o',
+                    color='k', ms=8, mec='w', mew=0.8, zorder=8)
+            ax.plot(year_julian_x, year_ace, '-',
+                    color='w', linewidth=2.8, zorder=6)
+            ax.plot(year_julian_x, year_ace, '-', color='k', linewidth=2.0,
+                    zorder=6, label=f'{plot_year_title} ACE ({np.max(year_ace):.1f})')
+            ax.plot(year_julian_x[year_genesis], year_ace[year_genesis], 'D',
+                    color='k', ms=5, mec='w', mew=0.5, zorder=7, label='TC Genesis')
+
+        # Plot comparison years
         if compare_years is not None:
-            
-            if isinstance(compare_years, int): compare_years = [compare_years]
-                
+
+            if isinstance(compare_years, int):
+                compare_years = [compare_years]
+
             for year in compare_years:
-                
-                year_julian = np.copy(julian)
+
+                year_julian_x = np.copy(julian_x)
                 year_ace = ace[str(year)]['ace']
                 year_genesis = ace[str(year)]['genesis_index']
 
-                #Check to see if this is current year
+                # Check to see if this is current year
                 cur_year = (dt.now()).year
                 if year == cur_year:
-                    cur_julian = int(convert_to_julian( (dt.now()).replace(year=2019,minute=0,second=0) ))*4 - int(rolling_sum*4)
-                    year_julian = year_julian[:cur_julian+1]
-                    year_ace = year_ace[:cur_julian+1]
-                    year_genesis = year_genesis[:cur_julian+1]
-                    ax.plot(year_julian[-1],year_ace[-1],'o',color='#333333',alpha=0.3,ms=6,zorder=5)
+                    cur_julian = int(convert_to_julian((dt.now()).replace(
+                        year=2019, minute=0, second=0))) * 4 - int(rolling_sum * 4)
+                    year_julian_x = year_julian_x[:cur_julian + 1]
+                    year_ace = year_ace[:cur_julian + 1]
+                    year_genesis = year_genesis[:cur_julian + 1]
+                    ax.plot(year_julian_x[-1], year_ace[-1], 'o',
+                            color='#333333', alpha=0.3, ms=6, zorder=5)
 
                 if len(compare_years) <= 5:
-                    ax.plot(year_julian,year_ace,'-',color='k',linewidth=1.0,alpha=0.5,zorder=3,label=f'{year} ACE ({np.max(year_ace):.1f})')
-                    ax.plot(year_julian[year_genesis],year_ace[year_genesis],'D',color='#333333',ms=3,alpha=0.3,zorder=4)
-                    ax.text(year_julian[-2],year_ace[-2]+2,str(year),fontsize=7,fontweight='bold',alpha=0.7,ha='right',va='bottom')
+                    label_year = str(
+                        year) if self.basin not in constants.SOUTH_HEMISPHERE_BASINS else f'{year-1}-{year}'
+
+                    ax.plot(year_julian_x, year_ace, '-', color='k', linewidth=1.0,
+                            alpha=0.5, zorder=3, label=f'{year} ACE ({np.max(year_ace):.1f})')
+                    ax.plot(year_julian_x[year_genesis], year_ace[year_genesis],
+                            'D', color='#333333', ms=3, alpha=0.3, zorder=4)
+                    ax.text(year_julian_x[-2], year_ace[-2] + 2, str(year), fontsize=7,
+                            fontweight='bold', alpha=0.7, ha='right', va='bottom')
                 else:
-                    ax.plot(year_julian,year_ace,'-',color='k',linewidth=1.0,alpha=0.15,zorder=3)
-            
-        
-        #Plot all climatological values
+                    ax.plot(year_julian_x, year_ace, '-', color='k',
+                            linewidth=1.0, alpha=0.15, zorder=3)
+
+        # Plot all climatological values
         pmin_masked = np.array(pmin)
-        pmin_masked = np.ma.masked_where(pmin_masked==0,pmin_masked)
-        ax.plot(julian,pmax,'--',color='r',zorder=2,label=f'Max ({np.max(pmax):.1f})')
-        ax.plot(julian,pmin_masked,'--',color='b',zorder=2,label=f'Min ({np.max(pmin):.1f})')
-        ax.fill_between(julian,p10,p90,color='#60CE56',alpha=0.3,zorder=2,label='Climo 10-90%')
-        ax.fill_between(julian,p25,p75,color='#16A147',alpha=0.3,zorder=2,label='Climo 25-75%')
-        ax.fill_between(julian,p40,p60,color='#00782A',alpha=0.3,zorder=2,label='Climo 40-60%')
+        pmin_masked = np.ma.masked_where(pmin_masked == 0, pmin_masked)
+        ax.plot(julian_x, pmax, '--', color='r', zorder=2,
+                label=f'Max ({np.max(pmax):.1f})')
+        ax.plot(julian_x, pmin_masked, '--', color='b',
+                zorder=2, label=f'Min ({np.max(pmin):.1f})')
+        ax.fill_between(julian_x, p10, p90, color='#60CE56',
+                        alpha=0.3, zorder=2, label='Climo 10-90%')
+        ax.fill_between(julian_x, p25, p75, color='#16A147',
+                        alpha=0.3, zorder=2, label='Climo 25-75%')
+        ax.fill_between(julian_x, p40, p60, color='#00782A',
+                        alpha=0.3, zorder=2, label='Climo 40-60%')
 
-        #Add legend & plot credit
+        # Add legend & plot credit
         ax.legend(loc=2)
         endash = u"\u2013"
-        
+
         credit_text = plot_credit()
-        add_credit(ax,credit_text)
-        ax.text(0.99,0.99,f'Climatology from {climo_year_range[0]}{endash}{climo_year_range[-1]}',fontsize=9,color='k',alpha=0.7,
-                transform=ax.transAxes,ha='right',va='top',zorder=10)
-        
-        #Show/save plot and close
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight',facecolor='w')
-        
-        if return_dict:
-            return {'ax':ax,'ace':ace}
-        else:
-            return ax
+        add_credit(ax, credit_text)
+        ax.text(0.99, 0.99, f'Climatology from {climo_bounds[0]}{endash}{climo_bounds[-1]}', fontsize=9, color='k', alpha=0.7,
+                transform=ax.transAxes, ha='right', va='top', zorder=10)
+
+        # Show/save plot and close
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight', facecolor='w')
 
-    def hurricane_days_climo(self,plot_year=None,compare_years=None,start_year=1950,rolling_sum=0,category=None,return_dict=False,plot=True,save_path=None):
-        
+        return ax
+
+    def hurricane_days_climo(self, plot_year=None, compare_years=None, climo_bounds=None, month_range=None, rolling_sum=0, category=None, return_dict=False, save_path=None):
         r"""
         Creates a climatology of tropical storm/hurricane/major hurricane days.
-        
+
         Parameters
         ----------
         plot_year : int
-            Year to highlight. If current year, plot will be drawn through today.
+            Year to highlight. If current year, plot will be drawn through today. If none, no year will be highlighted.
         compare_years : int or list
             Seasons to compare against. Can be either a single season (int), or a range or list of seasons (list).
-        start_year : int
-            Year to begin calculating the climatology over. Default is 1950.
+        climo_bounds : tuple
+            Start and end years to compute the climatology over. Default is from 1950 to last year.
+        month_range : tuple
+            Start and end months to plot (e.g., ``(5,10)``). Default is peak hurricane season by basin.
         rolling_sum : int
             Days to calculate a rolling sum over. Default is 0 (annual running sum).
         category : int
-            SSHWS Category to generate the data and plot for. Use 0 for tropical storm. If none (default), a plot will be generated for all categories.
+            SSHWS Category to generate the data and plot for. Use 0 for tropical storm. If None (default), a plot will be generated for all categories.
         return_dict : bool
-            Determines whether to return data from this function. Default is False.
-        plot : bool
-            Determines whether to generate a plot or not. If False, function simply returns data dictionary.
+            If False (default), plot axes will be returned. If True, a dictionary containing the raw data is returned.
         save_path : str
             Determines the file path to save the image to. If blank or none, image will be directly shown.
-        
+
         Returns
         -------
         axes or dict
-            By default, the axes is returned. If return_dict is True, a dictionary containing the axes and data about the climatology is returned.
+            By default, the plot axes is returned. If return_dict is True, a dictionary containing the raw data is returned.
+
+        Notes
+        -----
+        If in southern hemisphere, year is the 2nd year of the season (e.g., 1975 for 1974-1975).
         """
-        
-        #Create empty dict
+
+        # Retrieve current year
+        cur_year = dt.now().year
+
+        if climo_bounds is None:
+            climo_bounds = (1950, dt.now().year - 1)
+
+        # Create empty dict
         tc_days = {}
-        
-        #Function for counting TC days above a wind threshold
-        def duration_thres(arr,thres):
+
+        # Function for counting TC days above a wind threshold
+        def duration_thres(arr, thres):
             arr2 = np.zeros((arr.shape))
-            arr2[arr>=thres] = (6.0/24.0)
+            arr2[arr >= thres] = (6.0 / 24.0)
             return arr2
-        
-        #Iterate over every year of HURDAT available
+
+        # Iterate over every year of HURDAT available
         end_year = self.data[self.keys[-1]]['year']
-        years = range(start_year,end_year+1)
+        years = [yr for yr in range(
+            1851, cur_year + 1) if (min(climo_bounds) <= yr <= max(climo_bounds)) or yr == plot_year]
         for year in years:
-            
-            #Get info for this year
-            season = self.get_season(year)
-            year_info = season.summary()
-            
-            #Generate list of dates for this year
-            year_dates = np.array([dt.strptime(((pd.to_datetime(i)).strftime('%Y%m%d%H')),'%Y%m%d%H') for i in np.arange(dt(year,1,1),dt(year+1,1,1),timedelta(hours=6))])
-            
-            #Remove 2/29 from dates
+
+            # Get info for this year
+            if self.basin == 'all':
+                storm_ids = [key for key in self.data.keys() if year in [
+                    i.year for i in self.data[key]['time']]]
+            else:
+                try:
+                    season = self.get_season(year)
+                    storm_ids = season.summary()['id']
+                except:
+                    continue
+
+            # Generate list of dates for this year
+            if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                year_dates = np.array([dt.strptime(((pd.to_datetime(i)).strftime('%Y%m%d%H')), '%Y%m%d%H')
+                                      for i in np.arange(dt(year - 1, 7, 1), dt(year, 7, 1), timedelta(hours=6))])
+            else:
+                year_dates = np.array([dt.strptime(((pd.to_datetime(i)).strftime('%Y%m%d%H')), '%Y%m%d%H')
+                                      for i in np.arange(dt(year, 1, 1), dt(year + 1, 1, 1), timedelta(hours=6))])
+
+            # Remove 2/29 from dates
             if calendar.isleap(year):
-                year_dates = year_dates[year_dates != dt(year,2,29,0)]
-                year_dates = year_dates[year_dates != dt(year,2,29,6)]
-                year_dates = year_dates[year_dates != dt(year,2,29,12)]
-                year_dates = year_dates[year_dates != dt(year,2,29,18)]
-            
-            #Additional empty arrays
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 0)]
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 6)]
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 12)]
+                year_dates = year_dates[year_dates != dt(year, 2, 29, 18)]
+
+            # Additional empty arrays
             temp_arr = np.zeros((year_dates.shape))
             cumulative = {}
-            all_thres = ['ts','c1','c2','c3','c4','c5']
+            all_thres = ['ts', 'c1', 'c2', 'c3', 'c4', 'c5']
             for thres in all_thres:
                 cumulative[thres] = np.copy(temp_arr)
             year_genesis = []
-            
-            #Get list of storms for this year
-            storm_ids = year_info['id']
+
+            # Get list of storms for this year
             for storm in storm_ids:
-                
-                #Get HURDAT data for this storm
+
+                # Get HURDAT data for this storm
                 storm_data = self.data[storm]
-                storm_date_y = np.array([int(i.strftime('%Y')) for i in storm_data['date']])
-                storm_date_h = np.array([i.strftime('%H%M') for i in storm_data['date']])
-                storm_date = np.array(storm_data['date'])
+                storm_date_y = np.array([int(i.strftime('%Y'))
+                                        for i in storm_data['time']])
+                storm_date_h = np.array([i.strftime('%H%M')
+                                        for i in storm_data['time']])
+                storm_date_m = [i.strftime('%m%d') for i in storm_data['time']]
+                storm_date = np.array(storm_data['time'])
                 storm_type = np.array(storm_data['type'])
                 storm_vmax = np.array(storm_data['vmax'])
-                
-                #Subset to remove obs not useful for calculation
-                idx1 = ((storm_type == 'SS') | (storm_type == 'TS') | (storm_type == 'HU'))
+
+                # Subset to remove obs not useful for calculation
+                idx1 = ((storm_type == 'SS') | (storm_type == 'TS') | (
+                    storm_type == 'HU') | (storm_type == 'TY') | (storm_type == 'ST'))
                 idx2 = ~np.isnan(storm_vmax)
-                idx3 = ((storm_date_h == '0000') | (storm_date_h == '0600') | (storm_date_h == '1200') | (storm_date_h == '1800'))
+                idx3 = ((storm_date_h == '0000') | (storm_date_h == '0600') | (
+                    storm_date_h == '1200') | (storm_date_h == '1800'))
                 idx4 = storm_date_y == year
+                if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                    idx4[idx4 == False] = True
                 storm_date = storm_date[(idx1) & (idx2) & (idx3) & (idx4)]
                 storm_type = storm_type[(idx1) & (idx2) & (idx3) & (idx4)]
                 storm_vmax = storm_vmax[(idx1) & (idx2) & (idx3) & (idx4)]
-                if len(storm_vmax) == 0: continue #Continue if doesn't apply to this storm
-                
-                #Append storm days to cumulative sum
+                if len(storm_vmax) == 0:
+                    continue  # Continue if doesn't apply to this storm
+
+                # Account for storms on february 29th by pushing them forward 1 day
+                if '0229' in storm_date_m:
+                    storm_date_temp = []
+                    for idate in storm_date:
+                        dt_date = pd.to_datetime(idate)
+                        if dt_date.strftime('%m%d') == '0229' or dt_date.strftime('%m') == '03':
+                            dt_date += timedelta(hours=24)
+                        storm_date_temp.append(dt_date)
+                    storm_date = storm_date_temp
+
+                # Append storm days to cumulative sum
                 idx = np.nonzero(np.in1d(year_dates, storm_date))
-                cumulative['ts'][idx] += duration_thres(storm_vmax,34.0)
-                cumulative['c1'][idx] += duration_thres(storm_vmax,64.0)
-                cumulative['c2'][idx] += duration_thres(storm_vmax,83.0)
-                cumulative['c3'][idx] += duration_thres(storm_vmax,96.0)
-                cumulative['c4'][idx] += duration_thres(storm_vmax,113.0)
-                cumulative['c5'][idx] += duration_thres(storm_vmax,137.0)
-                year_genesis.append(np.where(year_dates == storm_date[0])[0][0])
-                
-            #Calculate cumulative sum of year
+                cumulative['ts'][idx] += duration_thres(storm_vmax, 34.0)
+                cumulative['c1'][idx] += duration_thres(storm_vmax, 64.0)
+                cumulative['c2'][idx] += duration_thres(storm_vmax, 83.0)
+                cumulative['c3'][idx] += duration_thres(storm_vmax, 96.0)
+                cumulative['c4'][idx] += duration_thres(storm_vmax, 113.0)
+                cumulative['c5'][idx] += duration_thres(storm_vmax, 137.0)
+                year_genesis.append(
+                    np.where(year_dates == storm_date[0])[0][0])
+
+            # Calculate cumulative sum of year
             if rolling_sum == 0:
                 year_genesis = np.array(year_genesis)
-            
-                #Attach to dict
+
+                # Attach to dict
                 tc_days[str(year)] = {}
-                tc_days[str(year)]['date'] = year_dates
+                tc_days[str(year)]['time'] = year_dates
                 tc_days[str(year)]['genesis_index'] = year_genesis
-                
-                #Loop through all thresholds
+
+                # Loop through all thresholds
                 for thres in all_thres:
                     tc_days[str(year)][thres] = np.cumsum(cumulative[thres])
             else:
-                year_genesis = np.array(year_genesis) - ((rolling_sum*4)-1)
-                
-                #Attach to dict
+                year_genesis = np.array(year_genesis) - ((rolling_sum * 4) - 1)
+
+                # Attach to dict
                 tc_days[str(year)] = {}
-                tc_days[str(year)]['date'] = year_dates[((rolling_sum*4)-1):]
+                tc_days[str(year)]['time'] = year_dates[(
+                    (rolling_sum * 4) - 1):]
                 tc_days[str(year)]['genesis_index'] = year_genesis
-                
-                #Loop through all thresholds
+
+                # Loop through all thresholds
                 for thres in all_thres:
-                    tc_days[str(year)][thres] = np.sum(rolling_window(cumulative[thres],rolling_sum*4),axis=1)
-         
-        #------------------------------------------------------------------------------------------
-        
-        #Construct non-leap year julian day array
-        julian = np.arange(365*4.0) / 4.0
+                    tc_days[str(year)][thres] = np.sum(rolling_window(
+                        cumulative[thres], rolling_sum * 4), axis=1)
+
+        # ------------------------------------------------------------------------------------------
+
+        # Construct non-leap year julian day array
+        julian_x = np.arange(365 * 4.0) / 4.0
+        julian = np.arange(365 * 4.0) / 4.0
+        if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+            julian = list(julian[182 * 4:]) + list(julian[:182 * 4])
         if rolling_sum != 0:
-            julian = julian[((rolling_sum*4)-1):]
-          
-        #Get julian days for a non-leap year
+            julian = julian[((rolling_sum * 4) - 1):]
+            julian_x = julian_x[((rolling_sum * 4) - 1):]
+
+        # Get julian days for a non-leap year
         months_julian = months_in_julian(2019)
         julian_start = months_julian['start']
         julian_midpoint = months_julian['midpoint']
         julian_name = months_julian['name']
-        
-        #Determine type of plot to make
-        category_match = {0:'ts',1:'c1',2:'c2',3:'c3',4:'c4',5:'c5'}
+        julian_months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
+        if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+            julian_start = list(
+                np.array(julian_start[6:]) - 181) + list(np.array(julian_start[:6]) + 183)
+            julian_midpoint = list(np.array(
+                julian_midpoint[6:]) - 181) + list(np.array(julian_midpoint[:6]) + 183)
+            julian_name = julian_name[6:] + julian_name[:6]
+            julian_months = [7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6]
+
+        # Determine type of plot to make
+        category_match = {0: 'ts', 1: 'c1', 2: 'c2', 3: 'c3', 4: 'c4', 5: 'c5'}
         if category is None:
             cat = 0
         else:
-            cat = category_match.get(category,'c1')
-        
-        #Construct percentile arrays
+            cat = category_match.get(category, 'c1')
+
+        # Construct percentile arrays
         if cat == 0:
             p50 = {}
             for thres in all_thres:
-                all_tc_days = np.zeros((len(years),len(julian)))
+                all_tc_days = np.zeros((len(years), len(julian)))
                 for year in years:
                     all_tc_days[years.index(year)] = tc_days[str(year)][thres]
-                p50[thres] = np.percentile(all_tc_days,50,axis=0)
-                p50[thres] = np.average(all_tc_days,axis=0)
+                p50[thres] = np.percentile(all_tc_days, 50, axis=0)
+                p50[thres] = np.average(all_tc_days, axis=0)
         else:
-            all_tc_days = np.zeros((len(years),len(julian)))
+            all_tc_days = np.zeros((len(years), len(julian)))
             for year in years:
                 all_tc_days[years.index(year)] = tc_days[str(year)][cat]
-            pmin,p10,p25,p40,p60,p75,p90,pmax = np.percentile(all_tc_days,[0,10,25,40,60,75,90,100],axis=0)
-        
-        #Return if not plotting
-        if plot == False:
-            if return_dict:
-                return tc_days
-            else:
-                return
-        
-        #------------------------------------------------------------------------------------------
-        
-        #Create figure
-        fig,ax=plt.subplots(figsize=(9,7),dpi=200)
-        
-        #Set up x-axis
-        ax.grid(axis='y',linewidth=0.5,color='k',alpha=0.2,zorder=1,linestyle='--')
+            pmin, p10, p25, p40, p60, p75, p90, pmax = np.percentile(
+                all_tc_days, [0, 10, 25, 40, 60, 75, 90, 100], axis=0)
+
+        # Return if not plotting
+        if return_dict:
+            return tc_days
+
+        # ------------------------------------------------------------------------------------------
+
+        # Create figure
+        fig, ax = plt.subplots(figsize=(9, 7), dpi=200)
+
+        # Set up x-axis
+        ax.grid(axis='y', linewidth=0.5, color='k',
+                alpha=0.2, zorder=1, linestyle='--')
         ax.set_xticks(julian_midpoint)
         ax.set_xticklabels(julian_name)
-        for i,(istart,iend) in enumerate(zip(julian_start[:-1][::2],julian_start[1:][::2])):
-            ax.axvspan(istart,iend,color='#e4e4e4',alpha=0.5,zorder=0)
-        
-        #Limit plot from May onward
-        ax.set_xlim(julian_start[4],julian[-1])
-        
-        #Format plot title by category
-        category_names = {'ts':'Tropical Storm','c1':'Category 1','c2':'Category 2','c3':'Category 3','c4':'Category 4','c5':'Category 5'}
+        for i, (istart, iend) in enumerate(zip(julian_start[:-1][::2], julian_start[1:][::2])):
+            ax.axvspan(istart, iend, color='#e4e4e4', alpha=0.5, zorder=0)
+
+        # Set x-axis bounds
+        if month_range is None:
+            ax.set_xlim(julian_start[4], julian_x[-1])
+        else:
+            start_month = julian_months.index(int(month_range[0]))
+            end_month = julian_months.index(int(month_range[1]))
+            start_julian = julian_x[0] if start_month == 0 else julian_start[start_month]
+            end_julian = julian_x[-1] if end_month == len(
+                julian_months) - 1 else julian_start[end_month + 1]
+            ax.set_xlim(start_julian, end_julian)
+
+        # Format plot title by category
+        category_names = {'ts': 'Tropical Storm', 'c1': 'Category 1',
+                          'c2': 'Category 2', 'c3': 'Category 3', 'c4': 'Category 4', 'c5': 'Category 5'}
         if cat == 0:
             add_str = "Tropical Cyclone"
         else:
             add_str = category_names.get(cat)
-        
-        #Add plot title
+
+        # Add plot title
+        basin_title = self.basin.title().replace(
+            '_', ' ') if self.basin != 'all' else 'Global'
+        plot_year_title = ''
         if plot_year is None:
-            title_string = f"{self.basin.title().replace('_',' ')} Accumulated {add_str} Days"
+            title_string = f"{basin_title} Accumulated {add_str} Days"
         else:
+            plot_year_title = str(
+                plot_year) if self.basin not in constants.SOUTH_HEMISPHERE_BASINS else f'{plot_year-1}-{plot_year}'
             cur_year = (dt.now()).year
             if plot_year == cur_year:
                 add_current = f" (through {(dt.now()).strftime('%b %d')})"
             else:
                 add_current = ""
-            title_string = f"{plot_year} {self.basin.title().replace('_',' ')} Accumulated {add_str} Days{add_current}"
+            title_string = f"{plot_year_title} {basin_title} Accumulated {add_str} Days{add_current}"
         if rolling_sum != 0:
             title_add = f"\n{rolling_sum}-Day Running Sum"
         else:
             title_add = ""
-        ax.set_title(f"{title_string}{title_add}",fontsize=12,fontweight='bold',loc='left')
-        
-        #Plot requested year
+        ax.set_title(f"{title_string}{title_add}", fontsize=12,
+                     fontweight='bold', loc='left')
+
+        # Plot requested year
         if plot_year is not None:
-            
+
             if cat == 0:
                 year_labels = []
                 for icat in all_thres[::-1]:
-                    year_julian = np.copy(julian)
+                    year_julian_x = np.copy(julian_x)
                     year_tc_days = tc_days[str(plot_year)][icat]
 
-                    #Check to see if this is current year
+                    # Check to see if this is current year
                     cur_year = (dt.now()).year
                     if plot_year == cur_year:
-                        cur_julian = int(convert_to_julian( (dt.now()).replace(year=2019,minute=0,second=0) ))*4 - int(rolling_sum*4)
-                        year_julian = year_julian[:cur_julian+1]
-                        year_tc_days = year_tc_days[:cur_julian+1]
-                        ax.plot(year_julian[-1],year_tc_days[-1],'o',color=get_colors_sshws(icat),ms=8,mec='k',mew=0.8,zorder=8)
+
+                        start_time = dt(2019, 1, 1)
+                        if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                            start_time = dt(2018, 7, 1)
+                        end_time = (dt.now()).replace(
+                            year=2019, minute=0, second=0)
+                        temp_julian = ((end_time - start_time).days +
+                                       (end_time - start_time).seconds / 86400.0) + 1
+                        cur_julian = int(temp_julian) * 4 - \
+                            int(rolling_sum * 4)
+
+                        year_julian_x = year_julian_x[:cur_julian + 1]
+                        year_tc_days = year_tc_days[:cur_julian + 1]
+                        ax.plot(year_julian_x[-1], year_tc_days[-1], 'o', color=get_colors_sshws(
+                            icat), ms=8, mec='k', mew=0.8, zorder=8)
 
                     year_tc_days_masked = np.array(year_tc_days)
-                    year_tc_days_masked = np.ma.masked_where(year_tc_days_masked==0,year_tc_days_masked)
-                    ax.plot(year_julian,year_tc_days_masked,'-',color='k',linewidth=2.8,zorder=6)
-                    ax.plot(year_julian,year_tc_days_masked,'-',color=get_colors_sshws(icat),linewidth=2.0,zorder=6)
+                    year_tc_days_masked = np.ma.masked_where(
+                        year_tc_days_masked == 0, year_tc_days_masked)
+                    ax.plot(year_julian_x, year_tc_days_masked,
+                            '-', color='k', linewidth=2.8, zorder=6)
+                    ax.plot(year_julian_x, year_tc_days_masked, '-',
+                            color=get_colors_sshws(icat), linewidth=2.0, zorder=6)
                     year_labels.append(f"{np.max(year_tc_days):.1f}")
-                    
+
             else:
-                year_julian = np.copy(julian)
+                year_julian_x = np.copy(julian_x)
                 year_tc_days = tc_days[str(plot_year)][cat]
                 year_genesis = tc_days[str(plot_year)]['genesis_index']
 
-                #Check to see if this is current year
+                # Check to see if this is current year
                 cur_year = (dt.now()).year
                 if plot_year == cur_year:
-                    cur_julian = int(convert_to_julian( (dt.now()).replace(year=2019,minute=0,second=0) ))*4 - int(rolling_sum*4)
-                    year_julian = year_julian[:cur_julian+1]
-                    year_tc_days = year_tc_days[:cur_julian+1]
-                    year_genesis = year_genesis[:cur_julian+1]
-                    ax.plot(year_julian[-1],year_tc_days[-1],'o',color='#FF7CFF',ms=8,mec='#750775',mew=0.8,zorder=8)
-
-                ax.plot(year_julian,year_tc_days,'-',color='#750775',linewidth=2.8,zorder=6)
-                ax.plot(year_julian,year_tc_days,'-',color='#FF7CFF',linewidth=2.0,zorder=6,label=f'{plot_year} ({np.max(year_tc_days):.1f} days)')
-                ax.plot(year_julian[year_genesis],year_tc_days[year_genesis],'D',color='#FF7CFF',ms=5,mec='#750775',mew=0.5,zorder=7,label='TC Genesis')
-            
-        #Plot comparison years
+
+                    start_time = dt(2019, 1, 1)
+                    if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+                        start_time = dt(2018, 7, 1)
+                    end_time = (dt.now()).replace(
+                        year=2019, minute=0, second=0)
+                    temp_julian = ((end_time - start_time).days +
+                                   (end_time - start_time).seconds / 86400.0) + 1
+                    cur_julian = int(temp_julian) * 4 - int(rolling_sum * 4)
+
+                    year_julian_x = year_julian_x[:cur_julian + 1]
+                    year_tc_days = year_tc_days[:cur_julian + 1]
+                    year_genesis = year_genesis[year_genesis <= cur_julian]
+                    ax.plot(year_julian_x[-1], year_tc_days[-1], 'o',
+                            color='#FF7CFF', ms=8, mec='#750775', mew=0.8, zorder=8)
+
+                ax.plot(year_julian_x, year_tc_days, '-',
+                        color='#750775', linewidth=2.8, zorder=6)
+                ax.plot(year_julian_x, year_tc_days, '-', color='#FF7CFF', linewidth=2.0,
+                        zorder=6, label=f'{plot_year} ({np.max(year_tc_days):.1f} days)')
+                ax.plot(year_julian_x[year_genesis], year_tc_days[year_genesis], 'D',
+                        color='#FF7CFF', ms=5, mec='#750775', mew=0.5, zorder=7, label='TC Genesis')
+
+        # Plot comparison years
         if compare_years is not None and cat != 0:
-            
-            if isinstance(compare_years, int): compare_years = [compare_years]
-                
+
+            if isinstance(compare_years, int):
+                compare_years = [compare_years]
+
             for year in compare_years:
-                
-                year_julian = np.copy(julian)
+
+                year_julian_x = np.copy(julian_x)
                 year_tc_days = tc_days[str(year)][cat]
                 year_genesis = tc_days[str(year)]['genesis_index']
 
-                #Check to see if this is current year
+                # Check to see if this is current year
                 cur_year = (dt.now()).year
                 if year == cur_year:
-                    cur_julian = int(convert_to_julian( (dt.now()).replace(year=2019,minute=0,second=0) ))*4 - int(rolling_sum*4)
-                    year_julian = year_julian[:cur_julian+1]
-                    year_tc_days = year_tc_days[:cur_julian+1]
-                    year_genesis = year_genesis[:cur_julian+1]
-                    ax.plot(year_julian[-1],year_tc_days[-1],'o',color='#333333',alpha=0.3,ms=6,zorder=5)
+                    cur_julian = int(convert_to_julian((dt.now()).replace(
+                        year=2019, minute=0, second=0))) * 4 - int(rolling_sum * 4)
+                    year_julian_x = year_julian_x[:cur_julian + 1]
+                    year_tc_days = year_tc_days[:cur_julian + 1]
+                    year_genesis = year_genesis[:cur_julian + 1]
+                    ax.plot(year_julian_x[-1], year_tc_days[-1], 'o',
+                            color='#333333', alpha=0.3, ms=6, zorder=5)
 
                 if len(compare_years) <= 5:
-                    ax.plot(year_julian,year_tc_days,'-',color='k',linewidth=1.0,alpha=0.5,zorder=3,label=f'{year} ({np.max(year_tc_days):.1f} days)')
-                    ax.plot(year_julian[year_genesis],year_tc_days[year_genesis],'D',color='#333333',ms=3,alpha=0.3,zorder=4)
-                    ax.text(year_julian[-2],year_tc_days[-2]+2,str(year),fontsize=7,fontweight='bold',alpha=0.7,ha='right',va='bottom')
+                    ax.plot(year_julian_x, year_tc_days, '-', color='k', linewidth=1.0,
+                            alpha=0.5, zorder=3, label=f'{year} ({np.max(year_tc_days):.1f} days)')
+                    ax.plot(year_julian_x[year_genesis], year_tc_days[year_genesis],
+                            'D', color='#333333', ms=3, alpha=0.3, zorder=4)
+                    ax.text(year_julian_x[-2], year_tc_days[-2] + 2, str(
+                        year), fontsize=7, fontweight='bold', alpha=0.7, ha='right', va='bottom')
                 else:
-                    ax.plot(year_julian,year_tc_days,'-',color='k',linewidth=1.0,alpha=0.15,zorder=3)
-            
-        
-        #Plot all climatological values
+                    ax.plot(year_julian_x, year_tc_days, '-', color='k',
+                            linewidth=1.0, alpha=0.15, zorder=3)
+
+        # Plot all climatological values
         if cat == 0:
             if plot_year is None:
                 add_str = ["" for i in all_thres]
             else:
                 add_str = [f" | {plot_year}: {i}" for i in year_labels[::-1]]
             xnums = np.zeros((p50['ts'].shape))
-            ax.fill_between(julian,p50['c1'],p50['ts'],color=get_colors_sshws(34),alpha=0.3,zorder=2,label=f'TS (Avg: {np.max(p50["ts"]):.1f}{add_str[0]})')
-            ax.fill_between(julian,p50['c2'],p50['c1'],color=get_colors_sshws(64),alpha=0.3,zorder=2,label=f'C1 (Avg: {np.max(p50["c1"]):.1f}{add_str[1]})')
-            ax.fill_between(julian,p50['c3'],p50['c2'],color=get_colors_sshws(83),alpha=0.3,zorder=2,label=f'C2 (Avg: {np.max(p50["c2"]):.1f}{add_str[2]})')
-            ax.fill_between(julian,p50['c4'],p50['c3'],color=get_colors_sshws(96),alpha=0.3,zorder=2,label=f'C3 (Avg: {np.max(p50["c3"]):.1f}{add_str[3]})')
-            ax.fill_between(julian,p50['c5'],p50['c4'],color=get_colors_sshws(113),alpha=0.3,zorder=2,label=f'C4 (Avg: {np.max(p50["c4"]):.1f}{add_str[4]})')
-            ax.fill_between(julian,xnums,p50['c5'],color=get_colors_sshws(137),alpha=0.3,zorder=2,label=f'C5 (Avg: {np.max(p50["c5"]):.1f}{add_str[5]})')
+            ax.fill_between(julian_x, p50['c1'], p50['ts'], color=get_colors_sshws(
+                34), alpha=0.3, zorder=2, label=f'TS (Avg: {np.max(p50["ts"]):.1f}{add_str[0]})')
+            ax.fill_between(julian_x, p50['c2'], p50['c1'], color=get_colors_sshws(
+                64), alpha=0.3, zorder=2, label=f'C1 (Avg: {np.max(p50["c1"]):.1f}{add_str[1]})')
+            ax.fill_between(julian_x, p50['c3'], p50['c2'], color=get_colors_sshws(
+                83), alpha=0.3, zorder=2, label=f'C2 (Avg: {np.max(p50["c2"]):.1f}{add_str[2]})')
+            ax.fill_between(julian_x, p50['c4'], p50['c3'], color=get_colors_sshws(
+                96), alpha=0.3, zorder=2, label=f'C3 (Avg: {np.max(p50["c3"]):.1f}{add_str[3]})')
+            ax.fill_between(julian_x, p50['c5'], p50['c4'], color=get_colors_sshws(
+                113), alpha=0.3, zorder=2, label=f'C4 (Avg: {np.max(p50["c4"]):.1f}{add_str[4]})')
+            ax.fill_between(julian_x, xnums, p50['c5'], color=get_colors_sshws(
+                137), alpha=0.3, zorder=2, label=f'C5 (Avg: {np.max(p50["c5"]):.1f}{add_str[5]})')
         else:
             pmin_masked = np.array(pmin)
-            pmin_masked = np.ma.masked_where(pmin_masked==0,pmin_masked)
-            ax.plot(julian,pmax,'--',color='r',zorder=2,label=f'Max ({np.max(pmax):.1f} days)')
-            ax.plot(julian,pmin_masked,'--',color='b',zorder=2,label=f'Min ({np.max(pmin):.1f} days)')
-            ax.fill_between(julian,p10,p90,color='#60CE56',alpha=0.3,zorder=2,label='Climo 10-90%')
-            ax.fill_between(julian,p25,p75,color='#16A147',alpha=0.3,zorder=2,label='Climo 25-75%')
-            ax.fill_between(julian,p40,p60,color='#00782A',alpha=0.3,zorder=2,label='Climo 40-60%')
+            pmin_masked = np.ma.masked_where(pmin_masked == 0, pmin_masked)
+            ax.plot(julian_x, pmax, '--', color='r', zorder=2,
+                    label=f'Max ({np.max(pmax):.1f} days)')
+            ax.plot(julian_x, pmin_masked, '--', color='b',
+                    zorder=2, label=f'Min ({np.max(pmin):.1f} days)')
+            ax.fill_between(julian_x, p10, p90, color='#60CE56',
+                            alpha=0.3, zorder=2, label='Climo 10-90%')
+            ax.fill_between(julian_x, p25, p75, color='#16A147',
+                            alpha=0.3, zorder=2, label='Climo 25-75%')
+            ax.fill_between(julian_x, p40, p60, color='#00782A',
+                            alpha=0.3, zorder=2, label='Climo 40-60%')
 
-        #Add legend & plot credit
+        # Add legend & plot credit
         ax.legend(loc=2)
         endash = u"\u2013"
-        ax.text(0.99,0.01,plot_credit(),fontsize=6,color='k',alpha=0.7,
-                transform=ax.transAxes,ha='right',va='bottom',zorder=10)
-        ax.text(0.99,0.99,f'Climatology from {start_year}{endash}{end_year}',fontsize=8,color='k',alpha=0.7,
-                transform=ax.transAxes,ha='right',va='top',zorder=10)
-        
-        #Show/save plot and close
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        if return_dict:
-            return {'ax':ax,'data':tc_days}
-        else:
-            return ax
-    
-    def wind_pres_relationship(self,storm=None,year_range=None,return_dict=False,plot=True,save_path=None):
-        
+        ax.text(0.99, 0.01, plot_credit(), fontsize=6, color='k', alpha=0.7,
+                transform=ax.transAxes, ha='right', va='bottom', zorder=10)
+        ax.text(0.99, 0.99, f'Climatology from {climo_bounds[0]}{endash}{climo_bounds[-1]}', fontsize=8, color='k', alpha=0.7,
+                transform=ax.transAxes, ha='right', va='top', zorder=10)
+
+        # Show/save plot and close
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        return ax
+
+    def wind_pres_relationship(self, storm=None, climo_bounds=None, return_dict=False, save_path=None):
         r"""
         Creates a climatology of maximum sustained wind speed vs minimum MSLP relationships.
-        
+
         Parameters
         ----------
         storm : str or tuple
             Storm to plot. Can be either string of storm ID (e.g., "AL052019"), or tuple with storm name and year (e.g., ("Matthew",2016)).
-        year_range : list or tuple
-            List or tuple representing the start and end years (e.g., (1950,2018)). Default is the start and end of dataset.
+        climo_bounds : list or tuple
+            List or tuple representing the start and end years (e.g., ``(1950,2018)``). Default is the start and end of dataset.
         return_dict : bool
-            Determines whether to return data from this function. Default is False.
-        plot : bool
-            Determines whether to generate a plot or not. If False, function simply returns ace dictionary.
+            If False (default), plot axes will be returned. If True, a dictionary containing the raw data is returned.
         save_path : str
             Determines the file path to save the image to. If blank or none, image will be directly shown.
-        
+
         Returns
         -------
         ax or dict
             By default, the plot axes is returned. If return_dict is True, a dictionary containing data about the wind vs. MSLP relationship climatology is returned.
+
+        Notes
+        -----
+        Climatology is only valid for time steps during which cyclones were tropical or subtropical.
         """
-        
-        #Define empty dictionary
+
+        # Define empty dictionary
         relationship = {}
-        
-        #Determine year range of dataset
-        if year_range is None:
+
+        # Determine year range of dataset
+        if climo_bounds is None:
             start_year = self.data[self.keys[0]]['year']
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
-            if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
-            start_year = int(year_range[0])
-            if start_year < self.data[self.keys[0]]['year']: start_year = self.data[self.keys[0]]['year']
-            end_year = int(year_range[1])
-            if end_year > self.data[self.keys[-1]]['year']: end_year = self.data[self.keys[-1]]['year']
+        elif isinstance(climo_bounds, (list, tuple)):
+            if len(climo_bounds) != 2:
+                raise ValueError(
+                    "climo_bounds must be a tuple or list with 2 elements: (start_year, end_year)")
+            start_year = int(climo_bounds[0])
+            if start_year < self.data[self.keys[0]]['year']:
+                start_year = self.data[self.keys[0]]['year']
+            end_year = int(climo_bounds[1])
+            if end_year > self.data[self.keys[-1]]['year']:
+                end_year = self.data[self.keys[-1]]['year']
         else:
-            raise TypeError("year_range must be of type tuple or list")
-        
-        #Get velocity & pressure pairs for all storms in dataset
-        vp = filter_storms_vp(self,year_min=start_year,year_max=end_year)
+            raise TypeError("climo_bounds must be of type tuple or list")
+
+        # Get velocity & pressure pairs for all storms in dataset
+        vp = filter_storms_vp(self, year_min=start_year, year_max=end_year)
         relationship['vp'] = vp
 
-        #Create 2D histogram of v+p relationship
-        counts,yedges,xedges = np.histogram2d(*zip(*vp),[np.arange(800,1050,5)-2.5,np.arange(0,250,5)-2.5])
+        # Create 2D histogram of v+p relationship
+        counts, yedges, xedges = np.histogram2d(
+            *zip(*vp), [np.arange(800, 1050, 5) - 2.5, np.arange(0, 250, 5) - 2.5])
         relationship['counts'] = counts
         relationship['yedges'] = yedges
         relationship['xedges'] = xedges
-        
-        #Return if plot is not requested
-        if plot == False:
-            if return_dict:
-                return relationship
-            else:
-                return
-        
-        #Create figure
-        fig,ax = plt.subplots(figsize=(12,9.5),dpi = 200)
 
-        #Plot climatology
-        CS = plt.pcolor(xedges,yedges,counts**0.3,vmin=0,vmax=np.amax(counts)**.3,cmap='gnuplot2_r')
-        plt.plot(xedges,[testfit(vp,x,2) for x in xedges],'k--',linewidth=2)
-        
-        #Plot storm, if specified
+        if return_dict:
+            return relationship
+
+        # Create figure
+        fig, ax = plt.subplots(figsize=(12, 9.5), dpi=200)
+
+        # Plot climatology
+        CS = plt.pcolor(xedges, yedges, counts**0.3, vmin=0,
+                        vmax=np.amax(counts)**.3, cmap='gnuplot2_r')
+        plt.plot(xedges, [testfit(vp, x, 2)
+                 for x in xedges], 'k--', linewidth=2)
+
+        # Plot storm, if specified
         if storm is not None:
-            
-            #Check if storm is str or tuple
+
+            # Check if storm is str or tuple
             if isinstance(storm, str):
                 pass
             elif isinstance(storm, tuple):
-                storm = self.get_storm_id((storm[0],storm[1]))
+                storm = self.get_storm_id((storm[0], storm[1]))
             else:
-                raise RuntimeError("Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
-                
-            #Plot storm
+                raise RuntimeError(
+                    "Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
+
+            # Plot storm
             storm_data = self.data[storm]
             V = np.array(storm_data['vmax'])
             P = np.array(storm_data['mslp'])
             T = np.array(storm_data['type'])
 
             def get_color(itype):
                 if itype in constants.TROPICAL_STORM_TYPES:
-                    return ['#00EE00','palegreen'] #lime
+                    return ['#00EE00', 'palegreen']  # lime
                 else:
-                    return ['#00A600','#3BD73B']
-                
+                    return ['#00A600', '#3BD73B']
+
             def getMarker(itype):
                 mtype = '^'
                 if itype in constants.SUBTROPICAL_ONLY_STORM_TYPES:
                     mtype = 's'
                 elif itype in constants.TROPICAL_ONLY_STORM_TYPES:
                     mtype = 'o'
                 return mtype
-            
+
             xt_label = False
             tr_label = False
-            for i,(iv,ip,it) in enumerate(zip(V[:-1],P[:-1],T[:-1])):
+            for i, (iv, ip, it) in enumerate(zip(V[:-1], P[:-1], T[:-1])):
                 check = False
-                if it in constants.TROPICAL_STORM_TYPES and tr_label == True: check = True
-                if not it in constants.TROPICAL_STORM_TYPES and xt_label == True: check = True
+                if it in constants.TROPICAL_STORM_TYPES and tr_label:
+                    check = True
+                if not it in constants.TROPICAL_STORM_TYPES and xt_label:
+                    check = True
                 if check:
-                    plt.scatter(iv, ip, marker='o',s=80,color=get_color(it)[0],edgecolor='k',zorder=9)
+                    plt.scatter(iv, ip, marker='o', s=80, color=get_color(
+                        it)[0], edgecolor='k', zorder=9)
                 else:
-                    if it in constants.TROPICAL_STORM_TYPES and tr_label == False:
+                    if it in constants.TROPICAL_STORM_TYPES and not tr_label:
                         tr_label = True
                         label_content = f"{storm_data['name'].title()} {storm_data['year']} (Tropical)"
-                    if it not in constants.TROPICAL_STORM_TYPES and xt_label == False:
+                    if it not in constants.TROPICAL_STORM_TYPES and not xt_label:
                         xt_label = True
                         label_content = f"{storm_data['name'].title()} {storm_data['year']} (Non-Tropical)"
-                    plt.scatter(iv, ip, marker='o',s=80,color=get_color(it)[0],edgecolor='k',label=label_content,zorder=9)
-            
-            plt.scatter(V[-1], P[-1], marker='D',s=80,color=get_color(it)[0],edgecolor='k',linewidth=2,zorder=9)
-            
-            for i,(iv,ip,it,mv,mp,mt) in enumerate(zip(V[1:],P[1:],T[1:],V[:-1],P[:-1],T[:-1])):
-                plt.quiver(mv, mp, iv-mv, ip-mp, scale_units='xy', angles='xy',
-                           scale=1, width=0.005, color=get_color(it)[1],zorder=8)
-            
-            #Add legend
-            plt.legend(loc='upper right',scatterpoints=1,prop={'weight':'bold','size':14})
-            
-        
-        #Additional plot settings
-        plt.xlabel('Maximum sustained winds (kt)',fontsize=14)
-        plt.ylabel('Minimum central pressure (hPa)',fontsize=14)
-        plt.title(f"TC Pressure vs. Wind \n {self.basin.title().replace('_',' ')} | "+\
-                  f"{start_year}-{end_year}",fontsize=18,fontweight='bold')
-        plt.xticks(np.arange(20,200,20))
-        plt.yticks(np.arange(880,1040,20))
+                    plt.scatter(iv, ip, marker='o', s=80, color=get_color(
+                        it)[0], edgecolor='k', label=label_content, zorder=9)
+
+            plt.scatter(V[-1], P[-1], marker='D', s=80, color=get_color(it)
+                        [0], edgecolor='k', linewidth=2, zorder=9)
+
+            for i, (iv, ip, it, mv, mp, mt) in enumerate(zip(V[1:], P[1:], T[1:], V[:-1], P[:-1], T[:-1])):
+                plt.quiver(mv, mp, iv - mv, ip - mp, scale_units='xy', angles='xy',
+                           scale=1, width=0.005, color=get_color(it)[1], zorder=8)
+
+            # Add legend
+            plt.legend(loc='upper right', scatterpoints=1,
+                       prop={'weight': 'bold', 'size': 14})
+
+        # Additional plot settings
+        plt.xlabel('Maximum sustained winds (kt)', fontsize=14)
+        plt.ylabel('Minimum central pressure (hPa)', fontsize=14)
+        plt.title(f"TC Pressure vs. Wind \n {self.basin.title().replace('_',' ')} | " +
+                  f"{start_year}-{end_year}", fontsize=18, fontweight='bold')
+        plt.xticks(np.arange(20, 200, 20))
+        plt.yticks(np.arange(880, 1040, 20))
         plt.tick_params(labelsize=14)
         plt.grid()
-        plt.axis([0,200,860,1040])
-        cbar=fig.colorbar(CS)
-        cbar.ax.set_ylabel('Historical Frequency',fontsize=14)
+        plt.axis([0, 200, 860, 1040])
+        cbar = fig.colorbar(CS)
+        cbar.ax.set_ylabel('Historical Frequency', fontsize=14)
         cbar.ax.tick_params(labelsize=14)
-        cbar.set_ticks(np.array([i for i in [0,5,50,200,500,1000,2000] if i<np.amax(counts)])**0.3, update_ticks=True)
-        cbar.set_ticklabels([i for i in [0,5,50,200,500,1000,2000] if i<np.amax(counts)], update_ticks=True)
+        cbar.set_ticks(np.array(
+            [i for i in [0, 5, 50, 200, 500, 1000, 2000] if i < np.amax(counts)])**0.3)
+        cbar.set_ticklabels(
+            [i for i in [0, 5, 50, 200, 500, 1000, 2000] if i < np.amax(counts)])
+
+        # add credit
+        credit_text = Plot().plot_credit()
+        plt.text(0.99, 0.01, credit_text, fontsize=9, color='k', alpha=0.7, backgroundcolor='w',
+                 transform=plt.gca().transAxes, ha='right', va='bottom', zorder=10)
+
+        # Show/save plot and close
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
 
-        #add credit
-        credit_text = Plot().plot_credit()        
-        plt.text(0.99,0.01,credit_text,fontsize=9,color='k',alpha=0.7,backgroundcolor='w',\
-                transform=plt.gca().transAxes,ha='right',va='bottom',zorder=10)        
-        
-        #Show/save plot and close
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        if return_dict:
-            return {'ax':ax,'data':relationship}
-        else:
-            return ax
-    
-    def rank_storm(self,metric,return_df=True,ascending=False,domain=None,year_range=None,date_range=None,subtropical=True):
-        
+        return ax
+
+    def rank_storm(self, metric, return_df=True, ascending=False, domain=None, year_range=None, date_range=None, subtropical=True):
         r"""
         Ranks storm by a specified metric.
-        
+
         Parameters
         ----------
         metric : str
             Metric to rank storms by. Can be any of the following:
-            
+
             * **ace** = rank storms by ACE
             * **start_lat** = starting latitude of cyclone
             * **start_lon** = starting longitude of cyclone
             * **end_lat** = ending latitude of cyclone
             * **end_lon** = ending longitude of cyclone
             * **start_date** = formation date of cyclone
             * **start_date_indomain** = first time step a cyclone entered the domain
@@ -2292,517 +2679,586 @@
             Geographic domain. Default is entire basin. Please refer to :ref:`options-domain` for available domain options.
         year_range : list or tuple
             List or tuple representing the start and end years (e.g., (1950,2018)). Default is start and end years of dataset.
         date_range : list or tuple
             List or tuple representing the start and end dates in 'month/day' format (e.g., (6/1,8/15)). Default is entire year.
         subtropical : bool
             Whether to include subtropical storms in the ranking. Default is True.
-        
+
         Returns
         -------
         pandas.DataFrame
             Returns a pandas DataFrame containing ranked storms. If pandas is not installed, a dict will be returned instead.
         """
-        
+
         if self.source == 'ibtracs':
-            warnings.warn("This function is not currently configured to work for the ibtracs dataset.")
-        
-        #Revise metric if threshold included
+            warnings.warn(
+                "This function is not currently configured to work for the ibtracs dataset.")
+
+        # Revise metric if threshold included
         if 'wind_ge' in metric:
             thresh = int(metric.split("_")[2])
             metric = 'wind_ge'
-        
-        #Error check for metric
+
+        # Error check for metric
         metric = metric.lower()
-        metric_bank = {'ace':{'output':['ace'],'subset_type':'domain'},
-                       'start_lat':{'output':['lat','lon','type'],'subset_type':'start'},
-                       'start_lon':{'output':['lon','lat','type'],'subset_type':'start'},
-                       'end_lat':{'output':['lat','lon','type'],'subset_type':'end'},
-                       'end_lon':{'output':['lon','lat','type'],'subset_type':'end'},
-                       'start_date':{'output':['date','lat','lon','type'],'subset_type':'start'},
-                       'start_date_indomain':{'output':['date','lat','lon','type'],'subset_type':'domain'},
-                       'max_wind':{'output':['vmax','mslp','lat','lon'],'subset_type':'domain'},
-                       'min_mslp':{'output':['mslp','vmax','lat','lon'],'subset_type':'domain'},
-                       'wind_ge':{'output':['lat','lon','mslp','vmax','date'],'subset_type':'start'},
-                      }
+        metric_bank = {'ace': {'output': ['ace'], 'subset_type': 'domain'},
+                       'start_lat': {'output': ['lat', 'lon', 'type'], 'subset_type': 'start'},
+                       'start_lon': {'output': ['lon', 'lat', 'type'], 'subset_type': 'start'},
+                       'end_lat': {'output': ['lat', 'lon', 'type'], 'subset_type': 'end'},
+                       'end_lon': {'output': ['lon', 'lat', 'type'], 'subset_type': 'end'},
+                       'start_date': {'output': ['time', 'lat', 'lon', 'type'], 'subset_type': 'start'},
+                       'start_date_indomain': {'output': ['time', 'lat', 'lon', 'type'], 'subset_type': 'domain'},
+                       'max_wind': {'output': ['vmax', 'mslp', 'lat', 'lon'], 'subset_type': 'domain'},
+                       'min_mslp': {'output': ['mslp', 'vmax', 'lat', 'lon'], 'subset_type': 'domain'},
+                       'wind_ge': {'output': ['lat', 'lon', 'mslp', 'vmax', 'time'], 'subset_type': 'start'},
+                       }
         if metric not in metric_bank.keys():
-            raise ValueError("Metric requested for sorting is not available. Please reference the documentation for acceptable entries for 'metric'.")
-        
-        #Determine year range of dataset
+            raise ValueError(
+                "Metric requested for sorting is not available. Please reference the documentation for acceptable entries for 'metric'.")
+
+        # Determine year range of dataset
         if year_range is None:
             start_year = self.data[self.keys[0]]['year']
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
+        elif isinstance(year_range, (list, tuple)):
             if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
+                raise ValueError(
+                    "year_range must be a tuple or list with 2 elements: (start_year, end_year)")
             start_year = int(year_range[0])
             end_year = int(year_range[1])
         else:
             raise TypeError("year_range must be of type tuple or list")
-            
-        #Initialize empty dict
+
+        # Initialize empty dict
         analyze_list = metric_bank[metric]['output']
-        analyze_list.insert(1,'id'); analyze_list.insert(2,'name'); analyze_list.insert(3,'year');
-        analyze_dict = {key:[] for key in analyze_list}
-            
-        #Iterate over every storm in dataset
+        analyze_list.insert(1, 'id')
+        analyze_list.insert(2, 'name')
+        analyze_list.insert(3, 'year')
+        analyze_dict = {key: [] for key in analyze_list}
+
+        # Iterate over every storm in dataset
         for storm in self.keys:
-            
-            #Get entry for this storm
+
+            # Get entry for this storm
             storm_data = self.data[storm]
-            
-            #Filter by year
-            if storm_data['year'] < start_year or storm_data['year'] > end_year: continue
-            
-            #Filter for purely tropical/subtropical storm locations
+
+            # Filter by year
+            if storm_data['year'] < start_year or storm_data['year'] > end_year:
+                continue
+
+            # Filter for purely tropical/subtropical storm locations
             type_array = np.array(storm_data['type'])
             if subtropical:
-                idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+                idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+                    type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
             else:
-                idx = np.where((type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
-            
-            if len(idx[0]) == 0: continue
+                idx = np.where((type_array == 'TD') | (type_array == 'TS') | (
+                    type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
+
+            if len(idx[0]) == 0:
+                continue
             lat_tropical = np.array(storm_data['lat'])[idx]
             lon_tropical = np.array(storm_data['lon'])[idx]
-            date_tropical = np.array(storm_data['date'])[idx]
+            time_tropical = np.array(storm_data['time'])[idx]
             type_tropical = np.array(storm_data['type'])[idx]
             vmax_tropical = np.array(storm_data['vmax'])[idx]
             mslp_tropical = np.array(storm_data['mslp'])[idx]
             basin_tropical = np.array(storm_data['wmo_basin'])[idx]
-            
-            #Filter geographically
+
+            # Filter geographically
             if domain is not None:
-                if isinstance(domain,dict):
+                if isinstance(domain, dict):
                     keys = domain.keys()
                     check = [False, False, False, False]
                     for key in keys:
-                        if key[0].lower() == 'n': check[0] = True; bound_n = domain[key]
-                        if key[0].lower() == 's': check[1] = True; bound_s = domain[key]
-                        if key[0].lower() == 'e': check[2] = True; bound_e = domain[key]
-                        if key[0].lower() == 'w': check[3] = True; bound_w = domain[key]
+                        if key[0].lower() == 'n':
+                            check[0] = True
+                            bound_n = domain[key]
+                        if key[0].lower() == 's':
+                            check[1] = True
+                            bound_s = domain[key]
+                        if key[0].lower() == 'e':
+                            check[2] = True
+                            bound_e = domain[key]
+                        if key[0].lower() == 'w':
+                            check[3] = True
+                            bound_w = domain[key]
                     if False in check:
                         msg = "Custom domains must be of type dict with arguments for 'n', 's', 'e' and 'w'."
                         raise ValueError(msg)
-                    idx = np.where((lat_tropical >= bound_s) & (lat_tropical <= bound_n) & (lon_tropical >= bound_w) & (lon_tropical <= bound_e))
-                elif isinstance(domain,str):
-                    idx = np.where(basin_tropical==domain)
+                    idx = np.where((lat_tropical >= bound_s) & (lat_tropical <= bound_n) & (
+                        lon_tropical >= bound_w) & (lon_tropical <= bound_e))
+                elif isinstance(domain, str):
+                    idx = np.where(basin_tropical == domain)
                 else:
                     msg = "domain must be of type str or dict."
                     raise TypeError(msg)
-                if len(idx[0]) == 0: continue
-                
-                #Check for subset type
+                if len(idx[0]) == 0:
+                    continue
+
+                # Check for subset type
                 subset_type = metric_bank[metric]['subset_type']
                 if subset_type == 'domain':
                     lat_tropical = lat_tropical[idx]
                     lon_tropical = lon_tropical[idx]
-                    date_tropical = date_tropical[idx]
+                    time_tropical = time_tropical[idx]
                     type_tropical = type_tropical[idx]
                     vmax_tropical = vmax_tropical[idx]
                     mslp_tropical = mslp_tropical[idx]
                     basin_tropical = basin_tropical[idx]
-            
-            #Filter by time
+
+            # Filter by time
             if date_range is not None:
-                start_time = dt.strptime(f"{storm_data['year']}/{date_range[0]}",'%Y/%m/%d')
-                end_time = dt.strptime(f"{storm_data['year']}/{date_range[1]}",'%Y/%m/%d')
-                idx = np.array([i for i in range(len(lat_tropical)) if date_tropical[i] >= start_time and date_tropical[i] <= end_time])
-                if len(idx) == 0: continue
-                
-                #Check for subset type
+                start_time = dt.strptime(
+                    f"{storm_data['year']}/{date_range[0]}", '%Y/%m/%d')
+                end_time = dt.strptime(
+                    f"{storm_data['year']}/{date_range[1]}", '%Y/%m/%d') + timedelta(hours=23)
+                idx = np.array([i for i in range(len(
+                    lat_tropical)) if time_tropical[i] >= start_time and time_tropical[i] <= end_time])
+                if len(idx) == 0:
+                    continue
+
+                # Check for subset type
                 subset_type = metric_bank[metric]['subset_type']
                 if subset_type == 'domain':
                     lat_tropical = lat_tropical[idx]
                     lon_tropical = lon_tropical[idx]
-                    date_tropical = date_tropical[idx]
+                    time_tropical = time_tropical[idx]
                     type_tropical = type_tropical[idx]
                     vmax_tropical = vmax_tropical[idx]
                     mslp_tropical = mslp_tropical[idx]
                     basin_tropical = basin_tropical[idx]
-            
-            #Filter by requested metric
+
+            # Filter by requested metric
             if metric == 'ace':
-                
-                if storm_data['ace'] == 0: continue
-                analyze_dict['ace'].append(np.round(storm_data['ace'],4))
-                
-            elif metric in ['start_lat','end_lat','start_lon','end_lon']:
-                
+
+                if storm_data['ace'] == 0:
+                    continue
+                storm_ace = 0.0
+                for i, (i_time, i_vmax) in enumerate(zip(time_tropical, vmax_tropical)):
+                    if i_time.strftime('%H%M') not in constants.STANDARD_HOURS:
+                        continue
+                    storm_ace += accumulated_cyclone_energy(i_vmax)
+                analyze_dict['ace'].append(round(storm_ace, 4))
+
+            elif metric in ['start_lat', 'end_lat', 'start_lon', 'end_lon']:
+
                 use_idx = 0 if 'start' in metric else -1
                 analyze_dict['lat'].append(lat_tropical[use_idx])
                 analyze_dict['lon'].append(lon_tropical[use_idx])
                 analyze_dict['type'].append(type_tropical[use_idx])
-                
+
             elif metric in ['start_date']:
-                
+
                 analyze_dict['lat'].append(lat_tropical[0])
                 analyze_dict['lon'].append(lon_tropical[0])
                 analyze_dict['type'].append(type_tropical[0])
-                analyze_dict['date'].append(date_tropical[0].replace(year=2016))
-                
-            elif metric in ['max_wind','min_mslp']:
-                
-                #Find max wind or min MSLP
-                if metric == 'max_wind' and all_nan(vmax_tropical) == True: continue
-                if metric == 'min_mslp' and all_nan(mslp_tropical) == True: continue
-                use_idx = np.where(vmax_tropical==np.nanmax(vmax_tropical))[0][0]
-                if metric == 'min_mslp': use_idx = np.where(mslp_tropical==np.nanmin(mslp_tropical))[0][0]
-                
+                analyze_dict['time'].append(
+                    time_tropical[0].replace(year=2016))
+
+            elif metric in ['max_wind', 'min_mslp']:
+
+                # Find max wind or min MSLP
+                if metric == 'max_wind' and all_nan(vmax_tropical):
+                    continue
+                if metric == 'min_mslp' and all_nan(mslp_tropical):
+                    continue
+                use_idx = np.where(
+                    vmax_tropical == np.nanmax(vmax_tropical))[0][0]
+                if metric == 'min_mslp':
+                    use_idx = np.where(
+                        mslp_tropical == np.nanmin(mslp_tropical))[0][0]
+
                 analyze_dict['lat'].append(lat_tropical[use_idx])
                 analyze_dict['lon'].append(lon_tropical[use_idx])
                 analyze_dict['mslp'].append(mslp_tropical[use_idx])
                 analyze_dict['vmax'].append(vmax_tropical[use_idx])
-            
+
             elif metric in ['wind_ge']:
-                
-                #Find max wind or min MSLP
-                if metric == 'wind_ge' and all_nan(vmax_tropical) == True: continue
-                if metric == 'wind_ge' and np.nanmax(vmax_tropical) < thresh: continue
-                use_idx = np.where(vmax_tropical>=thresh)[0][0]
-                
+
+                # Find max wind or min MSLP
+                if metric == 'wind_ge' and all_nan(vmax_tropical):
+                    continue
+                if metric == 'wind_ge' and np.nanmax(vmax_tropical) < thresh:
+                    continue
+                use_idx = np.where(vmax_tropical >= thresh)[0][0]
+
                 analyze_dict['lat'].append(lat_tropical[use_idx])
                 analyze_dict['lon'].append(lon_tropical[use_idx])
-                analyze_dict['date'].append(date_tropical[use_idx])
+                analyze_dict['time'].append(time_tropical[use_idx])
                 analyze_dict['mslp'].append(mslp_tropical[use_idx])
                 analyze_dict['vmax'].append(vmax_tropical[use_idx])
-            
-            #Append generic storm attributes
+
+            # Append generic storm attributes
             analyze_dict['id'].append(storm)
             analyze_dict['name'].append(storm_data['name'])
             analyze_dict['year'].append(int(storm_data['year']))
-            
-        #Error check
+
+        # Error check
         if len(analyze_dict[analyze_list[0]]) == 0:
-            raise RuntimeError("No storms were found given the requested criteria.")
-        
-        #Sort in requested order
+            raise RuntimeError(
+                "No storms were found given the requested criteria.")
+
+        # Sort in requested order
         arg_idx = np.argsort(analyze_dict[analyze_list[0]])
-        if ascending == False: arg_idx = arg_idx[::-1]
-            
-        #Sort all variables in requested order
+        if not ascending:
+            arg_idx = arg_idx[::-1]
+
+        # Sort all variables in requested order
         for key in analyze_dict.keys():
             analyze_dict[key] = (np.array(analyze_dict[key])[arg_idx])
-        
-        #Enter into new ranked dict
+
+        # Enter into new ranked dict
         ranked_dict = {}
         for i in range(len(analyze_dict['id'])):
-            ranked_dict[i+1] = {key:analyze_dict[key][i] for key in analyze_list}
-            if 'date' in ranked_dict[i+1].keys():
-                ranked_dict[i+1]['date'] = ranked_dict[i+1]['date'].replace(year=ranked_dict[i+1]['year'])
-            
-        #Return ranked dictionary
-        try:
-            import pandas as pd
-            return (pd.DataFrame(ranked_dict).transpose())[analyze_list]
-        except:
-            return ranked_dict
-        
-    def storm_ace_vs_season(self,storm,year_range=None):
-        
+            ranked_dict[i + 1] = {key: analyze_dict[key][i]
+                                  for key in analyze_list}
+            if 'time' in ranked_dict[i + 1].keys():
+                ranked_dict[i + 1]['time'] = ranked_dict[i +
+                                                         1]['time'].replace(year=ranked_dict[i + 1]['year'])
+
+        # Return ranked dictionary
+        return (pd.DataFrame(ranked_dict).transpose())[analyze_list]
+
+    def storm_ace_vs_season(self, storm, year_range=None):
         r"""
         Retrives a list of entire hurricane seasons with lower ACE than the storm provided.
-        
+
         Parameters
         ----------
         storm : str or tuple
             Storm to rank seasons against. Can be either string of storm ID (e.g., "AL052019"), or tuple with storm name and year (e.g., ("Matthew",2016)).
         year_range : list or tuple
             List or tuple representing the start and end years (e.g., (1950,2018)). Default is 1950 through the last year in the dataset.
-        
+
         Returns
         -------
         dict
             Dictionary containing the seasons with less ACE than the requested storm.
         """
-        
-        #Warning for ibtracs
+
+        # Warning for ibtracs
         if self.source == 'ibtracs':
             warning_str = "This function is not currently configured to optimally work for the ibtracs dataset."
             warnings.warn(warning_str)
 
-        #Determine year range of dataset
+        # Determine year range of dataset
         if year_range is None:
             start_year = self.data[self.keys[0]]['year']
-            if start_year < 1950: start_year = 1950
+            if start_year < 1950:
+                start_year = 1950
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
+        elif isinstance(year_range, (list, tuple)):
             if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
+                raise ValueError(
+                    "year_range must be a tuple or list with 2 elements: (start_year, end_year)")
             start_year = int(year_range[0])
-            if start_year < self.data[self.keys[0]]['year']: start_year = self.data[self.keys[0]]['year']
+            if start_year < self.data[self.keys[0]]['year']:
+                start_year = self.data[self.keys[0]]['year']
             end_year = int(year_range[1])
-            if end_year > self.data[self.keys[-1]]['year']: end_year = self.data[self.keys[-1]]['year']
+            if end_year > self.data[self.keys[-1]]['year']:
+                end_year = self.data[self.keys[-1]]['year']
         else:
             raise TypeError("year_range must be of type tuple or list")
-            
-        #Check if storm is str or tuple
+
+        # Check if storm is str or tuple
         if isinstance(storm, str):
             pass
         elif isinstance(storm, tuple):
-            storm = self.get_storm_id((storm[0],storm[1]))
+            storm = self.get_storm_id((storm[0], storm[1]))
         else:
-            raise RuntimeError("Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
-            
-        #Get ACE for this storm
+            raise RuntimeError(
+                "Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
+
+        # Get ACE for this storm
         storm_data = self.data[storm]
-        
-        #Retrieve ACE for this event
+
+        # Retrieve ACE for this event
         storm_name = storm_data['name']
         storm_year = storm_data['year']
-        storm_ace = np.round(storm_data['ace'],4)
-        
-        #Initialize empty dict
-        ace_rank = {'year':[],'ace':[]}
-            
-        #Iterate over every season
-        for year in range(start_year,end_year+1):
+        storm_ace = np.round(storm_data['ace'], 4)
+
+        # Initialize empty dict
+        ace_rank = {'year': [], 'ace': []}
+
+        # Iterate over every season
+        for year in range(start_year, end_year + 1):
             season = self.get_season(year)
             year_data = season.summary()
             year_ace = year_data['season_ace']
-            
-            #Compare year ACE against storm ACE
+
+            # Compare year ACE against storm ACE
             if year_ace < storm_ace:
-                
+
                 ace_rank['year'].append(year)
                 ace_rank['ace'].append(year_ace)
-                
+
         return ace_rank
 
-    def filter_storms(self,storm=None,year_range=(0,9999),date_range=('1/1','12/31'),thresh={},domain=None,interpolate_data=False,return_keys=True):
-        
+    def filter_storms(self, storm=None, year_range=None, date_range=None, thresh={}, domain=None, interpolate_data=False, return_keys=True):
         r"""
         Filters all storms by various thresholds.
-        
+
         Parameters
         ----------
+        storm : list or str
+            Single storm ID or list of storm IDs (e.g., ``'AL012022'``, ``['AL012022','AL022022']``) to search through. If None, defaults to searching through the entire dataset.
         year_range : list or tuple
-            List or tuple representing the start and end years (e.g., (1950,2018)). Default is start and end years of dataset.
+            List or tuple representing the start and end years (e.g., ``(1950,2018)``). Default is start and end years of dataset.
         date_range : list or tuple
-            List or tuple representing the start and end dates as a string in 'month/day' format (e.g., ('6/1','8/15')). Default is ('1/1','12/31') or full year.
+            List or tuple representing the start and end dates as a string in 'month/day' format (e.g., ``('6/1','8/15')``). Default is ``('1/1','12/31')`` or full year.
         thresh : dict
             Keywords include:
-                
+
             * **sample_min** - minimum number of storms in a grid box for "request" to be applied. For the functions 'percentile' and 'average', 'sample_min' defaults to 5 and will override any value less than 5.
             * **v_min** - minimum wind for a given point to be included in "request".
             * **p_max** - maximum pressure for a given point to be included in "request".
             * **dv_min** - minimum change in wind over dt_window for a given point to be included in "request".
             * **dp_max** - maximum change in pressure over dt_window for a given point to be included in "request".
             * **dt_window** - time window over which change variables are calculated (hours). Default is 24.
             * **dt_align** - alignment of dt_window for change variables -- 'start','middle','end' -- e.g. 'end' for dt_window=24 associates a TC point with change over the past 24 hours. Default is middle.
-            
+
             Units of all wind variables = kt, and pressure variables = hPa. These are added to the subtitle.
         domain : str
             Geographic domain. Default is entire basin. Please refer to :ref:`options-domain` for available domain options.
         interpolate_data : bool
             Whether to interpolate track data to hourly. Default is False.
         return_keys : bool
             If True, returns a list of storm IDs that match the specified criteria. Otherwise returns a pandas.DataFrame object with all matching data points. Default is True.
-        
+
         Returns
         -------
         list or pandas.DataFrame
             Check return_keys for more information.
         """
-        
-        #Add interpolation automatically if requested threshold necessitates it
-        check_keys = [True if i in thresh else False for i in ['dv_min','dv_max','dp_min','dp_max','speed_min','speed_max']]
-        if True in check_keys: interpolate_data = True
-        
-        #Update thresh based on input
-        default_thresh={'sample_min':1,'p_max':9999,'p_min':0,'v_min':0,'v_max':9999,'dv_min':-9999,'dp_max':9999,'dv_max':9999,'dp_min':-9999,'speed_max':9999,'speed_min':-9999,'dt_window':24,'dt_align':'middle'}
+
+        # Add default year aned date ranges
+        if year_range is None:
+            year_range = (0, 9999)
+        if date_range is None:
+            date_range = ('1/1', '12/31')
+
+        # Add interpolation automatically if requested threshold necessitates it
+        check_keys = [True if i in thresh else False for i in [
+            'dv_min', 'dv_max', 'dp_min', 'dp_max', 'speed_min', 'speed_max']]
+        if True in check_keys:
+            interpolate_data = True
+
+        # Update thresh based on input
+        default_thresh = {'sample_min': 1, 'p_max': 9999, 'p_min': 0, 'v_min': 0, 'v_max': 9999, 'dv_min': -9999, 'dp_max': 9999,
+                          'dv_max': 9999, 'dp_min': -9999, 'speed_max': 9999, 'speed_min': -9999, 'dt_window': 24, 'dt_align': 'middle'}
         for key in thresh:
             default_thresh[key] = thresh[key]
         thresh = default_thresh
 
-        #Determine domain over which to filter data
+        # Determine domain over which to filter data
         if domain is None:
             lon_min = 0
             lon_max = 360
             lat_min = -90
             lat_max = 90
         else:
             keys = domain.keys()
             check = [False, False, False, False]
             for key in keys:
-                if key[0].lower() == 'n': check[0] = True; lat_max = domain[key]
-                if key[0].lower() == 's': check[1] = True; lat_min = domain[key]
-                if key[0].lower() == 'e': check[2] = True; lon_max = domain[key]
-                if key[0].lower() == 'w': check[3] = True; lon_min = domain[key]
+                if key[0].lower() == 'n':
+                    check[0] = True
+                    lat_max = domain[key]
+                if key[0].lower() == 's':
+                    check[1] = True
+                    lat_min = domain[key]
+                if key[0].lower() == 'e':
+                    check[2] = True
+                    lon_max = domain[key]
+                if key[0].lower() == 'w':
+                    check[3] = True
+                    lon_min = domain[key]
             if False in check:
                 msg = "Custom domains must be of type dict with arguments for 'n', 's', 'e' and 'w'."
                 raise ValueError(msg)
-            if lon_max < 0: lon_max += 360.0
-            if lon_min < 0: lon_min += 360.0
-
-        #Determine year and date range
-        year_min,year_max = year_range
-        date_min,date_max = [dt.strptime(i,'%m/%d') for i in date_range]
-        date_max += timedelta(days=1,seconds=-1)
-        
-        #Determine if a date falls within the date range
-        def date_range_test(t,t_min,t_max):
-            if date_min<date_max:
-                test1 = (t>=t_min.replace(year=t.year))
-                test2 = (t<=t_max.replace(year=t.year))
+            if lon_max < 0:
+                lon_max += 360.0
+            if lon_min < 0:
+                lon_min += 360.0
+
+        # Determine year and date range
+        year_min, year_max = year_range
+        date_min, date_max = [dt.strptime(i, '%m/%d') for i in date_range]
+        date_max += timedelta(days=1, seconds=-1)
+
+        # Determine if a date falls within the date range
+        def date_range_test(t, t_min, t_max):
+            if date_min < date_max:
+                test1 = (t >= t_min.replace(year=t.year))
+                test2 = (t <= t_max.replace(year=t.year))
                 return test1 & test2
             else:
-                test1 = (t_min.replace(year=t.year)<=t<dt(t.year+1,1,1))
-                test2 = (dt(t.year,1,1)<=t<=t_max.replace(year=t.year))
+                test1 = (t_min.replace(year=t.year)
+                         <= t < dt(t.year + 1, 1, 1))
+                test2 = (dt(t.year, 1, 1) <= t <= t_max.replace(year=t.year))
                 return test1 | test2
-        
-        #Create empty dictionary to store output in
+
+        # Create empty dictionary to store output in
         points = {}
-        for name in ['vmax','mslp','type','lat','lon','date','season','stormid','ace']+ \
-                    ['dmslp_dt','dvmax_dt','acie','dx_dt','dy_dt','speed']*int(interpolate_data):
+        for name in ['vmax', 'mslp', 'type', 'lat', 'lon', 'time', 'season', 'stormid', 'ace'] + \
+                    ['dmslp_dt', 'dvmax_dt', 'acie', 'dx_dt', 'dy_dt', 'speed'] * int(interpolate_data):
             points[name] = []
-        
-        #Iterate over every storm in TrackDataset
+
+        # Iterate over every storm in TrackDataset
         if storm is not None:
-            if isinstance(storm,list):
-                if isinstance(storm[0],tuple):
+            if isinstance(storm, list):
+                if isinstance(storm[0], tuple):
                     stormkeys = [self.get_storm_id(s) for s in storm]
                 else:
-                    stormkeys=storm
-            elif isinstance(storm,tuple):
+                    stormkeys = storm
+            elif isinstance(storm, tuple):
                 stormkeys = [self.get_storm_id(storm)]
             else:
                 stormkeys = [storm]
         else:
             stormkeys = self.keys
-        
+
         for key in stormkeys:
-            
-            #Only interpolate storms within the provided temporal range
-            if self.data[key]['year'] <= (year_range[0]-1) or self.data[key]['year'] >= (year_range[-1]+1): continue
-            subset_dates = np.array(self.data[key]['date'])[np.array([i in constants.TROPICAL_STORM_TYPES for i in self.data[key]['type']])]
-            if len(subset_dates) == 0: continue
-            verify_dates = [date_range_test(i,date_min,date_max) for i in subset_dates]
-            if True not in verify_dates: continue
-            
-            #Interpolate temporally if requested
+
+            # Only interpolate storms within the provided temporal range
+            if self.data[key]['year'] <= (year_range[0] - 1) or self.data[key]['year'] >= (year_range[-1] + 1):
+                continue
+            subset_dates = np.array(self.data[key]['time'])[np.array(
+                [i in constants.TROPICAL_STORM_TYPES for i in self.data[key]['type']])]
+            if len(subset_dates) == 0:
+                continue
+            verify_dates = [date_range_test(
+                i, date_min, date_max) for i in subset_dates]
+            if True not in verify_dates:
+                continue
+
+            # Interpolate temporally if requested
             if interpolate_data:
                 try:
                     istorm = self.data_interp[key]
                 except:
-                    istorm = interp_storm(self.data[key].copy(),hours=1,dt_window=thresh['dt_window'],dt_align=thresh['dt_align'])
+                    istorm = interp_storm(self.data[key].copy(
+                    ), hours=1, dt_window=thresh['dt_window'], dt_align=thresh['dt_align'])
                     self.data_interp[key] = istorm.copy()
                 timeres = 1
             else:
                 istorm = self.data[key]
                 timeres = 6
-            
-            #Iterate over every timestep of the storm
-            for i in range(len(istorm['date'])):
-                
-                #Filter to only tropical cyclones, and filter by dates & coordiates
+
+            # Iterate over every timestep of the storm
+            for i in range(len(istorm['time'])):
+
+                # Filter to only tropical cyclones, and filter by dates & coordiates
                 if istorm['type'][i] in constants.TROPICAL_STORM_TYPES \
-                and lat_min<=istorm['lat'][i]<=lat_max and lon_min<=istorm['lon'][i]%360<=lon_max \
-                and year_min<=istorm['date'][i].year<=year_max \
-                and date_range_test(istorm['date'][i],date_min,date_max):
-                    
-                    #Append data points
+                    and lat_min <= istorm['lat'][i] <= lat_max and lon_min <= istorm['lon'][i] % 360 <= lon_max \
+                    and year_min <= istorm['time'][i].year <= year_max \
+                        and date_range_test(istorm['time'][i], date_min, date_max):
+
+                    # Append data points
                     points['vmax'].append(istorm['vmax'][i])
                     points['mslp'].append(istorm['mslp'][i])
                     points['type'].append(istorm['type'][i])
                     points['lat'].append(istorm['lat'][i])
                     points['lon'].append(istorm['lon'][i])
-                    points['date'].append(istorm['date'][i])
+                    points['time'].append(istorm['time'][i])
                     points['season'].append(istorm['season'])
                     points['stormid'].append(key)
-                    if istorm['vmax'][i]>34:
-                        points['ace'].append(istorm['vmax'][i]**2*1e-4*timeres/6)
+                    if istorm['vmax'][i] > 34:
+                        points['ace'].append(
+                            istorm['vmax'][i]**2 * 1e-4 * timeres / 6)
                     else:
-                        points['ace'].append(0)                        
-                        
-                    #Append separately for interpolated data
+                        points['ace'].append(0)
+
+                    # Append separately for interpolated data
                     if interpolate_data:
                         points['dvmax_dt'].append(istorm['dvmax_dt'][i])
-                        points['acie'].append([0,istorm['dvmax_dt'][i]**2*1e-4*timeres/6][istorm['dvmax_dt'][i]>0])
+                        points['acie'].append(
+                            [0, istorm['dvmax_dt'][i]**2 * 1e-4 * timeres / 6][istorm['dvmax_dt'][i] > 0])
                         points['dmslp_dt'].append(istorm['dmslp_dt'][i])
                         points['dx_dt'].append(istorm['dx_dt'][i])
                         points['dy_dt'].append(istorm['dy_dt'][i])
                         points['speed'].append(istorm['speed'][i])
-        
-        #Create a DataFrame from the dictionary
+
+        # Create a DataFrame from the dictionary
         p = pd.DataFrame.from_dict(points)
-        
-        #Filter by thresholds
-        if thresh['v_min']>0:
-            p = p.loc[(p['vmax']>=thresh['v_min'])]
-        if thresh['v_max']<9999:
-            p = p.loc[(p['vmax']<=thresh['v_max'])]
-        if thresh['p_max']<9999:
-            p = p.loc[(p['mslp']<=thresh['p_max'])]
-        if thresh['p_min']>0:
-            p = p.loc[(p['mslp']>=thresh['p_min'])]
+
+        # Filter by thresholds
+        if thresh['v_min'] > 0:
+            p = p.loc[(p['vmax'] >= thresh['v_min'])]
+        if thresh['v_max'] < 9999:
+            p = p.loc[(p['vmax'] <= thresh['v_max'])]
+        if thresh['p_max'] < 9999:
+            p = p.loc[(p['mslp'] <= thresh['p_max'])]
+        if thresh['p_min'] > 0:
+            p = p.loc[(p['mslp'] >= thresh['p_min'])]
         if interpolate_data:
-            if thresh['dv_min']>-9999:
-                p = p.loc[(p['dvmax_dt']>=thresh['dv_min'])]
-            if thresh['dp_max']<9999:
-                p = p.loc[(p['dmslp_dt']<=thresh['dp_max'])]
-            if thresh['dv_max']<9999:
-                p = p.loc[(p['dvmax_dt']<=thresh['dv_max'])]
-            if thresh['dp_min']>-9999:
-                p = p.loc[(p['dmslp_dt']>=thresh['dp_min'])]
-            if thresh['speed_max']<9999:
-                p = p.loc[(p['speed']>=thresh['speed_max'])]
-            if thresh['speed_min']>-9999:
-                p = p.loc[(p['speed']>=thresh['speed_min'])]
-        
-        #Determine how to return data
+            if thresh['dv_min'] > -9999:
+                p = p.loc[(p['dvmax_dt'] >= thresh['dv_min'])]
+            if thresh['dp_max'] < 9999:
+                p = p.loc[(p['dmslp_dt'] <= thresh['dp_max'])]
+            if thresh['dv_max'] < 9999:
+                p = p.loc[(p['dvmax_dt'] <= thresh['dv_max'])]
+            if thresh['dp_min'] > -9999:
+                p = p.loc[(p['dmslp_dt'] >= thresh['dp_min'])]
+            if thresh['speed_max'] < 9999:
+                p = p.loc[(p['speed'] >= thresh['speed_max'])]
+            if thresh['speed_min'] > -9999:
+                p = p.loc[(p['speed'] >= thresh['speed_min'])]
+
+        # Determine how to return data
         if return_keys:
             return [g[0] for g in p.groupby("stormid")]
         else:
             return p
 
-    def gridded_stats(self,request,thresh={},storm=None,year_range=None,year_range_subtract=None,year_average=False,
-                      date_range=('1/1','12/31'),binsize=1,domain=None,ax=None,
-                      return_array=False,cartopy_proj=None,**kwargs):
-        
+    def gridded_stats(self, request, thresh={}, storm=None, year_range=None, year_range_subtract=None, year_average=False,
+                      date_range=('1/1', '12/31'), binsize=1, domain=None, ax=None,
+                      return_array=False, cartopy_proj=None, **kwargs):
         r"""
         Creates a plot of gridded statistics.
-        
+
         Parameters
         ----------
         request : str
             This string is a descriptor for what you want to plot.
             It will be used to define the variable (e.g. 'wind' --> 'vmax') and the function (e.g. 'maximum' --> np.max()).
             This string is also used as the plot title.
-            
+
             Variable words to use in request:
-                
+
             * **wind** - (kt). Sustained wind.
             * **pressure** - (hPa). Minimum pressure.
             * **wind change** - (kt/time). Must be followed by an integer value denoting the length of the time window '__ hours' (e.g., "wind change in 24 hours").
             * **pressure change** - (hPa/time). Must be followed by an integer value denoting the length of the time window '__ hours' (e.g., "pressure change in 24 hours").
             * **storm motion** - (km/hour). Can be followed a length of time window. Otherwise defaults to 24 hours.
-            
+
             Units of all wind variables are knots and pressure variables are hPa. These are added into the title.
-            
+
             Function words to use in request:
-                
+
             * **maximum**
             * **minimum**
             * **average** 
             * **percentile** - Percentile must be preceded by an integer [0,100].
             * **number** - Number of storms in grid box satisfying filter thresholds.
-            
+
             Example usage: "maximum wind change in 24 hours", "50th percentile wind", "number of storms"
-            
+
         thresh : dict, optional
             Keywords include:
-                
+
             * **sample_min** - minimum number of storms in a grid box for the request to be applied. For the functions 'percentile' and 'average', 'sample_min' defaults to 5 and will override any value less than 5.
             * **v_min** - minimum wind for a given point to be included in the request.
             * **p_max** - maximum pressure for a given point to be included in the request.
             * **dv_min** - minimum change in wind over dt_window for a given point to be included in the request.
             * **dp_max** - maximum change in pressure over dt_window for a given point to be included in the request.
             * **dt_window** - time window over which change variables are calculated (hours). Default is 24.
             * **dt_align** - alignment of dt_window for change variables -- 'start','middle','end' -- e.g. 'end' for dt_window=24 associates a TC point with change over the past 24 hours. Default is middle.
-            
+
             Units of all wind variables = kt, and pressure variables = hPa. These are added to the subtitle.
 
         year_range : list or tuple, optional
             List or tuple representing the start and end years (e.g., (1950,2018)). Default is start and end years of dataset.
         year_range_subtract : list or tuple, optional
             A year range to subtract from the previously specified "year_range". If specified, will create a difference plot.
         year_average : bool, optional
@@ -2815,30 +3271,30 @@
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
         return_array : bool, optional
             If True, returns the gridded 2D array used to generate the plot. Default is False.
         cartopy_proj : ccrs, optional
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
-        
+
         Other Parameters
         ----------------
         prop : dict, optional
             Customization properties of plot. Please refer to :ref:`options-prop-gridded` for available options.
         map_prop : dict, optional
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         By default, the plot axes is returned. If "return_array" are set to True, a dictionary is returned containing both the axes and data array.
-        
+
         Notes
         -----
         The following properties are available for customizing the plot, via ``prop``:
-        
+
         .. list-table:: 
             :widths: 25 75
             :header-rows: 1
 
             * - Property
               - Description
             * - plot_values
@@ -2850,1040 +3306,1120 @@
             * - clevs
               - Contour levels for the plot. Default is minimum and maximum values in the grid.
             * - left_title
               - Title string for the left side of the plot. Default is the string passed via the 'request' keyword argument.
             * - right_title
               - Title string for the right side of the plot. Default is 'All storms'.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
 
-        default_prop = {'smooth':None}
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        default_prop = {'smooth': None}
         for key in prop.keys():
             default_prop[key] = prop[key]
         prop = default_prop
-        
-        #Update thresh based on input
-        default_thresh={'sample_min':np.nan,'p_max':np.nan,'v_min':np.nan,'dv_min':np.nan,'dp_max':np.nan,'dv_max':np.nan,'dp_min':np.nan,'dt_window':24,'dt_align':'middle'}
+
+        # Update thresh based on input
+        default_thresh = {'sample_min': np.nan, 'p_max': np.nan, 'v_min': np.nan, 'dv_min': np.nan,
+                          'dp_max': np.nan, 'dv_max': np.nan, 'dp_min': np.nan, 'dt_window': 24, 'dt_align': 'middle'}
         for key in thresh:
             default_thresh[key] = thresh[key]
         thresh = default_thresh
-        
-        #Retrieve the requested function, variable for computing stats, and plot title. These modify thresh if necessary.
-        thresh,func = find_func(request,thresh)
-        thresh,varname = find_var(request,thresh)
-        thresh,plot_subtitle = construct_title(thresh)
+
+        # Retrieve the requested function, variable for computing stats, and plot title. These modify thresh if necessary.
+        thresh, func = find_func(request, thresh)
+        thresh, varname = find_var(request, thresh)
+        thresh, plot_subtitle = construct_title(thresh)
         if storm is not None:
-            thresh['sample_min']=1
+            thresh['sample_min'] = 1
             plot_subtitle = ''
-            
-        #Determine whether request includes a vector (i.e., TC motion vector)
-        VEC_FLAG = isinstance(varname,tuple)
-        
-        #Determine year range of plot
+
+        # Determine whether request includes a vector (i.e., TC motion vector)
+        VEC_FLAG = isinstance(varname, tuple)
+
+        # Determine year range of plot
         def get_year_range(y_r):
             start_year = self.data[self.keys[0]]['year']
-            end_year = self.data[self.keys[-1]]['year']  
+            end_year = self.data[self.keys[-1]]['year']
             if y_r is None:
-                new_y_r = (start_year,end_year)
+                new_y_r = (start_year, end_year)
             else:
-                if isinstance(y_r,(list,tuple)) == False:
+                if not isinstance(y_r, (list, tuple)):
                     msg = "\"year_range\" and \"year_range_subtract\" must be of type list or tuple."
                     raise ValueError(msg)
                 if year_range_subtract is not None and len(year_range_subtract) != 2:
-                    msg = "\"year_range\" and \"year_range_subtract\" must contain 2 elements."                
+                    msg = "\"year_range\" and \"year_range_subtract\" must contain 2 elements."
                     raise ValueError(msg)
-                new_y_r = (max((start_year,min(y_r))),min((end_year,max(y_r))))
+                new_y_r = (max((start_year, min(y_r))),
+                           min((end_year, max(y_r))))
             return new_y_r
-        
+
         year_range = get_year_range(year_range)
-        
-        #Start date in numpy datetime64
-        startdate = np.datetime64('2000-'+'-'.join([f'{int(d):02}' for d in date_range[0].split('/')]))
-        
-        #Determine year range to subtract, if making a difference plot
+
+        # Start date in numpy datetime64
+        startdate = np.datetime64(
+            '2000-' + '-'.join([f'{int(d):02}' for d in date_range[0].split('/')]))
+
+        # Determine year range to subtract, if making a difference plot
         if year_range_subtract is not None:
             year_range_subtract = get_year_range(year_range_subtract)
-        
-        #---------------------------------------------------------------------------------------------------
-        
-        #Perform analysis either once or twice depending on year_range_subtract
+
+        # ---------------------------------------------------------------------------------------------------
+
+        # Perform analysis either once or twice depending on year_range_subtract
         if year_range_subtract is None:
             years_analysis = [year_range]
         else:
-            years_analysis = [year_range,year_range_subtract]
+            years_analysis = [year_range, year_range_subtract]
         grid_x_years = []
         grid_y_years = []
         grid_z_years = []
-        
+
         for year_range_temp in years_analysis:
 
-            #Obtain all data points for the requested threshold and year/date ranges. Interpolate data to hourly.
+            # Obtain all data points for the requested threshold and year/date ranges. Interpolate data to hourly.
             print("--> Getting filtered storm tracks")
-            points = self.filter_storms(storm,year_range_temp,date_range,thresh=thresh,interpolate_data=True,return_keys=False)
+            points = self.filter_storms(
+                storm, year_range_temp, date_range, thresh=thresh, interpolate_data=True, return_keys=False)
 
-            #Round lat/lon points down to nearest bin
-            to_bin = lambda x: np.floor(x / binsize) * binsize
+            # Round lat/lon points down to nearest bin
+            def to_bin(x): return np.floor(x / binsize) * binsize
             points["latbin"] = points.lat.map(to_bin)
             points["lonbin"] = points.lon.map(to_bin)
 
-            #---------------------------------------------------------------------------------------------------
+            # ---------------------------------------------------------------------------------------------------
 
-            #Group by latbin,lonbin,stormid
+            # Group by latbin,lonbin,stormid
             print("--> Grouping by lat/lon/storm")
-            groups = points.groupby(["latbin","lonbin","stormid","season"])
+            groups = points.groupby(["latbin", "lonbin", "stormid", "season"])
 
-            #Loops through groups, and apply stat func to storms
-            #Constructs a new dataframe containing the lat/lon bins, storm ID and the plotting variable
-            new_df = {'latbin':[],'lonbin':[],'stormid':[],'season':[],varname:[]}
+            # Loops through groups, and apply stat func to storms
+            # Constructs a new dataframe containing the lat/lon bins, storm ID and the plotting variable
+            new_df = {'latbin': [], 'lonbin': [],
+                      'stormid': [], 'season': [], varname: []}
             for g in groups:
-                #Apply function to all time steps in which a storm tracks within a gridbox
+                # Apply function to all time steps in which a storm tracks within a gridbox
                 if VEC_FLAG:
-                    new_df[varname].append([func(g[1][v].values) for v in varname])
+                    new_df[varname].append(
+                        [func(g[1][v].values) for v in varname])
                 elif varname == 'date':
-                    new_df[varname].append(func([date_diff(dt(2000,t.month,t.day),startdate)\
-                          for t in pd.DatetimeIndex(g[1][varname].values)]))
+                    new_df[varname].append(func([date_diff(dt(2000, t.month, t.day), startdate)
+                                                 for t in pd.DatetimeIndex(g[1][varname].values)]))
                 else:
-                    new_df[varname].append(func(g[1][varname].values))                    
+                    new_df[varname].append(func(g[1][varname].values))
                 new_df['latbin'].append(g[0][0])
                 new_df['lonbin'].append(g[0][1])
                 new_df['stormid'].append(g[0][2])
                 new_df['season'].append(g[0][3])
             new_df = pd.DataFrame.from_dict(new_df)
 
-            #---------------------------------------------------------------------------------------------------
+            # ---------------------------------------------------------------------------------------------------
 
-            #Group again by latbin,lonbin
-            #Construct two 1D lists: zi (grid values) and coords, that correspond to the 2D grid
+            # Group again by latbin,lonbin
+            # Construct two 1D lists: zi (grid values) and coords, that correspond to the 2D grid
             groups = new_df.groupby(["latbin", "lonbin"])
 
-            #Apply the function to all storms that pass through a gridpoint
+            # Apply the function to all storms that pass through a gridpoint
             if VEC_FLAG:
-                zi = [[func(v) for v in zip(*g[1][varname])] if len(g[1]) >= thresh['sample_min'] else [np.nan]*2 for g in groups]
+                zi = [[func(v) for v in zip(*g[1][varname])] if len(g[1])
+                      >= thresh['sample_min'] else [np.nan] * 2 for g in groups]
             elif varname == 'date':
-                zi = [func(g[1][varname]) if len(g[1]) >= thresh['sample_min'] else np.nan for g in groups]
-                zi = [mdates.date2num(startdate+z) for z in zi]                
+                zi = [func(g[1][varname]) if len(g[1]) >=
+                      thresh['sample_min'] else np.nan for g in groups]
+                zi = [mdates.date2num(startdate + z) for z in zi]
             else:
-                zi = [func(g[1][varname]) if len(g[1]) >= thresh['sample_min'] else np.nan for g in groups]
+                zi = [func(g[1][varname]) if len(g[1]) >=
+                      thresh['sample_min'] else np.nan for g in groups]
 
-            #Construct a 1D array of coordinates
+            # Construct a 1D array of coordinates
             coords = [g[0] for g in groups]
 
-            #Construct a 2D longitude and latitude grid, using the specified binsize resolution
+            # Construct a 2D longitude and latitude grid, using the specified binsize resolution
             if prop['smooth'] is not None:
-                all_lats = [(round(l/binsize)*binsize) for key in self.data.keys() for l in self.data[key]['lat']]
-                all_lons = [(round(l/binsize)*binsize)%360 for key in self.data.keys() for l in self.data[key]['lon']]
-                xi = np.arange(min(all_lons)-binsize,max(all_lons)+2*binsize,binsize)
-                yi = np.arange(min(all_lats)-binsize,max(all_lats)+2*binsize,binsize)
+                all_lats = [(round(l / binsize) * binsize)
+                            for key in self.data.keys() for l in self.data[key]['lat']]
+                all_lons = [(round(l / binsize) * binsize) %
+                            360 for key in self.data.keys() for l in self.data[key]['lon']]
+                xi = np.arange(min(all_lons) - binsize,
+                               max(all_lons) + 2 * binsize, binsize)
+                yi = np.arange(min(all_lats) - binsize,
+                               max(all_lats) + 2 * binsize, binsize)
                 if self.basin == 'all':
-                    xi = np.arange(0,360+binsize,binsize)
-                    yi = np.arange(-90,90+binsize,binsize)
+                    xi = np.arange(0, 360 + binsize, binsize)
+                    yi = np.arange(-90, 90 + binsize, binsize)
             else:
-                xi = np.arange(np.nanmin(points["lonbin"])-binsize,np.nanmax(points["lonbin"])+2*binsize,binsize)
-                yi = np.arange(np.nanmin(points["latbin"])-binsize,np.nanmax(points["latbin"])+2*binsize,binsize)
-            grid_x, grid_y = np.meshgrid(xi,yi)
+                xi = np.arange(np.nanmin(
+                    points["lonbin"]) - binsize, np.nanmax(points["lonbin"]) + 2 * binsize, binsize)
+                yi = np.arange(np.nanmin(
+                    points["latbin"]) - binsize, np.nanmax(points["latbin"]) + 2 * binsize, binsize)
+            grid_x, grid_y = np.meshgrid(xi, yi)
             grid_x_years.append(grid_x)
             grid_y_years.append(grid_y)
 
-            #Construct a 2D grid for the z value, depending on whether vector or scalar quantity
+            # Construct a 2D grid for the z value, depending on whether vector or scalar quantity
             if VEC_FLAG:
                 grid_z_u = np.ones(grid_x.shape) * np.nan
                 grid_z_v = np.ones(grid_x.shape) * np.nan
-                for c,z in zip(coords,zi):
-                    grid_z_u[np.where((grid_y==c[0]) & (grid_x==c[1]))] = z[0]
-                    grid_z_v[np.where((grid_y==c[0]) & (grid_x==c[1]))] = z[1]
-                grid_z = [grid_z_u,grid_z_v]
+                for c, z in zip(coords, zi):
+                    grid_z_u[np.where((grid_y == c[0]) &
+                                      (grid_x == c[1]))] = z[0]
+                    grid_z_v[np.where((grid_y == c[0]) &
+                                      (grid_x == c[1]))] = z[1]
+                grid_z = [grid_z_u, grid_z_v]
             else:
-                grid_z = np.ones(grid_x.shape)*np.nan
-                for c,z in zip(coords,zi):
-                    grid_z[np.where((grid_y==c[0]) & (grid_x==c[1]))] = z
+                grid_z = np.ones(grid_x.shape) * np.nan
+                for c, z in zip(coords, zi):
+                    grid_z[np.where((grid_y == c[0]) & (grid_x == c[1]))] = z
 
-            #Set zero values to nan's if necessary
+            # Set zero values to nan's if necessary
             if varname == 'type':
-                grid_z[np.where(grid_z==0)] = np.nan
-            
-            #Add to list of grid_z's
+                grid_z[np.where(grid_z == 0)] = np.nan
+
+            # Add to list of grid_z's
             grid_z_years.append(grid_z)
-        
-        #---------------------------------------------------------------------------------------------------
-        
-        #Calculate difference between plots, if specified
+
+        # ---------------------------------------------------------------------------------------------------
+
+        # Calculate difference between plots, if specified
         if len(grid_z_years) == 2:
-            try:
-                #Import xarray and construct DataArray
-                import xarray as xr
-                
-                #Determine whether to use averages
-                if year_average:
-                    years_listed = len(range(year_range[0],year_range[1]+1))
-                    grid_z_years[0] = grid_z_years[0] / years_listed
-                    years_listed = len(range(year_range_subtract[0],year_range_subtract[1]+1))
-                    grid_z_years[1] = grid_z_years[1] / years_listed
-                   
-                #Construct DataArrays
-                grid_z_1 = xr.DataArray(np.nan_to_num(grid_z_years[0]),coords=[grid_y_years[0].T[0],grid_x_years[0][0]],dims=['lat','lon'])
-                grid_z_2 = xr.DataArray(np.nan_to_num(grid_z_years[1]),coords=[grid_y_years[1].T[0],grid_x_years[1][0]],dims=['lat','lon'])
-                
-                #Compute difference grid
-                grid_z = grid_z_1 - grid_z_2
-                
-                #Reconstruct lat & lon grids
-                xi = grid_z.lon.values
-                yi = grid_z.lat.values
-                grid_z = grid_z.values
-                grid_x, grid_y = np.meshgrid(xi,yi)
-                
-                #Determine NaNs
-                grid_z_years[0][np.isnan(grid_z_years[0])] = -9999
-                grid_z_years[1][np.isnan(grid_z_years[1])] = -8999
-                grid_z_years[0][grid_z_years[0]!=-9999] = 0
-                grid_z_years[1][grid_z_years[1]!=-8999] = 0
-                grid_z_1 = xr.DataArray(np.nan_to_num(grid_z_years[0]),coords=[grid_y_years[0].T[0],grid_x_years[0][0]],dims=['lat','lon'])
-                grid_z_2 = xr.DataArray(np.nan_to_num(grid_z_years[1]),coords=[grid_y_years[1].T[0],grid_x_years[1][0]],dims=['lat','lon'])
-                grid_z_check = (grid_z_1 - grid_z_2).values
-                grid_z[grid_z_check==-1000] = np.nan
-                print(np.nanmin(grid_z))
-                
-            except ImportError as e:
-                raise RuntimeError("Error: xarray is not available. Install xarray in order to use the subtract year functionality.") from e
+            # Determine whether to use averages
+            if year_average:
+                years_listed = len(range(year_range[0], year_range[1] + 1))
+                grid_z_years[0] = grid_z_years[0] / years_listed
+                years_listed = len(
+                    range(year_range_subtract[0], year_range_subtract[1] + 1))
+                grid_z_years[1] = grid_z_years[1] / years_listed
+
+            # Construct DataArrays
+            grid_z_1 = xr.DataArray(np.nan_to_num(grid_z_years[0]), coords=[
+                                    grid_y_years[0].T[0], grid_x_years[0][0]], dims=['lat', 'lon'])
+            grid_z_2 = xr.DataArray(np.nan_to_num(grid_z_years[1]), coords=[
+                                    grid_y_years[1].T[0], grid_x_years[1][0]], dims=['lat', 'lon'])
+
+            # Compute difference grid
+            grid_z = grid_z_1 - grid_z_2
+
+            # Reconstruct lat & lon grids
+            xi = grid_z.lon.values
+            yi = grid_z.lat.values
+            grid_z = grid_z.values
+            grid_x, grid_y = np.meshgrid(xi, yi)
+
+            # Determine NaNs
+            grid_z_years[0][np.isnan(grid_z_years[0])] = -9999
+            grid_z_years[1][np.isnan(grid_z_years[1])] = -8999
+            grid_z_years[0][grid_z_years[0] != -9999] = 0
+            grid_z_years[1][grid_z_years[1] != -8999] = 0
+            grid_z_1 = xr.DataArray(np.nan_to_num(grid_z_years[0]), coords=[
+                                    grid_y_years[0].T[0], grid_x_years[0][0]], dims=['lat', 'lon'])
+            grid_z_2 = xr.DataArray(np.nan_to_num(grid_z_years[1]), coords=[
+                                    grid_y_years[1].T[0], grid_x_years[1][0]], dims=['lat', 'lon'])
+            grid_z_check = (grid_z_1 - grid_z_2).values
+            grid_z[grid_z_check == -1000] = np.nan
+            print(np.nanmin(grid_z))
+
         else:
-            #Determine whether to use averages
+            # Determine whether to use averages
             if year_average:
-                years_listed = len(range(year_range[0],year_range[1]+1))
+                years_listed = len(range(year_range[0], year_range[1] + 1))
                 grid_z = grid_z / years_listed
-        
-        #Create instance of plot object
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection using basin
+
+        # Create cartopy projection using basin
         if domain is None:
             domain = self.basin
         if cartopy_proj is None:
             if max(points['lon']) > 150 or min(points['lon']) < -150:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=180.0)
             else:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-        
-        #Format left title for plot
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
+
+        # Format left title for plot
         endash = u"\u2013"
         dot = u"\u2022"
         title_L = request.lower()
-        for name in ['wind','vmax']:
-            title_L = title_L.replace(name,'wind (kt)')
-        for name in ['pressure','mslp']:
-            title_L = title_L.replace(name,'pressure (hPa)')
-        for name in ['heading','motion']:
-            title_L = title_L.replace(name,f'heading (kt) over {thresh["dt_window"]} hours')
-        for name in ['speed','movement']:
-            title_L = title_L.replace(name,f'forward speed (kt) over {thresh["dt_window"]} hours')
+        for name in ['wind', 'vmax']:
+            title_L = title_L.replace(name, 'wind (kt)')
+        for name in ['pressure', 'mslp']:
+            title_L = title_L.replace(name, 'pressure (hPa)')
+        for name in ['heading', 'motion']:
+            title_L = title_L.replace(
+                name, f'heading (kt) over {thresh["dt_window"]} hours')
+        for name in ['speed', 'movement']:
+            title_L = title_L.replace(
+                name, f'forward speed (kt) over {thresh["dt_window"]} hours')
         if request.find('change') >= 0:
-            title_L = title_L+f", {thresh['dt_align']}"
+            title_L = title_L + f", {thresh['dt_align']}"
         title_L = title_L[0].upper() + title_L[1:] + plot_subtitle
-        
-        #Format right title for plot
+
+        # Format right title for plot
         if storm is not None:
-            if isinstance(storm,list):
+            if isinstance(storm, list):
                 title_R = 'Storm Composite'
             else:
-                if isinstance(storm,str):
+                if isinstance(storm, str):
                     storm = basin.get_storm_tuple(storm)
                 title_R = f'{storm[0]} {storm[1]}'
         else:
-            date_range = [dt.strptime(d,'%m/%d').strftime('%b/%d') for d in date_range]
-            if np.subtract(*year_range)==0:
+            date_range = [dt.strptime(d, '%m/%d').strftime('%b/%d')
+                          for d in date_range]
+            if np.subtract(*year_range) == 0:
                 y_r_title = f'{year_range[0]}'
             else:
                 y_r_title = f'{year_range[0]} {endash} {year_range[1]}'
-            add_avg = ' year-avg' if year_average == True else ''
+            add_avg = ' year-avg' if year_average else ''
             if year_range_subtract is None:
                 title_R = f'{date_range[0].replace("/"," ")} {endash} {date_range[1].replace("/"," ")} {dot} {y_r_title}{add_avg}'
             else:
-                if np.subtract(*year_range_subtract)==0:
+                if np.subtract(*year_range_subtract) == 0:
                     y_r_s_title = f'{year_range_subtract[0]}'
                 else:
                     y_r_s_title = f'{year_range_subtract[0]} {endash} {year_range_subtract[1]}'
                 title_R = f'{date_range[0].replace("/"," ")} {endash} {date_range[1].replace("/"," ")}\n{y_r_title}{add_avg} minus {y_r_s_title}{add_avg}'
-        prop['title_L'],prop['title_R'] = title_L,title_R
-        
-        #Change the masking for variables that go out to zero near the edge of the data
+        prop['title_L'], prop['title_R'] = title_L, title_R
+
+        # Change the masking for variables that go out to zero near the edge of the data
         if prop['smooth'] is not None:
-            
-            #Replace NaNs with zeros to apply Gaussian filter
+
+            # Replace NaNs with zeros to apply Gaussian filter
             grid_z_zeros = grid_z.copy()
             grid_z_zeros[np.isnan(grid_z)] = 0
-            initial_mask = grid_z.copy() #Save initial mask
+            initial_mask = grid_z.copy()  # Save initial mask
             initial_mask[np.isnan(grid_z)] = -9999
-            grid_z_zeros = gfilt(grid_z_zeros,sigma=prop['smooth'])
-            
-            
+            grid_z_zeros = gfilt(grid_z_zeros, sigma=prop['smooth'])
+
             if len(grid_z_years) == 2:
-                #grid_z_1_zeros = np.asarray(grid_z_1)
-                #grid_z_1_zeros[grid_z_1==-9999]=0
-                #grid_z_1_zeros = gfilt(grid_z_1_zeros,sigma=prop['smooth'])
-                
-                #grid_z_2_zeros = np.asarray(grid_z_2)
-                #grid_z_2_zeros[grid_z_2==-8999]=0
-                #grid_z_2_zeros = gfilt(grid_z_2_zeros,sigma=prop['smooth'])
-                #grid_z_zeros = grid_z_1_zeros - grid_z_2_zeros
-                #test_zeros = (grid_z_1_zeros<.02*np.nanmax(grid_z_1_zeros)) & (grid_z_2_zeros<.02*np.nanmax(grid_z_2_zeros))
+                # grid_z_1_zeros = np.asarray(grid_z_1)
+                # grid_z_1_zeros[grid_z_1==-9999]=0
+                # grid_z_1_zeros = gfilt(grid_z_1_zeros,sigma=prop['smooth'])
+
+                # grid_z_2_zeros = np.asarray(grid_z_2)
+                # grid_z_2_zeros[grid_z_2==-8999]=0
+                # grid_z_2_zeros = gfilt(grid_z_2_zeros,sigma=prop['smooth'])
+                # grid_z_zeros = grid_z_1_zeros - grid_z_2_zeros
+                # test_zeros = (grid_z_1_zeros<.02*np.nanmax(grid_z_1_zeros)) & (grid_z_2_zeros<.02*np.nanmax(grid_z_2_zeros))
                 pass
-            
-            elif varname not in [('dx_dt','dy_dt'),'speed','mslp']:
-                
-                #Apply cutoff at 2% of maximum
-                test_zeros = (grid_z_zeros<.02*np.amax(grid_z_zeros))
+
+            elif varname not in [('dx_dt', 'dy_dt'), 'speed', 'mslp']:
+
+                # Apply cutoff at 2% of maximum
+                test_zeros = (grid_z_zeros < .02 * np.amax(grid_z_zeros))
                 grid_z_zeros[test_zeros] = -9999
                 initial_mask = grid_z_zeros.copy()
-                
-            grid_z_zeros[initial_mask==-9999] = np.nan
+
+            grid_z_zeros[initial_mask == -9999] = np.nan
             grid_z = grid_z_zeros.copy()
-        
-        #Plot gridded field
-        plot_ax = self.plot_obj.plot_gridded(grid_x,grid_y,grid_z,varname,VEC_FLAG,domain,ax=ax,prop=prop,map_prop=map_prop)
-        
-        #Format grid into xarray if specified
-        if return_array:
-            try:
-                #Import xarray and construct DataArray, replacing NaNs with zeros
-                import xarray as xr
-                arr = xr.DataArray(np.nan_to_num(grid_z),coords=[grid_y.T[0],grid_x[0]],dims=['lat','lon'])
-                return arr
-            except ImportError as e:
-                raise RuntimeError("Error: xarray is not available. Install xarray in order to use the 'return_array' flag.") from e
 
-        #Return axis
+        # Plot gridded field
+        plot_ax = self.plot_obj.plot_gridded(
+            grid_x, grid_y, grid_z, varname, VEC_FLAG, domain, ax=ax, prop=prop, map_prop=map_prop)
+
+        # Format grid into xarray if specified
         if return_array:
-            return {'ax':plot_ax,'array':arr}
+            arr = xr.DataArray(np.nan_to_num(grid_z), coords=[
+                               grid_y.T[0], grid_x[0]], dims=['lat', 'lon'])
+            return arr
+        
+        # Return axis
+        if return_array:
+            return {'ax': plot_ax, 'array': arr}
         else:
             return plot_ax
 
-    
-    def assign_storm_tornadoes(self,dist_thresh=1000,tornado_path='spc'):
-        
+    def assign_storm_tornadoes(self, dist_thresh=1000, tornado_path='spc'):
         r"""
         Assigns tornadoes to all North Atlantic tropical cyclones from TornadoDataset.
-        
+
         Parameters
         ----------
         dist_thresh : int
             Distance threshold (in kilometers) from the tropical cyclone track over which to attribute tornadoes to the TC. Default is 1000 km.
         tornado_path : str
             Source to read tornado data from. Default is "spc", which reads from the online Storm Prediction Center (SPC) 1950-present tornado database. Can change this to a local file.
-        
+
         Notes
         -----
         If you intend on analyzing tornadoes for multiple tropical cyclones using a Storm object, it is recommended to run this function first to avoid the need to re-read the entire tornado database for each Storm object.
         """
-        
-        #Check to ensure data source is over North Atlantic
+
+        # Check to ensure data source is over North Atlantic
         if self.basin != "north_atlantic":
-            raise RuntimeError("Tropical cyclone tornado data is only available for the North Atlantic basin.")
-        
-        #Check to see if tornado data already exists in this instance
+            raise RuntimeError(
+                "Tropical cyclone tornado data is only available for the North Atlantic basin.")
+
+        # Check to see if tornado data already exists in this instance
         self.TorDataset = TornadoDataset(tornado_path=tornado_path)
         self.tornado_dist_thresh = dist_thresh
-        
-        #Iterate through all storms in dataset and assign them tornadoes, if they exist
+
+        # Iterate through all storms in dataset and assign them tornadoes, if they exist
         timer_start = dt.now()
         print(f'--> Starting to assign tornadoes to storms')
-        for i,key in enumerate(self.keys):
-            
-            #Skip years prior to 1950
-            if self.data[key]['year'] < 1950: continue
-                
-            #Get tornado data for storm
+        for i, key in enumerate(self.keys):
+
+            # Skip years prior to 1950
+            if self.data[key]['year'] < 1950:
+                continue
+
+            # Get tornado data for storm
             storm_obj = self.get_storm(key)
-            tor_data = self.TorDataset.get_storm_tornadoes(storm_obj,dist_thresh=dist_thresh)
-            tor_data = self.TorDataset.rotateToHeading(storm_obj,tor_data)
+            tor_data = self.TorDataset.get_storm_tornadoes(
+                storm_obj, dist_thresh=dist_thresh)
+            tor_data = self.TorDataset.rotateToHeading(storm_obj, tor_data)
             self.data_tors[key] = tor_data
-            
-            #Check if storm contains tornadoes
+
+            # Check if storm contains tornadoes
             if len(tor_data) > 0:
                 self.keys_tors[i] = 1
-                
-        #Update user on status
-        print(f'--> Completed assigning tornadoes to storm (%.2f seconds)' % (dt.now()-timer_start).total_seconds())
-        
-    def plot_TCtors_rotated(self,storms,mag_thresh=0,return_df=False,save_path=None):
-        
+
+        # Update user on status
+        print(f'--> Completed assigning tornadoes to storm (%.2f seconds)' %
+              (dt.now() - timer_start).total_seconds())
+
+    def plot_TCtors_rotated(self, storms, mag_thresh=0, return_df=False, save_path=None):
         r"""
         Plot tracks of tornadoes relative to the storm motion vector of the tropical cyclone.
-        
+
         Parameters
         ----------
         storms : list or str
             Storm(s) for which to plot motion-relative tornado data for. Can be either a list of storm IDs/tuples for which to create a composite of, or a string "all" for all storms containing tornado data.
         mag_thresh : int
             Minimum threshold for tornado rating.
         return_df : bool
             Whether to return the pandas DataFrame containing the composite tornado data. Default is False.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Returns
         -------
         ax or dict
             By default, the plot axes is returned. If "return_df" is set to True, returns a dict containing both the data and the axes plot
-        
+
         Notes
         -----
         The motion vector is oriented upwards (in the +y direction).
         """
-        
-        #Error check
+
+        # Error check
         try:
             self.TorDataset
         except:
-            raise RuntimeError("No tornado data has been attributed to this dataset. Please run \"TrackDataset.assign_storm_tornadoes()\" first.")
-        
-        #Error check
-        if isinstance(mag_thresh,int) == False:
+            raise RuntimeError(
+                "No tornado data has been attributed to this dataset. Please run \"TrackDataset.assign_storm_tornadoes()\" first.")
+
+        # Error check
+        if not isinstance(mag_thresh, int):
             raise TypeError("mag_thresh must be of type int.")
-        elif mag_thresh not in [0,1,2,3,4,5]:
+        elif mag_thresh not in [0, 1, 2, 3, 4, 5]:
             raise ValueError("mag_thresh must be between 0 and 5.")
-        
-        #Get IDs of all storms to composite
+
+        # Get IDs of all storms to composite
         if storms == 'all':
-            storms = [self.keys[i] for i in range(len(self.keys)) if self.keys_tors[i] == 1]
+            storms = [self.keys[i]
+                      for i in range(len(self.keys)) if self.keys_tors[i] == 1]
         else:
-            if len(storms)==2 and isinstance(storms[-1],int):
+            if len(storms) == 2 and isinstance(storms[-1], int):
                 use_storms = [self.get_storm_id(storms)]
             else:
-                use_storms = [i if isinstance(i,str) == True else self.get_storm_id(i) for i in storms]
-            storms = [i for i in use_storms if i in self.keys and self.keys_tors[self.keys.index(i)] == 1]
-            
+                use_storms = [i if isinstance(i, str) else self.get_storm_id(i) for i in storms]
+            storms = [
+                i for i in use_storms if i in self.keys and self.keys_tors[self.keys.index(i)] == 1]
+
         if len(storms) == 0:
-            raise RuntimeError("None of the requested storms produced any tornadoes.")
-        
-        #Get stormTors formatted with requested storm(s)
+            raise RuntimeError(
+                "None of the requested storms produced any tornadoes.")
+
+        # Get stormTors formatted with requested storm(s)
         stormTors = (self.data_tors[storms[0]]).copy()
-        stormTors['storm_id'] = [storms[0]]*len(stormTors)
+        stormTors['storm_id'] = [storms[0]] * len(stormTors)
         if len(storms) > 1:
             for storm in storms[1:]:
                 storm_df = self.data_tors[storm]
-                storm_df['storm_id'] = [storm]*len(storm_df)
+                storm_df['storm_id'] = [storm] * len(storm_df)
                 stormTors = stormTors.append(storm_df)
-        
-        #Create figure for plotting
-        plt.figure(figsize=(9,9),dpi=150)
+
+        # Create figure for plotting
+        plt.figure(figsize=(9, 9), dpi=150)
         ax = plt.subplot()
-        
-        #Default EF color scale
+
+        # Default EF color scale
         EFcolors = get_colors_ef('default')
-        
-        #Number of storms exceeding mag_thresh
-        num_storms = len(np.unique(stormTors.loc[stormTors['mag']>=mag_thresh]['storm_id'].values))
-        
-        #Filter for mag >= mag_thresh, and sort by mag so highest will be plotted on top
-        stormTors = stormTors.loc[stormTors['mag']>=mag_thresh].sort_values('mag')
 
-        #Plot all tornado tracks in motion relative coords
-        for _,row in stormTors.iterrows():
-            plt.plot([row['rot_xdist_s'],row['rot_xdist_e']+.01],[row['rot_ydist_s'],row['rot_ydist_e']+.01],\
-                     lw=2,c=EFcolors[row['mag']])
-            
-        #Plot dist_thresh radius
+        # Number of storms exceeding mag_thresh
+        num_storms = len(
+            np.unique(stormTors.loc[stormTors['mag'] >= mag_thresh]['storm_id'].values))
+
+        # Filter for mag >= mag_thresh, and sort by mag so highest will be plotted on top
+        stormTors = stormTors.loc[stormTors['mag']
+                                  >= mag_thresh].sort_values('mag')
+
+        # Plot all tornado tracks in motion relative coords
+        for _, row in stormTors.iterrows():
+            plt.plot([row['rot_xdist_s'], row['rot_xdist_e'] + .01], [row['rot_ydist_s'], row['rot_ydist_e'] + .01],
+                     lw=2, c=EFcolors[row['mag']])
+
+        # Plot dist_thresh radius
         dist_thresh = self.tornado_dist_thresh
         ax.set_facecolor('#F6F6F6')
-        circle = plt.Circle((0,0), dist_thresh, color='w')
+        circle = plt.Circle((0, 0), dist_thresh, color='w')
         ax.add_artist(circle)
         an = np.linspace(0, 2 * np.pi, 100)
-        ax.plot(dist_thresh * np.cos(an), dist_thresh * np.sin(an),'k')
-        ax.plot([-dist_thresh,dist_thresh],[0,0],'k--',lw=.5)
-        ax.plot([0,0],[-dist_thresh,dist_thresh],'k--',lw=.5)
-        
-        #Plot motion vector
-        plt.arrow(0, -dist_thresh*.1, 0, dist_thresh*.2, length_includes_head=True,
-          head_width=45, head_length=45,fc='k',lw=2,zorder=100)
-        
-        #Labels
+        ax.plot(dist_thresh * np.cos(an), dist_thresh * np.sin(an), 'k')
+        ax.plot([-dist_thresh, dist_thresh], [0, 0], 'k--', lw=.5)
+        ax.plot([0, 0], [-dist_thresh, dist_thresh], 'k--', lw=.5)
+
+        # Plot motion vector
+        plt.arrow(0, -dist_thresh * .1, 0, dist_thresh * .2, length_includes_head=True,
+                  head_width=45, head_length=45, fc='k', lw=2, zorder=100)
+
+        # Labels
         ax.set_aspect('equal', 'box')
-        ax.set_xlabel('Left/Right of Storm Heading (km)',fontsize=13)
-        ax.set_ylabel('Behind/Ahead of Storm Heading (km)',fontsize=13)
-        ax.set_title(f'Composite motion-relative tornadoes\nMin threshold: EF-{mag_thresh} | n={num_storms} storms',fontsize=14,fontweight='bold')
+        ax.set_xlabel('Left/Right of Storm Heading (km)', fontsize=13)
+        ax.set_ylabel('Behind/Ahead of Storm Heading (km)', fontsize=13)
+        ax.set_title(
+            f'Composite motion-relative tornadoes\nMin threshold: EF-{mag_thresh} | n={num_storms} storms', fontsize=14, fontweight='bold')
         ax.tick_params(axis='both', which='major', labelsize=11.5)
-        
-        #Add legend
-        handles=[]
-        for ef,color in enumerate(EFcolors):
+
+        # Add legend
+        handles = []
+        for ef, color in enumerate(EFcolors):
             if ef >= mag_thresh:
-                count = len(stormTors[stormTors['mag']==ef])
-                handles.append(mlines.Line2D([], [], linestyle='-',color=color,label=f'EF-{ef} ({count})'))
-        ax.legend(handles=handles,loc='lower left',fontsize=11.5)
-        
-        #Add attribution
-        ax.text(0.99,0.01,plot_credit(),fontsize=8,color='k',alpha=0.7,
-                transform=ax.transAxes,ha='right',va='bottom',zorder=10)
-        
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return data
+                count = len(stormTors[stormTors['mag'] == ef])
+                handles.append(mlines.Line2D([], [], linestyle='-',
+                               color=color, label=f'EF-{ef} ({count})'))
+        ax.legend(handles=handles, loc='lower left', fontsize=11.5)
+
+        # Add attribution
+        ax.text(0.99, 0.01, plot_credit(), fontsize=8, color='k', alpha=0.7,
+                transform=ax.transAxes, ha='right', va='bottom', zorder=10)
+
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return data
         if return_df:
-            return {'ax':ax,'df':stormTors}
+            return {'ax': ax, 'df': stormTors}
         else:
             return ax
 
-    
     def to_dataframe(self):
-
         r"""
         Retrieve a Pandas DataFrame for all seasons within TrackDataset.
-        
+
         Returns
         -------
         pandas.DataFrame
             Returns a Pandas DataFrame containing requested data.
         """
 
-        #Get start and end seasons in this TrackDataset object
+        # Get start and end seasons in this TrackDataset object
         start_season = self.data[self.keys[0]]['year']
         end_season = self.data[self.keys[-1]]['year']
 
-        #Create empty dict to be used for making pandas DataFrame object
-        ds = {'season':[],'all_storms':[],'named_storms':[],'hurricanes':[],'major_hurricanes':[],'ace':[],'start_time':[],'end_time':[]}
+        # Create empty dict to be used for making pandas DataFrame object
+        ds = {'season': [], 'all_storms': [], 'named_storms': [], 'hurricanes': [
+        ], 'major_hurricanes': [], 'ace': [], 'start_time': [], 'end_time': []}
 
-        #Iterate over all seasons in the TrackDataset object
-        for season in range(start_season,end_season+1):
+        # Iterate over all seasons in the TrackDataset object
+        for season in range(start_season, end_season + 1):
 
-            #Get season summary, if season can be retrieved
+            # Get season summary, if season can be retrieved
             try:
                 season_summary = self.get_season(season).summary()
             except:
                 continue
-            if len(season_summary['id']) == 0: continue
+            if len(season_summary['id']) == 0:
+                continue
 
-            #Add information to dict
+            # Add information to dict
             ds['season'].append(season)
             ds['all_storms'].append(season_summary['season_storms'])
             ds['named_storms'].append(season_summary['season_named'])
             ds['hurricanes'].append(season_summary['season_hurricane'])
             ds['major_hurricanes'].append(season_summary['season_major'])
             ds['ace'].append(season_summary['season_ace'])
             ds['start_time'].append(season_summary['season_start'])
             ds['end_time'].append(season_summary['season_end'])
 
-        #Convert entire dict to a DataFrame
+        # Convert entire dict to a DataFrame
         ds = pd.DataFrame(ds)
 
-        #Return dataset
+        # Return dataset
         return ds.set_index('season')
 
-    def climatology(self,year_range=(1991,2020)):
-
+    def climatology(self, climo_bounds=(1991, 2020)):
         r"""
         Create a climatology for this dataset given start and end seasons. If none passed, defaults to 1991-2020.
-        
+
         Parameters
         ----------
-        year_range : list or tuple, optional
+        climo_bounds : list or tuple, optional
             Start and end year for the climatology range. Default is (1991,2020).
-        
+
         Returns
         -------
         dict
             Dictionary containing the climatology for this dataset.
+
+        Notes
+        -----
+        If in southern hemisphere, year is the 2nd year of the season (e.g., 1975 for 1974-1975).
         """
 
-        #Error check
-        if not isinstance(year_range,(list,tuple)):
-            raise TypeError("year_range must be of type list or tuple.")
-        if len(year_range) != 2:
-            raise TypeError("year_range must have two elements, start and end year.")
-        start_season,end_season = year_range
+        # Error check
+        if not isinstance(climo_bounds, (list, tuple)):
+            raise TypeError("climo_bounds must be of type list or tuple.")
+        if len(climo_bounds) != 2:
+            raise TypeError(
+                "climo_bounds must have two elements, start and end year.")
+        start_season, end_season = climo_bounds
         if start_season >= end_season:
             raise ValueError("start_season cannot be greater than end_season.")
-        if not isinstance(start_season,(int,np.int,np.integer,float,np.floating)) or not isinstance(end_season,(int,np.int,np.integer,float,np.floating)):
+        if not is_number(start_season) or not is_number(end_season):
             raise TypeError("start_season and end_season must be of type int.")
         if (end_season - start_season) < 5:
-            raise ValueError("A minimum of 5 seasons is required for constructing a climatology.")
+            raise ValueError(
+                "A minimum of 5 seasons is required for constructing a climatology.")
 
-        #Retrieve data for all seasons in this dataset
+        # Retrieve data for all seasons in this dataset
         full_climo = self.to_dataframe()
-        subset_climo = full_climo.loc[start_season:end_season+1]
+        subset_climo = full_climo.loc[start_season:end_season + 1]
 
-        #Convert dates to julian days
-        julian_start = [convert_to_julian(pd.to_datetime(i)) for i in subset_climo['start_time'].values]
-        julian_end = [convert_to_julian(pd.to_datetime(i)) for i in subset_climo['end_time'].values]
-        julian_end = [i+365 if i < 100 else i for i in julian_end]
-        subset_climo = subset_climo.drop(columns=['start_time','end_time'])
+        # Convert dates to julian days
+        julian_start = [convert_to_julian(pd.to_datetime(
+            i)) for i in subset_climo['start_time'].values]
+        julian_end = [convert_to_julian(pd.to_datetime(i))
+                      for i in subset_climo['end_time'].values]
+        if self.basin in constants.SOUTH_HEMISPHERE_BASINS:
+            julian_start = [i + 365 if i < 100 else i for i in julian_start]
+            julian_end = [i - 365 if i > 300 else i for i in julian_end]
+        elif self.basin != 'all':
+            julian_end = [i + 365 if i < 100 else i for i in julian_end]
+        subset_climo = subset_climo.drop(columns=['start_time', 'end_time'])
         subset_climo['start_time'] = julian_start
         subset_climo['end_time'] = julian_end
 
         subset_climo_means = (subset_climo.mean(axis=0)).round(1)
 
         climatology = {}
-        for key in ['all_storms','named_storms','hurricanes','major_hurricanes','ace']:
+        for key in ['all_storms', 'named_storms', 'hurricanes', 'major_hurricanes', 'ace']:
             climatology[key] = subset_climo_means[key]
-        for key in ['start_time','end_time']:
-            climatology[key] = dt(dt.now().year-1,12,31)+timedelta(hours=24*subset_climo_means[key])
+        for key in ['start_time', 'end_time']:
+            climatology[key] = dt(dt.now().year - 1, 12, 31) + \
+                timedelta(hours=24 * subset_climo_means[key])
 
         return climatology
 
-    def season_composite(self,seasons,climo_bounds=None):
-
+    def season_composite(self, seasons, climo_bounds=None):
         r"""
         Create composite statistics for a list of seasons.
-        
+
         Parameters
         ----------
         seasons : list
-            List of seasons to create a composite of. For Southern Hemisphere, season is the start of the two-year period.
+            List of seasons to create a composite of.
         climo_bounds : list or tuple
             List or tuple of start and end years of climatology bounds. If none, defaults to (1991,2020).
-        
+
         Returns
         -------
         dict
             Dictionary containing the composite of the requested seasons.
+
+        Notes
+        -----
+        If in southern hemisphere, year is the 2nd year of the season (e.g., 1975 for 1974-1975).
         """
 
-        if isinstance(seasons,list) == False:
+        if not isinstance(seasons, list):
             raise TypeError("'seasons' must be of type list.")
 
         if climo_bounds is None:
-            climo_bounds = (1991,2020)
+            climo_bounds = (1991, 2020)
 
         summary = self.get_season(seasons).summary()
 
         climatology = self.climatology(climo_bounds)
         full_climo = self.to_dataframe()
-        subset_climo = full_climo.loc[climo_bounds[0]:climo_bounds[1]+1]
+        subset_climo = full_climo.loc[climo_bounds[0]:climo_bounds[1] + 1]
 
-        #Create composite dictionary
-        map_keys = {'all_storms':'season_storms',
-                    'named_storms':'season_named',
-                    'hurricanes':'season_hurricane',
-                    'major_hurricanes':'season_major',
-                    'ace':'season_ace',
-                   }
+        # Create composite dictionary
+        map_keys = {'all_storms': 'season_storms',
+                    'named_storms': 'season_named',
+                    'hurricanes': 'season_hurricane',
+                    'major_hurricanes': 'season_major',
+                    'ace': 'season_ace',
+                    }
         composite = {}
         for key in map_keys.keys():
 
-            #Get list from seasons
+            # Get list from seasons
             season_list = summary[map_keys.get(key)]
             season_climo = climatology[key]
             season_fullclimo = subset_climo[key].values
 
-            #Create dictionary of relevant calculations for this entry
-            composite[key] = {'list':season_list,
-                              'average':np.round(np.average(season_list),1),
-                              'composite_anomaly':np.round(np.average(season_list)-season_climo,1),
-                              'percentile_ranks':[np.round(stats.percentileofscore(season_fullclimo,i),1) for i in season_list],
-                             }
+            # Create dictionary of relevant calculations for this entry
+            composite[key] = {'list': season_list,
+                              'average': np.round(np.average(season_list), 1),
+                              'composite_anomaly': np.round(np.average(season_list) - season_climo, 1),
+                              'percentile_ranks': [np.round(stats.percentileofscore(season_fullclimo, i), 1) for i in season_list],
+                              }
 
         return composite
-    
-    def analogs_from_point(self,point,radius,units='km',thresh={},non_tropical=False,year_range=None,date_range=None):
-        
+
+    def analogs_from_point(self, point, radius, units='km', thresh={}, non_tropical=False, year_range=None, date_range=None):
         r"""
         Retrieve historical TC tracks surrounding a point.
-        
+
         Parameters
         ----------
         point : tuple
             Tuple ordered by (latitude, longitude).
         radius : int or float
             Radius in kilometers surrounding the point to search for storms.
         units : str, optional
             Units of distance for radius. Can be "miles" or "km". Default is "km".
         thresh : dict
             Dict for threshold(s) that storms within the requested radius must meet. The following options are available:
-            
+
             * **v_min** - Search for sustained wind (kt) above this threshold
             * **v_max** - Search for sustained wind (kt) below this threshold
             * **p_min** - Search for MSLP (hPa) below this threshold
             * **p_max** - Search for MSLP (hPa) above this threshold
         non_tropical : bool
             If True, non-tropical (e.g., tropical disturbance, extra-tropical cyclone) points are included in the search. Default is False.
         year_range : tuple
             Year range over which to search. If None, defaults to entire dataset.
         date_range : tuple
             Start and end dates, formatted as a "month/day" string. If None, defaults to year round.
-        
+
         Returns
         -------
         dict
             Dict of tropical cyclones that meet the criteria, with storm ID as the key and its closest distance to point as the value.
-        
+
         Notes
         -----
         This function automatically interpolates all storm data within this TrackDataset instance to hourly, if this hasn't already been done previously.
         """
-        
-        #Determine year range of dataset
-        if year_range == None:
+
+        # Determine year range of dataset
+        if year_range is None:
             start_year = self.data[self.keys[0]]['year']
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
+        elif isinstance(year_range, (list, tuple)):
             if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
+                raise ValueError(
+                    "year_range must be a tuple or list with 2 elements: (start_year, end_year)")
             start_year = int(year_range[0])
-            if start_year < self.data[self.keys[0]]['year']: start_year = self.data[self.keys[0]]['year']
+            if start_year < self.data[self.keys[0]]['year']:
+                start_year = self.data[self.keys[0]]['year']
             end_year = int(year_range[1])
-            if end_year > self.data[self.keys[-1]]['year']: end_year = self.data[self.keys[-1]]['year']
+            if end_year > self.data[self.keys[-1]]['year']:
+                end_year = self.data[self.keys[-1]]['year']
         else:
             raise TypeError("year_range must be of type tuple or list")
-        
-        #Determine date range
-        if date_range == None:
-            date_range = ('1/1','12/31')
-        
-        #Units error check
-        if units not in ['km','miles']:
+
+        # Determine date range
+        if date_range is None:
+            date_range = ('1/1', '12/31')
+
+        # Units error check
+        if units not in ['km', 'miles']:
             raise ValueError("units must be 'km' or 'miles'.")
         unit_factor = 1.0 if units == 'km' else 0.621371
-        
-        #Interpolate all storm data, if hasn't been done already
+
+        # Interpolate all storm data, if hasn't been done already
         self.__interpolate_storms(self.keys)
-        
+
         data = {}
         for key in self.keys:
-            if self.data[key]['year'] > end_year or self.data[key]['year'] < start_year: continue
-            storm_data = [[great_circle(point,(self.data_interp[key]['lat'][i],self.data_interp[key]['lon'][i])).kilometers,self.data_interp[key]['vmax'][i],self.data_interp[key]['mslp'][i],self.data_interp[key]['date'][i]] for i in range(len(self.data_interp[key]['lat'])) if self.data_interp[key]['type'][i] in constants.TROPICAL_STORM_TYPES or non_tropical == True]
-            storm_data = [i for i in storm_data if i[0] <= radius*unit_factor]
-            storm_data = [i for i in storm_data if i[3] >= dt.strptime(date_range[0],'%m/%d').replace(year=i[3].year) and i[3] <= dt.strptime(date_range[1],'%m/%d').replace(year=i[3].year)]
-            if len(storm_data) == 0: continue
+            if self.data[key]['year'] > end_year or self.data[key]['year'] < start_year:
+                continue
+            storm_data = [[great_circle(point, (self.data_interp[key]['lat'][i], self.data_interp[key]['lon'][i])).kilometers, self.data_interp[key]['vmax'][i], self.data_interp[key]['mslp'][i],
+                           self.data_interp[key]['time'][i]] for i in range(len(self.data_interp[key]['lat'])) if self.data_interp[key]['type'][i] in constants.TROPICAL_STORM_TYPES or non_tropical == True]
+            storm_data = [i for i in storm_data if i[0]
+                          <= radius * unit_factor]
+            storm_data = [i for i in storm_data if i[3] >= dt.strptime(date_range[0], '%m/%d').replace(
+                year=i[3].year) and i[3] <= (dt.strptime(date_range[1], '%m/%d') + timedelta(hours=23)).replace(year=i[3].year)]
+            if len(storm_data) == 0:
+                continue
             if 'v_min' in thresh.keys():
                 storm_data = [i for i in storm_data if i[1] >= thresh['v_min']]
             if 'v_max' in thresh.keys():
                 storm_data = [i for i in storm_data if i[1] <= thresh['v_max']]
             if 'p_min' in thresh.keys():
                 storm_data = [i for i in storm_data if i[2] >= thresh['p_min']]
             if 'p_max' in thresh.keys():
                 storm_data = [i for i in storm_data if i[2] <= thresh['p_max']]
-            if len(storm_data) == 0: continue
-            
-            data[key] = np.round(np.nanmin([i[0]*unit_factor for i in storm_data]),1)
-        
+            if len(storm_data) == 0:
+                continue
+
+            data[key] = np.round(
+                np.nanmin([i[0] * unit_factor for i in storm_data]), 1)
+
         return data
-        
-    def analogs_from_shape(self,points,thresh={},non_tropical=False,year_range=None,date_range=None):
-        
+
+    def analogs_from_shape(self, points, thresh={}, non_tropical=False, year_range=None, date_range=None):
         r"""
         Retrieve historical TC tracks within a bounded region.
-        
+
         Parameters
         ----------
         points : list
             List of tuples ordered by (latitude, longitude) corresponding to the bounded region.
         thresh : dict
             Dict for threshold(s) that storms within the requested radius must meet. The following options are available:
-            
+
             * **v_min** - Search for sustained wind (kt) above this threshold
             * **v_max** - Search for sustained wind (kt) below this threshold
             * **p_min** - Search for MSLP (hPa) below this threshold
             * **p_max** - Search for MSLP (hPa) above this threshold
         non_tropical : bool
             If True, non-tropical (e.g., tropical disturbance, extra-tropical cyclone) points are included in the search. Default is False.
         year_range : tuple
             Year range over which to search. If None, defaults to entire dataset.
         date_range : tuple
             Start and end dates, formatted as a "month/day" string. If None, defaults to year round.
-        
+
         Returns
         -------
         list
             List of tropical cyclones that meet the criteria.
-        
+
         Notes
         -----
         This function automatically interpolates all storm data within this TrackDataset instance to hourly, if this hasn't already been done previously.
         """
-        
-        #Determine year range of dataset
-        if year_range == None:
+
+        # Determine year range of dataset
+        if year_range is None:
             start_year = self.data[self.keys[0]]['year']
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
+        elif isinstance(year_range, (list, tuple)):
             if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
+                raise ValueError(
+                    "year_range must be a tuple or list with 2 elements: (start_year, end_year)")
             start_year = int(year_range[0])
-            if start_year < self.data[self.keys[0]]['year']: start_year = self.data[self.keys[0]]['year']
+            if start_year < self.data[self.keys[0]]['year']:
+                start_year = self.data[self.keys[0]]['year']
             end_year = int(year_range[1])
-            if end_year > self.data[self.keys[-1]]['year']: end_year = self.data[self.keys[-1]]['year']
+            if end_year > self.data[self.keys[-1]]['year']:
+                end_year = self.data[self.keys[-1]]['year']
         else:
             raise TypeError("year_range must be of type tuple or list")
-        
-        #Determine date range
-        if date_range == None:
-            date_range = ('1/1','12/31')
-        
-        #Interpolate all storm data, if hasn't been done already
+
+        # Determine date range
+        if date_range is None:
+            date_range = ('1/1', '12/31')
+
+        # Interpolate all storm data, if hasn't been done already
         self.__interpolate_storms(self.keys)
-        
-        #Check for last entry of tuple
-        if points[-1] != points[0]: points.append(points[0])
+
+        # Check for last entry of tuple
+        if points[-1] != points[0]:
+            points.append(points[0])
         p = path.Path(points)
-        
-        #Coerce points longitudes to -180 to 180
+
+        # Coerce points longitudes to -180 to 180
         for point in points:
-            if point[1] > 180.0: point[1] = point[1] - 360.0
-        
+            if point[1] > 180.0:
+                point[1] = point[1] - 360.0
+
         data = []
         for key in self.keys:
             lon_shift = self.data_interp[key]['lon'] + 0.0
             lon_shift[lon_shift > 180.0] = lon_shift[lon_shift > 180.0] - 360.0
-            if self.data[key]['year'] > end_year or self.data[key]['year'] < start_year: continue
-            storm_data = [[p.contains_point((self.data_interp[key]['lat'][i],lon_shift[i])),self.data_interp[key]['vmax'][i],self.data_interp[key]['mslp'][i],self.data_interp[key]['date'][i]] for i in range(len(self.data_interp[key]['lat'])) if self.data_interp[key]['type'][i] in constants.TROPICAL_STORM_TYPES or non_tropical == True]
+            if self.data[key]['year'] > end_year or self.data[key]['year'] < start_year:
+                continue
+            storm_data = [[p.contains_point((self.data_interp[key]['lat'][i], lon_shift[i])), self.data_interp[key]['vmax'][i], self.data_interp[key]['mslp'][i], self.data_interp[key]['time'][i]] for i in range(
+                len(self.data_interp[key]['lat'])) if self.data_interp[key]['type'][i] in constants.TROPICAL_STORM_TYPES or non_tropical == True]
             storm_data = [i for i in storm_data if i[0] == True]
-            storm_data = [i for i in storm_data if i[3] >= dt.strptime(date_range[0],'%m/%d').replace(year=i[3].year) and i[3] <= dt.strptime(date_range[1],'%m/%d').replace(year=i[3].year)]
-            if len(storm_data) == 0: continue
+            storm_data = [i for i in storm_data if i[3] >= dt.strptime(date_range[0], '%m/%d').replace(
+                year=i[3].year) and i[3] <= (dt.strptime(date_range[1], '%m/%d') + timedelta(hours=23)).replace(year=i[3].year)]
+            if len(storm_data) == 0:
+                continue
             if 'v_min' in thresh.keys():
                 storm_data = [i for i in storm_data if i[1] >= thresh['v_min']]
             if 'v_max' in thresh.keys():
                 storm_data = [i for i in storm_data if i[1] <= thresh['v_max']]
             if 'p_min' in thresh.keys():
                 storm_data = [i for i in storm_data if i[2] >= thresh['p_min']]
             if 'p_max' in thresh.keys():
                 storm_data = [i for i in storm_data if i[2] <= thresh['p_max']]
-            if len(storm_data) == 0: continue
-                
+            if len(storm_data) == 0:
+                continue
+
             data.append(key)
-        
+
         return data
-        
-    def plot_analogs_from_point(self,point,radius,units='km',thresh={},non_tropical=False,year_range=None,date_range=None,**kwargs):
-        
+
+    def plot_analogs_from_point(self, point, radius, units='km', thresh={}, non_tropical=False, year_range=None, date_range=None, **kwargs):
         r"""
         Plot historical TC tracks surrounding a point.
-        
+
         Parameters
         ----------
         point : tuple
             Tuple ordered by (latitude, longitude).
         radius : int or float
             Radius in kilometers surrounding the point to search for storms.
         units : str, optional
             Units of distance for radius. Can be "miles" or "km". Default is "km".
         thresh : dict
             Dict for threshold(s) that storms within the requested radius must meet. The following options are available:
-            
+
             * **v_min** - Search for sustained wind (kt) above this threshold
             * **v_max** - Search for sustained wind (kt) below this threshold
             * **p_min** - Search for MSLP (hPa) below this threshold
             * **p_max** - Search for MSLP (hPa) above this threshold
         non_tropical : bool
             If True, non-tropical (e.g., tropical disturbance, extra-tropical cyclone) points are included in the search. Default is False.
         year_range : tuple
             Year range over which to search. If None, defaults to entire dataset.
         date_range : tuple
             Start and end dates, formatted as a "month/day" string. If None, defaults to year round.
-        
+
         Other Parameters
         ----------------
         **kwargs
             Refer to ``tropycal.tracks.TrackDataset.plot_storms`` for plotting keyword arguments.
-        
+
         Returns
         -------
         ax
             Axes instance of the plot.
-        
+
         Notes
         -----
         This function automatically interpolates all storm data within this TrackDataset instance to hourly, if this hasn't already been done previously.
         """
-        
-        #Determine year range of dataset
-        if year_range == None:
+
+        # Determine year range of dataset
+        if year_range is None:
             start_year = self.data[self.keys[0]]['year']
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
+        elif isinstance(year_range, (list, tuple)):
             if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
+                raise ValueError(
+                    "year_range must be a tuple or list with 2 elements: (start_year, end_year)")
             start_year = int(year_range[0])
-            if start_year < self.data[self.keys[0]]['year']: start_year = self.data[self.keys[0]]['year']
+            if start_year < self.data[self.keys[0]]['year']:
+                start_year = self.data[self.keys[0]]['year']
             end_year = int(year_range[1])
-            if end_year > self.data[self.keys[-1]]['year']: end_year = self.data[self.keys[-1]]['year']
+            if end_year > self.data[self.keys[-1]]['year']:
+                end_year = self.data[self.keys[-1]]['year']
         else:
             raise TypeError("year_range must be of type tuple or list")
-        
-        #Determine date range
-        if date_range == None:
-            date_range = ('1/1','12/31')
-        
-        #Reconfigure domain to be centered around circle
+
+        # Determine date range
+        if date_range is None:
+            date_range = ('1/1', '12/31')
+
+        # Reconfigure domain to be centered around circle
         import cartopy.geodesic as geodesic
         unit_factor = 1.0 if units == 'km' else 0.621371
-        circle_points = geodesic.Geodesic().circle(lon=point[1],lat=point[0],radius=radius*1000*unit_factor,n_samples=360,endpoint=False)
+        circle_points = geodesic.Geodesic().circle(
+            lon=point[1], lat=point[0], radius=radius * 1000 * unit_factor, n_samples=360, endpoint=False)
         domain = kwargs.pop('domain', None)
-        if domain == None:
+        if domain is None:
             lons = [i[0] for i in circle_points]
             lats = [i[1] for i in circle_points]
-            bounds = dynamic_map_extent(np.nanmin(lons),np.nanmax(lons),np.nanmin(lats),np.nanmax(lats))
-            kwargs['domain'] = {'w':bounds[0],'e':bounds[1],'s':bounds[2],'n':bounds[3]}
+            bounds = dynamic_map_extent(np.nanmin(lons), np.nanmax(
+                lons), np.nanmin(lats), np.nanmax(lats))
+            kwargs['domain'] = {'w': bounds[0],
+                                'e': bounds[1], 's': bounds[2], 'n': bounds[3]}
         else:
             kwargs['domain'] = domain
-        
-        #Retrieve storms and plot on axes
-        storms = self.analogs_from_point(point,radius,units,thresh,non_tropical,year_range,date_range).keys()
-        ax = self.plot_storms(storms,**kwargs)
-        
-        #Plot circle and dot
+
+        # Retrieve storms and plot on axes
+        storms = self.analogs_from_point(
+            point, radius, units, thresh, non_tropical, year_range, date_range).keys()
+        ax = self.plot_storms(storms, **kwargs)
+
+        # Plot circle and dot
         import cartopy
         import shapely
         ms = 12
         linewidth = 2.5
         color = 'k'
-        ax.plot(point[1],point[0],'o',mfc=color,mec=color,ms=ms,zorder=30)
+        ax.plot(point[1], point[0], 'o', mfc=color,
+                mec=color, ms=ms, zorder=30)
         geom = shapely.geometry.Polygon(circle_points)
-        ax.add_geometries((geom,), crs=cartopy.crs.PlateCarree(), facecolor='none', edgecolor=color, linewidth=linewidth, zorder=30)
-        
-        #Change title
-        title = kwargs.pop('title','')
+        ax.add_geometries((geom,), crs=cartopy.crs.PlateCarree(
+        ), facecolor='none', edgecolor=color, linewidth=linewidth, zorder=30)
+
+        # Change title
+        title = kwargs.pop('title', '')
         if title == '':
             endash = u"\u2013"
             dot = u"\u2022"
             degree_sign = u'\N{DEGREE SIGN}'
             lat_formatter = f"{point[0]:.1f}{degree_sign}N"
-            if point[0] < 0: lat_formatter = f"{abs(point[0]):.1f}{degree_sign}S"
+            if point[0] < 0:
+                lat_formatter = f"{abs(point[0]):.1f}{degree_sign}S"
             lon_formatter = f"{point[1]:.1f}{degree_sign}E"
-            if point[1] < 0: lon_formatter = f"{abs(point[1]):.1f}{degree_sign}W"
-            if point[1] > 180: lon_formatter = f"{abs(point[1]-360.0):.1f}{degree_sign}W"
-            start_day = dt.strptime(date_range[0],'%m/%d').strftime('%b %d')
-            end_day = dt.strptime(date_range[1],'%m/%d').strftime('%b %d')
-            ax.set_title(f"TCs Within {radius} {units} of {lat_formatter}, {lon_formatter}",loc='left',fontsize=17,fontweight='bold')
-            ax.set_title(f"Number of storms: {len(storms)}\n{start_day} {endash} {end_day} {dot} {start_year} {endash} {end_year}",loc='right',fontsize=13)
-        
+            if point[1] < 0:
+                lon_formatter = f"{abs(point[1]):.1f}{degree_sign}W"
+            if point[1] > 180:
+                lon_formatter = f"{abs(point[1]-360.0):.1f}{degree_sign}W"
+            start_day = dt.strptime(date_range[0], '%m/%d').strftime('%b %d')
+            end_day = dt.strptime(date_range[1], '%m/%d').strftime('%b %d')
+            ax.set_title(f"TCs Within {radius} {units} of {lat_formatter}, {lon_formatter}",
+                         loc='left', fontsize=17, fontweight='bold')
+            ax.set_title(
+                f"Number of storms: {len(storms)}\n{start_day} {endash} {end_day} {dot} {start_year} {endash} {end_year}", loc='right', fontsize=13)
+
         return ax
 
-    def plot_analogs_from_shape(self,points,thresh={},non_tropical=False,year_range=None,date_range=None,**kwargs):
-        
+    def plot_analogs_from_shape(self, points, thresh={}, non_tropical=False, year_range=None, date_range=None, **kwargs):
         r"""
         Plot historical TC tracks surrounding a point.
-        
+
         Parameters
         ----------
         points : list
             List of tuples ordered by (latitude, longitude) corresponding to the bounded region.
         thresh : dict
             Dict for threshold(s) that storms within the requested radius must meet. The following options are available:
-            
+
             * **v_min** - Search for sustained wind (kt) above this threshold
             * **v_max** - Search for sustained wind (kt) below this threshold
             * **p_min** - Search for MSLP (hPa) below this threshold
             * **p_max** - Search for MSLP (hPa) above this threshold
         non_tropical : bool
             If True, non-tropical (e.g., tropical disturbance, extra-tropical cyclone) points are included in the search. Default is False.
         year_range : tuple
             Year range over which to search. If None, defaults to entire dataset.
         date_range : tuple
             Start and end dates, formatted as a "month/day" string. If None, defaults to year round.
-        
+
         Other Parameters
         ----------------
         linewidth : int or float
             Width of bounded shape line. Defaults to 2.0.
         color : str
             Color of bounded shape line. Defaults to black.
         **kwargs
             Refer to ``tropycal.tracks.TrackDataset.plot_storms`` for plotting keyword arguments.
-        
+
         Returns
         -------
         ax
             Axes instance of the plot.
-        
+
         Notes
         -----
         This function automatically interpolates all storm data within this TrackDataset instance to hourly, if this hasn't already been done previously.
         """
-        
-        #Determine year range of dataset
-        if year_range == None:
+
+        # Determine year range of dataset
+        if year_range is None:
             start_year = self.data[self.keys[0]]['year']
             end_year = self.data[self.keys[-1]]['year']
-        elif isinstance(year_range,(list,tuple)):
+        elif isinstance(year_range, (list, tuple)):
             if len(year_range) != 2:
-                raise ValueError("year_range must be a tuple or list with 2 elements: (start_year, end_year)")
+                raise ValueError(
+                    "year_range must be a tuple or list with 2 elements: (start_year, end_year)")
             start_year = int(year_range[0])
-            if start_year < self.data[self.keys[0]]['year']: start_year = self.data[self.keys[0]]['year']
+            if start_year < self.data[self.keys[0]]['year']:
+                start_year = self.data[self.keys[0]]['year']
             end_year = int(year_range[1])
-            if end_year > self.data[self.keys[-1]]['year']: end_year = self.data[self.keys[-1]]['year']
+            if end_year > self.data[self.keys[-1]]['year']:
+                end_year = self.data[self.keys[-1]]['year']
         else:
             raise TypeError("year_range must be of type tuple or list")
-        
-        #Determine date range
-        if date_range == None:
-            date_range = ('1/1','12/31')
-        
-        #Reconfigure domain to be centered around circle
+
+        # Determine date range
+        if date_range is None:
+            date_range = ('1/1', '12/31')
+
+        # Reconfigure domain to be centered around circle
         domain = kwargs.pop('domain', None)
-        if domain == None:
+        if domain is None:
             lons = [i[1] for i in points]
             lats = [i[0] for i in points]
-            bounds = dynamic_map_extent(np.nanmin(lons),np.nanmax(lons),np.nanmin(lats),np.nanmax(lats))
-            kwargs['domain'] = {'w':bounds[0],'e':bounds[1],'s':bounds[2],'n':bounds[3]}
+            bounds = dynamic_map_extent(np.nanmin(lons), np.nanmax(
+                lons), np.nanmin(lats), np.nanmax(lats))
+            kwargs['domain'] = {'w': bounds[0],
+                                'e': bounds[1], 's': bounds[2], 'n': bounds[3]}
         else:
             kwargs['domain'] = domain
-        
-        #Retrieve storms and plot on axes
-        storms = self.analogs_from_shape(points,thresh,non_tropical,year_range,date_range)
-        ax = self.plot_storms(storms,**kwargs)
-        
-        #Plot circle and dot
+
+        # Retrieve storms and plot on axes
+        storms = self.analogs_from_shape(
+            points, thresh, non_tropical, year_range, date_range)
+        ax = self.plot_storms(storms, **kwargs)
+
+        # Plot circle and dot
         import cartopy.crs as ccrs
         linewidth = 3.0
         color = 'k'
-        if points[-1] != points[0]: points.append(points[0])
-        ax.plot([i[1] for i in points],[i[0] for i in points],color=color,linewidth=linewidth,zorder=30,transform=ccrs.PlateCarree())
-        
-        #Change title
-        title = kwargs.pop('title','')
+        if points[-1] != points[0]:
+            points.append(points[0])
+        ax.plot([i[1] for i in points], [i[0] for i in points], color=color,
+                linewidth=linewidth, zorder=30, transform=ccrs.PlateCarree())
+
+        # Change title
+        title = kwargs.pop('title', '')
         if title == '':
             endash = u"\u2013"
             dot = u"\u2022"
-            start_day = dt.strptime(date_range[0],'%m/%d').strftime('%b %d')
-            end_day = dt.strptime(date_range[1],'%m/%d').strftime('%b %d')
-            ax.set_title(f"TC Tracks Within Bounded Region",loc='left',fontsize=17,fontweight='bold')
-            ax.set_title(f"Number of storms: {len(storms)}\n{start_day} {endash} {end_day} {dot} {start_year} {endash} {end_year}",loc='right',fontsize=13)
-        
+            start_day = dt.strptime(date_range[0], '%m/%d').strftime('%b %d')
+            end_day = dt.strptime(date_range[1], '%m/%d').strftime('%b %d')
+            ax.set_title(f"TC Tracks Within Bounded Region",
+                         loc='left', fontsize=17, fontweight='bold')
+            ax.set_title(
+                f"Number of storms: {len(storms)}\n{start_day} {endash} {end_day} {dot} {start_year} {endash} {end_year}", loc='right', fontsize=13)
+
         return ax
 
-    def plot_summary(self,time,domain=None,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+    def plot_summary(self, time, domain=None, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Plot a summary map of past tropical cyclone and NHC potential development activity. Only valid for areas in NHC's area of responsibility at this time.
-        
+
         Parameters
         ----------
         time : datetime
             Valid time for the summary plot.
         domain : str
             Domain for the plot. Default is current basin. Please refer to :ref:`options-domain` for available domain options.
         ax : axes, optional
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs, optional
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str, optional
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         two_prop : dict
             Customization properties of NHC Tropical Weather Outlook (TWO). Please refer to :ref:`options-summary` for available options.
         storm_prop : dict
             Customization properties of active storms. Please refer to :ref:`options-summary` for available options.
         cone_prop : dict
             Customization properties of cone of uncertainty. Please refer to :ref:`options-summary` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
 
         The following properties are available for plotting NHC Tropical Weather Outlook (TWO), via ``two_prop``.
 
         .. list-table:: 
            :widths: 25 75
@@ -3944,195 +4480,296 @@
            * - fillcolor
              - Fill color for forecast dots. Default is color by SSHWS category ("category").
            * - label_category
              - Boolean for whether to plot SSHWS category on top of forecast dots. Default is True.
            * - ms
              - Marker size for forecast dots. Default is 12.
         """
-        
-        #Error check
-        if self.source != 'hurdat':
-            raise RuntimeError("This function is only available for NHC's area of responsibility at this time.")
-        
-        #Set basin
-        if domain == None:
+
+        # Set basin
+        if domain is None:
             domain = self.basin
-        
-        #Find closest NHC shapefile
+
+        # Find closest NHC shapefile
         shapefiles = get_two_archive(time)
-        if shapefiles['areas'] == None:
-            two_prop = {'plot':False}
+        if shapefiles['areas'] is None:
+            two_prop = {'plot': False}
         else:
-            two_prop = kwargs.pop('two_prop',{})
-        
-        #Search all valid storms at the time
+            two_prop = kwargs.pop('two_prop', {})
+
+        # Search all valid storms at the time
         print("--> Reading storm data")
         storms = []
         forecasts = []
         for key in self.keys:
-            
-            #First filter
-            if time < self.data[key]['date'][0]: continue
-            if self.data[key]['date'][-1] < time: continue
-           
-            #Second filter
-            diff = [(time-i).total_seconds()/3600 for i in self.data[key]['date']]
-            diff_maxes = [i for i in diff if i >= 0]
-            idx = diff.index(np.nanmin(diff_maxes))
-            
-            #Get forecast
+
+            # Temporal filter
+            if time < self.data[key]['time'][0]:
+                continue
+            if self.data[key]['time'][-1] < time:
+                continue
+
+            # Filter for ibtracs sources (use best track instead of operational track)
+            if self.source != 'hurdat':
+                diff = [(time - i).total_seconds() /
+                        3600 for i in self.data[key]['time']]
+                diff_maxes = [i for i in diff if i >= 0]
+                if len(diff_maxes) == 0:
+                    continue
+                idx = diff.index(np.nanmin(diff_maxes))
+                if np.nanmin(diff) > 0:
+                    continue
+                if diff[idx] < (-9.0 / 24.0):
+                    continue
+                if diff[idx] > (9.0 / 24.0):
+                    continue
+                storm = self.get_storm(key).sel(
+                    time=(self.data[key]['time'][0], time))
+                if True in np.isin(list(storm.type), list(constants.TROPICAL_STORM_TYPES)):
+                    storms.append(storm)
+                continue
+
+            # Get forecast
             storm = self.get_storm(key)
             storm.get_operational_forecasts()
 
-            #Get all NHC forecast entries
+            # Get all NHC forecast entries
             nhc_forecasts = storm.forecast_dict['OFCL']
             carq_forecasts = storm.forecast_dict['CARQ']
 
-            #Get list of all NHC forecast initializations
+            # Get list of all NHC forecast initializations
             nhc_forecast_init = [k for k in nhc_forecasts.keys()]
             carq_forecast_init = [k for k in carq_forecasts.keys()]
 
-            #Find closest matching time to the provided forecast date, or time
-            nhc_forecast_init_dt = [dt.strptime(k,'%Y%m%d%H') if 3 not in nhc_forecasts[k]['fhr'] else dt.strptime(k,'%Y%m%d%H')+timedelta(hours=3) for k in nhc_forecast_init]
-            time_diff = [(i-time).days + (i-time).seconds/86400 for i in nhc_forecast_init_dt]            
+            # Find closest matching time to the provided forecast date, or time
+            nhc_forecast_init_dt = [dt.strptime(k, '%Y%m%d%H') if 3 not in nhc_forecasts[k]['fhr'] else dt.strptime(
+                k, '%Y%m%d%H') + timedelta(hours=3) for k in nhc_forecast_init]
+            time_diff = [(i - time).days + (i - time).seconds /
+                         86400 for i in nhc_forecast_init_dt]
             time_diff = np.array([i for i in time_diff if i <= 0.0])
-            if len(time_diff) == 0: continue
+            if len(time_diff) == 0:
+                continue
             closest_idx = np.abs(time_diff).argmin()
             forecast_dict = nhc_forecasts[nhc_forecast_init[closest_idx]]
-            advisory_num = closest_idx+1
-            
-            #Second filter
-            if np.nanmin(time_diff) > 0: continue
-            if time_diff[closest_idx] < (-9.0/24.0): continue
-            if time_diff[closest_idx] > (9.0/24.0): continue
+            advisory_num = closest_idx + 1
+
+            # Second filter
+            if np.nanmin(time_diff) > 0:
+                continue
+            if time_diff[closest_idx] < (-9.0 / 24.0):
+                continue
+            if time_diff[closest_idx] > (9.0 / 24.0):
+                continue
 
-            #Get observed track as per NHC analyses
-            track_dict = {'lat':[],'lon':[],'vmax':[],'type':[],'mslp':[],'date':[],'extra_obs':[],'special':[],'ace':0.0}
+            # Get observed track as per NHC analyses
+            track_dict = {'lat': [], 'lon': [], 'vmax': [], 'type': [], 'mslp': [
+            ], 'time': [], 'extra_obs': [], 'special': [], 'ace': 0.0}
             use_carq = True
             for k in nhc_forecast_init:
-                if k not in carq_forecasts.keys(): continue
-                if carq_forecasts[k]['init'] > time: continue
+                if k not in carq_forecasts.keys():
+                    continue
+                if carq_forecasts[k]['init'] > time:
+                    continue
                 hrs = nhc_forecasts[k]['fhr']
-                hrs_carq = carq_forecasts[k]['fhr'] if k in carq_forecast_init else []
+                hrs_carq = carq_forecasts[k]['fhr'] if k in carq_forecast_init else [
+                ]
 
-                #Account for old years when hour 0 wasn't included directly
-                #if 0 not in hrs and k in carq_forecast_init and 0 in hrs_carq:
+                # Account for old years when hour 0 wasn't included directly
+                # if 0 not in hrs and k in carq_forecast_init and 0 in hrs_carq:
                 if storm.dict['year'] < 2000 and k in carq_forecast_init and 0 in hrs_carq:
 
                     use_carq = True
                     hr_idx = hrs_carq.index(0)
                     track_dict['lat'].append(carq_forecasts[k]['lat'][hr_idx])
                     track_dict['lon'].append(carq_forecasts[k]['lon'][hr_idx])
-                    track_dict['vmax'].append(carq_forecasts[k]['vmax'][hr_idx])
+                    track_dict['vmax'].append(
+                        carq_forecasts[k]['vmax'][hr_idx])
                     track_dict['mslp'].append(np.nan)
-                    track_dict['date'].append(carq_forecasts[k]['init'])
+                    track_dict['time'].append(carq_forecasts[k]['init'])
 
                     itype = carq_forecasts[k]['type'][hr_idx]
-                    if itype == "": itype = get_storm_type(carq_forecasts[k]['vmax'][0],False)
+                    if itype == "":
+                        itype = get_storm_type(
+                            carq_forecasts[k]['vmax'][0], False)
                     track_dict['type'].append(itype)
 
                     hr = carq_forecasts[k]['init'].strftime("%H%M")
-                    track_dict['extra_obs'].append(0) if hr in ['0300','0900','1500','2100'] else track_dict['extra_obs'].append(1)
+                    track_dict['extra_obs'].append(0) if hr in [
+                        '0300', '0900', '1500', '2100'] else track_dict['extra_obs'].append(1)
                     track_dict['special'].append("")
 
                 else:
                     use_carq = False
                     if 3 in hrs:
                         hr_idx = hrs.index(3)
                         hr_add = 3
                     else:
                         hr_idx = 0
                         hr_add = 0
-                    if nhc_forecasts[k]['init']+timedelta(hours=hr_add) > time: continue
+                    if nhc_forecasts[k]['init'] + timedelta(hours=hr_add) > time:
+                        continue
                     track_dict['lat'].append(nhc_forecasts[k]['lat'][hr_idx])
                     track_dict['lon'].append(nhc_forecasts[k]['lon'][hr_idx])
                     track_dict['vmax'].append(nhc_forecasts[k]['vmax'][hr_idx])
                     track_dict['mslp'].append(np.nan)
-                    track_dict['date'].append(nhc_forecasts[k]['init']+timedelta(hours=hr_add))
+                    track_dict['time'].append(
+                        nhc_forecasts[k]['init'] + timedelta(hours=hr_add))
 
                     itype = nhc_forecasts[k]['type'][hr_idx]
-                    if itype == "": itype = get_storm_type(nhc_forecasts[k]['vmax'][0],False)
+                    if itype == "":
+                        itype = get_storm_type(
+                            nhc_forecasts[k]['vmax'][0], False)
                     track_dict['type'].append(itype)
 
                     hr = nhc_forecasts[k]['init'].strftime("%H%M")
-                    track_dict['extra_obs'].append(0) if hr in ['0300','0900','1500','2100'] else track_dict['extra_obs'].append(1)
+                    track_dict['extra_obs'].append(0) if hr in [
+                        '0300', '0900', '1500', '2100'] else track_dict['extra_obs'].append(1)
                     track_dict['special'].append("")
 
-            #Add main elements from storm dict
-            for key in ['id','operational_id','name','year']:
+            # Add main elements from storm dict
+            for key in ['id', 'operational_id', 'name', 'year']:
                 track_dict[key] = storm.dict[key]
 
-            #Add carq to forecast dict as hour 0, if available
-            if use_carq == True and forecast_dict['init'] in track_dict['date']:
-                insert_idx = track_dict['date'].index(forecast_dict['init'])
+            # Add carq to forecast dict as hour 0, if available
+            if use_carq and forecast_dict['init'] in track_dict['time']:
+                insert_idx = track_dict['time'].index(forecast_dict['init'])
                 if 0 in forecast_dict['fhr']:
                     forecast_dict['lat'][0] = track_dict['lat'][insert_idx]
                     forecast_dict['lon'][0] = track_dict['lon'][insert_idx]
                     forecast_dict['vmax'][0] = track_dict['vmax'][insert_idx]
                     forecast_dict['mslp'][0] = track_dict['mslp'][insert_idx]
                     forecast_dict['type'][0] = track_dict['type'][insert_idx]
                 else:
-                    forecast_dict['fhr'].insert(0,0)
-                    forecast_dict['lat'].insert(0,track_dict['lat'][insert_idx])
-                    forecast_dict['lon'].insert(0,track_dict['lon'][insert_idx])
-                    forecast_dict['vmax'].insert(0,track_dict['vmax'][insert_idx])
-                    forecast_dict['mslp'].insert(0,track_dict['mslp'][insert_idx])
-                    forecast_dict['type'].insert(0,track_dict['type'][insert_idx])
-            
-            #Fix forecast dict if hour 3 is available
-            if use_carq == False and 3 in forecast_dict['fhr']:
+                    forecast_dict['fhr'].insert(0, 0)
+                    forecast_dict['lat'].insert(
+                        0, track_dict['lat'][insert_idx])
+                    forecast_dict['lon'].insert(
+                        0, track_dict['lon'][insert_idx])
+                    forecast_dict['vmax'].insert(
+                        0, track_dict['vmax'][insert_idx])
+                    forecast_dict['mslp'].insert(
+                        0, track_dict['mslp'][insert_idx])
+                    forecast_dict['type'].insert(
+                        0, track_dict['type'][insert_idx])
+
+            # Fix forecast dict if hour 3 is available
+            if not use_carq and 3 in forecast_dict['fhr']:
                 idx_3 = forecast_dict['fhr'].index(3)
-                for iter_key in ['fhr','lat','lon','vmax','mslp','type']:
+                for iter_key in ['fhr', 'lat', 'lon', 'vmax', 'mslp', 'type']:
                     forecast_dict[iter_key] = forecast_dict[iter_key][idx_3:]
                 forecast_dict['fhr'][0] = 0
-            
-            #Add other info to forecast dict
+
+            # Add other info to forecast dict
             forecast_dict['advisory_num'] = advisory_num
             forecast_dict['basin'] = storm.basin
-            
-            #Append to storms
+
+            # Append to storms
             track_dict['prob_2day'] = 'N/A'
             track_dict['risk_2day'] = 'N/A'
-            track_dict['prob_5day'] = 'N/A'
-            track_dict['risk_5day'] = 'N/A'
-            
-            #Fill empty observed dicts (assuming storm just formed)
+            track_dict['prob_7day'] = 'N/A'
+            track_dict['risk_7day'] = 'N/A'
+
+            # Fill empty observed dicts (assuming storm just formed)
             if len(track_dict['lat']) == 0:
-                for iter_key in ['lat','lon','vmax','mslp','type']:
+                for iter_key in ['lat', 'lon', 'vmax', 'mslp', 'type']:
                     track_dict[iter_key] = [forecast_dict[iter_key][0]]
-            
-            #Fix name if applicable
+
+            # Fix name if applicable
             if np.nanmax(track_dict['vmax']) < 34:
                 if track_dict['operational_id'] != '':
-                    track_dict['name'] = num_to_text(int(track_dict['operational_id'][2:4])).upper()
+                    track_dict['name'] = num_to_text(
+                        int(track_dict['operational_id'][2:4])).upper()
                 else:
-                    track_dict['name'] = num_to_text(int(track_dict['id'][2:4])).upper()
-            
+                    track_dict['name'] = num_to_text(
+                        int(track_dict['id'][2:4])).upper()
+
             track_dict['basin'] = storm.basin
             storms.append(Storm(track_dict))
             forecasts.append(forecast_dict)
-        
-        #Retrieve kwargs
-        invest_prop = {'plot':False}
-        storm_prop = kwargs.pop('storm_prop',{})
-        cone_prop = kwargs.pop('cone_prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Create instance of plot object
+
+        # Retrieve kwargs
+        invest_prop = {'plot': False}
+        storm_prop = kwargs.pop('storm_prop', {})
+        cone_prop = kwargs.pop('cone_prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0) #0.0
-        
-        #Plot
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)  # 0.0
+
+        # Plot
         print("--> Generating plot")
-        ax = self.plot_obj.plot_summary(storms,forecasts,shapefiles,time,domain,
-                                        ax,save_path,two_prop,invest_prop,storm_prop,cone_prop,map_prop)
-        
+        ax = self.plot_obj.plot_summary(storms, forecasts, shapefiles, time, domain,
+                                        ax, save_path, two_prop, invest_prop, storm_prop, cone_prop, map_prop)
+
         return ax
-        
+
+    def search_time(self, time, extratropical=False, vmin=None, pmax=None):
+        r"""
+        Searches for all storms active during the requested time.
+
+        Parameters
+        ----------
+        time : datetime.datetime
+            Datetime object denoting the requested time.
+        extratropical : bool, optional
+            If False, search excludes extratropical cyclones during the requested time. Default is False.
+        vmin : int, optional
+            Minimum threshold for sustained wind (knots) to filter.
+        pmax : int, optional
+            Maximum threshold for cyclone minimum MSLP (hPa) to filter.
+
+        Returns
+        -------
+        pandas.DataFrame
+            DataFrame containing the storm IDs found during the requested time and additional relevant storm attributes.
+        """
+
+        # Iterate over all keys
+        return_data = {'id': [], 'name': [], 'lat': [], 'lon': [],
+                       'vmax': [], 'mslp': [], 'type': [], 'wmo_basin': []}
+        for key in self.keys:
+
+            # First filter
+            if time < self.data[key]['time'][0]:
+                continue
+            if self.data[key]['time'][-1] < time:
+                continue
+            diff = [(time - i).total_seconds() /
+                    3600 for i in self.data[key]['time']]
+            diff_maxes = [i for i in diff if i >= 0]
+            if len(diff_maxes) == 0:
+                continue
+            idx = diff.index(np.nanmin(diff_maxes))
+            if np.nanmin(diff) > 0:
+                continue
+            if diff[idx] >= 6.0:
+                continue
+
+            # Second filter
+            if not extratropical and self.data[key]['type'][idx] not in constants.TROPICAL_STORM_TYPES:
+                continue
+
+            # Third filter
+            if vmin is not None:
+                if np.isnan(self.data[key]['vmax'][idx]) or self.data[key]['vmax'][idx] < vmin:
+                    continue
+            if pmax is not None:
+                if np.isnan(self.data[key]['mslp'][idx]) or self.data[key]['mslp'][idx] > pmax:
+                    continue
+
+            # Return data
+            return_data['id'].append(key)
+            return_data['name'].append(self.data[key]['name'])
+            for iter_key in [k for k in return_data.keys()][2:]:
+                return_data[iter_key].append(self.data[key][iter_key][idx])
+
+        return pd.DataFrame(return_data)
```

### Comparing `tropycal-0.6.1/src/tropycal/tracks/plot.py` & `tropycal-1.0/src/tropycal/tracks/plot.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,55 +1,52 @@
-import os, sys
-import calendar
+import os
 import numpy as np
 import scipy.interpolate as interp
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 import scipy.ndimage as ndimage
 import networkx as nx
-from scipy.ndimage import gaussian_filter as gfilt
 
-#Import internal scripts
+# Import internal scripts
 from ..plot import Plot
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 from .. import constants
 
 try:
     import cartopy.feature as cfeature
     from cartopy import crs as ccrs
-    from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
-except:
-    warnings.warn("Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Cartopy is not installed in your python environment. Plotting functions will not work.")
 
 try:
     import matplotlib.colors as mcolors
     import matplotlib.lines as mlines
     import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
     import matplotlib.patches as mpatches
-    from matplotlib.offsetbox import TextArea, DrawingArea, OffsetImage, AnnotationBbox
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
 
-def plot_dot(ax,lon,lat,date,vmax,i_type,zorder,storm_data,prop,i):
 
+def plot_dot(ax, lon, lat, time, vmax, i_type, zorder, storm_data, prop, i):
     r"""
     Plot a dot on the map per user settings.
 
     Parameters
     ----------
     lon : int, float
         Longitude of the dot
     lat : int, float
         Latitude of the dot
-    date : datetime.datetime
+    time : datetime.datetime
         Datetime object corresponding to the time of the dot in UTC
     vmax : int, float
         Sustained wind in knots
     i_type : str
         Storm type
 
     Other Parameters
@@ -62,204 +59,244 @@
         Dictionary containing plot properties.
 
     Returns
     -------
     segmented_colors : bool
         Information for colorbar generation on whether a segmented colormap was used or not.
     """
-    
-    #Return cmap & levels if needed
+
+    # Return cmap & levels if needed
     extra = {}
 
-    #Determine fill color, with SSHWS scale used as default
+    # Determine fill color, with SSHWS scale used as default
     if prop['fillcolor'] == 'category':
         segmented_colors = True
         fill_color = get_colors_sshws(np.nan_to_num(vmax))
 
-    #Use user-defined colormap if another storm variable
-    elif isinstance(prop['fillcolor'],str) == True and prop['fillcolor'] in ['vmax','mslp','dvmax_dt','speed']:
+    # Use user-defined colormap if another storm variable
+    elif isinstance(prop['fillcolor'], str) and prop['fillcolor'] in ['vmax', 'mslp', 'dvmax_dt', 'speed']:
         segmented_colors = True
         color_variable = storm_data[prop['fillcolor']]
-        if prop['levels'] is None: #Auto-determine color levels if needed
-            prop['levels'] = [np.nanmin(color_variable),np.nanmax(color_variable)]
-        cmap,levels = get_cmap_levels(prop['fillcolor'],prop['cmap'],prop['levels'])
-        fill_color = cmap((color_variable-min(levels))/(max(levels)-min(levels)))[i]
+        if prop['levels'] is None:  # Auto-determine color levels if needed
+            prop['levels'] = [
+                np.nanmin(color_variable), np.nanmax(color_variable)]
+        cmap, levels = get_cmap_levels(
+            prop['fillcolor'], prop['cmap'], prop['levels'])
+        fill_color = cmap((color_variable - min(levels)) /
+                          (max(levels) - min(levels)))[i]
         extra['cmap'] = cmap
         extra['levels'] = levels
 
-    #Otherwise go with user input as is
+    # Otherwise go with user input as is
     else:
         segmented_colors = False
         fill_color = prop['fillcolor']
 
-    #Determine dot type
+    # Determine dot type
     marker_type = '^'
     if i_type in constants.SUBTROPICAL_ONLY_STORM_TYPES:
         marker_type = 's'
     elif i_type in constants.TROPICAL_ONLY_STORM_TYPES:
         marker_type = 'o'
 
-    #Plot marker
-    ax.plot(lon,lat,marker_type,mfc=fill_color,mec='k',mew=0.5,
-                 zorder=zorder,ms=prop['ms'],transform=ccrs.PlateCarree())
+    # Plot marker
+    ax.plot(lon, lat, marker_type, mfc=fill_color, mec='k', mew=0.5,
+            zorder=zorder, ms=prop['ms'], transform=ccrs.PlateCarree())
 
     return ax, segmented_colors, extra
 
-def rgb(self,rgb):
-    r,g,b = rgb
-    r = int(r)
-    g = int(g)
-    b = int(b)
-    return '#%02x%02x%02x' % (r, g, b)
-
-def add_legend(ax,fig,prop,segmented_colors,levels=None,cmap=None,storm=None):
-
-    #Linecolor category with dots
-    if prop['fillcolor'] == 'category' and prop['dots'] == True:
-        ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color='w')
-        sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color='w')
-        td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical Depression', marker='o', color=get_colors_sshws(33))
-        ts = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical Storm', marker='o', color=get_colors_sshws(34))
-        c1 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 1', marker='o', color=get_colors_sshws(64))
-        c2 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 2', marker='o', color=get_colors_sshws(83))
-        c3 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 3', marker='o', color=get_colors_sshws(96))
-        c4 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 4', marker='o', color=get_colors_sshws(113))
-        c5 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 5', marker='o', color=get_colors_sshws(137))
-        l = ax.legend(handles=[ex,sb,td,ts,c1,c2,c3,c4,c5], prop={'size':11.5}, loc=1)
+
+def add_legend(ax, fig, prop, segmented_colors, levels=None, cmap=None, storm=None):
+
+    # Linecolor category with dots
+    if prop['fillcolor'] == 'category' and prop['dots']:
+        ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                           mec='k', mew=0.5, label='Non-Tropical', marker='^', color='w')
+        sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                           mec='k', mew=0.5, label='Subtropical', marker='s', color='w')
+        td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k', mew=0.5,
+                           label='Tropical Depression', marker='o', color=get_colors_sshws(33))
+        ts = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Tropical Storm', marker='o', color=get_colors_sshws(34))
+        c1 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Category 1', marker='o', color=get_colors_sshws(64))
+        c2 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Category 2', marker='o', color=get_colors_sshws(83))
+        c3 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Category 3', marker='o', color=get_colors_sshws(96))
+        c4 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Category 4', marker='o', color=get_colors_sshws(113))
+        c5 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Category 5', marker='o', color=get_colors_sshws(137))
+        l = ax.legend(handles=[ex, sb, td, ts, c1, c2,
+                      c3, c4, c5], prop={'size': 11.5}, loc=1)
         l.set_zorder(1001)
 
-    #Linecolor category without dots
-    elif prop['linecolor'] == 'category' and prop['dots'] == False:
-        ex = mlines.Line2D([], [], linestyle='dotted', label='Non-Tropical', color='k')
-        td = mlines.Line2D([], [], linestyle='solid', label='Sub/Tropical Depression', color=get_colors_sshws(33))
-        ts = mlines.Line2D([], [], linestyle='solid', label='Sub/Tropical Storm', color=get_colors_sshws(34))
-        c1 = mlines.Line2D([], [], linestyle='solid', label='Category 1', color=get_colors_sshws(64))
-        c2 = mlines.Line2D([], [], linestyle='solid', label='Category 2', color=get_colors_sshws(83))
-        c3 = mlines.Line2D([], [], linestyle='solid', label='Category 3', color=get_colors_sshws(96))
-        c4 = mlines.Line2D([], [], linestyle='solid', label='Category 4', color=get_colors_sshws(113))
-        c5 = mlines.Line2D([], [], linestyle='solid', label='Category 5', color=get_colors_sshws(137))
-        l = ax.legend(handles=[ex,td,ts,c1,c2,c3,c4,c5], prop={'size':11.5}, loc=1)
+    # Linecolor category without dots
+    elif prop['linecolor'] == 'category' and not prop['dots']:
+        ex = mlines.Line2D([], [], linestyle='dotted',
+                           label='Non-Tropical', color='k')
+        td = mlines.Line2D([], [], linestyle='solid',
+                           label='Sub/Tropical Depression', color=get_colors_sshws(33))
+        ts = mlines.Line2D([], [], linestyle='solid',
+                           label='Sub/Tropical Storm', color=get_colors_sshws(34))
+        c1 = mlines.Line2D([], [], linestyle='solid',
+                           label='Category 1', color=get_colors_sshws(64))
+        c2 = mlines.Line2D([], [], linestyle='solid',
+                           label='Category 2', color=get_colors_sshws(83))
+        c3 = mlines.Line2D([], [], linestyle='solid',
+                           label='Category 3', color=get_colors_sshws(96))
+        c4 = mlines.Line2D([], [], linestyle='solid',
+                           label='Category 4', color=get_colors_sshws(113))
+        c5 = mlines.Line2D([], [], linestyle='solid',
+                           label='Category 5', color=get_colors_sshws(137))
+        l = ax.legend(handles=[ex, td, ts, c1, c2, c3,
+                      c4, c5], prop={'size': 11.5}, loc=1)
         l.set_zorder(1001)
 
-    #Non-segmented custom colormap with dots
-    elif prop['dots'] == True and segmented_colors == False:
-        ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color=prop['fillcolor'])
-        sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color=prop['fillcolor'])
-        td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical', marker='o', color=prop['fillcolor'])
-        handles=[ex,sb,td]
-        l = ax.legend(handles=handles,fontsize=11.5, prop={'size':11.5}, loc=1)
+    # Non-segmented custom colormap with dots
+    elif prop['dots'] and not segmented_colors:
+        ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Non-Tropical', marker='^', color=prop['fillcolor'])
+        sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Subtropical', marker='s', color=prop['fillcolor'])
+        td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                           mew=0.5, label='Tropical', marker='o', color=prop['fillcolor'])
+        handles = [ex, sb, td]
+        l = ax.legend(handles=handles, fontsize=11.5,
+                      prop={'size': 11.5}, loc=1)
         l.set_zorder(1001)
 
-    #Non-segmented custom colormap without dots
-    elif prop['dots'] == False and segmented_colors == False:
-        ex = mlines.Line2D([], [], linestyle='dotted',label='Non-Tropical', color=prop['linecolor'])
-        td = mlines.Line2D([], [], linestyle='solid',label='Tropical', color=prop['linecolor'])
-        handles=[ex,td]
-        l = ax.legend(handles=handles,fontsize=11.5, prop={'size':11.5}, loc=1)
+    # Non-segmented custom colormap without dots
+    elif not prop['dots'] and not segmented_colors:
+        ex = mlines.Line2D([], [], linestyle='dotted',
+                           label='Non-Tropical', color=prop['linecolor'])
+        td = mlines.Line2D([], [], linestyle='solid',
+                           label='Tropical', color=prop['linecolor'])
+        handles = [ex, td]
+        l = ax.legend(handles=handles, fontsize=11.5,
+                      prop={'size': 11.5}, loc=1)
         l.set_zorder(1001)
 
-    #Custom colormap with dots
-    elif prop['dots'] == True and segmented_colors == True:
-        ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color='w')
-        sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color='w')
-        td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical', marker='o', color='w')
-        handles=[ex,sb,td]
+    # Custom colormap with dots
+    elif prop['dots'] and segmented_colors:
+        ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                           mec='k', mew=0.5, label='Non-Tropical', marker='^', color='w')
+        sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                           mec='k', mew=0.5, label='Subtropical', marker='s', color='w')
+        td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                           mec='k', mew=0.5, label='Tropical', marker='o', color='w')
+        handles = [ex, sb, td]
         for _ in range(7):
-            handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-        l = ax.legend(handles=handles,fontsize=11.5)
+            handles.append(mlines.Line2D(
+                [], [], linestyle='-', label='', lw=0))
+        l = ax.legend(handles=handles, fontsize=11.5)
         l.set_zorder(1001)
         plt.draw()
 
-        #Get the bbox
+        # Get the bbox
         try:
             bb = l.legendPatch.get_bbox().inverse_transformed(fig.transFigure)
         except:
             bb = l.legendPatch.get_bbox().transformed(fig.transFigure.inverted())
 
-        #Define colorbar axis
-        cax = fig.add_axes([bb.x0+0.47*bb.width, bb.y0+.057*bb.height, 0.015, .65*bb.height])
+        # Define colorbar axis
+        cax = fig.add_axes(
+            [bb.x0 + 0.47 * bb.width, bb.y0 + .057 * bb.height, 0.015, .65 * bb.height])
         norm = mlib.colors.Normalize(vmin=min(levels), vmax=max(levels))
         cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-        cbar = fig.colorbar(cbmap,cax=cax,orientation='vertical',\
-                                 ticks=levels)
+        cbar = fig.colorbar(cbmap, cax=cax, orientation='vertical',
+                            ticks=levels)
 
         cax.tick_params(labelsize=11.5)
         cax.yaxis.set_ticks_position('left')
-        cbar.set_label(prop['fillcolor'],fontsize=11.5,rotation=90)
+        cbar.set_label(prop['fillcolor'], fontsize=11.5, rotation=90)
 
         rect_offset = 0.0
         if prop['cmap'] == 'category' and prop['fillcolor'] == 'vmax':
-            cax.yaxis.set_ticks(np.linspace(min(levels),max(levels),len(levels)))
+            cax.yaxis.set_ticks(np.linspace(
+                min(levels), max(levels), len(levels)))
             cax.yaxis.set_ticklabels(levels)
             cax2 = cax.twinx()
             cax2.yaxis.set_ticks_position('right')
-            cax2.yaxis.set_ticks((np.linspace(0,1,len(levels))[:-1]+np.linspace(0,1,len(levels))[1:])*.5)
-            cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+            cax2.yaxis.set_ticks((np.linspace(0, 1, len(levels))[
+                                 :-1] + np.linspace(0, 1, len(levels))[1:]) * .5)
+            cax2.set_yticklabels(
+                ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
             cax2.tick_params('both', length=0, width=0, which='major')
             cax.yaxis.set_ticks_position('left')
             rect_offset = 0.7
-        if prop['fillcolor'] == 'date':
-            cax.set_yticklabels([f'{mdates.num2date(i):%b %-d}' for i in levels],fontsize=11.5)
+        if prop['fillcolor'] == 'time':
+            cax.set_yticklabels(
+                [f'{mdates.num2date(i):%b %-d}' for i in levels], fontsize=11.5)
 
-    #Custom colormap without dots
+    # Custom colormap without dots
     else:
-        ex = mlines.Line2D([], [], linestyle='dotted',label='Non-Tropical', color='k')
-        td = mlines.Line2D([], [], linestyle='solid',label='Tropical', color='k')
-        handles=[ex,td]
+        ex = mlines.Line2D([], [], linestyle='dotted',
+                           label='Non-Tropical', color='k')
+        td = mlines.Line2D([], [], linestyle='solid',
+                           label='Tropical', color='k')
+        handles = [ex, td]
         for _ in range(7):
-            handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-        l = ax.legend(handles=handles,fontsize=11.5)
+            handles.append(mlines.Line2D(
+                [], [], linestyle='-', label='', lw=0))
+        l = ax.legend(handles=handles, fontsize=11.5)
         l.set_zorder(1001)
         plt.draw()
 
-        #Get the bbox
+        # Get the bbox
         try:
             bb = l.legendPatch.get_bbox().inverse_transformed(fig.transFigure)
         except:
             bb = l.legendPatch.get_bbox().transformed(fig.transFigure.inverted())
 
-        #Define colorbar axis
-        cax = fig.add_axes([bb.x0+0.47*bb.width, bb.y0+.057*bb.height, 0.015, .65*bb.height])
+        # Define colorbar axis
+        cax = fig.add_axes(
+            [bb.x0 + 0.47 * bb.width, bb.y0 + .057 * bb.height, 0.015, .65 * bb.height])
         norm = mlib.colors.Normalize(vmin=min(levels), vmax=max(levels))
         cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-        cbar = fig.colorbar(cbmap,cax=cax,orientation='vertical',\
-                                 ticks=levels)
+        cbar = fig.colorbar(cbmap, cax=cax, orientation='vertical',
+                            ticks=levels)
 
         cax.tick_params(labelsize=11.5)
         cax.yaxis.set_ticks_position('left')
-        cbarlab = make_var_label(prop['linecolor'],storm)
-        cbar.set_label(cbarlab,fontsize=11.5,rotation=90)
+        cbarlab = make_var_label(prop['linecolor'], storm)
+        cbar.set_label(cbarlab, fontsize=11.5, rotation=90)
 
         rect_offset = 0.0
         if prop['cmap'] == 'category' and prop['linecolor'] == 'vmax':
-            cax.yaxis.set_ticks(np.linspace(min(levels),max(levels),len(levels)))
+            cax.yaxis.set_ticks(np.linspace(
+                min(levels), max(levels), len(levels)))
             cax.yaxis.set_ticklabels(levels)
             cax2 = cax.twinx()
             cax2.yaxis.set_ticks_position('right')
-            cax2.yaxis.set_ticks((np.linspace(0,1,len(levels))[:-1]+np.linspace(0,1,len(levels))[1:])*.5)
-            cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+            cax2.yaxis.set_ticks((np.linspace(0, 1, len(levels))[
+                                 :-1] + np.linspace(0, 1, len(levels))[1:]) * .5)
+            cax2.set_yticklabels(
+                ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
             cax2.tick_params('both', length=0, width=0, which='major')
             cax.yaxis.set_ticks_position('left')
             rect_offset = 0.7
-        if prop['linecolor'] == 'date':
-            cax.set_yticklabels([f'{mdates.num2date(i):%b %-d}' for i in levels],fontsize=11.5)
+        if prop['linecolor'] == 'time':
+            cax.set_yticklabels(
+                [f'{mdates.num2date(i):%b %-d}' for i in levels], fontsize=11.5)
+
+    return ax, fig
 
-    return ax,fig
 
 class TrackPlot(Plot):
-    
+
     def __init__(self):
-        
+
         self.use_credit = True
-    
-    def plot_storms(self,storms,domain="dynamic",title="TC Track Composite",plot_all_dots=False,track_labels=False,ax=None,save_path=None,prop={},map_prop={}):
-        
+
+    def plot_storms(self, storms, domain="dynamic", title="TC Track Composite", plot_all_dots=False, track_labels=False, ax=None, save_path=None, prop={}, map_prop={}):
         r"""
         Creates a plot of a single or multiple storm tracks.
-        
+
         Parameters
         ----------
         storms : list
             List of requested storms. List can contain either strings of storm ID (e.g., "AL052019"), tuples with storm name and year (e.g., ("Matthew",2016)), or dict entries.
         domain : str
             Domain for the plot. Can be one of the following:
             "dynamic" - default. Dynamically focuses the domain using the storm track(s) plotted.
@@ -271,299 +308,338 @@
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        #Set default properties
-        default_prop={'dots':True,'fillcolor':'category','cmap':None,'levels':None,'linecolor':'k','linewidth':1.0,'ms':7.5,'plot_names':False}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # Set default properties
+        default_prop = {'dots': True, 'fillcolor': 'category', 'cmap': None, 'levels': None,
+                        'linecolor': 'k', 'linewidth': 1.0, 'ms': 7.5, 'plot_names': False}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # --------------------------------------------------------------------------------------
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = None
         min_lat = None
         max_lon = None
         min_lon = None
-        
-        #Iterate through all storms provided
+
+        # Iterate through all storms provided
         for storm in storms:
 
-            #Check for storm type, then get data for storm
-            if isinstance(storm, str) == True:
+            # Check for storm type, then get data for storm
+            if isinstance(storm, str):
                 storm_data = self.data[storm]
-            elif isinstance(storm, tuple) == True:
-                storm = self.get_storm_id(storm[0],storm[1])
+            elif isinstance(storm, tuple):
+                storm = self.get_storm_id(storm[0], storm[1])
                 storm_data = self.data[storm]
-            elif isinstance(storm, dict) == True:
+            elif isinstance(storm, dict):
                 storm_data = storm
             else:
-                raise RuntimeError("Error: Storm must be a string (e.g., 'AL052019'), tuple (e.g., ('Matthew',2016)), or dict.")
+                raise RuntimeError(
+                    "Error: Storm must be a string (e.g., 'AL052019'), tuple (e.g., ('Matthew',2016)), or dict.")
 
-            #Retrieve storm data
+            # Retrieve storm data
             lats = storm_data['lat']
             lons = storm_data['lon']
             vmax = storm_data['vmax']
             styp = storm_data['type']
-            sdate = storm_data['date']
+            sdate = storm_data['time']
 
-            #Account for cases crossing dateline
+            # Account for cases crossing dateline
             if self.proj.proj4_params['lon_0'] == 180.0:
                 new_lons = np.array(lons)
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 lons = new_lons.tolist()
-            
-            #Force dynamic_tropical to tropical if an invest
+
+            # Force dynamic_tropical to tropical if an invest
             invest_bool = False
-            if 'invest' in storm_data.keys() and storm_data['invest'] == True:
+            if 'invest' in storm_data.keys() and storm_data['invest']:
                 invest_bool = True
-                if domain == 'dynamic_tropical': domain = 'dynamic'
+                if domain == 'dynamic_tropical':
+                    domain = 'dynamic'
 
-            #Add to coordinate extrema
+            # Add to coordinate extrema
             if domain == 'dynamic_tropical':
                 type_array = np.array(storm_data['type'])
-                idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
+                idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+                    type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
                 use_lats = (np.array(storm_data['lat'])[idx]).tolist()
                 use_lons = (np.array(lons)[idx]).tolist()
             else:
                 use_lats = storm_data['lat']
                 use_lons = np.copy(lons).tolist()
 
-            #Add to coordinate extrema
+            # Add to coordinate extrema
             if max_lat is None:
                 max_lat = max(use_lats)
             else:
-                if max(use_lats) > max_lat: max_lat = max(use_lats)
+                if max(use_lats) > max_lat:
+                    max_lat = max(use_lats)
             if min_lat is None:
                 min_lat = min(use_lats)
             else:
-                if min(use_lats) < min_lat: min_lat = min(use_lats)
+                if min(use_lats) < min_lat:
+                    min_lat = min(use_lats)
             if max_lon is None:
                 max_lon = max(use_lons)
             else:
-                if max(use_lons) > max_lon: max_lon = max(use_lons)
+                if max(use_lons) > max_lon:
+                    max_lon = max(use_lons)
             if min_lon is None:
                 min_lon = min(use_lons)
             else:
-                if min(use_lons) < min_lon: min_lon = min(use_lons)
+                if min(use_lons) < min_lon:
+                    min_lon = min(use_lons)
+
+            # Add storm label at start and end points
+            if prop['plot_names']:
+                self.ax.text(lons[0] + 0.0, storm_data['lat'][0] + 1.0, f"{storm_data['name'].upper()} {storm_data['year']}",
+                             fontsize=9, clip_on=True, zorder=1000, alpha=0.7, ha='center', va='center', transform=ccrs.PlateCarree())
+                self.ax.text(lons[-1] + 0.0, storm_data['lat'][-1] + 1.0, f"{storm_data['name'].upper()} {storm_data['year']}",
+                             fontsize=9, clip_on=True, zorder=1000, alpha=0.7, ha='center', va='center', transform=ccrs.PlateCarree())
 
-            #Add storm label at start and end points
-            if prop['plot_names'] == True:
-                self.ax.text(lons[0]+0.0,storm_data['lat'][0]+1.0,f"{storm_data['name'].upper()} {storm_data['year']}",
-                             fontsize=9,clip_on=True,zorder=1000,alpha=0.7,ha='center',va='center',transform=ccrs.PlateCarree())
-                self.ax.text(lons[-1]+0.0,storm_data['lat'][-1]+1.0,f"{storm_data['name'].upper()} {storm_data['year']}",
-                             fontsize=9,clip_on=True,zorder=1000,alpha=0.7,ha='center',va='center',transform=ccrs.PlateCarree())
-            
-            #Iterate over storm data to plot
+            # Iterate over storm data to plot
             levels = None
             cmap = None
-            for i,(i_lat,i_lon,i_vmax,i_mslp,i_date,i_type) in enumerate(zip(storm_data['lat'],lons,storm_data['vmax'],storm_data['mslp'],storm_data['date'],storm_data['type'])):
+            for i, (i_lat, i_lon, i_vmax, i_mslp, i_time, i_type) in enumerate(zip(storm_data['lat'], lons, storm_data['vmax'], storm_data['mslp'], storm_data['time'], storm_data['type'])):
 
-                #Determine line color, with SSHWS scale used as default
+                # Determine line color, with SSHWS scale used as default
                 if prop['linecolor'] == 'category':
                     segmented_colors = True
                     line_color = get_colors_sshws(np.nan_to_num(i_vmax))
 
-                #Use user-defined colormap if another storm variable
-                elif isinstance(prop['linecolor'],str) == True and prop['linecolor'] in ['vmax','mslp','dvmax_dt','speed']:
+                # Use user-defined colormap if another storm variable
+                elif isinstance(prop['linecolor'], str) and prop['linecolor'] in ['vmax', 'mslp', 'dvmax_dt', 'speed']:
                     segmented_colors = True
                     try:
                         color_variable = storm_data[prop['linecolor']]
                     except:
-                        raise ValueError("Storm object must be interpolated to hourly using 'storm.interp().plot(...)' in order to use 'dvmax_dt' or 'speed' for fill color")
-                    if prop['levels'] is None: #Auto-determine color levels if needed
-                        prop['levels'] = [np.nanmin(color_variable),np.nanmax(color_variable)]
-                    cmap,levels = get_cmap_levels(prop['linecolor'],prop['cmap'],prop['levels'])
-                    line_color = cmap((color_variable-min(levels))/(max(levels)-min(levels)))[i]
+                        raise ValueError(
+                            "Storm object must be interpolated to hourly using 'storm.interp().plot(...)' in order to use 'dvmax_dt' or 'speed' for fill color")
+                    if prop['levels'] is None:  # Auto-determine color levels if needed
+                        prop['levels'] = [
+                            np.nanmin(color_variable), np.nanmax(color_variable)]
+                    cmap, levels = get_cmap_levels(
+                        prop['linecolor'], prop['cmap'], prop['levels'])
+                    line_color = cmap(
+                        (color_variable - min(levels)) / (max(levels) - min(levels)))[i]
 
-                #Otherwise go with user input as is
+                # Otherwise go with user input as is
                 else:
                     segmented_colors = False
                     line_color = prop['linecolor']
 
-                #For tropical/subtropical types, color-code if requested
+                # For tropical/subtropical types, color-code if requested
                 if i > 0:
-                    if i_type in constants.TROPICAL_STORM_TYPES and storm_data['type'][i-1] in constants.TROPICAL_STORM_TYPES:
+                    if i_type in constants.TROPICAL_STORM_TYPES and storm_data['type'][i - 1] in constants.TROPICAL_STORM_TYPES:
 
-                        #Plot underlying black and overlying colored line
-                        self.ax.plot([lons[i-1],lons[i]],[storm_data['lat'][i-1],storm_data['lat'][i]],'-',
-                                      linewidth=prop['linewidth']*1.33,color='k',zorder=3,
-                                      transform=ccrs.PlateCarree())
-                        self.ax.plot([lons[i-1],lons[i]],[storm_data['lat'][i-1],storm_data['lat'][i]],'-',
-                                      linewidth=prop['linewidth'],color=line_color,zorder=4,
-                                      transform=ccrs.PlateCarree())
+                        # Plot underlying black and overlying colored line
+                        self.ax.plot([lons[i - 1], lons[i]], [storm_data['lat'][i - 1], storm_data['lat'][i]], '-',
+                                     linewidth=prop['linewidth'] * 1.33, color='k', zorder=3,
+                                     transform=ccrs.PlateCarree())
+                        self.ax.plot([lons[i - 1], lons[i]], [storm_data['lat'][i - 1], storm_data['lat'][i]], '-',
+                                     linewidth=prop['linewidth'], color=line_color, zorder=4,
+                                     transform=ccrs.PlateCarree())
 
-                    #For non-tropical types, plot dotted lines
+                    # For non-tropical types, plot dotted lines
                     else:
 
-                        #Restrict line width to 1.5 max
+                        # Restrict line width to 1.5 max
                         line_width = prop['linewidth'] + 0.0
-                        if line_width > 1.5: line_width = 1.5
+                        if line_width > 1.5:
+                            line_width = 1.5
 
-                        #Plot dotted line
-                        self.ax.plot([lons[i-1],lons[i]],[storm_data['lat'][i-1],storm_data['lat'][i]],':',
-                                      linewidth=line_width,color=line_color,zorder=4,
-                                      transform=ccrs.PlateCarree(),
-                                      path_effects=[path_effects.Stroke(linewidth=line_width*1.33, foreground='k'),
-                                                    path_effects.Normal()])
-
-                #Plot dots if requested
-                if prop['dots'] == True:
-                    if plot_all_dots == False and i_date.strftime('%H%M') not in constants.STANDARD_HOURS: continue
-                    self.ax,segmented_colors,extra = plot_dot(self.ax,i_lon,i_lat,i_date,i_vmax,i_type,
-                                                              zorder=5,storm_data=storm_data,prop=prop,i=i)
-                    if 'cmap' in extra.keys(): cmap = extra['cmap']
-                    if 'levels' in extra.keys(): levels = extra['levels']
-                
-                #Label track dots
+                        # Plot dotted line
+                        self.ax.plot([lons[i - 1], lons[i]], [storm_data['lat'][i - 1], storm_data['lat'][i]], ':',
+                                     linewidth=line_width, color=line_color, zorder=4,
+                                     transform=ccrs.PlateCarree(),
+                                     path_effects=[path_effects.Stroke(linewidth=line_width * 1.33, foreground='k'),
+                                                   path_effects.Normal()])
+
+                # Plot dots if requested
+                if prop['dots']:
+                    if not plot_all_dots and i_time.strftime('%H%M') not in constants.STANDARD_HOURS:
+                        continue
+                    self.ax, segmented_colors, extra = plot_dot(self.ax, i_lon, i_lat, i_time, i_vmax, i_type,
+                                                                zorder=5, storm_data=storm_data, prop=prop, i=i)
+                    if 'cmap' in extra.keys():
+                        cmap = extra['cmap']
+                    if 'levels' in extra.keys():
+                        levels = extra['levels']
+
+                # Label track dots
                 if track_labels in ['valid_utc']:
                     if track_labels == 'valid_utc':
                         strformat = '%H UTC \n%-m/%-d'
-                        labels = {t.strftime(strformat):(x,y) for t,x,y in zip(sdate,lons,lats) if t.hour==0}
-                        track = {t.strftime(strformat):(x,y) for t,x,y in zip(sdate,lons,lats)}
+                        labels = {t.strftime(strformat): (x, y) for t, x, y in zip(
+                            sdate, lons, lats) if t.hour == 0}
+                        track = {t.strftime(strformat): (x, y)
+                                 for t, x, y in zip(sdate, lons, lats)}
                     self.plot_track_labels(self.ax, labels, track, k=.9)
 
-        #--------------------------------------------------------------------------------------
-        
-        #Storm-centered plot domain
+        # --------------------------------------------------------------------------------------
+
+        # Storm-centered plot domain
         if domain == "dynamic" or domain == "dynamic_tropical":
-            
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
+
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
+
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
         if len(storms) > 1:
-            if title != "": self.ax.set_title(f"{title}",loc='left',fontsize=17,fontweight='bold')
+            if title != "":
+                self.ax.set_title(f"{title}", loc='left',
+                                  fontsize=17, fontweight='bold')
         else:
-            #Add left title
+            # Add left title
             type_array = np.array(storm_data['type'])
-            idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))
-            
-            #Check if storm is an invest with a leading 9
+            idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+                type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))
+
+            # Check if storm is an invest with a leading 9
             add_ptc_flag = False
             check_invest = False
             if len(storm_data['id']) > 4 and str(storm_data['id'][2]) == "9":
                 check_invest = True
-            
+
             if len(storm_data['id']) == 0 and len(idx[0]) == 0:
                 idx = np.array([True for i in type_array])
                 tropical_vmax = np.array(storm_data['vmax'])
-                self.ax.set_title(f"Cyclone {storm_data['name']}",loc='left',fontsize=17,fontweight='bold')
-            elif check_invest == False and (invest_bool == False or len(idx[0]) > 0):
+                self.ax.set_title(
+                    f"Cyclone {storm_data['name']}", loc='left', fontsize=17, fontweight='bold')
+            elif not check_invest and (invest_bool == False or len(idx[0]) > 0):
                 tropical_vmax = np.array(storm_data['vmax'])[idx]
 
-                #Coerce to include non-TC points if storm hasn't been designated yet
+                # Coerce to include non-TC points if storm hasn't been designated yet
                 if len(tropical_vmax) == 0 and len(storm_data['id']) > 4:
                     add_ptc_flag = True
                     idx = np.where((type_array == 'LO') | (type_array == 'DB'))
                     tropical_vmax = np.array(storm_data['vmax'])[idx]
 
-                subtrop = classify_subtropical(np.array(storm_data['type']))
-                peak_idx = storm_data['vmax'].index(np.nanmax(tropical_vmax))
-                peak_basin = storm_data['wmo_basin'][peak_idx]
-                storm_type = get_storm_classification(np.nanmax(tropical_vmax),subtrop,peak_basin)
-                if add_ptc_flag == True: storm_type = "Potential Tropical Cyclone"
-                self.ax.set_title(f"{storm_type} {storm_data['name']}",loc='left',fontsize=17,fontweight='bold')
+                if all_nan(tropical_vmax):
+                    storm_type = 'Tropical Cyclone'
+                else:
+                    subtrop = classify_subtropical(
+                        np.array(storm_data['type']))
+                    peak_idx = storm_data['vmax'].index(
+                        np.nanmax(tropical_vmax))
+                    peak_basin = storm_data['wmo_basin'][peak_idx]
+                    storm_type = get_storm_classification(
+                        np.nanmax(tropical_vmax), subtrop, peak_basin)
+                    if add_ptc_flag:
+                        storm_type = "Potential Tropical Cyclone"
+                self.ax.set_title(
+                    f"{storm_type} {storm_data['name']}", loc='left', fontsize=17, fontweight='bold')
             else:
-                #Use all indices for invests
+                # Use all indices for invests
                 idx = np.array([True for i in type_array])
                 tropical_vmax = np.array(storm_data['vmax'])
 
-                #Determine letter in front of invest
+                # Determine letter in front of invest
                 add_letter = 'L'
                 if storm_data['id'][0] == 'C':
                     add_letter = 'C'
                 elif storm_data['id'][0] == 'E':
                     add_letter = 'E'
                 elif storm_data['id'][0] == 'W':
                     add_letter = 'W'
                 elif storm_data['id'][0] == 'I':
                     add_letter = 'I'
                 elif storm_data['id'][0] == 'S':
                     add_letter = 'S'
 
-                #Add title
-                self.ax.set_title(f"INVEST {storm_data['id'][2:4]}{add_letter}",loc='left',fontsize=17,fontweight='bold')
+                # Add title
+                self.ax.set_title(
+                    f"INVEST {storm_data['id'][2:4]}{add_letter}", loc='left', fontsize=17, fontweight='bold')
 
-            #Add right title
+            # Add right title
             ace = storm_data['ace']
-            if add_ptc_flag == True: ace = 0.0
+            if add_ptc_flag:
+                ace = 0.0
             type_array = np.array(storm_data['type'])
-            
-            #Get storm extrema for display
+
+            # Get storm extrema for display
             mslp_key = 'mslp' if 'wmo_mslp' not in storm_data.keys() else 'wmo_mslp'
-            if all_nan(np.array(storm_data[mslp_key])[idx]) == True:
+            if all_nan(np.array(storm_data[mslp_key])[idx]):
                 min_pres = "N/A"
             else:
-                min_pres = int(np.nan_to_num(np.nanmin(np.array(storm_data[mslp_key])[idx])))
-            if all_nan(np.array(storm_data['vmax'])[idx]) == True:
+                min_pres = int(np.nan_to_num(
+                    np.nanmin(np.array(storm_data[mslp_key])[idx])))
+            if all_nan(np.array(storm_data['vmax'])[idx]):
                 max_wind = "N/A"
             else:
-                max_wind = int(np.nan_to_num(np.nanmax(np.array(storm_data['vmax'])[idx])))
-            start_date = dt.strftime(np.array(storm_data['date'])[idx][0],'%d %b %Y')
-            end_date = dt.strftime(np.array(storm_data['date'])[idx][-1],'%d %b %Y')
+                max_wind = int(np.nan_to_num(
+                    np.nanmax(np.array(storm_data['vmax'])[idx])))
+            start_time = dt.strftime(
+                np.array(storm_data['time'])[idx][0], '%d %b %Y')
+            end_time = dt.strftime(np.array(storm_data['time'])[
+                                   idx][-1], '%d %b %Y')
             endash = u"\u2013"
             dot = u"\u2022"
-            self.ax.set_title(f"{start_date} {endash} {end_date}\n{max_wind} kt {dot} {min_pres} hPa {dot} {ace:.1f} ACE",loc='right',fontsize=13)
+            self.ax.set_title(
+                f"{start_time} {endash} {end_time}\n{max_wind} kt {dot} {min_pres} hPa {dot} {ace:.1f} ACE", loc='right', fontsize=13)
 
-        #--------------------------------------------------------------------------------------
-        
-        #Add plot credit
-        warning_text=""
+        # --------------------------------------------------------------------------------------
+
+        # Add plot credit
+        warning_text = ""
         if storm_data['source'] == 'ibtracs' and storm_data['source_info'] == 'World Meteorological Organization (official)':
             warning_text = f"This plot uses 10-minute averaged WMO official wind data converted\nto 1-minute average (factor of 0.88). Use this wind data with caution.\n\n"
 
-            self.ax.text(0.99,0.01,warning_text,fontsize=9,color='k',alpha=0.7,
-            transform=self.ax.transAxes,ha='right',va='bottom',zorder=10)
-        
+            self.ax.text(0.99, 0.01, warning_text, fontsize=9, color='k', alpha=0.7,
+                         transform=self.ax.transAxes, ha='right', va='bottom', zorder=10)
+
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
-        
-        #--------------------------------------------------------------------------------------
-                
-        #Add legend
-        self.ax,self.fig = add_legend(self.ax,self.fig,prop,segmented_colors,levels,cmap,storm_data)
-                
-        #-----------------------------------------------------------------------------------------
-        
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return axis if specified, otherwise display figure
+
+        # --------------------------------------------------------------------------------------
+
+        # Add legend
+        self.ax, self.fig = add_legend(
+            self.ax, self.fig, prop, segmented_colors, levels, cmap, storm_data)
+
+        # -----------------------------------------------------------------------------------------
+
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return axis if specified, otherwise display figure
         return self.ax
-        
-    def plot_storm_nhc(self,forecast,track=None,track_labels='fhr',cone_days=5,domain="dynamic_forecast",ax=None,save_path=
-None,prop={},map_prop={}):
-        
+
+    def plot_storm_nhc(self, forecast, track=None, track_labels='fhr', cone_days=5, domain="dynamic_forecast", ax=None, save_path=None, prop={}, map_prop={}):
         r"""
         Creates a plot of the operational NHC forecast track along with observed track data.
-        
+
         Parameters
         ----------
         forecast : dict
             Dict entry containing forecast data.
         track : dict
             Dict entry containing observed track data. Default is none.
         track_labels : str
@@ -582,1297 +658,1423 @@
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        #Set default properties
-        default_prop={'dots':True,'fillcolor':'category','linecolor':'k','linewidth':1.0,'ms':7.5,'cone_lw':1.0,'cone_alpha':0.6}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # Set default properties
+        default_prop = {'dots': True, 'fillcolor': 'category', 'linecolor': 'k',
+                        'linewidth': 1.0, 'ms': 7.5, 'cone_lw': 1.0, 'cone_alpha': 0.6}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # --------------------------------------------------------------------------------------
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = None
         min_lat = None
         max_lon = None
         min_lon = None
-        
-        #Add storm or multiple storms
+
+        # Add storm or multiple storms
         if track != "":
-            
-            #Check for storm type, then get data for storm
-            if isinstance(track, dict) == True:
+
+            # Check for storm type, then get data for storm
+            if isinstance(track, dict):
                 storm_data = track
             else:
                 raise RuntimeError("Error: track must be of type dict.")
-                
-            #Retrieve storm data
+
+            # Retrieve storm data
             lats = storm_data['lat']
             lons = storm_data['lon']
             vmax = storm_data['vmax']
             styp = storm_data['type']
-            sdate = storm_data['date']
-            
-            #Check if there's enough data points to plot
+            sdate = storm_data['time']
+
+            # Check if there's enough data points to plot
             matching_times = [i for i in sdate if i <= forecast['init']]
             check_length = len(matching_times)
             if check_length >= 2:
 
-                #Subset until time of forecast
+                # Subset until time of forecast
                 matching_times = [i for i in sdate if i <= forecast['init']]
-                plot_idx = sdate.index(matching_times[-1])+1
+                plot_idx = sdate.index(matching_times[-1]) + 1
                 lats = storm_data['lat'][:plot_idx]
                 lons = storm_data['lon'][:plot_idx]
                 vmax = storm_data['vmax'][:plot_idx]
                 styp = storm_data['type'][:plot_idx]
-                sdate = storm_data['date'][:plot_idx]
+                sdate = storm_data['time'][:plot_idx]
 
-                #Account for cases crossing dateline
+                # Account for cases crossing dateline
                 if self.proj.proj4_params['lon_0'] == 180.0:
                     new_lons = np.array(lons)
-                    new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                    new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                     lons = new_lons.tolist()
-                
-                #Connect to 1st forecast location
+
+                # Connect to 1st forecast location
                 fcst_hr = np.array(forecast['fhr'])
                 start_slice = 0
-                if 3 in fcst_hr: start_slice = 3
-                iter_hr = np.array(forecast['fhr'])[fcst_hr>=start_slice][0]
-                fcst_lon = np.array(forecast['lon'])[fcst_hr>=start_slice][0]
-                fcst_lat = np.array(forecast['lat'])[fcst_hr>=start_slice][0]
-                fcst_type = np.array(forecast['type'])[fcst_hr>=start_slice][0]
-                fcst_vmax = np.array(forecast['vmax'])[fcst_hr>=start_slice][0]
-                if fcst_type == "": fcst_type = get_storm_type(fcst_vmax,False)
+                if 3 in fcst_hr:
+                    start_slice = 3
+                iter_hr = np.array(forecast['fhr'])[fcst_hr >= start_slice][0]
+                fcst_lon = np.array(forecast['lon'])[fcst_hr >= start_slice][0]
+                fcst_lat = np.array(forecast['lat'])[fcst_hr >= start_slice][0]
+                fcst_type = np.array(forecast['type'])[
+                    fcst_hr >= start_slice][0]
+                fcst_vmax = np.array(forecast['vmax'])[
+                    fcst_hr >= start_slice][0]
+                if fcst_type == "":
+                    fcst_type = get_storm_type(fcst_vmax, False)
                 if self.proj.proj4_params['lon_0'] == 180.0:
-                    if fcst_lon < 0: fcst_lon = fcst_lon + 360.0
+                    if fcst_lon < 0:
+                        fcst_lon = fcst_lon + 360.0
                 lons.append(fcst_lon)
                 lats.append(fcst_lat)
                 vmax.append(fcst_vmax)
                 styp.append(fcst_type)
-                sdate.append(sdate[-1]+timedelta(hours=start_slice))
+                sdate.append(sdate[-1] + timedelta(hours=start_slice))
 
-                #Add to coordinate extrema
+                # Add to coordinate extrema
                 if domain != "dynamic_forecast":
                     if max_lat is None:
                         max_lat = max(lats)
                     else:
-                        if max(lats) > max_lat: max_lat = max(lats)
+                        if max(lats) > max_lat:
+                            max_lat = max(lats)
                     if min_lat is None:
                         min_lat = min(lats)
                     else:
-                        if min(lats) < min_lat: min_lat = min(lats)
+                        if min(lats) < min_lat:
+                            min_lat = min(lats)
                     if max_lon is None:
                         max_lon = max(lons)
                     else:
-                        if max(lons) > max_lon: max_lon = max(lons)
+                        if max(lons) > max_lon:
+                            max_lon = max(lons)
                     if min_lon is None:
                         min_lon = min(lons)
                     else:
-                        if min(lons) < min_lon: min_lon = min(lons)
+                        if min(lons) < min_lon:
+                            min_lon = min(lons)
                 else:
-                    max_lat = lats[-1]+0.2
-                    min_lat = lats[-2]-0.2
-                    max_lon = lons[-1]+0.2
-                    min_lon = lons[-2]-0.2
+                    max_lat = lats[-1] + 0.2
+                    min_lat = lats[-2] - 0.2
+                    max_lon = lons[-1] + 0.2
+                    min_lon = lons[-2] - 0.2
 
-                #Plot storm line as specified
+                # Plot storm line as specified
                 if prop['linecolor'] == 'category':
                     type6 = np.array(styp)
-                    for i in (np.arange(len(lats[1:]))+1):
+                    for i in (np.arange(len(lats[1:])) + 1):
                         ltype = 'solid'
-                        if type6[i] not in constants.TROPICAL_STORM_TYPES: ltype = 'dotted'
-                        self.ax.plot([lons[i-1],lons[i]],[lats[i-1],lats[i]],
-                                      '-',color=get_colors_sshws(np.nan_to_num(vmax[i])),linewidth=prop['linewidth'],linestyle=ltype,
-                                      transform=ccrs.PlateCarree(),
-                                      path_effects=[path_effects.Stroke(linewidth=prop['linewidth']*1.25, foreground='k'), path_effects.Normal()])
+                        if type6[i] not in constants.TROPICAL_STORM_TYPES:
+                            ltype = 'dotted'
+                        self.ax.plot([lons[i - 1], lons[i]], [lats[i - 1], lats[i]],
+                                     '-', color=get_colors_sshws(np.nan_to_num(vmax[i])), linewidth=prop['linewidth'], linestyle=ltype,
+                                     transform=ccrs.PlateCarree(),
+                                     path_effects=[path_effects.Stroke(linewidth=prop['linewidth'] * 1.25, foreground='k'), path_effects.Normal()])
                 else:
-                    self.ax.plot(lons,lats,'-',color=prop['linecolor'],linewidth=prop['linewidth'],transform=ccrs.PlateCarree())
+                    self.ax.plot(
+                        lons, lats, '-', color=prop['linecolor'], linewidth=prop['linewidth'], transform=ccrs.PlateCarree())
 
-                #Plot storm dots as specified
-                if prop['dots'] == True:
-                    for i,(ilon,ilat,iwnd,itype) in enumerate(zip(lons,lats,vmax,styp)):
+                # Plot storm dots as specified
+                if prop['dots']:
+                    for i, (ilon, ilat, iwnd, itype) in enumerate(zip(lons, lats, vmax, styp)):
                         mtype = '^'
                         if itype in constants.SUBTROPICAL_ONLY_STORM_TYPES:
                             mtype = 's'
                         elif itype in constants.TROPICAL_ONLY_STORM_TYPES:
                             mtype = 'o'
                         if prop['fillcolor'] == 'category':
                             ncol = get_colors_sshws(np.nan_to_num(iwnd))
                         else:
                             ncol = 'k'
-                        self.ax.plot(ilon,ilat,mtype,color=ncol,mec='k',mew=0.5,ms=prop['ms'],transform=ccrs.PlateCarree())
+                        self.ax.plot(ilon, ilat, mtype, color=ncol, mec='k',
+                                     mew=0.5, ms=prop['ms'], transform=ccrs.PlateCarree())
 
-        #--------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------
 
-        #Error check cone days
-        if isinstance(cone_days,int) == False:
+        # Error check cone days
+        if not isinstance(cone_days, int):
             raise TypeError("Error: cone_days must be of type int")
-        if cone_days not in [5,4,3,2]:
-            raise ValueError("Error: cone_days must be an int between 2 and 5.")
-        
-        #Error check forecast dict
-        if isinstance(forecast, dict) == False:
+        if cone_days not in [5, 4, 3, 2]:
+            raise ValueError(
+                "Error: cone_days must be an int between 2 and 5.")
+
+        # Error check forecast dict
+        if not isinstance(forecast, dict):
             raise RuntimeError("Error: Forecast must be of type dict")
-            
-        #Determine first forecast index
+
+        # Determine first forecast index
         fcst_hr = np.array(forecast['fhr'])
         start_slice = 0
-        if 3 in fcst_hr: start_slice = 3
-        check_duration = fcst_hr[(fcst_hr>=start_slice) & (fcst_hr<=cone_days*24)]
+        if 3 in fcst_hr:
+            start_slice = 3
+        check_duration = fcst_hr[(fcst_hr >= start_slice) & (
+            fcst_hr <= cone_days * 24)]
 
-        #Check for sufficiently many hours
+        # Check for sufficiently many hours
         if len(check_duration) > 1:
 
-            #Generate forecast cone for forecast data
+            # Generate forecast cone for forecast data
             dateline = False
-            if self.proj.proj4_params['lon_0'] == 180.0: dateline = True
-            cone = generate_nhc_cone(forecast,forecast['basin'],dateline,cone_days)
+            if self.proj.proj4_params['lon_0'] == 180.0:
+                dateline = True
+            cone = generate_nhc_cone(
+                forecast, forecast['basin'], dateline, cone_days)
 
-            #Contour fill cone & account for dateline crossing
-            if 'cone' in forecast.keys() and forecast['cone'] == False:
+            # Contour fill cone & account for dateline crossing
+            if 'cone' in forecast.keys() and not forecast['cone']:
                 pass
             else:
-                cone_lon = cone['lon']
-                cone_lat = cone['lat']
-                cone_lon_2d = cone['lon2d']
-                cone_lat_2d = cone['lat2d']
                 if self.proj.proj4_params['lon_0'] == 180.0:
-                    new_lons = np.array(cone_lon_2d)
-                    new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
-                    cone_lon_2d = new_lons.tolist()
-                    new_lons = np.array(cone_lon)
-                    new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
-                    cone_lon = new_lons.tolist() 
-                cone_2d = cone['cone']
-                cone_2d = ndimage.gaussian_filter(cone_2d,sigma=0.5,order=0)
-                self.ax.contourf(cone_lon_2d,cone_lat_2d,cone_2d,[0.9,1.1],colors=['#ffffff','#ffffff'],alpha=prop['cone_alpha'],zorder=2,transform=ccrs.PlateCarree())
-                self.ax.contour(cone_lon_2d,cone_lat_2d,cone_2d,[0.9],linewidths=prop['cone_lw'],colors=['k'],zorder=3,transform=ccrs.PlateCarree())
-
-            #Plot center line & account for dateline crossing
-            center_lon = cone['center_lon']
-            center_lat = cone['center_lat']
-            if self.proj.proj4_params['lon_0'] == 180.0:
-                new_lons = np.array(center_lon)
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
-                center_lon = new_lons.tolist()
-            self.ax.plot(center_lon,center_lat,color='k',linewidth=2.0,zorder=4,transform=ccrs.PlateCarree())
-
-            #Retrieve forecast dots
-            iter_hr = np.array(forecast['fhr'])[(fcst_hr>=start_slice) & (fcst_hr<=cone_days*24)]
-            fcst_lon = np.array(forecast['lon'])[(fcst_hr>=start_slice) & (fcst_hr<=cone_days*24)]
-            fcst_lat = np.array(forecast['lat'])[(fcst_hr>=start_slice) & (fcst_hr<=cone_days*24)]
-            fcst_type = np.array(forecast['type'])[(fcst_hr>=start_slice) & (fcst_hr<=cone_days*24)]
-            fcst_vmax = np.array(forecast['vmax'])[(fcst_hr>=start_slice) & (fcst_hr<=cone_days*24)]
-            
-            #Account for cases crossing dateline
+                    new_lons = np.array(cone['lon2d'])
+                    new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
+                    cone['lon2d'] = new_lons
+                    center_lon = np.array(cone['center_lon'])
+                    center_lon[center_lon < 0] = center_lon[center_lon < 0] + 360.0
+                    cone['center_lon'] = center_lon
+                plot_cone(self.ax,cone,plot_center_line=True,center_linewidth=2.0,zorder=3)
+
+            # Retrieve forecast dots
+            iter_hr = np.array(forecast['fhr'])[
+                (fcst_hr >= start_slice) & (fcst_hr <= cone_days * 24)]
+            fcst_lon = np.array(forecast['lon'])[
+                (fcst_hr >= start_slice) & (fcst_hr <= cone_days * 24)]
+            fcst_lat = np.array(forecast['lat'])[
+                (fcst_hr >= start_slice) & (fcst_hr <= cone_days * 24)]
+            fcst_type = np.array(forecast['type'])[
+                (fcst_hr >= start_slice) & (fcst_hr <= cone_days * 24)]
+            fcst_vmax = np.array(forecast['vmax'])[
+                (fcst_hr >= start_slice) & (fcst_hr <= cone_days * 24)]
+
+            # Account for cases crossing dateline
             if self.proj.proj4_params['lon_0'] == 180.0:
                 new_lons = np.array(fcst_lon)
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 fcst_lon = new_lons.tolist()
 
-            #Plot forecast dots
-            for i,(ilon,ilat,itype,iwnd,ihr) in enumerate(zip(fcst_lon,fcst_lat,fcst_type,fcst_vmax,iter_hr)):
+            # Plot forecast dots
+            for i, (ilon, ilat, itype, iwnd, ihr) in enumerate(zip(fcst_lon, fcst_lat, fcst_type, fcst_vmax, iter_hr)):
                 mtype = '^'
                 if itype in constants.SUBTROPICAL_ONLY_STORM_TYPES:
                     mtype = 's'
-                elif itype in ['TD','TS','HU','']:
+                elif itype in list(constants.TROPICAL_ONLY_STORM_TYPES) + ['']:
                     mtype = 'o'
                 if prop['fillcolor'] == 'category':
                     ncol = get_colors_sshws(np.nan_to_num(iwnd))
                 else:
                     ncol = 'k'
-                #Marker width
-                mew = 0.5; use_zorder=5
+                # Marker width
+                mew = 0.5
+                use_zorder = 5
                 if i == 0:
-                    mew = 2.0; use_zorder=10
-                self.ax.plot(ilon,ilat,mtype,color=ncol,mec='k',mew=mew,ms=prop['ms']*1.3,transform=ccrs.PlateCarree(),zorder=use_zorder)
-
-            #Label forecast dots
-            if track_labels in ['fhr','valid_utc','valid_edt','fhr_wind_kt','fhr_wind_mph']:
-                valid_dates = [forecast['init']+timedelta(hours=int(i)) for i in iter_hr]
+                    mew = 2.0
+                    use_zorder = 10
+                self.ax.plot(ilon, ilat, mtype, color=ncol, mec='k', mew=mew,
+                             ms=prop['ms'] * 1.3, transform=ccrs.PlateCarree(), zorder=use_zorder)
+
+            # Label forecast dots
+            if track_labels in ['fhr', 'valid_utc', 'valid_edt', 'fhr_wind_kt', 'fhr_wind_mph']:
+                valid_dates = [forecast['init'] +
+                               timedelta(hours=int(i)) for i in iter_hr]
                 if track_labels == 'fhr':
                     labels = [str(i) for i in iter_hr]
                 if track_labels == 'fhr_wind_kt':
-                    labels = [f"Hour {iter_hr[i]}\n{fcst_vmax[i]} kt" for i in range(len(iter_hr))]
+                    labels = [
+                        f"Hour {iter_hr[i]}\n{fcst_vmax[i]} kt" for i in range(len(iter_hr))]
                 if track_labels == 'fhr_wind_mph':
-                    labels = [f"Hour {iter_hr[i]}\n{knots_to_mph(fcst_vmax[i])} mph" for i in range(len(iter_hr))]
+                    labels = [f"Hour {iter_hr[i]}\n{knots_to_mph(fcst_vmax[i])} mph" for i in range(
+                        len(iter_hr))]
                 if track_labels == 'valid_edt':
-                    labels = [str(int(i.strftime('%I'))) + ' ' + i.strftime('%p %a') for i in [j-timedelta(hours=4) for j in valid_dates]]
+                    labels = [str(int(i.strftime('%I'))) + ' ' + i.strftime('%p %a')
+                              for i in [j - timedelta(hours=4) for j in valid_dates]]
                     edt_warning = True
                 if track_labels == 'valid_utc':
-                    labels = [f"{i.strftime('%H UTC')}\n{str(i.month)}/{str(i.day)}" for i in valid_dates]
-                self.plot_nhc_labels(self.ax, fcst_lon, fcst_lat, labels, k=1.2)
-                
-            #Add cone coordinates to coordinate extrema
-            if 'cone' in forecast.keys() and forecast['cone'] == False:
+                    labels = [
+                        f"{i.strftime('%H UTC')}\n{str(i.month)}/{str(i.day)}" for i in valid_dates]
+                self.plot_nhc_labels(
+                    self.ax, fcst_lon, fcst_lat, labels, k=1.2)
+
+            # Add cone coordinates to coordinate extrema
+            if 'cone' in forecast.keys() and not forecast['cone']:
                 if domain == "dynamic_forecast" or max_lat is None:
                     max_lat = max(center_lat)
                     min_lat = min(center_lat)
                     max_lon = max(center_lon)
                     min_lon = min(center_lon)
                 else:
-                    if max(center_lat) > max_lat: max_lat = max(center_lat)
-                    if min(center_lat) < min_lat: min_lat = min(center_lat)
-                    if max(center_lon) > max_lon: max_lon = max(center_lon)
-                    if min(center_lon) < min_lon: min_lon = min(center_lon)
+                    if max(center_lat) > max_lat:
+                        max_lat = max(center_lat)
+                    if min(center_lat) < min_lat:
+                        min_lat = min(center_lat)
+                    if max(center_lon) > max_lon:
+                        max_lon = max(center_lon)
+                    if min(center_lon) < min_lon:
+                        min_lon = min(center_lon)
             else:
                 if domain == "dynamic_forecast" or max_lat is None:
                     max_lat = max(cone_lat)
                     min_lat = min(cone_lat)
                     max_lon = max(cone_lon)
                     min_lon = min(cone_lon)
                 else:
-                    if max(cone_lat) > max_lat: max_lat = max(cone_lat)
-                    if min(cone_lat) < min_lat: min_lat = min(cone_lat)
-                    if max(cone_lon) > max_lon: max_lon = max(cone_lon)
-                    if min(cone_lon) < min_lon: min_lon = min(cone_lon)
+                    if max(cone_lat) > max_lat:
+                        max_lat = max(cone_lat)
+                    if min(cone_lat) < min_lat:
+                        min_lat = min(cone_lat)
+                    if max(cone_lon) > max_lon:
+                        max_lon = max(cone_lon)
+                    if min(cone_lon) < min_lon:
+                        min_lon = min(cone_lon)
 
-        #--------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------
 
-        #Storm-centered plot domain
+        # Storm-centered plot domain
         if domain == "dynamic" or domain == 'dynamic_forecast':
-            
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
-            
-        #Pre-generated or custom domain
+
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
+
+        # Pre-generated or custom domain
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Identify storm type (subtropical, hurricane, etc)
+
+        # --------------------------------------------------------------------------------------
+
+        # Identify storm type (subtropical, hurricane, etc)
         first_fcst_wind = np.array(forecast['vmax'])[fcst_hr >= start_slice][0]
         first_fcst_mslp = np.array(forecast['mslp'])[fcst_hr >= start_slice][0]
         first_fcst_type = np.array(forecast['type'])[fcst_hr >= start_slice][0]
-        if all_nan(first_fcst_wind) == True:
+        if all_nan(first_fcst_wind):
             storm_type = 'Unknown'
         else:
             subtrop = first_fcst_type in constants.SUBTROPICAL_ONLY_STORM_TYPES
             cur_wind = first_fcst_wind + 0
-            storm_type = get_storm_classification(np.nan_to_num(cur_wind),subtrop,'north_atlantic')
-        
-        #Identify storm name (and storm type, if post-tropical or potential TC)
-        matching_times = [i for i in storm_data['date'] if i <= forecast['init']]
+            storm_type = get_storm_classification(
+                np.nan_to_num(cur_wind), subtrop, 'north_atlantic')
+
+        # Identify storm name (and storm type, if post-tropical or potential TC)
+        matching_times = [i for i in storm_data['time']
+                          if i <= forecast['init']]
         if check_length < 2:
-            if all_nan(first_fcst_wind) == True:
+            if all_nan(first_fcst_wind):
                 storm_name = storm_data['name']
             else:
                 storm_name = num_to_text(int(storm_data['id'][2:4])).upper()
-                if first_fcst_wind >= 34 and first_fcst_type in constants.TROPICAL_STORM_TYPES: storm_name = storm_data['name'];
-                if first_fcst_type not in constants.TROPICAL_STORM_TYPES: storm_type = 'Potential Tropical Cyclone'
+                if first_fcst_wind >= 34 and first_fcst_type in constants.TROPICAL_STORM_TYPES:
+                    storm_name = storm_data['name']
+                if first_fcst_type not in constants.TROPICAL_STORM_TYPES:
+                    storm_type = 'Potential Tropical Cyclone'
         else:
             storm_name = num_to_text(int(storm_data['id'][2:4])).upper()
             storm_type = 'Potential Tropical Cyclone'
             storm_tropical = False
-            if all_nan(vmax) == True:
+            if all_nan(vmax):
                 storm_type = 'Unknown'
                 storm_name = storm_data['name']
             else:
-                for i,(iwnd,ityp) in enumerate(zip(vmax,styp)):
+                for i, (iwnd, ityp) in enumerate(zip(vmax, styp)):
                     if ityp in constants.TROPICAL_STORM_TYPES:
                         storm_tropical = True
                         subtrop = ityp in constants.SUBTROPICAL_ONLY_STORM_TYPES
-                        storm_type = get_storm_classification(np.nan_to_num(iwnd),subtrop,'north_atlantic')
-                        if np.isnan(iwnd) == True: storm_type = 'Unknown'
+                        storm_type = get_storm_classification(
+                            np.nan_to_num(iwnd), subtrop, 'north_atlantic')
+                        if np.isnan(iwnd):
+                            storm_type = 'Unknown'
                     else:
-                        if storm_tropical == True: storm_type = 'Post Tropical Cyclone'
+                        if storm_tropical:
+                            storm_type = 'Post Tropical Cyclone'
                     if ityp in constants.NAMED_TROPICAL_STORM_TYPES:
                         storm_name = storm_data['name']
-        
-        #Fix storm types for non-NHC basins
+
+        # Fix storm types for non-NHC basins
         if 'cone' in forecast.keys():
-            storm_type = get_storm_classification(first_fcst_wind,False,forecast['basin'])
-        
-        #Add left title
-        self.ax.set_title(f"{storm_type} {storm_name}",loc='left',fontsize=17,fontweight='bold')
+            storm_type = get_storm_classification(
+                first_fcst_wind, False, forecast['basin'])
+
+        # Add left title
+        self.ax.set_title(f"{storm_type} {storm_name}",
+                          loc='left', fontsize=17, fontweight='bold')
 
         endash = u"\u2013"
         dot = u"\u2022"
-        
-        #Get current advisory information
-        first_fcst_wind = "N/A" if np.isnan(first_fcst_wind) == True else int(first_fcst_wind)
-        first_fcst_mslp = "N/A" if np.isnan(first_fcst_mslp) == True else int(first_fcst_mslp)
-        
-        #Get time of advisory
+
+        # Get current advisory information
+        first_fcst_wind = "N/A" if np.isnan(first_fcst_wind) else int(first_fcst_wind)
+        first_fcst_mslp = "N/A" if np.isnan(first_fcst_mslp) else int(first_fcst_mslp)
+
+        # Get time of advisory
         fcst_hr = forecast['fhr']
         start_slice = 0
-        if 3 in fcst_hr: start_slice = 1
-        forecast_date = (forecast['init']+timedelta(hours=fcst_hr[start_slice])).strftime("%H%M UTC %d %b %Y")
+        if 3 in fcst_hr:
+            start_slice = 1
+        forecast_date = (
+            forecast['init'] + timedelta(hours=fcst_hr[start_slice])).strftime("%H%M UTC %d %b %Y")
         forecast_id = forecast['advisory_num']
-        
+
         if forecast_id == -1:
             title_text = f"Current Intensity: {knots_to_mph(first_fcst_wind)} mph {dot} {first_fcst_mslp} hPa"
-            if 'cone' in forecast.keys() and forecast['cone'] == False:
+            if 'cone' in forecast.keys() and not forecast['cone']:
                 title_text += f"\nJTWC Issued: {forecast_date}"
             else:
                 title_text += f"\nNHC Issued: {forecast_date}"
         else:
-            if first_fcst_wind != "N/A": first_fcst_wind = knots_to_mph(first_fcst_wind)
+            if first_fcst_wind != "N/A":
+                first_fcst_wind = knots_to_mph(first_fcst_wind)
             title_text = f"{first_fcst_wind} mph {dot} {first_fcst_mslp} hPa {dot} Forecast #{forecast_id}"
             title_text += f"\nForecast Issued: {forecast_date}"
-        
-        
-        #Add right title
-        self.ax.set_title(title_text,loc='right',fontsize=13)
-
-        #--------------------------------------------------------------------------------------
-        
-        #Add legend
+
+        # Add right title
+        self.ax.set_title(title_text, loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
+
+        # Add legend
         if prop['fillcolor'] == 'category' or prop['linecolor'] == 'category':
-            
-            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Non-Tropical', marker='^', color='w')
-            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Subtropical', marker='s', color='w')
-            uk = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Unknown', marker='o', color='w')
-            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical Depression', marker='o', color=get_colors_sshws(33))
-            ts = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Tropical Storm', marker='o', color=get_colors_sshws(34))
-            c1 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 1', marker='o', color=get_colors_sshws(64))
-            c2 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 2', marker='o', color=get_colors_sshws(83))
-            c3 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 3', marker='o', color=get_colors_sshws(96))
-            c4 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 4', marker='o', color=get_colors_sshws(113))
-            c5 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',mew=0.5, label='Category 5', marker='o', color=get_colors_sshws(137))
-            self.ax.legend(handles=[ex,sb,uk,td,ts,c1,c2,c3,c4,c5], prop={'size':11.5})
 
-        #Add forecast label warning
+            ex = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Non-Tropical', marker='^', color='w')
+            sb = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Subtropical', marker='s', color='w')
+            uk = mlines.Line2D([], [], linestyle='None', ms=prop['ms'],
+                               mec='k', mew=0.5, label='Unknown', marker='o', color='w')
+            td = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k', mew=0.5,
+                               label='Tropical Depression', marker='o', color=get_colors_sshws(33))
+            ts = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Tropical Storm', marker='o', color=get_colors_sshws(34))
+            c1 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 1', marker='o', color=get_colors_sshws(64))
+            c2 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 2', marker='o', color=get_colors_sshws(83))
+            c3 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 3', marker='o', color=get_colors_sshws(96))
+            c4 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 4', marker='o', color=get_colors_sshws(113))
+            c5 = mlines.Line2D([], [], linestyle='None', ms=prop['ms'], mec='k',
+                               mew=0.5, label='Category 5', marker='o', color=get_colors_sshws(137))
+            self.ax.legend(handles=[ex, sb, uk, td, ts,
+                           c1, c2, c3, c4, c5], prop={'size': 11.5})
+
+        # Add forecast label warning
         try:
-            if edt_warning == True:
+            if edt_warning:
                 warning_text = "All times displayed are in EDT\n\n"
             else:
                 warning_text = ""
         except:
             warning_text = ""
         try:
             warning_text += f"The cone of uncertainty in this product was generated internally using {cone['year']} official\nNHC cone radii. This cone differs slightly from the official NHC cone.\n\n"
         except:
             pass
-        
-        self.ax.text(0.99,0.01,warning_text,fontsize=9,color='k',alpha=0.7,
-                transform=self.ax.transAxes,ha='right',va='bottom',zorder=10)
-        
+
+        self.ax.text(0.99, 0.01, warning_text, fontsize=9, color='k', alpha=0.7,
+                     transform=self.ax.transAxes, ha='right', va='bottom', zorder=10)
+
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
-        
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(os.path.join(save_path,f"{storm_data['name']}_{storm_data['year']}_track.png"),bbox_inches='tight')
-        
-        #Return axis if specified, otherwise display figure
+
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(os.path.join(
+                save_path, f"{storm_data['name']}_{storm_data['year']}_track.png"), bbox_inches='tight')
+
+        # Return axis if specified, otherwise display figure
         return self.ax
 
-    def plot_models(self,forecast,plot_btk,storm_dict,forecast_dict,models,domain,ax,prop,map_prop,save_path):
-        
+    def plot_models(self, forecast, plot_btk, storm_dict, forecast_dict, models, domain, ax, prop, map_prop, save_path):
         r"""
         Plot multi-model forecast tracks.
         """
-        
-         #Set default properties
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        default_model = {'nhc':'k',
-                         'gfs':'#0000ff',
-                         'ecm':'#ff1493',
-                         'cmc':'#1e90ff',
-                         'ukm':'#00ff00',
-                         'hmon':'#ff8c00',
-                         'hwrf':'#66cdaa'}
-        default_prop = {'linewidth':2.5,'marker':'label','marker_hours':[24,48,72,96,120,144,168]}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        model_prop = self.add_prop(models,default_model)
-        self.plot_init(ax,map_prop)
-        
-        #Fix GFDL
+
+        # Set default properties
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+        default_model = {'nhc': 'k',
+                         'gfs': '#0000ff',
+                         'ecm': '#ff1493',
+                         'cmc': '#1e90ff',
+                         'ukm': '#00ff00',
+                         'hmon': '#ff8c00',
+                         'hwrf': '#66cdaa',
+                         'hafsa': '#C659F9',
+                         'hafsb': '#8915BB'}
+        default_prop = {'linewidth': 2.5, 'marker': 'label',
+                        'marker_hours': [24, 48, 72, 96, 120, 144, 168]}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        model_prop = self.add_prop(models, default_model)
+        self.plot_init(ax, map_prop)
+
+        # Fix GFDL
         if 'gfdl' in forecast_dict.keys():
             model_prop['gfdl'] = model_prop['hmon']
         if 'jtwc' in forecast_dict.keys():
             model_prop['jtwc'] = model_prop['nhc']
-        
-        #================================================================================================
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # ================================================================================================
+
+        # Keep record of lat/lon coordinate extrema
         lat_max_extrema = []
         lat_min_extrema = []
         lon_max_extrema = []
         lon_min_extrema = []
-        
-        #================================================================================================
-        
-        #Plot models
+
+        # ================================================================================================
+
+        # Plot models
         for model in forecast_dict.keys():
-            
-            #Plot forecast track
+
+            # Fix label for HAFS
+            if 'hafs' in model:
+                idx = model.index('hafs')
+                model_label = f"HAFS-{model[idx + len('hafs'):].upper()}"
+            else:
+                model_label = model.upper()
+
+            # Plot forecast track
             lons = forecast_dict[model]['lon']
             lats = forecast_dict[model]['lat']
-            self.ax.plot(lons,lats,color=model_prop[model],linewidth=prop['linewidth'],label=model.upper(),transform=ccrs.PlateCarree())
-            
-            #Add labels if requested
-            if prop['marker'] != None and len(prop['marker_hours']) >= 1:
+            self.ax.plot(
+                lons, lats, color=model_prop[model], linewidth=prop['linewidth'], label=model_label, transform=ccrs.PlateCarree())
+
+            # Add labels if requested
+            if prop['marker'] is not None and len(prop['marker_hours']) >= 1:
                 for hour in prop['marker_hours']:
-                    if hour not in forecast_dict[model]['fhr']: continue
+                    if hour not in forecast_dict[model]['fhr']:
+                        continue
                     idx = forecast_dict[model]['fhr'].index(hour)
                     if prop['marker'] == 'label':
-                        self.ax.text(lons[idx],lats[idx],str(hour),ha='center',va='center',zorder=100,transform=ccrs.PlateCarree())
+                        self.ax.text(lons[idx], lats[idx], str(
+                            hour), ha='center', va='center', zorder=100, transform=ccrs.PlateCarree())
                     elif prop['marker'] == 'dot':
-                        self.ax.plot(lons[idx],lats[idx],'o',ms=prop['linewidth']*3,zorder=100,
-                                     mfc=model_prop[model],mec='k',transform=ccrs.PlateCarree())
+                        self.ax.plot(lons[idx], lats[idx], 'o', ms=prop['linewidth'] * 3, zorder=100,
+                                     mfc=model_prop[model], mec='k', transform=ccrs.PlateCarree())
                     else:
-                        raise ValueError("Acceptable values for 'marker' prop are 'label' or 'dot'.")
-            
-            #Add to lat/lon extrema
+                        raise ValueError(
+                            "Acceptable values for 'marker' prop are 'label' or 'dot'.")
+
+            # Add to lat/lon extrema
             lat_max_extrema.append(np.nanmax(lats))
             lat_min_extrema.append(np.nanmin(lats))
             lon_max_extrema.append(np.nanmax(lons))
             lon_min_extrema.append(np.nanmin(lons))
-        
-        #Plot best track if requested
+
+        # Plot best track if requested
         if plot_btk:
-            
-            #Account for cases crossing dateline
+
+            # Account for cases crossing dateline
             if self.proj.proj4_params['lon_0'] == 180.0:
                 new_lons = np.array(storm_dict['lon'])
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 use_lons = new_lons.tolist()
             else:
                 use_lons = storm_dict['lon']
-            
-            #Determine maximum forecast hour
-            max_fhr = max([max(forecast_dict[model]['fhr']) for model in forecast_dict.keys()])
-            
-            #Determine range of forecast data in best track
-            idx_start = storm_dict['date'].index(forecast)
+
+            # Determine maximum forecast hour
+            max_fhr = max([max(forecast_dict[model]['fhr'])
+                          for model in forecast_dict.keys()])
+
+            # Determine range of forecast data in best track
+            idx_start = storm_dict['time'].index(forecast)
             end_date = forecast + timedelta(hours=max_fhr)
-            if end_date in storm_dict['date']:
-                idx_end = storm_dict['date'].index(end_date)
+            if end_date in storm_dict['time']:
+                idx_end = storm_dict['time'].index(end_date)
             else:
-                idx_end = len(storm_dict['date'])
-            
-            #Plot best track
-            lons = use_lons[idx_start:idx_end+1]
-            lats = storm_dict['lat'][idx_start:idx_end+1]
-            storm_dates = storm_dict['date'][idx_start:idx_end+1]
-            self.ax.plot(lons,lats,':',color='k',linewidth=prop['linewidth']*0.8,label='Best Track',transform=ccrs.PlateCarree())
-            
-            #Add to lat/lon extrema
+                idx_end = len(storm_dict['time'])
+
+            # Plot best track
+            lons = use_lons[idx_start:idx_end + 1]
+            lats = storm_dict['lat'][idx_start:idx_end + 1]
+            storm_times = storm_dict['time'][idx_start:idx_end + 1]
+            self.ax.plot(lons, lats, ':', color='k',
+                         linewidth=prop['linewidth'] * 0.8, label='Best Track', transform=ccrs.PlateCarree())
+
+            # Add to lat/lon extrema
             lat_max_extrema.append(np.nanmax(lats))
             lat_min_extrema.append(np.nanmin(lats))
             lon_max_extrema.append(np.nanmax(lons))
             lon_min_extrema.append(np.nanmin(lons))
-            
-            #Add labels if requested
-            if prop['marker'] != None and len(prop['marker_hours']) >= 1:
+
+            # Add labels if requested
+            if prop['marker'] is not None and len(prop['marker_hours']) >= 1:
                 for hour in prop['marker_hours']:
                     valid_date = forecast + timedelta(hours=hour)
-                    if valid_date not in storm_dates: continue
-                    idx = storm_dict['date'].index(valid_date)
+                    if valid_date not in storm_times:
+                        continue
+                    idx = storm_dict['time'].index(valid_date)
                     if prop['marker'] == 'label':
-                        self.ax.text(use_lons[idx],storm_dict['lat'][idx],
-                                     str(hour),ha='center',va='center',zorder=100,clip_on=True,transform=ccrs.PlateCarree())
+                        self.ax.text(use_lons[idx], storm_dict['lat'][idx],
+                                     str(hour), ha='center', va='center', zorder=100, clip_on=True, transform=ccrs.PlateCarree())
                     elif prop['marker'] == 'dot':
-                        self.ax.plot(use_lons[idx],storm_dict['lat'][idx],'o',ms=prop['linewidth']*3,zorder=100,
-                                     mfc='k',mec='k',transform=ccrs.PlateCarree())
+                        self.ax.plot(use_lons[idx], storm_dict['lat'][idx], 'o', ms=prop['linewidth'] * 3, zorder=100,
+                                     mfc='k', mec='k', transform=ccrs.PlateCarree())
                     else:
-                        raise ValueError("Acceptable values for 'marker' prop are 'label' or 'dot'.")
-        
-        #================================================================================================
-        
-        #Calcuate lat/lon extrema
+                        raise ValueError(
+                            "Acceptable values for 'marker' prop are 'label' or 'dot'.")
+
+        # ================================================================================================
+
+        # Calcuate lat/lon extrema
         lat_max_extrema = np.sort(lat_max_extrema)
         lat_min_extrema = np.sort(lat_min_extrema)
         lon_max_extrema = np.sort(lon_max_extrema)
         lon_min_extrema = np.sort(lon_min_extrema)
-        
-        max_lat = np.nanpercentile(lat_max_extrema,95)
-        min_lat = np.nanpercentile(lat_min_extrema,5)
-        max_lon = np.nanpercentile(lon_max_extrema,95)
-        min_lon = np.nanpercentile(lon_min_extrema,5)
-        
-        #================================================================================================
-
-        #Add legend
-        import matplotlib.patches as mpatches
-        import matplotlib.lines as mlines
-        l = self.ax.legend(loc=1,prop={'size':12})
+
+        max_lat = np.nanpercentile(lat_max_extrema, 95)
+        min_lat = np.nanpercentile(lat_min_extrema, 5)
+        max_lon = np.nanpercentile(lon_max_extrema, 95)
+        min_lon = np.nanpercentile(lon_min_extrema, 5)
+
+        # ================================================================================================
+
+        # Add legend
+        l = self.ax.legend(loc=1, prop={'size': 12})
         l.set_zorder(1001)
 
-        #Plot title
+        # Plot title
         plot_title = f"Model Forecast Tracks for {storm_dict['name'].title()}"
-        self.ax.set_title(plot_title,fontsize=16,loc='left',fontweight='bold')
+        self.ax.set_title(plot_title, fontsize=16,
+                          loc='left', fontweight='bold')
 
         title_str = f"Initialized {forecast.strftime('%H%M UTC %d %B %Y')}"
-        self.ax.set_title(title_str,fontsize=12,loc='right')
+        self.ax.set_title(title_str, fontsize=12, loc='right')
 
-        #--------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------
 
-        #Storm-centered plot domain
+        # Storm-centered plot domain
         if domain == "dynamic":
 
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
 
-        #Pre-generated or custom domain
+        # Pre-generated or custom domain
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
 
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
 
-        #--------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------
 
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
 
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(os.path.join(save_path),bbox_inches='tight')
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(os.path.join(save_path), bbox_inches='tight')
 
-        #Return axis if specified, otherwise display figure
+        # Return axis if specified, otherwise display figure
         return self.ax
-    
-    def plot_ensembles(self,forecast,storm_dict,fhr,interpolate,prop_ensemble_members,prop_ensemble_mean,prop_gfs,prop_btk,prop_ellipse,prop_density,nens,
-               domain,ds,ax,map_prop,save_path):
-        
+
+    def plot_ensembles(self, forecast, storm_dict, fhr, interpolate, prop_ensemble_members, prop_ensemble_mean, prop_gfs, prop_btk, prop_ellipse, prop_density, nens,
+                       domain, ds, ax, map_prop, save_path):
         r"""
         Plot GEFS ensemble forecast tracks.
         """
-        
-        #Set default properties
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        default_prop_ensemble_members = {'plot':True, 'linewidth':0.2, 'linecolor':'k','color_var':None,'cmap':None,'levels':None}
-        default_prop_ensemble_mean = {'plot':True, 'linewidth':3.0, 'linecolor':'k'}
-        default_prop_gfs = {'plot':True, 'linewidth':3.0, 'linecolor':'r'}
-        default_prop_btk = {'plot':True, 'linewidth':2.5, 'linecolor':'b'}
-        default_prop_ellipse = {'plot':True, 'linewidth':3.0, 'linecolor':'b'}
-        default_prop_density = {'plot':True, 'radius':200, 'cmap':plt.cm.plasma_r, 'levels':[1]+[i for i in range(10,101,10)]}
-        
-        #Initialize plot
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        prop_ensemble_members = self.add_prop(prop_ensemble_members,default_prop_ensemble_members)
-        prop_ensemble_mean = self.add_prop(prop_ensemble_mean,default_prop_ensemble_mean)
-        prop_gfs = self.add_prop(prop_gfs,default_prop_gfs)
-        prop_btk = self.add_prop(prop_btk,default_prop_btk)
-        prop_ellipse = self.add_prop(prop_ellipse,default_prop_ellipse)
-        prop_density = self.add_prop(prop_density,default_prop_density)
-        self.plot_init(ax,map_prop)
-        
-        #================================================================================================
-        
-        #Get valid time
+
+        # Set default properties
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+        default_prop_ensemble_members = {'plot': True, 'linewidth': 0.2,
+                                         'linecolor': 'k', 'color_var': None, 'cmap': None, 'levels': None}
+        default_prop_ensemble_mean = {
+            'plot': True, 'linewidth': 3.0, 'linecolor': 'k'}
+        default_prop_gfs = {'plot': True, 'linewidth': 3.0, 'linecolor': 'r'}
+        default_prop_btk = {'plot': True, 'linewidth': 2.5, 'linecolor': 'b'}
+        default_prop_ellipse = {'plot': True,
+                                'linewidth': 3.0, 'linecolor': 'b'}
+        default_prop_density = {'plot': True, 'radius': 200, 'cmap': plt.cm.plasma_r, 'levels': [
+            1] + [i for i in range(10, 101, 10)]}
+
+        # Initialize plot
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        prop_ensemble_members = self.add_prop(
+            prop_ensemble_members, default_prop_ensemble_members)
+        prop_ensemble_mean = self.add_prop(
+            prop_ensemble_mean, default_prop_ensemble_mean)
+        prop_gfs = self.add_prop(prop_gfs, default_prop_gfs)
+        prop_btk = self.add_prop(prop_btk, default_prop_btk)
+        prop_ellipse = self.add_prop(prop_ellipse, default_prop_ellipse)
+        prop_density = self.add_prop(prop_density, default_prop_density)
+        self.plot_init(ax, map_prop)
+
+        # ================================================================================================
+
+        # Get valid time
         hr = fhr
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # Keep record of lat/lon coordinate extrema
         lat_max_extrema = []
         lat_min_extrema = []
         lon_max_extrema = []
         lon_min_extrema = []
 
-        #================================================================================================
+        # ================================================================================================
 
-        #Function for temporal interpolation
-        import scipy.interpolate as interp
+        # Function for temporal interpolation
         def temporal_interpolation(value, orig_times, target_times):
-            f = interp.interp1d(orig_times,value)
+            f = interp.interp1d(orig_times, value)
             ynew = f(target_times)
             return ynew
-        
-        #Plot density
+
+        # Plot density
         density_colorbar = False
         if prop_density['plot']:
-            
-            #Error check radius
+
+            # Error check radius
             if prop_density['radius'] > 500 or prop_density['radius'] < 50:
                 raise ValueError("Radius must be between 50 and 500 km.")
-            
-            if hr == None or (hr != None and hr in ds['gefs']['fhr']):
 
-                #Create 0.25 degree grid for plotting
-                gridlats = np.arange(0,90,0.25)
-                if np.nanmax(storm_dict['lat']) < 0: gridlats = np.arange(-90,0,0.25)
-                gridlons = np.arange(-180.0,180.1,0.25)
-                if self.proj.proj4_params['lon_0'] == 180.0: gridlons = np.arange(0.0,360.1,0.25)
-                gridlons2d,gridlats2d = np.meshgrid(gridlons,gridlats)
+            if hr is None or (hr is not None and hr in ds['gefs']['fhr']):
+
+                # Create 0.25 degree grid for plotting
+                gridlats = np.arange(0, 90, 0.25)
+                if np.nanmax(storm_dict['lat']) < 0:
+                    gridlats = np.arange(-90, 0, 0.25)
+                gridlons = np.arange(-180.0, 180.1, 0.25)
+                if self.proj.proj4_params['lon_0'] == 180.0:
+                    gridlons = np.arange(0.0, 360.1, 0.25)
+                gridlons2d, gridlats2d = np.meshgrid(gridlons, gridlats)
                 griddata = np.zeros((gridlons2d.shape))
 
-                #Iterate over all ensemble members
-                if hr == None:
+                # Iterate over all ensemble members
+                if hr is None:
                     start_time = dt.now()
                     print("--> Starting to calculate track density")
                 for ens in range(nens):
 
-                    #Calculate for one hour
-                    if hr != None:
-                        
-                        #Proceed if hour is available
+                    # Calculate for one hour
+                    if hr is not None:
+
+                        # Proceed if hour is available
                         if hr in ds[f'gefs_{ens}']['fhr']:
                             idx = ds[f'gefs_{ens}']['fhr'].index(hr)
                             griddata += add_radius_quick(gridlats, gridlons, ds[f'gefs_{ens}']['lat'][idx],
-                                                   ds[f'gefs_{ens}']['lon'][idx], prop_density['radius'])
-                    
-                    #Calculate for cumulative
+                                                         ds[f'gefs_{ens}']['lon'][idx], prop_density['radius'])
+
+                    # Calculate for cumulative
                     else:
-                        
-                        #Ensemble temporary gridded field
+
+                        # Ensemble temporary gridded field
                         temp_grid = np.zeros((gridlons2d.shape))
 
-                        #Interpolate temporally to hourly if requested
+                        # Interpolate temporally to hourly if requested
                         if interpolate:
                             if len(ds[f'gefs_{ens}']['lat']) == 0:
                                 continue
                             elif len(ds[f'gefs_{ens}']['lat']) == 1:
                                 new_lats = ds[f'gefs_{ens}']['lat']
                                 new_lons = ds[f'gefs_{ens}']['lon']
                             else:
-                                new_hours = np.arange(min(ds[f'gefs_{ens}']['fhr']),max(ds[f'gefs_{ens}']['fhr']),1)
-                                new_lats = temporal_interpolation(ds[f'gefs_{ens}']['lat'],ds[f'gefs_{ens}']['fhr'],new_hours)
-                                new_lons = temporal_interpolation(ds[f'gefs_{ens}']['lon'],ds[f'gefs_{ens}']['fhr'],new_hours)
-
-                            #Iterate over all forecast hours
-                            for i,(i_lon,i_lat) in enumerate(zip(new_lons,new_lats)):
-                                radius_grid = add_radius_quick(gridlats, gridlons, i_lat, i_lon, prop_density['radius'])
+                                new_hours = np.arange(min(ds[f'gefs_{ens}']['fhr']), max(
+                                    ds[f'gefs_{ens}']['fhr']), 1)
+                                new_lats = temporal_interpolation(
+                                    ds[f'gefs_{ens}']['lat'], ds[f'gefs_{ens}']['fhr'], new_hours)
+                                new_lons = temporal_interpolation(
+                                    ds[f'gefs_{ens}']['lon'], ds[f'gefs_{ens}']['fhr'], new_hours)
+
+                            # Iterate over all forecast hours
+                            for i, (i_lon, i_lat) in enumerate(zip(new_lons, new_lats)):
+                                radius_grid = add_radius_quick(
+                                    gridlats, gridlons, i_lat, i_lon, prop_density['radius'])
                                 temp_grid = np.maximum(temp_grid, radius_grid)
-                        
-                        #Otherwise don't interpolate
+
+                        # Otherwise don't interpolate
                         else:
-                            
-                            #Iterate over all forecast hours
-                            for idx,iter_hr in enumerate(ds[f'gefs_{ens}']['fhr']):
+
+                            # Iterate over all forecast hours
+                            for idx, iter_hr in enumerate(ds[f'gefs_{ens}']['fhr']):
                                 radius_grid = add_radius(gridlats2d, gridlons2d, ds[f'gefs_{ens}']['lat'][idx],
                                                          ds[f'gefs_{ens}']['lon'][idx], prop_density['radius'])
                                 temp_grid = np.maximum(temp_grid, radius_grid)
 
-                        #Add temporary grid to full grid
+                        # Add temporary grid to full grid
                         griddata += temp_grid
 
-                #Convert density to percent
-                if hr == None:
+                # Convert density to percent
+                if hr is None:
                     time_elapsed = dt.now() - start_time
-                    tsec = str(round(time_elapsed.total_seconds(),2))
-                    print(f"--> Completed calculating track density ({tsec} seconds)")
+                    tsec = str(round(time_elapsed.total_seconds(), 2))
+                    print(
+                        f"--> Completed calculating track density ({tsec} seconds)")
                 density_percent = (griddata / nens) * 100.0
 
-                #Plot density
-                norm = mcolors.BoundaryNorm(prop_density['levels'], prop_density['cmap'].N)
+                # Plot density
+                norm = mcolors.BoundaryNorm(
+                    prop_density['levels'], prop_density['cmap'].N)
                 cs = self.ax.contourf(gridlons, gridlats, density_percent, prop_density['levels'],
                                       cmap=prop_density['cmap'], norm=norm, alpha=0.6, transform=ccrs.PlateCarree())
-                cbar = add_colorbar(cs,ticks=prop_density['levels'],ax=self.ax)
+                cbar = add_colorbar(
+                    cs, ticks=prop_density['levels'], ax=self.ax)
                 cbar.ax.tick_params(labelsize=12)
                 density_colorbar = True
 
-        #-------------------------------------------------------------------
-        #Plot ellipse
-        if hr != None and hr in ds['gefs']['fhr'] and prop_ellipse['plot']:
+        # -------------------------------------------------------------------
+        # Plot ellipse
+        if hr is not None and hr in ds['gefs']['fhr'] and prop_ellipse['plot']:
             idx = ds['gefs']['fhr'].index(hr)
 
-            #Account for cases crossing dateline
+            # Account for cases crossing dateline
             if self.proj.proj4_params['lon_0'] == 180.0:
                 new_lons = np.array(ds['gefs']['ellipse_lon'][idx])
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 new_lons = new_lons.tolist()
             else:
                 new_lons = ds['gefs']['ellipse_lon'][idx]
-            
+
             try:
-                self.ax.plot(new_lons,ds['gefs']['ellipse_lat'][idx],'-',
-                             color='w', linewidth=prop_ellipse['linewidth']*1.2,
+                self.ax.plot(new_lons, ds['gefs']['ellipse_lat'][idx], '-',
+                             color='w', linewidth=prop_ellipse['linewidth'] * 1.2,
                              transform=ccrs.PlateCarree(), alpha=0.8)
-                self.ax.plot(new_lons,ds['gefs']['ellipse_lat'][idx],'-',
+                self.ax.plot(new_lons, ds['gefs']['ellipse_lat'][idx], '-',
                              color=prop_ellipse['linecolor'], linewidth=prop_ellipse['linewidth'],
                              transform=ccrs.PlateCarree(), alpha=0.8)
             except:
                 pass
 
-        #-------------------------------------------------------------------
-        #Plot GEFS member tracks
+        # -------------------------------------------------------------------
+        # Plot GEFS member tracks
         for i in range(nens):
 
-            #Update coordinate bounds
+            # Update coordinate bounds
             skip_bounds = False
             if hr in ds[f'gefs_{i}']['fhr']:
                 idx = ds[f'gefs_{i}']['fhr'].index(hr)
-                use_lats = ds[f'gefs_{i}']['lat'][:idx+1]
-                use_lons = ds[f'gefs_{i}']['lon'][:idx+1]
+                use_lats = ds[f'gefs_{i}']['lat'][:idx + 1]
+                use_lons = ds[f'gefs_{i}']['lon'][:idx + 1]
             elif len(ds[f'gefs_{i}']['fhr']) > 0:
                 idx = 0
-                if hr == None:
+                if hr is None:
                     idx = len(ds[f'gefs_{i}']['lon'])
                 else:
                     for idx_hr in ds[f'gefs_{i}']['fhr']:
-                        if idx_hr <= hr: idx = ds[f'gefs_{i}']['fhr'].index(idx_hr)
-                use_lons = ds[f'gefs_{i}']['lon'][:idx+1]
-                use_lats = ds[f'gefs_{i}']['lat'][:idx+1]
+                        if idx_hr <= hr:
+                            idx = ds[f'gefs_{i}']['fhr'].index(idx_hr)
+                use_lons = ds[f'gefs_{i}']['lon'][:idx + 1]
+                use_lats = ds[f'gefs_{i}']['lat'][:idx + 1]
             else:
                 skip_bounds = True
 
-            if skip_bounds == False:
+            if not skip_bounds:
                 lat_max_extrema.append(np.nanmax(use_lats))
                 lat_min_extrema.append(np.nanmin(use_lats))
                 lon_max_extrema.append(np.nanmax(use_lons))
                 lon_min_extrema.append(np.nanmin(use_lons))
 
-                #Plot cumulative track
+                # Plot cumulative track
                 if len(ds[f'gefs_{i}']['fhr']) > 0:
-                    if prop_ensemble_members['color_var'] not in ['vmax','mslp']:
-                        self.ax.plot(ds[f'gefs_{i}']['lon'][:idx+1], ds[f'gefs_{i}']['lat'][:idx+1], 
+                    if prop_ensemble_members['color_var'] not in ['vmax', 'mslp']:
+                        self.ax.plot(ds[f'gefs_{i}']['lon'][:idx + 1], ds[f'gefs_{i}']['lat'][:idx + 1],
                                      linewidth=prop_ensemble_members['linewidth'],
                                      color=prop_ensemble_members['linecolor'], transform=ccrs.PlateCarree())
                     else:
-                        #Color by variable
+                        # Color by variable
                         cmap = prop_ensemble_members['cmap']
                         levels = prop_ensemble_members['levels']
-                        norm = mcolors.BoundaryNorm(levels,cmap.N)
-                        for j in range(1,idx+1):
-                            if j >= len(ds[f'gefs_{i}'][prop_ensemble_members['color_var']]): continue
+                        norm = mcolors.BoundaryNorm(levels, cmap.N)
+                        for j in range(1, idx + 1):
+                            if j >= len(ds[f'gefs_{i}'][prop_ensemble_members['color_var']]):
+                                continue
                             i_val = ds[f'gefs_{i}'][prop_ensemble_members['color_var']][j]
-                            color = 'w' if np.isnan(i_val) else cmap(norm(i_val))
-                            self.ax.plot([ds[f'gefs_{i}']['lon'][j-1],ds[f'gefs_{i}']['lon'][j]],
-                                         [ds[f'gefs_{i}']['lat'][j-1],ds[f'gefs_{i}']['lat'][j]], 
+                            color = 'w' if np.isnan(
+                                i_val) else cmap(norm(i_val))
+                            self.ax.plot([ds[f'gefs_{i}']['lon'][j - 1], ds[f'gefs_{i}']['lon'][j]],
+                                         [ds[f'gefs_{i}']['lat'][j - 1],
+                                             ds[f'gefs_{i}']['lat'][j]],
                                          linewidth=prop_ensemble_members['linewidth'],
                                          color=color, transform=ccrs.PlateCarree())
-                        
-                        #Add colorbar
-                        if density_colorbar == False:
+
+                        # Add colorbar
+                        if not density_colorbar:
                             cs = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
                             cs.set_array([])
-                            cbar = add_colorbar(cs,ticks=prop_ensemble_members['levels'],ax=self.ax)
+                            cbar = add_colorbar(
+                                cs, ticks=prop_ensemble_members['levels'], ax=self.ax)
                             cbar.ax.tick_params(labelsize=12)
                             density_colorbar = True
-                
-                #Plot latest dot if applicable
+
+                # Plot latest dot if applicable
                 if hr in ds[f'gefs_{i}']['fhr']:
                     idx = ds[f'gefs_{i}']['fhr'].index(hr)
                     self.ax.plot(ds[f'gefs_{i}']['lon'][idx], ds[f'gefs_{i}']['lat'][idx], 'o', ms=4,
-                                 mfc=prop_ensemble_members['linecolor'],mec='k',
-                                 alpha=0.6,transform=ccrs.PlateCarree())
+                                 mfc=prop_ensemble_members['linecolor'], mec='k',
+                                 alpha=0.6, transform=ccrs.PlateCarree())
 
-        #-------------------------------------------------------------------
-        #Plot best track
+        # -------------------------------------------------------------------
+        # Plot best track
         if prop_btk['plot']:
-            
-            #Account for cases crossing dateline
+
+            # Account for cases crossing dateline
             if self.proj.proj4_params['lon_0'] == 180.0:
                 new_lons = np.array(storm_dict['lon'])
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 new_lons = new_lons.tolist()
             else:
                 new_lons = storm_dict['lon']
-            
-            #Get valid time
-            valid_time = np.nan if hr == None else forecast + timedelta(hours=hr)
+
+            # Get valid time
+            valid_time = np.nan if hr is None else forecast + timedelta(hours=hr)
             end_time = forecast + timedelta(hours=240) if storm_dict['year'] >= 2015 else forecast + timedelta(hours=240)
-            
-            #Update coordinate bounds
+
+            # Update coordinate bounds
             skip_bounds = False
-            idx_start = storm_dict['date'].index(forecast)
-            if valid_time in storm_dict['date']:
-                idx = storm_dict['date'].index(valid_time)
-                
-                use_lats = storm_dict['lat'][idx_start:idx+1]
-                use_lons = new_lons[idx_start:idx+1]
+            idx_start = storm_dict['time'].index(
+                forecast) if forecast in storm_dict['time'] else 0
+            if valid_time in storm_dict['time']:
+                idx = storm_dict['time'].index(valid_time)
+
+                use_lats = storm_dict['lat'][idx_start:idx + 1]
+                use_lons = new_lons[idx_start:idx + 1]
             else:
                 idx = 0
-                if hr == None:
-                    if end_time in storm_dict['date']:
-                        idx = storm_dict['date'].index(end_time)
+                if hr is None:
+                    if end_time in storm_dict['time']:
+                        idx = storm_dict['time'].index(end_time)
                     else:
                         idx = len(storm_dict['lat'])
                 else:
-                    for idx_date in storm_dict['date']:
-                        if idx_date <= valid_time: idx = storm_dict['date'].index(idx_date)
-                use_lats = storm_dict['lat'][idx_start:idx+1]
-                use_lons = new_lons[idx_start:idx+1]
+                    for idx_date in storm_dict['time']:
+                        if idx_date <= valid_time:
+                            idx = storm_dict['time'].index(idx_date)
+                use_lats = storm_dict['lat'][idx_start:idx + 1]
+                use_lons = new_lons[idx_start:idx + 1]
 
-            if skip_bounds == False:
+            if not skip_bounds:
                 lat_max_extrema.append(np.nanmax(use_lats))
                 lat_min_extrema.append(np.nanmin(use_lats))
                 lon_max_extrema.append(np.nanmax(use_lons))
                 lon_min_extrema.append(np.nanmin(use_lons))
 
-            #Plot observed track before now
-            self.ax.plot(new_lons[:idx_start+1], storm_dict['lat'][:idx_start+1],
+            # Plot observed track before now
+            self.ax.plot(new_lons[:idx_start + 1], storm_dict['lat'][:idx_start + 1],
                          linewidth=1.5, color='k', linestyle=":", transform=ccrs.PlateCarree())
-            
-            if valid_time in storm_dict['date']:
-                idx = storm_dict['date'].index(valid_time)
-                self.ax.plot(new_lons[idx_start:idx+1], storm_dict['lat'][idx_start:idx+1],
-                             linewidth=prop_btk['linewidth'], color=prop_btk['linecolor'],transform=ccrs.PlateCarree())
+
+            if valid_time in storm_dict['time']:
+                idx = storm_dict['time'].index(valid_time)
+                self.ax.plot(new_lons[idx_start:idx + 1], storm_dict['lat'][idx_start:idx + 1],
+                             linewidth=prop_btk['linewidth'], color=prop_btk['linecolor'], transform=ccrs.PlateCarree())
                 self.ax.plot(new_lons[idx], storm_dict['lat'][idx], 'o', ms=12,
-                             mfc=prop_btk['linecolor'],mec='k', transform=ccrs.PlateCarree())
-            elif len(storm_dict['date']) > 0:
+                             mfc=prop_btk['linecolor'], mec='k', transform=ccrs.PlateCarree())
+            elif len(storm_dict['time']) > 0:
                 idx = 0
-                if hr == None:
-                    if end_time in storm_dict['date']:
-                        idx = storm_dict['date'].index(end_time)
+                if hr is None:
+                    if end_time in storm_dict['time']:
+                        idx = storm_dict['time'].index(end_time)
                     else:
                         idx = len(storm_dict['lat'])
                 else:
-                    for idx_date in storm_dict['date']:
-                        if idx_date <= valid_time: idx = storm_dict['date'].index(idx_date)
-                self.ax.plot(new_lons[idx_start:idx+1], storm_dict['lat'][idx_start:idx+1],
-                             linewidth=prop_btk['linewidth'], color=prop_btk['linecolor'],transform=ccrs.PlateCarree())
+                    for idx_date in storm_dict['time']:
+                        if idx_date <= valid_time:
+                            idx = storm_dict['time'].index(idx_date)
+                self.ax.plot(new_lons[idx_start:idx + 1], storm_dict['lat'][idx_start:idx + 1],
+                             linewidth=prop_btk['linewidth'], color=prop_btk['linecolor'], transform=ccrs.PlateCarree())
 
-        #-------------------------------------------------------------------
-        #Plot operational GFS track
+        # -------------------------------------------------------------------
+        # Plot operational GFS track
         if prop_gfs['plot']:
 
-            #Update coordinate bounds
+            # Update coordinate bounds
             skip_bounds = False
             if hr in ds['gfs']['fhr']:
                 idx = ds['gfs']['fhr'].index(hr)
-                use_lats = ds['gfs']['lat'][:idx+1]
-                use_lons = ds['gfs']['lon'][:idx+1]
+                use_lats = ds['gfs']['lat'][:idx + 1]
+                use_lons = ds['gfs']['lon'][:idx + 1]
             elif len(ds['gfs']['fhr']) > 0:
                 idx = 0
-                if hr == None:
+                if hr is None:
                     idx = len(ds['gfs']['lon'])
                 else:
                     for idx_hr in ds['gfs']['fhr']:
-                        if idx_hr <= hr: idx = ds['gfs']['fhr'].index(idx_hr)
-                use_lats = ds['gfs']['lat'][:idx+1]
-                use_lons = ds['gfs']['lon'][:idx+1]
+                        if idx_hr <= hr:
+                            idx = ds['gfs']['fhr'].index(idx_hr)
+                use_lats = ds['gfs']['lat'][:idx + 1]
+                use_lons = ds['gfs']['lon'][:idx + 1]
             else:
                 skip_bounds = True
 
-            if skip_bounds == False:
+            if not skip_bounds:
                 lat_max_extrema.append(np.nanmax(use_lats))
                 lat_min_extrema.append(np.nanmin(use_lats))
                 lon_max_extrema.append(np.nanmax(use_lons))
                 lon_min_extrema.append(np.nanmin(use_lons))
 
-            #Plot GFS forecast line and latest dot
+            # Plot GFS forecast line and latest dot
             if hr in ds['gfs']['fhr']:
                 idx = ds['gfs']['fhr'].index(hr)
-                self.ax.plot(ds['gfs']['lon'][:idx+1], ds['gfs']['lat'][:idx+1],
+                self.ax.plot(ds['gfs']['lon'][:idx + 1], ds['gfs']['lat'][:idx + 1],
                              linewidth=prop_gfs['linewidth'], color=prop_gfs['linecolor'], transform=ccrs.PlateCarree())
                 self.ax.plot(ds['gfs']['lon'][idx], ds['gfs']['lat'][idx], 'o', ms=12,
-                             mfc=prop_gfs['linecolor'],mec='k', transform=ccrs.PlateCarree())
+                             mfc=prop_gfs['linecolor'], mec='k', transform=ccrs.PlateCarree())
 
             elif len(ds['gfs']['fhr']) > 0:
                 idx = 0
-                if hr == None:
+                if hr is None:
                     idx = len(ds['gfs']['lon'])
                 else:
                     for idx_hr in ds['gfs']['fhr']:
-                        if idx_hr <= hr: idx = ds['gfs']['fhr'].index(idx_hr)
-                self.ax.plot(ds['gfs']['lon'][:idx+1], ds['gfs']['lat'][:idx+1],
+                        if idx_hr <= hr:
+                            idx = ds['gfs']['fhr'].index(idx_hr)
+                self.ax.plot(ds['gfs']['lon'][:idx + 1], ds['gfs']['lat'][:idx + 1],
                              linewidth=prop_gfs['linewidth'], color=prop_gfs['linecolor'], transform=ccrs.PlateCarree())
 
-        #-------------------------------------------------------------------
-        #Plot ensemble mean track
+        # -------------------------------------------------------------------
+        # Plot ensemble mean track
         if prop_ensemble_mean['plot']:
 
-            #Update coordinate bounds
+            # Update coordinate bounds
             skip_bounds = False
             if hr in ds['gefs']['fhr']:
                 idx = ds['gefs']['fhr'].index(hr)
-                use_lats = ds['gefs']['lat'][:idx+1]
-                use_lons = ds['gefs']['lon'][:idx+1]
+                use_lats = ds['gefs']['lat'][:idx + 1]
+                use_lons = ds['gefs']['lon'][:idx + 1]
             elif len(ds['gefs']['fhr']) > 0:
                 idx = 0
-                if hr == None:
+                if hr is None:
                     idx = len(ds['gefs']['lon'])
                 else:
                     for idx_hr in ds['gefs']['fhr']:
-                        if idx_hr <= hr: idx = ds['gefs']['fhr'].index(idx_hr)
-                use_lats = ds['gefs']['lat'][:idx+1]
-                use_lons = ds['gefs']['lon'][:idx+1]
+                        if idx_hr <= hr:
+                            idx = ds['gefs']['fhr'].index(idx_hr)
+                use_lats = ds['gefs']['lat'][:idx + 1]
+                use_lons = ds['gefs']['lon'][:idx + 1]
             else:
                 skip_bounds = True
 
-            if skip_bounds == False:
+            if not skip_bounds:
                 lat_max_extrema.append(np.nanmax(use_lats))
                 lat_min_extrema.append(np.nanmin(use_lats))
                 lon_max_extrema.append(np.nanmax(use_lons))
                 lon_min_extrema.append(np.nanmin(use_lons))
 
             if hr in ds['gefs']['fhr']:
                 idx = ds['gefs']['fhr'].index(hr)
-                self.ax.plot(ds['gefs']['lon'][:idx+1], ds['gefs']['lat'][:idx+1],
+                self.ax.plot(ds['gefs']['lon'][:idx + 1], ds['gefs']['lat'][:idx + 1],
                              linewidth=prop_ensemble_mean['linewidth'],
                              color=prop_ensemble_mean['linecolor'], transform=ccrs.PlateCarree())
                 self.ax.plot(ds['gefs']['lon'][idx], ds['gefs']['lat'][idx], 'o', ms=12,
-                             mfc=prop_ensemble_mean['linecolor'],mec='k', transform=ccrs.PlateCarree())
+                             mfc=prop_ensemble_mean['linecolor'], mec='k', transform=ccrs.PlateCarree())
             elif len(ds['gefs']['fhr']) > 0:
                 idx = 0
-                if hr == None:
+                if hr is None:
                     idx = len(ds['gefs']['lon'])
                 else:
                     for idx_hr in ds['gefs']['fhr']:
-                        if idx_hr <= hr: idx = ds['gefs']['fhr'].index(idx_hr)
-                self.ax.plot(ds['gefs']['lon'][:idx+1], ds['gefs']['lat'][:idx+1],
+                        if idx_hr <= hr:
+                            idx = ds['gefs']['fhr'].index(idx_hr)
+                self.ax.plot(ds['gefs']['lon'][:idx + 1], ds['gefs']['lat'][:idx + 1],
                              linewidth=prop_ensemble_mean['linewidth'],
                              color=prop_ensemble_mean['linecolor'], transform=ccrs.PlateCarree())
-        
-        #================================================================================================
-        
-        #Calcuate lat/lon extrema
+
+        # ================================================================================================
+
+        # Calcuate lat/lon extrema
         lat_max_extrema = np.sort(lat_max_extrema)
         lat_min_extrema = np.sort(lat_min_extrema)
         lon_max_extrema = np.sort(lon_max_extrema)
         lon_min_extrema = np.sort(lon_min_extrema)
-        
-        if hr == None:
-            max_lat = np.nanpercentile(lat_max_extrema,95)
-            min_lat = np.nanpercentile(lat_min_extrema,5)
-            max_lon = np.nanpercentile(lon_max_extrema,95)
-            min_lon = np.nanpercentile(lon_min_extrema,5)
+
+        if hr is None:
+            max_lat = np.nanpercentile(lat_max_extrema, 95)
+            min_lat = np.nanpercentile(lat_min_extrema, 5)
+            max_lon = np.nanpercentile(lon_max_extrema, 95)
+            min_lon = np.nanpercentile(lon_min_extrema, 5)
         else:
             max_lat = np.nanmax(lat_max_extrema)
             min_lat = np.nanmin(lat_min_extrema)
             max_lon = np.nanmax(lon_max_extrema)
             min_lon = np.nanmin(lon_min_extrema)
-        
-        #================================================================================================
-        
-        #Add legend
-        import matplotlib.patches as mpatches
-        import matplotlib.lines as mlines
-        p1 = mlines.Line2D([], [], color=prop_btk['linecolor'], linewidth=prop_btk['linewidth'], label='Best Track')
-        p2 = mlines.Line2D([], [], color=prop_gfs['linecolor'], linewidth=prop_gfs['linewidth'], label='Deterministic GFS')
-        p3 = mlines.Line2D([], [], color=prop_ensemble_mean['linecolor'], linewidth=prop_ensemble_mean['linewidth'], label='GEFS Mean')
-        p4 = mlines.Line2D([], [], color=prop_ensemble_members['linecolor'], linewidth=prop_ensemble_members['linewidth'], label='GEFS Members')
-        p5 = mlines.Line2D([], [], color='w', marker='o', ms=12, mec=prop_ellipse['linecolor'], mew=prop_ellipse['linewidth'], label='GEFS Ellipse')
+
+        # ================================================================================================
+
+        # Add legend
+        p1 = mlines.Line2D([], [], color=prop_btk['linecolor'],
+                           linewidth=prop_btk['linewidth'], label='Best Track')
+        p2 = mlines.Line2D([], [], color=prop_gfs['linecolor'],
+                           linewidth=prop_gfs['linewidth'], label='Deterministic GFS')
+        p3 = mlines.Line2D([], [], color=prop_ensemble_mean['linecolor'],
+                           linewidth=prop_ensemble_mean['linewidth'], label='GEFS Mean')
+        p4 = mlines.Line2D([], [], color=prop_ensemble_members['linecolor'],
+                           linewidth=prop_ensemble_members['linewidth'], label='GEFS Members')
+        p5 = mlines.Line2D([], [], color='w', marker='o', ms=12, mec=prop_ellipse['linecolor'],
+                           mew=prop_ellipse['linewidth'], label='GEFS Ellipse')
         handles_list = []
-        if prop_btk['plot']: handles_list.append(p1)
-        if prop_gfs['plot']: handles_list.append(p2)
-        if prop_ensemble_mean['plot']: handles_list.append(p3)
-        if prop_ensemble_members['plot']: handles_list.append(p4)
-        if hr != None and prop_ellipse['plot']: handles_list.append(p5)
+        if prop_btk['plot']:
+            handles_list.append(p1)
+        if prop_gfs['plot']:
+            handles_list.append(p2)
+        if prop_ensemble_mean['plot']:
+            handles_list.append(p3)
+        if prop_ensemble_members['plot']:
+            handles_list.append(p4)
+        if hr is not None and prop_ellipse['plot']:
+            handles_list.append(p5)
         if len(handles_list) > 0:
-            l = self.ax.legend(handles=handles_list,loc=1,prop={'size':12})
+            l = self.ax.legend(handles=handles_list, loc=1, prop={'size': 12})
             l.set_zorder(1001)
 
-        #Plot title
-        format_title = {'vmax':'Ensemble member sustained wind (knots)','mslp':'Ensemble member minimum MSLP (hPa)'}
+        # Plot title
+        format_title = {
+            'vmax': 'Ensemble member sustained wind (knots)', 'mslp': 'Ensemble member minimum MSLP (hPa)'}
         plot_title = f"GEFS Forecast Tracks for {storm_dict['name'].title()}"
-        if prop_density['plot']: plot_title += f"\nTrack Density ({np.int(prop_density['radius'])}-km radius)"
-        if prop_ensemble_members['color_var'] in ['vmax','mslp']: plot_title += f"\n{format_title.get(prop_ensemble_members['color_var'])}"
-        self.ax.set_title(plot_title,fontsize=16,loc='left',fontweight='bold')
+        if prop_density['plot']:
+            plot_title += f"\nTrack Density ({int(prop_density['radius'])}-km radius)"
+        if prop_ensemble_members['color_var'] in ['vmax', 'mslp']:
+            plot_title += f"\n{format_title.get(prop_ensemble_members['color_var'])}"
+        self.ax.set_title(plot_title, fontsize=16,
+                          loc='left', fontweight='bold')
 
-        if hr == None:
+        if hr is None:
             title_str = f"Initialized {forecast.strftime('%H%M UTC %d %B %Y')}"
         else:
             title_str = f"Hour {hr} | Valid {(forecast+timedelta(hours=hr)).strftime('%H%M UTC %d %B %Y')}\n"
             title_str += f"Initialized {forecast.strftime('%H%M UTC %d %B %Y')}"
-        self.ax.set_title(title_str,fontsize=12,loc='right')
+        self.ax.set_title(title_str, fontsize=12, loc='right')
 
-        #--------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------
 
-        #Storm-centered plot domain
+        # Storm-centered plot domain
         if domain == "dynamic":
 
-            bound_w,bound_e,bound_s,bound_n = self.dynamic_map_extent(min_lon,max_lon,min_lat,max_lat)
-            self.ax.set_extent([bound_w,bound_e,bound_s,bound_n], crs=ccrs.PlateCarree())
+            bound_w, bound_e, bound_s, bound_n = self.dynamic_map_extent(
+                min_lon, max_lon, min_lat, max_lat)
+            self.ax.set_extent(
+                [bound_w, bound_e, bound_s, bound_n], crs=ccrs.PlateCarree())
 
-        #Pre-generated or custom domain
+        # Pre-generated or custom domain
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
 
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
 
-        #--------------------------------------------------------------------------------------
+        # --------------------------------------------------------------------------------------
 
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
 
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(os.path.join(save_path),bbox_inches='tight')
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(os.path.join(save_path), bbox_inches='tight')
 
-        #Return axis if specified, otherwise display figure
+        # Return axis if specified, otherwise display figure
         return self.ax
 
-    def plot_season(self,season,domain=None,ax=None,save_path=None,prop={},map_prop={}):
-        
+    def plot_season(self, season, domain=None, ax=None, save_path=None, prop={}, map_prop={}):
         r"""
         Creates a plot of a single season.
-        
+
         Parameters
         ----------
         season : Season
             Instance of Season.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        #Set default properties
-        default_prop={'dots':False,'fillcolor':'category','cmap':None,'levels':None,
-                      'linecolor':'category','linewidth':1.0,'ms':7.5,'plot_names':True}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Keep record of lat/lon coordinate extrema
+
+        # Set default properties
+        default_prop = {'dots': False, 'fillcolor': 'category', 'cmap': None, 'levels': None,
+                        'linecolor': 'category', 'linewidth': 1.0, 'ms': 7.5, 'plot_names': True}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # --------------------------------------------------------------------------------------
+
+        # Keep record of lat/lon coordinate extrema
         max_lat = None
         min_lat = None
         max_lon = None
         min_lon = None
 
-        #Iterate over all storms in season object
+        # Iterate over all storms in season object
         sinfo = season.summary()
         storms = season.dict.keys()
-        for storm_idx,storm_key in enumerate(storms):
+        for storm_idx, storm_key in enumerate(storms):
 
-            #Get data for this storm
+            # Get data for this storm
             storm = season.dict[storm_key]
-            
-            #Retrieve storm data
+
+            # Retrieve storm data
             lats = storm['lat']
             lons = storm['lon']
             vmax = storm['vmax']
             styp = storm['type']
-            sdate = storm['date']
+            sdate = storm['time']
 
-            #Account for cases crossing dateline
+            # Account for cases crossing dateline
             if self.proj.proj4_params['lon_0'] == 180.0:
                 new_lons = np.array(lons)
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 lons = new_lons.tolist()
 
-            #Add to coordinate extrema
+            # Add to coordinate extrema
             if max_lat is None:
                 max_lat = max(lats)
             else:
-                if max(lats) > max_lat: max_lat = max(lats)
+                if max(lats) > max_lat:
+                    max_lat = max(lats)
             if min_lat is None:
                 min_lat = min(lats)
             else:
-                if min(lats) < min_lat: min_lat = min(lats)
+                if min(lats) < min_lat:
+                    min_lat = min(lats)
             if max_lon is None:
                 max_lon = max(lons)
             else:
-                if max(lons) > max_lon: max_lon = max(lons)
+                if max(lons) > max_lon:
+                    max_lon = max(lons)
             if min_lon is None:
                 min_lon = min(lons)
             else:
-                if min(lons) < min_lon: min_lon = min(lons)
-            
-            #Add storm label at start and end points
-            if prop['plot_names'] == True:
-                self.ax.text(lons[0]+0.0,storm['lat'][0]+1.0,storm['name'].upper(),
-                             fontsize=9,clip_on=True,zorder=1000,alpha=0.7,ha='center',va='center',transform=ccrs.PlateCarree())
-                self.ax.text(lons[-1]+0.0,storm['lat'][-1]+1.0,storm['name'].upper(),
-                             fontsize=9,clip_on=True,zorder=1000,alpha=0.7,ha='center',va='center',transform=ccrs.PlateCarree())
+                if min(lons) < min_lon:
+                    min_lon = min(lons)
 
-            #Iterate over storm data to plot
+            # Add storm label at start and end points
+            if prop['plot_names']:
+                self.ax.text(lons[0] + 0.0, storm['lat'][0] + 1.0, storm['name'].upper(),
+                             fontsize=9, clip_on=True, zorder=1000, alpha=0.7, ha='center', va='center', transform=ccrs.PlateCarree())
+                self.ax.text(lons[-1] + 0.0, storm['lat'][-1] + 1.0, storm['name'].upper(),
+                             fontsize=9, clip_on=True, zorder=1000, alpha=0.7, ha='center', va='center', transform=ccrs.PlateCarree())
+
+            # Iterate over storm data to plot
             levels = None
             cmap = None
-            for i,(i_lat,i_lon,i_vmax,i_mslp,i_date,i_type) in enumerate(zip(storm['lat'],lons,storm['vmax'],storm['mslp'],storm['date'],storm['type'])):
-                    
-                #Determine line color, with SSHWS scale used as default
+            for i, (i_lat, i_lon, i_vmax, i_mslp, i_time, i_type) in enumerate(zip(storm['lat'], lons, storm['vmax'], storm['mslp'], storm['time'], storm['type'])):
+
+                # Determine line color, with SSHWS scale used as default
                 if prop['linecolor'] == 'category':
                     segmented_colors = True
                     line_color = get_colors_sshws(np.nan_to_num(i_vmax))
-                
-                #Use user-defined colormap if another storm variable
-                elif isinstance(prop['linecolor'],str) == True and prop['linecolor'] in ['vmax','mslp','dvmax_dt','speed']:
+
+                # Use user-defined colormap if another storm variable
+                elif isinstance(prop['linecolor'], str) and prop['linecolor'] in ['vmax', 'mslp', 'dvmax_dt', 'speed']:
                     segmented_colors = True
                     try:
                         color_variable = storm[prop['linecolor']]
                     except:
-                        raise ValueError("Storm object must be interpolated to hourly using 'storm.interp().plot(...)' in order to use 'dvmax_dt' or 'speed' for fill color")
-                    if prop['levels'] is None: #Auto-determine color levels if needed
-                        prop['levels'] = [np.nanmin(color_variable),np.nanmax(color_variable)]
-                    cmap,levels = get_cmap_levels(prop['linecolor'],prop['cmap'],prop['levels'])
-                    line_color = cmap((color_variable-min(levels))/(max(levels)-min(levels)))[i]
-                
-                #Otherwise go with user input as is
+                        raise ValueError(
+                            "Storm object must be interpolated to hourly using 'storm.interp().plot(...)' in order to use 'dvmax_dt' or 'speed' for fill color")
+                    if prop['levels'] is None:  # Auto-determine color levels if needed
+                        prop['levels'] = [
+                            np.nanmin(color_variable), np.nanmax(color_variable)]
+                    cmap, levels = get_cmap_levels(
+                        prop['linecolor'], prop['cmap'], prop['levels'])
+                    line_color = cmap(
+                        (color_variable - min(levels)) / (max(levels) - min(levels)))[i]
+
+                # Otherwise go with user input as is
                 else:
                     segmented_colors = False
                     line_color = prop['linecolor']
 
-                #For tropical/subtropical types, color-code if requested
+                # For tropical/subtropical types, color-code if requested
                 if i > 0:
-                    if i_type in constants.TROPICAL_STORM_TYPES and storm['type'][i-1] in constants.TROPICAL_STORM_TYPES:
+                    if i_type in constants.TROPICAL_STORM_TYPES and storm['type'][i - 1] in constants.TROPICAL_STORM_TYPES:
 
-                        #Plot underlying black and overlying colored line
-                        self.ax.plot([lons[i-1],lons[i]],[storm['lat'][i-1],storm['lat'][i]],'-',
-                                      linewidth=prop['linewidth']*1.33,color='k',zorder=storm_idx*5,
-                                      transform=ccrs.PlateCarree())
-                        self.ax.plot([lons[i-1],lons[i]],[storm['lat'][i-1],storm['lat'][i]],'-',
-                                      linewidth=prop['linewidth'],color=line_color,zorder=i_vmax+(storm_idx*5),
-                                      transform=ccrs.PlateCarree())
+                        # Plot underlying black and overlying colored line
+                        self.ax.plot([lons[i - 1], lons[i]], [storm['lat'][i - 1], storm['lat'][i]], '-',
+                                     linewidth=prop['linewidth'] * 1.33, color='k', zorder=storm_idx * 5,
+                                     transform=ccrs.PlateCarree())
+                        self.ax.plot([lons[i - 1], lons[i]], [storm['lat'][i - 1], storm['lat'][i]], '-',
+                                     linewidth=prop['linewidth'], color=line_color, zorder=i_vmax + (
+                                         storm_idx * 5),
+                                     transform=ccrs.PlateCarree())
 
-                    #For non-tropical types, plot dotted lines
+                    # For non-tropical types, plot dotted lines
                     else:
 
-                        #Restrict line width to 1.5 max
+                        # Restrict line width to 1.5 max
                         line_width = prop['linewidth'] + 0.0
-                        if line_width > 1.5: line_width = 1.5
+                        if line_width > 1.5:
+                            line_width = 1.5
+
+                        # Plot dotted line
+                        self.ax.plot([lons[i - 1], lons[i]], [storm['lat'][i - 1], storm['lat'][i]], ':',
+                                     linewidth=line_width, color=line_color, zorder=i_vmax +
+                                     (storm_idx * 5),
+                                     transform=ccrs.PlateCarree(),
+                                     path_effects=[path_effects.Stroke(linewidth=line_width * 1.33, foreground='k'),
+                                                   path_effects.Normal()])
+
+                # Plot dots if requested
+                if prop['dots']:
+                    self.ax, segmented_colors, extra = plot_dot(self.ax, i_lon, i_lat, i_time, i_vmax, i_type,
+                                                                zorder=900 + i_vmax, storm_data=storm, prop=prop, i=i)
+                    if 'cmap' in extra.keys():
+                        cmap = extra['cmap']
+                    if 'levels' in extra.keys():
+                        levels = extra['levels']
 
-                        #Plot dotted line
-                        self.ax.plot([lons[i-1],lons[i]],[storm['lat'][i-1],storm['lat'][i]],':',
-                                      linewidth=line_width,color=line_color,zorder=i_vmax+(storm_idx*5),
-                                      transform=ccrs.PlateCarree(),
-                                      path_effects=[path_effects.Stroke(linewidth=line_width*1.33, foreground='k'),
-                                                    path_effects.Normal()])
-                
-                #Plot dots if requested
-                if prop['dots'] == True:
-                    self.ax,segmented_colors,extra = plot_dot(self.ax,i_lon,i_lat,i_date,i_vmax,i_type,
-                                                              zorder=900+i_vmax,storm_data=storm,prop=prop,i=i)
-                    if 'cmap' in extra.keys(): cmap = extra['cmap']
-                    if 'levels' in extra.keys(): levels = extra['levels']
-
-        #--------------------------------------------------------------------------------------
-        
-        #Pre-generated domains
+        # --------------------------------------------------------------------------------------
+
+        # Pre-generated domains
         if domain is None:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(season.basin)
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(
+                season.basin)
         else:
-            bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-            
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+            bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
+
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
         emdash = u"\u2014"
-        basin_name = ((season.basin).replace("_"," ")).title()
+        basin_name = ((season.basin).replace("_", " ")).title()
         if season.basin == 'all':
             season_title = f"{season.year} Global Tropical Cyclone Season"
         elif season.basin == 'both':
             season_title = f"{season.year} Atlantic-Pacific Hurricane Season"
-        elif season.basin in ['south_indian','south_atlantic','australia','south_pacific']:
+        elif season.basin in ['south_indian', 'south_atlantic', 'australia', 'south_pacific']:
             season_title = f"{season.year-1}{emdash}{season.year} {basin_name} Tropical Cyclone Season"
         elif season.basin in ['west_pacific']:
             season_title = f"{season.year} {basin_name.split(' ')[1]} Typhoon Season"
         else:
             season_title = f"{season.year} {basin_name.split(' ')[1]} Hurricane Season"
-        self.ax.set_title(season_title,loc='left',fontsize=17,fontweight='bold')
+        self.ax.set_title(season_title, loc='left',
+                          fontsize=17, fontweight='bold')
 
-        #Add right title
+        # Add right title
         endash = u"\u2013"
         dot = u"\u2022"
         count_named = sinfo['season_named']
         count_hurricane = sinfo['season_hurricane']
         count_major = sinfo['season_major']
         count_ace = sinfo['season_ace']
-        if isinstance(season.year,list) == True:
+        if isinstance(season.year, list):
             count_named = np.sum(sinfo['season_named'])
             count_hurricane = np.sum(sinfo['season_hurricane'])
             count_major = np.sum(sinfo['season_major'])
             count_ace = np.sum(sinfo['season_ace'])
-        self.ax.set_title(f"{count_named} named {dot} {count_hurricane} hurricanes {dot} {count_major} major\n{count_ace:.1f} Cumulative ACE",loc='right',fontsize=13)
+        self.ax.set_title(
+            f"{count_named} named {dot} {count_hurricane} hurricanes {dot} {count_major} major\n{count_ace:.1f} Cumulative ACE", loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
 
-        #--------------------------------------------------------------------------------------
-        
-        #Add plot credit
-        warning_text=""
+        # Add plot credit
+        warning_text = ""
         if storm['source'] == 'ibtracs' and storm['source_info'] == 'World Meteorological Organization (official)':
             warning_text = f"This plot uses 10-minute averaged WMO official wind data converted\nto 1-minute average (factor of 0.88). Use this wind data with caution.\n\n"
 
-            self.ax.text(0.99,0.01,warning_text,fontsize=9,color='k',alpha=0.7,
-            transform=self.ax.transAxes,ha='right',va='bottom',zorder=10)
-        
+            self.ax.text(0.99, 0.01, warning_text, fontsize=9, color='k', alpha=0.7,
+                         transform=self.ax.transAxes, ha='right', va='bottom', zorder=10)
+
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
-                
-        #--------------------------------------------------------------------------------------
-        
-        #Add legend
-        self.ax,self.fig = add_legend(self.ax,self.fig,prop,segmented_colors,levels,cmap,storm)
-                
-        #--------------------------------------------------------------------------------------
-        
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return axis if specified, otherwise display figure
+
+        # --------------------------------------------------------------------------------------
+
+        # Add legend
+        self.ax, self.fig = add_legend(
+            self.ax, self.fig, prop, segmented_colors, levels, cmap, storm)
+
+        # --------------------------------------------------------------------------------------
+
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return axis if specified, otherwise display figure
         return self.ax
-        
+
     def plot_track_labels(self, ax, labels, track, k=0.01):
 
         label_nodes = list(labels.keys())
-        labels['place1'] = (2*labels[label_nodes[0]][0]-labels[label_nodes[1]][0],\
-                          2*labels[label_nodes[0]][1]-labels[label_nodes[1]][1])
-        labels['place2'] = (2*labels[label_nodes[-1]][0]-labels[label_nodes[-2]][0],\
-                          2*labels[label_nodes[-1]][1]-labels[label_nodes[-2]][1])
+        labels['place1'] = (2 * labels[label_nodes[0]][0] - labels[label_nodes[1]][0],
+                            2 * labels[label_nodes[0]][1] - labels[label_nodes[1]][1])
+        labels['place2'] = (2 * labels[label_nodes[-1]][0] - labels[label_nodes[-2]][0],
+                            2 * labels[label_nodes[-1]][1] - labels[label_nodes[-2]][1])
         track['place1'] = labels['place1']
         track['place2'] = labels['place2']
-        
+
         G = nx.DiGraph()
         track_nodes = []
         init_pos = {}
-        
+
         for lab in track.keys():
             labG = 'track_{0}'.format(lab)
             G.add_node(labG)
             track_nodes.append(labG)
             init_pos[labG] = track[lab]
-            
+
         for lab in labels.keys():
             G.add_node(lab)
-            G.add_edge(lab,'track_{0}'.format(lab))
+            G.add_edge(lab, 'track_{0}'.format(lab))
             init_pos[lab] = labels[lab]
-            
+
         pos = nx.spring_layout(G, pos=init_pos, fixed=track_nodes, k=k)
 
         # undo spring_layout's rescaling
         pos_after = np.vstack([pos[d] for d in track_nodes])
         pos_before = np.vstack([init_pos[d] for d in track_nodes])
-        scale, shift_x = np.polyfit(pos_after[:,0], pos_before[:,0], 1)
-        scale, shift_y = np.polyfit(pos_after[:,1], pos_before[:,1], 1)
+        scale, shift_x = np.polyfit(pos_after[:, 0], pos_before[:, 0], 1)
+        scale, shift_y = np.polyfit(pos_after[:, 1], pos_before[:, 1], 1)
         shift = np.array([shift_x, shift_y])
         for key, val in pos.items():
-            pos[key] = (val*scale) + shift
+            pos[key] = (val * scale) + shift
 
         for label, _ in G.edges():
             if 'place' not in label:
                 self.ax.annotate(label,
-                            xy=init_pos[label], xycoords='data',
-                            xytext=pos[label], textcoords='data', fontweight='bold', ha='center', va='center',
-                            arrowprops=dict(arrowstyle="-",#->
-                                            shrinkA=0, shrinkB=0,
-                                            connectionstyle="arc3", 
-                                            color='k'),
-                            transform=ccrs.PlateCarree())
-    
+                                 xy=init_pos[label], xycoords='data',
+                                 xytext=pos[label], textcoords='data', fontweight='bold', ha='center', va='center',
+                                 arrowprops=dict(arrowstyle="-",  # ->
+                                                 shrinkA=0, shrinkB=0,
+                                                 connectionstyle="arc3",
+                                                 color='k'),
+                                 transform=ccrs.PlateCarree())
+
     def plot_nhc_labels(self, ax, x, y, labels, k=0.01):
 
         G = nx.DiGraph()
         data_nodes = []
         init_pos = {}
         for xi, yi, label in zip(x, y, labels):
             data_str = 'data_{0}'.format(label)
@@ -1884,42 +2086,41 @@
             init_pos[label] = (xi, yi)
 
         pos = nx.spring_layout(G, pos=init_pos, fixed=data_nodes, k=k)
 
         # undo spring_layout's rescaling
         pos_after = np.vstack([pos[d] for d in data_nodes])
         pos_before = np.vstack([init_pos[d] for d in data_nodes])
-        scale, shift_x = np.polyfit(pos_after[:,0], pos_before[:,0], 1)
-        scale, shift_y = np.polyfit(pos_after[:,1], pos_before[:,1], 1)
+        scale, shift_x = np.polyfit(pos_after[:, 0], pos_before[:, 0], 1)
+        scale, shift_y = np.polyfit(pos_after[:, 1], pos_before[:, 1], 1)
         shift = np.array([shift_x, shift_y])
         for key, val in pos.items():
-            pos[key] = (val*scale) + shift
+            pos[key] = (val * scale) + shift
 
-        #Apply coordinate transform
+        # Apply coordinate transform
         transform = ccrs.PlateCarree()._as_mpl_transform(self.ax)
-        
+
         start = False
         for label, data_str in G.edges():
-            if start == False:
+            if not start:
                 start = True
                 continue
-            self.ax.annotate(label, #xycoords="data"
-                        xy=pos[data_str], xycoords=transform,
-                        xytext=pos[label], textcoords=transform, fontweight='bold', ha='center', va='center',
-                        arrowprops=dict(arrowstyle="-",#->
-                                        shrinkA=0, shrinkB=0,
-                                        connectionstyle="arc3", 
-                                        color='k'),
-                        transform=ccrs.PlateCarree(),clip_on=True)
+            self.ax.annotate(label,  # xycoords="data"
+                             xy=pos[data_str], xycoords=transform,
+                             xytext=pos[label], textcoords=transform, fontweight='bold', ha='center', va='center',
+                             arrowprops=dict(arrowstyle="-",  # ->
+                                             shrinkA=0, shrinkB=0,
+                                             connectionstyle="arc3",
+                                             color='k'),
+                             transform=ccrs.PlateCarree(), clip_on=True)
 
-    def plot_gridded(self,xcoord,ycoord,zcoord,varname='type',VEC_FLAG=False,domain="north_atlantic",ax=None,prop={},map_prop={}):
-        
+    def plot_gridded(self, xcoord, ycoord, zcoord, varname='type', VEC_FLAG=False, domain="north_atlantic", ax=None, prop={}, map_prop={}):
         r"""
         Creates a plot of a single storm track.
-        
+
         Parameters
         ----------
         storm : str, tuple or dict
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), tuple with storm name and year (e.g., ("Matthew",2016)), or a dict entry.
         domain : str
             Domain for the plot. Default is TrackDataset basin. Can be one of the following:
             "north_atlantic" - North Atlantic Ocean basin
@@ -1930,488 +2131,581 @@
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         prop : dict
             Property of storm track lines.
         map_prop : dict
             Property of cartopy map.
         """
-        
-        #Set default properties
-        default_prop={'cmap':'category','levels':None,\
-                      'left_title':'','right_title':'All storms',
-                      'plot_values':False,'values_size':None}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        
-        #Initialize plot
-        prop = self.add_prop(prop,default_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #Determine if contour levels are automatically generated
-        auto_levels = True if prop['levels'] is None or prop['levels'] == [] else False
-
-        #Plot domain
-        bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+
+        # Set default properties
+        default_prop = {'cmap': 'category', 'levels': None,
+                        'left_title': '', 'right_title': 'All storms',
+                        'plot_values': False, 'values_size': None}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+
+        # Initialize plot
+        prop = self.add_prop(prop, default_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # Determine if contour levels are automatically generated
+        auto_levels = True if prop['levels'] is None or prop['levels'] == [
+        ] else False
+
+        # Plot domain
+        bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
-        
-        #--------------------------------------------------------------------------------------
+
+        # --------------------------------------------------------------------------------------
 
         if VEC_FLAG:
             vecmag = np.hypot(*zcoord)
             if prop['levels'] is None:
-                prop['levels'] = (np.nanmin(vecmag),np.nanmax(vecmag))
+                prop['levels'] = (np.nanmin(vecmag), np.nanmax(vecmag))
         elif prop['levels'] is None:
-            prop['levels'] = (np.nanmin(zcoord),np.nanmax(zcoord))
-        cmap,clevs = get_cmap_levels(varname,prop['cmap'],prop['levels'])
-        
-        #Generate contourf levels
+            prop['levels'] = (np.nanmin(zcoord), np.nanmax(zcoord))
+        cmap, clevs = get_cmap_levels(varname, prop['cmap'], prop['levels'])
+
+        # Generate contourf levels
         if len(clevs) == 2:
             y0 = min(clevs)
             y1 = max(clevs)
-            dy = (y1-y0)/8
-            scalemag = int(np.log(dy)/np.log(10))
-            dy_scaled = dy*10**-scalemag
-            dc = min([1,2,5,10], key=lambda x:abs(x-dy_scaled))
-            c0 = np.ceil(y0/dc*10**-scalemag)*dc*10**scalemag
-            c1 = np.floor(y1/dc*10**-scalemag)*dc*10**scalemag
-            clevs = np.arange(c0,c1+dc,dc)
-        
+            dy = (y1 - y0) / 8
+            scalemag = int(np.log(dy) / np.log(10))
+            dy_scaled = dy * 10**-scalemag
+            dc = min([1, 2, 5, 10], key=lambda x: abs(x - dy_scaled))
+            c0 = np.ceil(y0 / dc * 10**-scalemag) * dc * 10**scalemag
+            c1 = np.floor(y1 / dc * 10**-scalemag) * dc * 10**scalemag
+            clevs = np.arange(c0, c1 + dc, dc)
+
         if varname == 'vmax' and prop['cmap'] == 'category':
-            vmin = min(clevs); vmax = max(clevs)
+            vmin = min(clevs)
+            vmax = max(clevs)
         else:
-            vmin = min(prop['levels']); vmax = max(prop['levels'])
-        
-        #For difference/change plots with automatically generated contour levels, ensure that 0 is in the middle
-        if auto_levels == True:
-            if varname in ['dvmax_dt','dmslp_dt'] or '\n' in prop['title_R']:
-                max_val = np.max([np.abs(vmin),vmax])
-                vmin = np.round(max_val * -1.0,2)
-                vmax = np.round(max_val * 1.0,2)
-                clevs = [vmin,np.round(vmin*0.5,2),0,np.round(vmax*0.5,2),vmax]
-        
-        if len(xcoord.shape) and len(ycoord.shape)==1:
-            xcoord,ycoord = np.meshgrid(xcoord,ycoord)
-        
+            vmin = min(prop['levels'])
+            vmax = max(prop['levels'])
+
+        # For difference/change plots with automatically generated contour levels, ensure that 0 is in the middle
+        if auto_levels:
+            if varname in ['dvmax_dt', 'dmslp_dt'] or '\n' in prop['title_R']:
+                max_val = np.max([np.abs(vmin), vmax])
+                vmin = np.round(max_val * -1.0, 2)
+                vmax = np.round(max_val * 1.0, 2)
+                clevs = [vmin, np.round(vmin * 0.5, 2),
+                         0, np.round(vmax * 0.5, 2), vmax]
+
+        if len(xcoord.shape) and len(ycoord.shape) == 1:
+            xcoord, ycoord = np.meshgrid(xcoord, ycoord)
+
         if VEC_FLAG:
-            binsize = abs(xcoord[0,0]-xcoord[0,1])
-            cbmap = self.ax.pcolor(xcoord,ycoord,vecmag[:-1,:-1],cmap=cmap,vmin=min(clevs),vmax=max(clevs),
-                               transform=ccrs.PlateCarree())            
-            zcoord = zcoord/vecmag*binsize
-            x_center = (xcoord[:-1,:-1]+xcoord[1:,1:])*.5
-            y_center = (ycoord[:-1,:-1]+ycoord[1:,1:])*.5
-            u = zcoord[0][:-1,:-1]
-            v = zcoord[1][:-1,:-1]
+            binsize = abs(xcoord[0, 0] - xcoord[0, 1])
+            cbmap = self.ax.pcolor(xcoord, ycoord, vecmag[:-1, :-1], cmap=cmap, vmin=min(clevs), vmax=max(clevs),
+                                   transform=ccrs.PlateCarree())
+            zcoord = zcoord / vecmag * binsize
+            x_center = (xcoord[:-1, :-1] + xcoord[1:, 1:]) * .5
+            y_center = (ycoord[:-1, :-1] + ycoord[1:, 1:]) * .5
+            u = zcoord[0][:-1, :-1]
+            v = zcoord[1][:-1, :-1]
             if not prop['plot_values']:
-                self.ax.quiver(x_center,y_center,u,v,color='w',alpha=0.6,transform=ccrs.PlateCarree(),\
-                           pivot='mid',width=.001*binsize,headwidth=3.5,headlength=4.5,headaxislength=4)
+                self.ax.quiver(x_center, y_center, u, v, color='w', alpha=0.6, transform=ccrs.PlateCarree(),
+                               pivot='mid', width=.001 * binsize, headwidth=3.5, headlength=4.5, headaxislength=4)
             zcoord = vecmag
-        
+
         else:
             print('--> Generating plot')
-            #if varname=='date' and prop['smooth'] is not None:
+            # if varname=='date' and prop['smooth'] is not None:
             #    zcoord[np.isnan(zcoord)]=0
             #    zcoord=gfilt(zcoord,sigma=prop['smooth'])
             #    zcoord[zcoord<min(clevs)]=np.nan
-            
-            if prop['cmap']=='category' and varname=='vmax':
-                norm = mcolors.BoundaryNorm(clevs,cmap.N)
-                cbmap = self.ax.pcolor(xcoord,ycoord,zcoord[:-1,:-1],cmap=cmap,norm=norm,
+
+            if prop['cmap'] == 'category' and varname == 'vmax':
+                norm = mcolors.BoundaryNorm(clevs, cmap.N)
+                cbmap = self.ax.pcolor(xcoord, ycoord, zcoord[:-1, :-1], cmap=cmap, norm=norm,
                                        transform=ccrs.PlateCarree())
             else:
-                norm = mcolors.Normalize(vmin=vmin,vmax=vmax)
-                cbmap = self.ax.pcolor(xcoord,ycoord,zcoord[:-1,:-1],cmap=cmap,norm=norm,
+                norm = mcolors.Normalize(vmin=vmin, vmax=vmax)
+                cbmap = self.ax.pcolor(xcoord, ycoord, zcoord[:-1, :-1], cmap=cmap, norm=norm,
                                        transform=ccrs.PlateCarree())
         if prop['plot_values']:
-            binsize = abs(xcoord[0,0]-xcoord[0,1])
-            x_center = (xcoord[:-1,:-1]+xcoord[1:,1:])*.5
-            y_center = (ycoord[:-1,:-1]+ycoord[1:,1:])*.5
+            binsize = abs(xcoord[0, 0] - xcoord[0, 1])
+            x_center = (xcoord[:-1, :-1] + xcoord[1:, 1:]) * .5
+            y_center = (ycoord[:-1, :-1] + ycoord[1:, 1:]) * .5
             xs = x_center.flatten(order='C')
             ys = y_center.flatten(order='C')
-            zs = zcoord[:-1,:-1].flatten(order='C')
+            zs = zcoord[:-1, :-1].flatten(order='C')
             if prop['values_size'] is None:
-                fs = binsize*4
+                fs = binsize * 4
             else:
                 fs = prop['values_size']
-            for xtext,ytext,ztext in zip(xs,ys,zs):
-                if not np.isnan(ztext) and xtext%360>bound_w%360 and xtext%360<bound_e%360 and\
-                    ytext>bound_s and ytext<bound_n:
+            for xtext, ytext, ztext in zip(xs, ys, zs):
+                if not np.isnan(ztext) and xtext % 360 > bound_w % 360 and xtext % 360 < bound_e % 360 and\
+                        ytext > bound_s and ytext < bound_n:
                     square_color = cmap(norm(ztext))
-                    square_brightness = np.mean(square_color[:3])*square_color[3]
-                    text_color = 'k' if square_brightness>0.5 else 'w' 
-                    self.ax.text(xtext,ytext,ztext.astype(int),ha='center',va='center',fontsize=fs,\
-                                     color=text_color,alpha=0.8,transform=ccrs.PlateCarree(), zorder=2)
-                
-
-        #--------------------------------------------------------------------------------------
-
-        
-        #Phantom legend
-        handles=[]
+                    square_brightness = np.mean(
+                        square_color[:3]) * square_color[3]
+                    text_color = 'k' if square_brightness > 0.5 else 'w'
+                    self.ax.text(xtext, ytext, ztext.astype(int), ha='center', va='center', fontsize=fs,
+                                 color=text_color, alpha=0.8, transform=ccrs.PlateCarree(), zorder=2)
+
+        # --------------------------------------------------------------------------------------
+
+        # Phantom legend
+        handles = []
         for _ in range(10):
-            handles.append(mlines.Line2D([], [], linestyle='-',label='',lw=0))
-        l = self.ax.legend(handles=handles,loc='upper left',fancybox=True,framealpha=0,fontsize=11.5)
+            handles.append(mlines.Line2D(
+                [], [], linestyle='-', label='', lw=0))
+        l = self.ax.legend(handles=handles, loc='upper left',
+                           fancybox=True, framealpha=0, fontsize=11.5)
         plt.draw()
 
-        #Get the bbox
+        # Get the bbox
         try:
             bb = l.legendPatch.get_bbox().inverse_transformed(self.fig.transFigure)
         except:
             bb = l.legendPatch.get_bbox().transformed(self.fig.transFigure.inverted())
         bb_ax = self.ax.get_position()
 
-        #Define colorbar axis
-        cax = self.fig.add_axes([bb.x0+1.2*bb.width, bb.y0-.05*bb.height, 0.015, bb.height])
+        # Define colorbar axis
+        cax = self.fig.add_axes(
+            [bb.x0 + 1.2 * bb.width, bb.y0 - .05 * bb.height, 0.015, bb.height])
 #        cbmap = mlib.cm.ScalarMappable(norm=norm, cmap=cmap)
-        cbar = self.fig.colorbar(cbmap,cax=cax,orientation='vertical',\
+        cbar = self.fig.colorbar(cbmap, cax=cax, orientation='vertical',
                                  ticks=clevs)
-            
+
         """
         if len(prop['levels'])>2:
             cax.yaxis.set_ticks(np.linspace(min(clevs),max(clevs),len(clevs)))
             cax.yaxis.set_ticks(np.linspace(0,1,len(clevs)))
             cax.yaxis.set_ticklabels(clevs)
         else:
             cax.yaxis.set_ticks(clevs)
         """
         cax.tick_params(labelsize=11.5)
         cax.yaxis.set_ticks_position('left')
-    
+
         rect_offset = 0.0
-        if prop['cmap']=='category' and varname=='vmax':
-            cax.yaxis.set_ticks(np.linspace(min(clevs),max(clevs),len(clevs)))
+        if prop['cmap'] == 'category' and varname == 'vmax':
+            cax.yaxis.set_ticks(np.linspace(
+                min(clevs), max(clevs), len(clevs)))
             cax.yaxis.set_ticklabels(clevs)
             cax2 = cax.twinx()
             cax2.yaxis.set_ticks_position('right')
-            cax2.yaxis.set_ticks((np.linspace(0,1,len(clevs))[:-1]+np.linspace(0,1,len(clevs))[1:])*.5)
-            cax2.set_yticklabels(['TD','TS','Cat-1','Cat-2','Cat-3','Cat-4','Cat-5'],fontsize=11.5)
+            cax2.yaxis.set_ticks((np.linspace(0, 1, len(clevs))[
+                                 :-1] + np.linspace(0, 1, len(clevs))[1:]) * .5)
+            cax2.set_yticklabels(
+                ['TD', 'TS', 'Cat-1', 'Cat-2', 'Cat-3', 'Cat-4', 'Cat-5'], fontsize=11.5)
             cax2.tick_params('both', length=0, width=0, which='major')
             cax.yaxis.set_ticks_position('left')
-            
+
             rect_offset = 0.7
         if varname == 'date':
-            cax.set_yticklabels([f'{mdates.num2date(i):%b %-d}' for i in clevs],fontsize=11.5)
-            
-        rectangle = mpatches.Rectangle((bb.x0,bb.y0-0.1*bb.height),(2+rect_offset)*bb.width,1.1*bb.height,\
-                                       fc = 'w',edgecolor = '0.8',alpha = 0.8,\
+            cax.set_yticklabels(
+                [f'{mdates.num2date(i):%b %-d}' for i in clevs], fontsize=11.5)
+
+        rectangle = mpatches.Rectangle((bb.x0, bb.y0 - 0.1 * bb.height), (2 + rect_offset) * bb.width, 1.1 * bb.height,
+                                       fc='w', edgecolor='0.8', alpha=0.8,
                                        transform=self.fig.transFigure, zorder=3)
         self.ax.add_patch(rectangle)
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add left title
+
+        # --------------------------------------------------------------------------------------
+
+        # Add left title
         try:
-            self.ax.set_title(prop['title_L'],loc='left',fontsize=17,fontweight='bold')
+            self.ax.set_title(prop['title_L'], loc='left',
+                              fontsize=17, fontweight='bold')
         except:
             pass
-        
-        #Add right title
+
+        # Add right title
         try:
-            self.ax.set_title(prop['title_R'],loc='right',fontsize=15)
+            self.ax.set_title(prop['title_R'], loc='right', fontsize=15)
         except:
             pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add plot credit
+
+        # --------------------------------------------------------------------------------------
+
+        # Add plot credit
         text = self.plot_credit()
         self.add_credit(text)
-        
-        #Return axis if specified, otherwise display figure
+
+        # Return axis if specified, otherwise display figure
         return self.ax
 
-    def plot_summary(self,storms,forecasts,shapefiles,valid_date,domain,ax=None,save_path=None,two_prop={},invest_prop={},storm_prop={},cone_prop={},map_prop={}):
-        
+    def plot_summary(self, storms, forecasts, shapefiles, valid_date, domain, ax=None, save_path=None, two_prop={}, invest_prop={}, storm_prop={}, cone_prop={}, map_prop={}):
         r"""
         Creates a realtime summary plot.
         """
-        
-        #Set default properties
-        default_two_prop={'plot':True,'fontsize':12,'days':5,'ms':15}
-        default_invest_prop={'plot':True,'fontsize':12,'linewidth':0.8,'linecolor':'k','linestyle':'dotted','ms':14}
-        default_storm_prop={'plot':True,'fontsize':12,'linewidth':0.8,'linecolor':'k','linestyle':'dotted','fillcolor':'category','label_category':True,'ms':14}
-        default_cone_prop={'plot':True,'linewidth':1.5,'linecolor':'k','alpha':0.6,'days':5,'fillcolor':'category','label_category':True,'ms':12}
-        default_map_prop={'res':'m','land_color':'#FBF5EA','ocean_color':'#EDFBFF','linewidth':0.5,'linecolor':'k','figsize':(14,9),'dpi':200,'plot_gridlines':True}
-        if domain == 'all': default_map_prop['res'] = 'l'
-        
-        #Initialize plot
-        two_prop = self.add_prop(two_prop,default_two_prop)
-        invest_prop = self.add_prop(invest_prop,default_invest_prop)
-        storm_prop = self.add_prop(storm_prop,default_storm_prop)
-        cone_prop = self.add_prop(cone_prop,default_cone_prop)
-        map_prop = self.add_prop(map_prop,default_map_prop)
-        self.plot_init(ax,map_prop)
-        
-        #Plot domain
-        bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Format title
+
+        # Set default properties
+        default_two_prop = {'plot': True, 'fontsize': 12, 'days': 7, 'ms': 15}
+        default_invest_prop = {'plot': True, 'fontsize': 12,
+                               'linewidth': 0.8, 'linecolor': 'k', 'linestyle': 'dotted', 'ms': 14}
+        default_storm_prop = {'plot': True, 'fontsize': 12, 'linewidth': 0.8, 'linecolor': 'k',
+                              'linestyle': 'dotted', 'fillcolor': 'category', 'label_category': True, 'ms': 14}
+        default_cone_prop = {'plot': True, 'linewidth': 1.5, 'linecolor': 'k', 'alpha': 0.6,
+                             'days': 5, 'fillcolor': 'category', 'label_category': True, 'ms': 12}
+        default_map_prop = {'res': 'm', 'land_color': '#FBF5EA', 'ocean_color': '#EDFBFF',
+                            'linewidth': 0.5, 'linecolor': 'k', 'figsize': (14, 9), 'dpi': 200, 'plot_gridlines': True}
+        if domain == 'all':
+            default_map_prop['res'] = 'l'
+
+        # Initialize plot
+        two_prop = self.add_prop(two_prop, default_two_prop)
+        invest_prop = self.add_prop(invest_prop, default_invest_prop)
+        storm_prop = self.add_prop(storm_prop, default_storm_prop)
+        cone_prop = self.add_prop(cone_prop, default_cone_prop)
+        map_prop = self.add_prop(map_prop, default_map_prop)
+        self.plot_init(ax, map_prop)
+
+        # Plot domain
+        bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Format title
         add_title = ""
-        if two_prop['plot'] == True:
-            if two_prop['days'] == 2 or valid_date <= dt(2014,7,1):
+        if two_prop['plot']:
+            if two_prop['days'] == 2 or valid_date <= dt(2014, 7, 1):
                 add_title = " & NHC 2-Day Formation Outlook"
-            else:
+            elif valid_date <= dt(2023, 5, 1):
                 add_title = " & NHC 5-Day Formation Outlook"
-        
-        #--------------------------------------------------------------------------------------
-        
-        bbox_prop = {'facecolor':'white','alpha':0.5,'edgecolor':'black','boxstyle':'round,pad=0.3'}
-        
-        if two_prop['plot'] == True:
-            
-            #Store color
-            color_base = {'Low':'yellow','Medium':'orange','High':'red'}
+            else:
+                add_title = " & NHC 7-Day Formation Outlook"
+
+        # --------------------------------------------------------------------------------------
+
+        bbox_prop = {'facecolor': 'white', 'alpha': 0.5,
+                     'edgecolor': 'black', 'boxstyle': 'round,pad=0.3'}
+
+        if two_prop['plot']:
 
-            #Plot areas
-            if shapefiles['areas'] != None:
+            # Store color
+            color_base = {'Low': 'yellow', 'Medium': 'orange', 'High': 'red'}
+
+            # Plot areas
+            if shapefiles['areas'] is not None:
                 for record, geom in zip(shapefiles['areas'].records(), shapefiles['areas'].geometries()):
 
-                    #Read relevant data
-                    if 'RISK2DAY' in record.attributes.keys() or 'RISK5DAY' in record.attributes.keys():
+                    # Read relevant data
+                    if 'RISK2DAY' in record.attributes.keys() or 'RISK5DAY' in record.attributes.keys() or 'RISK7DAY' in record.attributes.keys():
                         if two_prop['days'] == 2:
-                            color = color_base.get(record.attributes['RISK2DAY'],'yellow')
+                            color = color_base.get(
+                                record.attributes['RISK2DAY'], 'yellow')
+                        elif 'RISK5DAY' in record.attributes.keys():
+                            color = color_base.get(
+                                record.attributes['RISK5DAY'], 'yellow')
                         else:
-                            color = color_base.get(record.attributes['RISK5DAY'],'yellow')
+                            color = color_base.get(
+                                record.attributes['RISK7DAY'], 'yellow')
                     else:
-                        color = color_base.get(record.attributes['GENCAT'],'yellow')
+                        color = color_base.get(
+                            record.attributes['GENCAT'], 'yellow')
 
-                    #Plot area
+                    # Plot area
                     self.ax.add_feature(cfeature.ShapelyFeature([geom], ccrs.PlateCarree()),
                                         facecolor=color, edgecolor=color, alpha=0.3, linewidth=1.5, zorder=3)
 
-                    #Plot hatching
+                    # Plot hatching
                     self.ax.add_feature(cfeature.ShapelyFeature([geom], ccrs.PlateCarree()),
                                         facecolor='none', edgecolor='k', linewidth=2.25, zorder=4)
                     self.ax.add_feature(cfeature.ShapelyFeature([geom], ccrs.PlateCarree()),
                                         facecolor='none', edgecolor=color, linewidth=1.5, zorder=4)
-                    
-                    #Add label if needed
+
+                    # Add label if needed
                     plot_coords = []
-                    if 'GENCAT' in record.attributes.keys() or shapefiles['points'] == None:
+                    if 'GENCAT' in record.attributes.keys() or shapefiles['points'] is None:
                         bounds = record.geometry.bounds
-                        plot_coords.append((bounds[0] + bounds[2]) * 0.5) #lon
-                        plot_coords.append(bounds[1]) #lat
+                        plot_coords.append(
+                            (bounds[0] + bounds[2]) * 0.5)  # lon
+                        plot_coords.append(bounds[1])  # lat
                         plot_coords.append(record.attributes['GENPROB'])
                     else:
                         found_areas = []
                         for i_record, i_point in zip(shapefiles['points'].records(), shapefiles['points'].geometries()):
                             found_areas.append(i_record.attributes['AREA'])
                         if 'AREA' in record.attributes.keys() and record.attributes['AREA'] not in found_areas:
                             bounds = record.geometry.bounds
-                            plot_coords.append((bounds[0] + bounds[2]) * 0.5) #lon
-                            plot_coords.append(bounds[1]) #lat
+                            plot_coords.append(
+                                (bounds[0] + bounds[2]) * 0.5)  # lon
+                            plot_coords.append(bounds[1])  # lat
                             if two_prop['days'] == 2:
-                                plot_coords.append(record.attributes['PROB2DAY'])
+                                plot_coords.append(
+                                    record.attributes['PROB2DAY'])
+                            elif 'PROB5DAY' in record.attributes.keys():
+                                plot_coords.append(
+                                    record.attributes['PROB5DAY'])
                             else:
-                                plot_coords.append(record.attributes['PROB5DAY'])
-                    
+                                plot_coords.append(
+                                    record.attributes['PROB7DAY'])
+
                     if len(plot_coords) > 0:
-                        #Transform coordinates for label
-                        x1, y1 = self.ax.projection.transform_point(plot_coords[0], plot_coords[1], ccrs.PlateCarree())
+                        # Transform coordinates for label
+                        x1, y1 = self.ax.projection.transform_point(
+                            plot_coords[0], plot_coords[1], ccrs.PlateCarree())
                         x2, y2 = self.ax.transData.transform((x1, y1))
                         x, y = self.ax.transAxes.inverted().transform((x2, y2))
 
                         # plot same point but using axes coordinates
                         text = plot_coords[2]
-                        a = self.ax.text(x,y-0.02,text,ha='center',va='top',transform=self.ax.transAxes,zorder=30,fontweight='bold',fontsize=two_prop['fontsize'],clip_on=True,bbox=bbox_prop)
-                        a.set_path_effects([path_effects.Stroke(linewidth=0.5,foreground='w'),path_effects.Normal()])
+                        a = self.ax.text(x, y - 0.02, text, ha='center', va='top', transform=self.ax.transAxes,
+                                         zorder=30, fontweight='bold', fontsize=two_prop['fontsize'], clip_on=True, bbox=bbox_prop)
+                        a.set_path_effects([path_effects.Stroke(
+                            linewidth=0.5, foreground='w'), path_effects.Normal()])
 
-            #Plot points
-            if shapefiles['points'] != None:
+            # Plot points
+            if shapefiles['points'] is not None:
                 for record, point in zip(shapefiles['points'].records(), shapefiles['points'].geometries()):
 
-                    #Read relevant data
                     lon = (list(point.coords)[0][0])
                     lat = (list(point.coords)[0][1])
-                    prob_2day = record.attributes['PROB2DAY'].replace(" ","")
-                    prob_5day = record.attributes['PROB5DAY'].replace(" ","")
-                    risk_2day = record.attributes['RISK2DAY'].replace(" ","")
-                    risk_5day = record.attributes['RISK5DAY'].replace(" ","")
 
-                    #Label area
+                    # Determine if 5 or 7 day outlook exists
+                    prob_2day = record.attributes['PROB2DAY'].replace(" ", "")
+                    risk_2day = record.attributes['RISK2DAY'].replace(" ", "")
+                    if 'PROB5DAY' in record.attributes.keys():
+                        prob_5day = record.attributes['PROB5DAY'].replace(
+                            " ", "")
+                        risk_5day = record.attributes['RISK5DAY'].replace(
+                            " ", "")
+                    else:
+                        prob_5day = record.attributes['PROB7DAY'].replace(
+                            " ", "")
+                        risk_5day = record.attributes['RISK7DAY'].replace(
+                            " ", "")
+
+                    # Label area
                     if two_prop['days'] == 2:
-                        color = color_base.get(risk_2day,'yellow')
+                        color = color_base.get(risk_2day, 'yellow')
                         text = prob_2day
                     else:
-                        color = color_base.get(risk_5day,'yellow')
+                        color = color_base.get(risk_5day, 'yellow')
                         text = prob_5day
-                    self.ax.plot(lon,lat,'X',ms=two_prop['ms'],color=color,mec='k',mew=1.5*(two_prop['ms']/15.0),transform=ccrs.PlateCarree(),zorder=20)
+                    self.ax.plot(lon, lat, 'X', ms=two_prop['ms'], color=color, mec='k', mew=1.5 * (
+                        two_prop['ms'] / 15.0), transform=ccrs.PlateCarree(), zorder=20)
 
-                    #Transform coordinates for label
-                    x1, y1 = self.ax.projection.transform_point(lon, lat, ccrs.PlateCarree())
+                    # Transform coordinates for label
+                    x1, y1 = self.ax.projection.transform_point(
+                        lon, lat, ccrs.PlateCarree())
                     x2, y2 = self.ax.transData.transform((x1, y1))
                     x, y = self.ax.transAxes.inverted().transform((x2, y2))
 
                     # plot same point but using axes coordinates
-                    a = self.ax.text(x,y-0.03,text,ha='center',va='top',transform=self.ax.transAxes,zorder=30,fontweight='bold',fontsize=two_prop['fontsize'],clip_on=True,bbox=bbox_prop)
-                    a.set_path_effects([path_effects.Stroke(linewidth=0.5,foreground='w'),path_effects.Normal()])
-        
-        #--------------------------------------------------------------------------------------
-        
-        if invest_prop['plot'] == True or storm_prop['plot'] == True:
-            
-            #Iterate over all storms
-            for storm_idx,storm in enumerate(storms):
-                
-                #Skip if it's already associated with a risk area, if TWO is being plotted
-                if storm.prob_2day != 'N/A' and two_prop['plot'] == True: continue
-                
-                #Plot invests
-                if storm.invest and invest_prop['plot'] == True:
-                    
-                    #Test
-                    self.ax.plot(storm.lon[-1],storm.lat[-1],'X',ms=invest_prop['ms'],color='k',transform=ccrs.PlateCarree(),zorder=20)
-                    
-                    #Transform coordinates for label
-                    x1, y1 = self.ax.projection.transform_point(storm.lon[-1], storm.lat[-1], ccrs.PlateCarree())
+                    a = self.ax.text(x, y - 0.03, text, ha='center', va='top', transform=self.ax.transAxes,
+                                     zorder=30, fontweight='bold', fontsize=two_prop['fontsize'], clip_on=True, bbox=bbox_prop)
+                    a.set_path_effects([path_effects.Stroke(
+                        linewidth=0.5, foreground='w'), path_effects.Normal()])
+
+        # --------------------------------------------------------------------------------------
+
+        if invest_prop['plot'] or storm_prop['plot']:
+
+            # Iterate over all storms
+            for storm_idx, storm in enumerate(storms):
+
+                # Skip if it's already associated with a risk area, if TWO is being plotted
+                if storm.realtime and storm.prob_2day != 'N/A' and two_prop['plot']:
+                    continue
+
+                # Plot invests
+                if storm.invest and invest_prop['plot']:
+
+                    # Test
+                    self.ax.plot(storm.lon[-1], storm.lat[-1], 'X', ms=invest_prop['ms'],
+                                 color='k', transform=ccrs.PlateCarree(), zorder=20)
+
+                    # Transform coordinates for label
+                    x1, y1 = self.ax.projection.transform_point(
+                        storm.lon[-1], storm.lat[-1], ccrs.PlateCarree())
                     x2, y2 = self.ax.transData.transform((x1, y1))
                     x, y = self.ax.transAxes.inverted().transform((x2, y2))
 
                     # plot same point but using axes coordinates
-                    a = self.ax.text(x,y-0.03,f"{storm.name.title()}",ha='center',va='top',transform=self.ax.transAxes,zorder=30,fontweight='bold',fontsize=invest_prop['fontsize'],clip_on=True,bbox=bbox_prop)
-                    a.set_path_effects([path_effects.Stroke(linewidth=0.5,foreground='w'),path_effects.Normal()])
-                    
-                    #Plot archive track
+                    a = self.ax.text(x, y - 0.03, f"{storm.name.title()}", ha='center', va='top', transform=self.ax.transAxes,
+                                     zorder=30, fontweight='bold', fontsize=invest_prop['fontsize'], clip_on=True, bbox=bbox_prop)
+                    a.set_path_effects([path_effects.Stroke(
+                        linewidth=0.5, foreground='w'), path_effects.Normal()])
+
+                    # Plot archive track
                     if invest_prop['linewidth'] > 0:
-                        self.ax.plot(storm.lon,storm.lat,color=invest_prop['linecolor'],linestyle=invest_prop['linestyle'],zorder=5,transform=ccrs.PlateCarree())
-                
-                #Plot TCs
-                elif storm.invest == False and storm_prop['plot'] == True:
-                    
-                    #Label dot
-                    #self.ax.plot(storm.lon[-1],storm.lat[-1],'o',ms=14,color='none',mec='k',mew=3.0,transform=ccrs.PlateCarree(),zorder=5)
-                    #self.ax.plot(storm.lon[-1],storm.lat[-1],'o',ms=14,color='none',mec='r',mew=2.0,transform=ccrs.PlateCarree(),zorder=6)
+                        self.ax.plot(
+                            storm.lon, storm.lat, color=invest_prop['linecolor'], linestyle=invest_prop['linestyle'], zorder=5, transform=ccrs.PlateCarree())
+
+                # Plot TCs
+                elif not storm.invest and storm_prop['plot']:
+
+                    # Label dot
                     category = str(wind_to_category(storm.vmax[-1]))
                     color = get_colors_sshws(storm.vmax[-1])
-                    if category == "0": category = 'S'
-                    if category == "-1": category = 'D'
+                    if category == "0":
+                        category = 'S'
+                    if category == "-1":
+                        category = 'D'
                     if np.isnan(storm.vmax[-1]):
                         category = 'U'
                         color = 'w'
-                    
+
                     if storm_prop['fillcolor'] == 'none':
-                        self.ax.plot(storm.lon[-1],storm.lat[-1],'o',ms=storm_prop['ms'],color='none',mec='k',mew=3.0,transform=ccrs.PlateCarree(),zorder=20)
-                        self.ax.plot(storm.lon[-1],storm.lat[-1],'o',ms=storm_prop['ms'],color='none',mec='r',mew=2.0,transform=ccrs.PlateCarree(),zorder=21)
-                    
+                        self.ax.plot(storm.lon[-1], storm.lat[-1], 'o', ms=storm_prop['ms'],
+                                     color='none', mec='k', mew=3.0, transform=ccrs.PlateCarree(), zorder=20)
+                        self.ax.plot(storm.lon[-1], storm.lat[-1], 'o', ms=storm_prop['ms'],
+                                     color='none', mec='r', mew=2.0, transform=ccrs.PlateCarree(), zorder=21)
+
                     else:
-                        if storm_prop['fillcolor'] != 'category': color = storm_prop['fillcolor']
-                        self.ax.plot(storm.lon[-1],storm.lat[-1],'o',ms=storm_prop['ms']*1.14,color='k',transform=ccrs.PlateCarree(),zorder=20)
-                        self.ax.plot(storm.lon[-1],storm.lat[-1],'o',ms=storm_prop['ms'],color=color,transform=ccrs.PlateCarree(),zorder=21)
-                        
-                        if storm_prop['label_category'] == True:
+                        if storm_prop['fillcolor'] != 'category':
+                            color = storm_prop['fillcolor']
+                        self.ax.plot(storm.lon[-1], storm.lat[-1], 'o', ms=storm_prop['ms']
+                                     * 1.14, color='k', transform=ccrs.PlateCarree(), zorder=20)
+                        self.ax.plot(storm.lon[-1], storm.lat[-1], 'o', ms=storm_prop['ms'],
+                                     color=color, transform=ccrs.PlateCarree(), zorder=21)
+
+                        if storm_prop['label_category']:
                             color = mcolors.to_rgb(color)
-                            red,green,blue = color
+                            red, green, blue = color
                             textcolor = 'w'
-                            if (red*0.299 + green*0.587 + blue*0.114) > (160.0/255.0): textcolor = 'k'
-                            self.ax.text(storm.lon[-1],storm.lat[-1],category,fontsize=storm_prop['ms']*0.83,ha='center',va='center',color=textcolor,
-                                         zorder=30,transform=ccrs.PlateCarree(),clip_on=True)
-                    
-                    #Transform coordinates for label
-                    x1, y1 = self.ax.projection.transform_point(storm.lon[-1], storm.lat[-1], ccrs.PlateCarree())
+                            if (red * 0.299 + green * 0.587 + blue * 0.114) > (160.0 / 255.0):
+                                textcolor = 'k'
+                            self.ax.text(storm.lon[-1], storm.lat[-1], category, fontsize=storm_prop['ms'] * 0.83, ha='center', va='center', color=textcolor,
+                                         zorder=30, transform=ccrs.PlateCarree(), clip_on=True)
+
+                    # Transform coordinates for label
+                    x1, y1 = self.ax.projection.transform_point(
+                        storm.lon[-1], storm.lat[-1], ccrs.PlateCarree())
                     x2, y2 = self.ax.transData.transform((x1, y1))
                     x, y = self.ax.transAxes.inverted().transform((x2, y2))
 
                     # plot same point but using axes coordinates
-                    a = self.ax.text(x,y-0.03,f"{storm.name.title()}",ha='center',va='top',transform=self.ax.transAxes,zorder=30,fontweight='bold',fontsize=storm_prop['fontsize'],clip_on=True,bbox=bbox_prop)
-                    a.set_path_effects([path_effects.Stroke(linewidth=0.5,foreground='w'),path_effects.Normal()])
-                    
-                    #Plot archive track
+                    a = self.ax.text(x, y - 0.03, f"{storm.name.title()}", ha='center', va='top', transform=self.ax.transAxes,
+                                     zorder=30, fontweight='bold', fontsize=storm_prop['fontsize'], clip_on=True, bbox=bbox_prop)
+                    a.set_path_effects([path_effects.Stroke(
+                        linewidth=0.5, foreground='w'), path_effects.Normal()])
+
+                    # Plot archive track
                     if storm_prop['linewidth'] > 0:
-                        self.ax.plot(storm.lon,storm.lat,color=storm_prop['linecolor'],linestyle=storm_prop['linestyle'],zorder=5,transform=ccrs.PlateCarree())
-                        
-                    #Plot cone
-                    if cone_prop['plot'] == True:
-                        
-                        #Retrieve cone
-                        forecast_dict = forecasts[storm_idx]
-                        
+
+                        # Fix longitudes for track if crossing dateline
+                        plot_lon = list(storm.lon)
+                        if np.nanmax(plot_lon) > 165 or np.nanmin(plot_lon) < -165:
+                            plot_lon = [i if i > 0 else i +
+                                        360.0 for i in plot_lon]
+                        self.ax.plot(plot_lon, storm.lat, color=storm_prop['linecolor'], linestyle=storm_prop['linestyle'],
+                                     zorder=5, transform=ccrs.PlateCarree())
+
+                    # Plot cone
+                    if cone_prop['plot']:
+
                         try:
-                            #Fix longitudes for cone if crossing dateline
+
+                            # Retrieve cone
+                            forecast_dict = forecasts[storm_idx]
+
+                            # Fix longitudes for cone if crossing dateline
                             if np.nanmax(forecast_dict['lon']) > 165 or np.nanmin(forecast_dict['lon']) < -165:
-                                forecast_dict['lon'] = [i if i > 0 else i + 360.0 for i in forecast_dict['lon']]
-                            cone = generate_nhc_cone(forecast_dict,storm.basin,cone_days=cone_prop['days'])
+                                forecast_dict['lon'] = [
+                                    i if i > 0 else i + 360.0 for i in forecast_dict['lon']]
+                            cone = generate_nhc_cone(
+                                forecast_dict, storm.basin, cone_days=cone_prop['days'])
 
-                            #Plot cone
+                            # Plot cone
                             if cone_prop['alpha'] > 0 and storm.basin in constants.NHC_BASINS:
                                 cone_2d = cone['cone']
-                                cone_2d = ndimage.gaussian_filter(cone_2d,sigma=0.5,order=0)
-                                self.ax.contourf(cone['lon2d'],cone['lat2d'],cone_2d,[0.9,1.1],colors=['#ffffff','#ffffff'],alpha=cone_prop['alpha'],zorder=4,transform=ccrs.PlateCarree())
-                                self.ax.contour(cone['lon2d'],cone['lat2d'],cone_2d,[0.9],linewidths=1.5,colors=['k'],zorder=4,transform=ccrs.PlateCarree())
+                                cone_2d = ndimage.gaussian_filter(
+                                    cone_2d, sigma=0.5, order=0)
+                                self.ax.contourf(cone['lon2d'], cone['lat2d'], cone_2d, [0.9, 1.1], colors=[
+                                                 '#ffffff', '#ffffff'], alpha=cone_prop['alpha'], zorder=4, transform=ccrs.PlateCarree())
+                                self.ax.contour(cone['lon2d'], cone['lat2d'], cone_2d, [
+                                                0.9], linewidths=1.5, colors=['k'], zorder=4, transform=ccrs.PlateCarree())
 
-                            #Plot center line & account for dateline crossing
+                            # Plot center line & account for dateline crossing
                             if cone_prop['linewidth'] > 0:
-                                self.ax.plot(cone['center_lon'],cone['center_lat'],color='w',linewidth=2.5,zorder=5,transform=ccrs.PlateCarree())
-                                self.ax.plot(cone['center_lon'],cone['center_lat'],color='k',linewidth=2.0,zorder=6,transform=ccrs.PlateCarree()) 
+                                self.ax.plot(cone['center_lon'], cone['center_lat'], color='w',
+                                             linewidth=2.5, zorder=5, transform=ccrs.PlateCarree())
+                                self.ax.plot(cone['center_lon'], cone['center_lat'], color='k',
+                                             linewidth=2.0, zorder=6, transform=ccrs.PlateCarree())
 
-                            #Plot forecast dots
+                            # Plot forecast dots
                             for idx in range(len(forecast_dict['lat'])):
-                                if forecast_dict['fhr'][idx]/24.0 > cone_prop['days']: continue
-                                if cone_prop['ms'] == 0: continue
-                                color = get_colors_sshws(forecast_dict['vmax'][idx])
-                                if np.isnan(forecast_dict['vmax'][idx]): color = 'w'
-                                if cone_prop['fillcolor'] != 'category': color = cone_prop['fillcolor']
-                                
+                                if forecast_dict['fhr'][idx] / 24.0 > cone_prop['days']:
+                                    continue
+                                if cone_prop['ms'] == 0:
+                                    continue
+                                color = get_colors_sshws(
+                                    forecast_dict['vmax'][idx])
+                                if np.isnan(forecast_dict['vmax'][idx]):
+                                    color = 'w'
+                                if cone_prop['fillcolor'] != 'category':
+                                    color = cone_prop['fillcolor']
+
                                 marker = 'o'
-                                if forecast_dict['type'][idx] not in constants.TROPICAL_STORM_TYPES: marker = '^'
-                                if np.isnan(forecast_dict['vmax'][idx]): marker = 'o'
-                                self.ax.plot(forecast_dict['lon'][idx],forecast_dict['lat'][idx],marker,ms=cone_prop['ms'],mfc=color,mec='k',zorder=7,transform=ccrs.PlateCarree(),clip_on=True)
-
-                                if cone_prop['label_category'] == True and marker == 'o':
-                                    category = str(wind_to_category(forecast_dict['vmax'][idx]))
-                                    if category == "0": category = 'S'
-                                    if category == "-1": category = 'D'
-                                    if np.isnan(forecast_dict['vmax'][idx]): category = 'U'
+                                if forecast_dict['type'][idx] not in constants.TROPICAL_STORM_TYPES:
+                                    marker = '^'
+                                if np.isnan(forecast_dict['vmax'][idx]):
+                                    marker = 'o'
+                                self.ax.plot(forecast_dict['lon'][idx], forecast_dict['lat'][idx], marker, ms=cone_prop['ms'],
+                                             mfc=color, mec='k', zorder=7, transform=ccrs.PlateCarree(), clip_on=True)
+
+                                if cone_prop['label_category'] and marker == 'o':
+                                    category = str(wind_to_category(
+                                        forecast_dict['vmax'][idx]))
+                                    if category == "0":
+                                        category = 'S'
+                                    if category == "-1":
+                                        category = 'D'
+                                    if np.isnan(forecast_dict['vmax'][idx]):
+                                        category = 'U'
 
                                     color = mcolors.to_rgb(color)
-                                    red,green,blue = color
+                                    red, green, blue = color
                                     textcolor = 'w'
-                                    if (red*0.299 + green*0.587 + blue*0.114) > (160.0/255.0): textcolor = 'k'
+                                    if (red * 0.299 + green * 0.587 + blue * 0.114) > (160.0 / 255.0):
+                                        textcolor = 'k'
 
-                                    self.ax.text(forecast_dict['lon'][idx],forecast_dict['lat'][idx],category,fontsize=cone_prop['ms']*0.81,ha='center',va='center',color=textcolor,
-                                                zorder=19,transform=ccrs.PlateCarree(),clip_on=True)
+                                    self.ax.text(forecast_dict['lon'][idx], forecast_dict['lat'][idx], category, fontsize=cone_prop['ms'] * 0.81, ha='center', va='center', color=textcolor,
+                                                 zorder=19, transform=ccrs.PlateCarree(), clip_on=True)
                         except:
                             pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Plot domain
-        bound_w,bound_e,bound_s,bound_n = self.set_projection(domain)
-        
-        #Plot parallels and meridians
-        #This is currently not supported for all cartopy projections.
+
+        # --------------------------------------------------------------------------------------
+
+        # Plot domain
+        bound_w, bound_e, bound_s, bound_n = self.set_projection(domain)
+
+        # Plot parallels and meridians
+        # This is currently not supported for all cartopy projections.
         if map_prop['plot_gridlines']:
             try:
-                self.plot_lat_lon_lines([bound_w,bound_e,bound_s,bound_n])
+                self.plot_lat_lon_lines([bound_w, bound_e, bound_s, bound_n])
             except:
                 pass
-        
-        #--------------------------------------------------------------------------------------
-        
-        #Add title
-        self.ax.set_title(f"Summary{add_title}",loc='left',fontsize=17,fontweight='bold')
-        self.ax.set_title(f"Valid: {valid_date.strftime('%H UTC %d %b %Y')}",loc='right',fontsize=13)
-
-        #--------------------------------------------------------------------------------------
-        
-        #Add credit
+
+        # --------------------------------------------------------------------------------------
+
+        # Add title
+        self.ax.set_title(f"Summary{add_title}",
+                          loc='left', fontsize=17, fontweight='bold')
+        self.ax.set_title(
+            f"Valid: {valid_date.strftime('%H UTC %d %b %Y')}", loc='right', fontsize=13)
+
+        # --------------------------------------------------------------------------------------
+
+        # Add credit
         credit_text = self.plot_credit()
         self.add_credit(credit_text)
-        
-        #--------------------------------------------------------------------------------------
-                
-        #Add legend
-        #self.add_legend(prop,segmented_colors,levels,cmap,storm_data)
-                
-        #-----------------------------------------------------------------------------------------
-        
-        #Save image if specified
-        if save_path is not None and isinstance(save_path,str) == True:
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return axis if specified, otherwise display figure
+
+        # --------------------------------------------------------------------------------------
+
+        # Add legend
+        # self.add_legend(prop,segmented_colors,levels,cmap,storm_data)
+
+        # -----------------------------------------------------------------------------------------
+
+        # Save image if specified
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return axis if specified, otherwise display figure
         return self.ax
```

### Comparing `tropycal-0.6.1/src/tropycal/tracks/season.py` & `tropycal-1.0/src/tropycal/tracks/season.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,26 +1,23 @@
 r"""Functionality for storing and analyzing a year/season of cyclones."""
 
 import numpy as np
-import pandas as pd
-import warnings
-from datetime import datetime as dt,timedelta
-from copy import copy
 
-#Import internal scripts
+# Import internal scripts
 from .plot import TrackPlot
 from .storm import Storm
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 from .. import constants
 
+
 class Season:
-    
+
     r"""
     Initializes an instance of Season, retrieved via ``TrackDataset.get_season()``.
 
     Parameters
     ----------
     season : dict
         Dict entry containing all storms within the requested season.
@@ -28,435 +25,484 @@
         Dict entry containing general information about the season.
 
     Returns
     -------
     Season
         Instance of a Season object.
     """
-    
+
     def __setitem__(self, key, value):
         self.__dict__[key] = value
-        
+
     def __getitem__(self, key):
         return self.__dict__[key]
-   
+
     def __add__(self, new):
-        #Add seasons
-        
-        #Ensure data sources and basins are the same
+        # Add seasons
+
+        # Ensure data sources and basins are the same
         if self.source_basin != new.source_basin:
             msg = 'Seasons can only be added for the same basin.'
             raise ValueError(msg)
         if self.source != new.source:
             msg = 'Seasons can only be added from the same source.'
             raise ValueError(msg)
-        if self.source == 'ibtracs':
-            msg = 'Only hurdat data sources are currently supported for this functionality.'
-            raise RuntimeError(msg)
-        
-        #Retrieve old & new dict entries
+
+        # Retrieve old & new dict entries
         dict_original = self.dict.copy()
         dict_new = new.dict.copy()
-        
-        #Retrieve copy of coordinates
+
+        # Retrieve copy of coordinates
         new_attrs = self.attrs.copy()
-        
-        #Add year to list of years
-        if isinstance(self.attrs['year'],int):
-            new_attrs['year'] = [self.year,new.year]
+
+        # Add year to list of years
+        if isinstance(self.attrs['year'], int):
+            new_attrs['year'] = [self.year, new.year]
         else:
             new_attrs['year'].append(new.year)
-        
-        #Sort list of years
+
+        # Sort list of years
         new_attrs['year'] = (np.sort(new_attrs['year'])).tolist()
-        
-        #Update dict
+
+        # Update dict
         dict_original.update(dict_new)
-        
-        #Iterate over every year to create a new dict
+
+        # Iterate over every year to create a new dict
         new_dict = {}
         for year in new_attrs['year']:
             for key in dict_original.keys():
-                if int(key[-4:]) == year:
+                if dict_original[key]['season'] == year:
                     new_dict[key] = dict_original[key]
-        
-        #Return new Season object
-        return Season(new_dict,new_attrs)
-    
+
+        # Return new Season object
+        return Season(new_dict, new_attrs)
+
     def __repr__(self):
-         
-        #Label object
+
+        # Label object
         summary = ["<tropycal.tracks.Season>"]
-        
-        #Format keys for summary
+
+        # Format keys for summary
         season_summary = self.summary()
-        summary_keys = {'Total Storms':season_summary['season_storms'],
-                        'Named Storms':season_summary['season_named'],
-                        'Hurricanes':season_summary['season_hurricane'],
-                        'Major Hurricanes':season_summary['season_major'],
-                        'Season ACE':season_summary['season_ace']}
+        summary_keys = {
+            'Total Storms': season_summary['season_storms'],
+            'Named Storms': season_summary['season_named'],
+            'Hurricanes': season_summary['season_hurricane'],
+            'Major Hurricanes': season_summary['season_major'],
+            'Season ACE': season_summary['season_ace'],
+        }
 
-        #Add season summary
+        # Add season summary
         summary.append("Season Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
-        #Add additional information
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
+        # Add additional information
         summary.append("\nMore Information:")
-        add_space = np.max([len(key) for key in self.attrs.keys()])+3
+        add_space = np.max([len(key) for key in self.attrs.keys()]) + 3
         for key in self.attrs.keys():
-            key_name = key+":"
+            key_name = key + ":"
             summary.append(f'{" "*4}{key_name:<{add_space}}{self.attrs[key]}')
 
         return "\n".join(summary)
-    
-    def __init__(self,season,info):
-        
-        #Save the dict entry of the season
+
+    def __init__(self, season, info):
+
+        # Save the dict entry of the season
         self.dict = season
-        
-        #Add other attributes about the storm
+
+        # Add other attributes about the storm
         keys = info.keys()
         self.attrs = {}
         for key in keys:
-            if isinstance(info[key], list) == False and isinstance(info[key], dict) == False:
+            if not isinstance(info[key], list) and not isinstance(info[key], dict):
                 self[key] = info[key]
                 self.attrs[key] = info[key]
-            if isinstance(info[key], list) == True and key == 'year':
+            if isinstance(info[key], list) and key == 'year':
                 self[key] = info[key]
                 self.attrs[key] = info[key]
-    
+
     def to_dataframe(self):
-        
         r"""
         Converts the season dict into a pandas DataFrame object.
-        
+
         Returns
         -------
         `pandas.DataFrame`
             A pandas DataFrame object containing information about the season.
         """
-        
-        #Try importing pandas
+
+        # Try importing pandas
         try:
             import pandas as pd
         except ImportError as e:
-            raise RuntimeError("Error: pandas is not available. Install pandas in order to use this function.") from e
-        
-        #Get season info
+            raise RuntimeError(
+                "Error: pandas is not available. Install pandas in order to use this function.") from e
+
+        # Get season info
         season_info = self.summary()
         season_info_keys = season_info['id']
-        
-        #Set up empty dict for dataframe
-        ds = {'id':[],'name':[],'vmax':[],'mslp':[],'category':[],'ace':[],'start_time':[],'end_time':[],'start_lat':[],'start_lon':[]}
-        
-        #Add every key containing a list into the dict
+
+        # Set up empty dict for dataframe
+        ds = {
+            'id': [],
+            'name': [],
+            'vmax': [],
+            'mslp': [],
+            'category': [],
+            'ace': [],
+            'start_time': [],
+            'end_time': [],
+            'start_lat': [],
+            'start_lon': [],
+        }
+
+        # Add every key containing a list into the dict
         keys = [k for k in self.dict.keys()]
         for key in keys:
-            #Get tropical duration
+            # Get tropical duration
             temp_type = np.array(self.dict[key]['type'])
-            tropical_idx = np.where((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (temp_type == 'TS') | (temp_type == 'HU'))
+            tropical_idx = np.where((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (
+                temp_type == 'TS') | (temp_type == 'HU') | (temp_type == 'TY') | (temp_type == 'ST'))
             if key in season_info_keys:
                 sidx = season_info_keys.index(key)
                 ds['id'].append(key)
                 ds['name'].append(self.dict[key]['name'])
                 ds['vmax'].append(season_info['max_wspd'][sidx])
                 ds['mslp'].append(season_info['min_mslp'][sidx])
                 ds['category'].append(season_info['category'][sidx])
-                ds['start_time'].append(np.array(self.dict[key]['date'])[tropical_idx][0])
-                ds['end_time'].append(np.array(self.dict[key]['date'])[tropical_idx][-1])
-                ds['start_lat'].append(np.array(self.dict[key]['lat'])[tropical_idx][0])
-                ds['start_lon'].append(np.array(self.dict[key]['lon'])[tropical_idx][0])
-                ds['ace'].append(np.round(season_info['ace'][sidx],1))
-                    
-        #Convert entire dict to a DataFrame
+                ds['start_time'].append(
+                    np.array(self.dict[key]['time'])[tropical_idx][0])
+                ds['end_time'].append(
+                    np.array(self.dict[key]['time'])[tropical_idx][-1])
+                ds['start_lat'].append(
+                    np.array(self.dict[key]['lat'])[tropical_idx][0])
+                ds['start_lon'].append(
+                    np.array(self.dict[key]['lon'])[tropical_idx][0])
+                ds['ace'].append(np.round(season_info['ace'][sidx], 1))
+
+        # Convert entire dict to a DataFrame
         ds = pd.DataFrame(ds)
 
-        #Return dataset
+        # Return dataset
         return ds
-    
-    def get_storm_id(self,storm):
-        
+
+    def get_storm_id(self, storm):
         r"""
         Returns the storm ID (e.g., "AL012019") given the storm name and year.
-        
+
         Parameters
         ----------
         storm : tuple
             Tuple containing the storm name and year (e.g., ("Matthew",2016)).
-            
+
         Returns
         -------
         str or list
             If a single storm was found, returns a string containing its ID. Otherwise returns a list of matching IDs.
         """
-        
-        #Error check
-        if isinstance(storm,tuple) == False:
+
+        # Error check
+        if not isinstance(storm, tuple):
             raise TypeError("storm must be of type tuple.")
         if len(storm) != 2:
-            raise ValueError("storm must contain 2 elements, name (str) and year (int)")
-        name,year = storm
-        
-        #Search for corresponding entry in keys
+            raise ValueError(
+                "storm must contain 2 elements, name (str) and year (int)")
+        name, year = storm
+
+        # Search for corresponding entry in keys
         keys_use = []
         for key in self.dict.keys():
             temp_year = self.dict[key]['year']
             if temp_year == year:
                 temp_name = self.dict[key]['name']
                 if temp_name == name.upper():
                     keys_use.append(key)
-                
-        #return key, or list of keys
-        if len(keys_use) == 1: keys_use = keys_use[0]
-        if len(keys_use) == 0: raise RuntimeError("Storm not found")
+
+        # return key, or list of keys
+        if len(keys_use) == 1:
+            keys_use = keys_use[0]
+        if len(keys_use) == 0:
+            raise RuntimeError("Storm not found")
         return keys_use
-    
-    def get_storm(self,storm):
-        
+
+    def get_storm(self, storm):
         r"""
         Retrieves a Storm object for the requested storm.
-        
+
         Parameters
         ----------
         storm : str or tuple
             Requested storm. Can be either string of storm ID (e.g., "AL052019"), or tuple with storm name and year (e.g., ("Matthew",2016)).
-        
+
         Returns
         -------
         tropycal.tracks.Storm
             Object containing information about the requested storm, and methods for analyzing and plotting the storm.
         """
-        
-        #Check if storm is str or tuple
+
+        # Check if storm is str or tuple
         if isinstance(storm, str):
             key = storm
         elif isinstance(storm, tuple):
-            key = self.get_storm_id((storm[0],storm[1]))
+            key = self.get_storm_id((storm[0], storm[1]))
         else:
-            raise RuntimeError("Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
-        
-        #Retrieve key of given storm
+            raise RuntimeError(
+                "Storm must be a string (e.g., 'AL052019') or tuple (e.g., ('Matthew',2016)).")
+
+        # Retrieve key of given storm
         if isinstance(key, str):
             return Storm(self.dict[key])
         else:
             error_message = ''.join([f"\n{i}" for i in key])
             error_message = f"Multiple IDs were identified for the requested storm. Choose one of the following storm IDs and provide it as the 'storm' argument instead of a tuple:{error_message}"
             raise RuntimeError(error_message)
-        
-    def plot(self,domain=None,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot(self, domain=None, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of this season.
-        
+
         Parameters
         ----------
         domain : str
             Domain for the plot. Default is basin-wide. Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
 
-        #Create instance of plot object
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         self.plot_obj = TrackPlot()
-        
-        if self.basin in ['east_pacific','west_pacific','south_pacific','australia','all']:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+
+        if self.basin in ['east_pacific', 'west_pacific', 'south_pacific', 'australia', 'all']:
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot storm
-        plot_ax = self.plot_obj.plot_season(self,domain,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_season(
+            self, domain, ax=ax, save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-        
+
     def summary(self):
-        
         r"""
         Generates a summary for this season with various cumulative statistics.
-        
+
         Returns
         -------
         dict
             Dictionary containing various statistics about this season.
         """
-        
-        #Determine if season object has a single or multiple seasons
-        multi_season = isinstance(self.year,list)
-        
-        #Initialize dict with info about all of year's storms
-        if multi_season == False:
-            summary_dict = {'id':[],'operational_id':[],'name':[],'max_wspd':[],'min_mslp':[],'category':[],'ace':[]}
+
+        # Determine if season object has a single or multiple seasons
+        multi_season = isinstance(self.year, list)
+
+        # Initialize dict with info about all of year's storms
+        if not multi_season:
+            summary_dict = {
+                'id': [],
+                'operational_id': [],
+                'name': [],
+                'max_wspd': [],
+                'min_mslp': [],
+                'category': [],
+                'ace': [],
+            }
         else:
-            summary_dict = {'id':[[] for i in range(len(self.year))],
-                           'operational_id':[[] for i in range(len(self.year))],
-                           'name':[[] for i in range(len(self.year))],
-                           'max_wspd':[[] for i in range(len(self.year))],
-                           'min_mslp':[[] for i in range(len(self.year))],
-                           'category':[[] for i in range(len(self.year))],
-                           'ace':[[] for i in range(len(self.year))],
-                           
-                           'seasons':self.year + [],
-                           'season_start':[0 for i in range(len(self.year))],
-                           'season_end':[0 for i in range(len(self.year))],
-                           'season_storms':[0 for i in range(len(self.year))],
-                           'season_named':[0 for i in range(len(self.year))],
-                           'season_hurricane':[0 for i in range(len(self.year))],
-                           'season_major':[0 for i in range(len(self.year))],
-                           'season_ace':[0 for i in range(len(self.year))],
-                           'season_subtrop_pure':[0 for i in range(len(self.year))],
-                           'season_subtrop_partial':[0 for i in range(len(self.year))],
-                          }
-        
-        #Iterate over season(s)
-        list_seasons = [self.year] if multi_season == False else self.year + []
-        for season_idx,iter_season in enumerate(list_seasons):
+            summary_dict = {
+                'id': [[] for i in range(len(self.year))],
+                'operational_id': [[] for i in range(len(self.year))],
+                'name': [[] for i in range(len(self.year))],
+                'max_wspd': [[] for i in range(len(self.year))],
+                'min_mslp': [[] for i in range(len(self.year))],
+                'category': [[] for i in range(len(self.year))],
+                'ace': [[] for i in range(len(self.year))],
+                'seasons': self.year + [],
+                'season_start': [0 for i in range(len(self.year))],
+                'season_end': [0 for i in range(len(self.year))],
+                'season_storms': [0 for i in range(len(self.year))],
+                'season_named': [0 for i in range(len(self.year))],
+                'season_hurricane': [0 for i in range(len(self.year))],
+                'season_major': [0 for i in range(len(self.year))],
+                'season_ace': [0 for i in range(len(self.year))],
+                'season_subtrop_pure': [0 for i in range(len(self.year))],
+                'season_subtrop_partial': [0 for i in range(len(self.year))],
+            }
+
+        # Iterate over season(s)
+        list_seasons = [self.year] if not multi_season else self.year + []
+        for season_idx, iter_season in enumerate(list_seasons):
 
-            #Search for corresponding entry in keys
+            # Search for corresponding entry in keys
             count_ss_pure = 0
             count_ss_partial = 0
             iterate_id = 1
             for key in self.dict.keys():
-                
-                #Skip if using multi-season object and storm is outside of this season
-                if multi_season == True and int(key[-4:]) != iter_season: continue
 
-                #Retrieve info about storm, only in this basin
+                # Skip if using multi-season object and storm is outside of this season
+                if multi_season and self.dict[key]['season'] != iter_season:
+                    continue
+
+                # Retrieve info about storm, only in this basin
                 temp_name = self.dict[key]['name']
                 temp_vmax = np.array(self.dict[key]['vmax'])
                 temp_mslp = np.array(self.dict[key]['mslp'])
                 temp_type = np.array(self.dict[key]['type'])
-                temp_time = np.array(self.dict[key]['date'])
+                temp_time = np.array(self.dict[key]['time'])
                 temp_basin = np.array(self.dict[key]['wmo_basin'])
-                temp_year = np.array([i.year for i in self.dict[key]['date']])
-                
-                #Calculate ACE within basin
+                temp_year = np.array([i.year for i in self.dict[key]['time']])
+
+                # Calculate ACE within basin
                 temp_ace = 0.0
-                for ace_i,(i_time,i_vmax,i_basin,i_type) in enumerate(zip(temp_time,temp_vmax,temp_basin,temp_type)):
-                    if self.basin not in ['all','both'] and i_basin != self.basin: continue
-                    if i_time.strftime('%H%M') not in constants.STANDARD_HOURS: continue
-                    if i_type not in constants.NAMED_TROPICAL_STORM_TYPES: continue
-                    if self.basin == 'all' and i_time.year != self.year: continue
-                    if np.isnan(i_vmax): continue
+                for ace_i, (i_time, i_vmax, i_basin, i_type) in enumerate(zip(temp_time, temp_vmax, temp_basin, temp_type)):
+                    if self.basin not in ['all', 'both'] and i_basin != self.basin:
+                        continue
+                    if i_time.strftime('%H%M') not in constants.STANDARD_HOURS:
+                        continue
+                    if i_type not in constants.NAMED_TROPICAL_STORM_TYPES:
+                        continue
+                    if self.basin == 'all' and i_time.year != self.year:
+                        continue
+                    if np.isnan(i_vmax):
+                        continue
                     temp_ace += accumulated_cyclone_energy(i_vmax)
-                temp_ace = np.round(temp_ace,1)
+                temp_ace = np.round(temp_ace, 1)
 
-                #Get indices of all tropical/subtropical time steps
+                # Get indices of all tropical/subtropical time steps
                 if self.basin == 'all':
-                    idx = np.where(((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (temp_type == 'TS') | (temp_type == 'HU')) & (temp_year == self.year))
+                    idx = np.where(((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (temp_type == 'TS') | (
+                        temp_type == 'HU') | (temp_type == 'TY') | (temp_type == 'ST')) & (temp_year == self.year))
                 elif self.basin == 'both':
-                    idx = np.where(((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (temp_type == 'TS') | (temp_type == 'HU')))
+                    idx = np.where(((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (
+                        temp_type == 'TS') | (temp_type == 'HU') | (temp_type == 'TY') | (temp_type == 'ST')))
                 else:
-                    idx = np.where(((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (temp_type == 'TS') | (temp_type == 'HU')) & (temp_basin == self.basin))
-                
-                #Get times during existence of trop/subtrop storms
-                if len(idx[0]) == 0: continue
+                    idx = np.where(((temp_type == 'SS') | (temp_type == 'SD') | (temp_type == 'TD') | (temp_type == 'TS') | (
+                        temp_type == 'HU') | (temp_type == 'TY') | (temp_type == 'ST')) & (temp_basin == self.basin))
+
+                # Get times during existence of trop/subtrop storms
+                if len(idx[0]) == 0:
+                    continue
                 trop_time = temp_time[idx]
-                
-                if multi_season == False:
+
+                if not multi_season:
                     if 'season_start' not in summary_dict.keys():
                         summary_dict['season_start'] = trop_time[0]
                     else:
-                        if trop_time[0] < summary_dict['season_start']: summary_dict['season_start'] = trop_time[0]
+                        if trop_time[0] < summary_dict['season_start']:
+                            summary_dict['season_start'] = trop_time[0]
                     if 'season_end' not in summary_dict.keys():
                         summary_dict['season_end'] = trop_time[-1]
                     else:
-                        if trop_time[-1] > summary_dict['season_end']: summary_dict['season_end'] = trop_time[-1]
+                        if trop_time[-1] > summary_dict['season_end']:
+                            summary_dict['season_end'] = trop_time[-1]
                 else:
                     if summary_dict['season_start'][season_idx] == 0:
                         summary_dict['season_start'][season_idx] = trop_time[0]
                     else:
-                        if trop_time[0] < summary_dict['season_start'][season_idx]: summary_dict['season_start'][season_idx] = trop_time[0]
+                        if trop_time[0] < summary_dict['season_start'][season_idx]:
+                            summary_dict['season_start'][season_idx] = trop_time[0]
                     if summary_dict['season_end'][season_idx] == 0:
                         summary_dict['season_end'][season_idx] = trop_time[-1]
                     else:
-                        if trop_time[-1] > summary_dict['season_end'][season_idx]: summary_dict['season_end'][season_idx] = trop_time[-1]
+                        if trop_time[-1] > summary_dict['season_end'][season_idx]:
+                            summary_dict['season_end'][season_idx] = trop_time[-1]
 
-                #Get max/min values and check for nan's
+                # Get max/min values and check for nan's
                 np_wnd = np.array(temp_vmax[idx])
                 np_slp = np.array(temp_mslp[idx])
                 if len(np_wnd[~np.isnan(np_wnd)]) == 0:
                     max_wnd = np.nan
                     max_cat = -1
                 else:
                     max_wnd = int(np.nanmax(temp_vmax[idx]))
                     max_cat = wind_to_category(np.nanmax(temp_vmax[idx]))
                 if len(np_slp[~np.isnan(np_slp)]) == 0:
                     min_slp = np.nan
                 else:
                     min_slp = int(np.nanmin(temp_mslp[idx]))
 
-                #Append to dict
-                if multi_season == False:
+                # Append to dict
+                if not multi_season:
                     summary_dict['id'].append(key)
                     summary_dict['name'].append(temp_name)
                     summary_dict['max_wspd'].append(max_wnd)
                     summary_dict['min_mslp'].append(min_slp)
                     summary_dict['category'].append(max_cat)
                     summary_dict['ace'].append(temp_ace)
-                    summary_dict['operational_id'].append(self.dict[key]['operational_id'])
+                    summary_dict['operational_id'].append(
+                        self.dict[key]['operational_id'])
                 else:
                     summary_dict['id'][season_idx].append(key)
                     summary_dict['name'][season_idx].append(temp_name)
                     summary_dict['max_wspd'][season_idx].append(max_wnd)
                     summary_dict['min_mslp'][season_idx].append(min_slp)
                     summary_dict['category'][season_idx].append(max_cat)
                     summary_dict['ace'][season_idx].append(temp_ace)
-                    summary_dict['operational_id'][season_idx].append(self.dict[key]['operational_id'])
+                    summary_dict['operational_id'][season_idx].append(
+                        self.dict[key]['operational_id'])
 
-                #Handle operational vs. non-operational storms
+                # Handle operational vs. non-operational storms
 
-                #Check for purely subtropical storms
-                if 'SS' in temp_type and True not in np.isin(temp_type,list(constants.TROPICAL_ONLY_STORM_TYPES)):
+                # Check for purely subtropical storms
+                if 'SS' in temp_type and True not in np.isin(temp_type, list(constants.TROPICAL_ONLY_STORM_TYPES)):
                     count_ss_pure += 1
 
-                #Check for partially subtropical storms
+                # Check for partially subtropical storms
                 if 'SS' in temp_type:
                     count_ss_partial += 1
 
-            #Add generic season info
-            if multi_season == False:
+            # Add generic season info
+            if not multi_season:
                 narray = np.array(summary_dict['max_wspd'])
                 narray = narray[~np.isnan(narray)]
-                summary_dict['season_storms'] = len(summary_dict['max_wspd'])
-                summary_dict['season_named'] = len(narray[narray>=34])
-                summary_dict['season_hurricane'] = len(narray[narray>=65])
-                summary_dict['season_major'] = len(narray[narray>=100])
-                summary_dict['season_ace'] = np.round(np.sum(summary_dict['ace']),1)
+                summary_dict['season_storms'] = len(narray[narray >= 0])
+                summary_dict['season_named'] = len(narray[narray >= 34])
+                summary_dict['season_hurricane'] = len(narray[narray >= 65])
+                summary_dict['season_major'] = len(narray[narray >= 100])
+                summary_dict['season_ace'] = np.round(
+                    np.sum(summary_dict['ace']), 1)
                 summary_dict['season_subtrop_pure'] = count_ss_pure
                 summary_dict['season_subtrop_partial'] = count_ss_partial
             else:
                 narray = np.array(summary_dict['max_wspd'][season_idx])
                 narray = narray[~np.isnan(narray)]
-                summary_dict['season_storms'][season_idx] = len(summary_dict['max_wspd'])
-                summary_dict['season_named'][season_idx] = len(narray[narray>=34])
-                summary_dict['season_hurricane'][season_idx] = len(narray[narray>=65])
-                summary_dict['season_major'][season_idx] = len(narray[narray>=100])
-                summary_dict['season_ace'][season_idx] = np.round(np.sum(summary_dict['ace'][season_idx]),1)
+                summary_dict['season_storms'][season_idx] = len(
+                    narray[narray >= 0])
+                summary_dict['season_named'][season_idx] = len(
+                    narray[narray >= 34])
+                summary_dict['season_hurricane'][season_idx] = len(
+                    narray[narray >= 65])
+                summary_dict['season_major'][season_idx] = len(
+                    narray[narray >= 100])
+                summary_dict['season_ace'][season_idx] = np.round(
+                    np.sum(summary_dict['ace'][season_idx]), 1)
                 summary_dict['season_subtrop_pure'][season_idx] = count_ss_pure
                 summary_dict['season_subtrop_partial'][season_idx] = count_ss_partial
-                
-        #Return object
+
+        # Return object
         return summary_dict
```

### Comparing `tropycal-0.6.1/src/tropycal/tracks/storm.py` & `tropycal-1.0/src/tropycal/tracks/storm.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,946 +1,972 @@
 r"""Functionality for storing and analyzing an individual storm."""
 
+import re
 import numpy as np
+import xarray as xr
 import pandas as pd
-import re
-import scipy.interpolate as interp
 import urllib
 import warnings
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 import requests
 import copy
 
-#Import internal scripts
+# Import internal scripts
 from .plot import TrackPlot
 from ..tornado import *
-from ..recon import *
+from ..recon import ReconDataset
+from ..ships import Ships
 
-#Import tools
+# Import tools
 from .tools import *
 from ..utils import *
 
 try:
     import zipfile
     import gzip
-    from io import StringIO, BytesIO
+    from io import BytesIO
     import tarfile
-except:
-    warnings.warn("Warning: The libraries necessary for online NHC forecast retrieval aren't available (gzip, io, tarfile).")
+except ImportError:
+    warnings.warn(
+        "Warning: The libraries necessary for online NHC forecast retrieval aren't available (gzip, io, tarfile).")
 
 try:
     import matplotlib.lines as mlines
-    import matplotlib.patheffects as path_effects
     import matplotlib.pyplot as plt
-    import matplotlib.ticker as mticker
-except:
-    warnings.warn("Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+except ImportError:
+    warnings.warn(
+        "Warning: Matplotlib is not installed in your python environment. Plotting functions will not work.")
+
 
 class Storm:
-    
+
     r"""
     Initializes an instance of Storm, retrieved via ``TrackDataset.get_storm()``.
 
     Parameters
     ----------
     storm : dict
         Dict entry of the requested storm.
-    
+
     Other Parameters
     ----------------
     stormTors : dict, optional
         Dict entry containing tornado data assicated with the storm. Populated directly from tropycal.tracks.TrackDataset.
 
     Returns
     -------
     Storm
         Instance of a Storm object.
-    
+
     Notes
     -----
     A Storm object is retrieved from TrackDataset's ``get_storm()`` method. For example, if the dataset read in is the default North Atlantic and the desired storm is Hurricane Michael (2018), it would be retrieved as follows:
-    
+
     .. code-block:: python
-    
+
         from tropycal import tracks
         basin = tracks.TrackDataset()
         storm = basin.get_storm(('michael',2018))
-    
+
     Now Hurricane Michael's data is stored in the variable ``storm``, which is an instance of Storm and can access all of the methods and attributes of a Storm object.
-    
-    All the variables associated with a Storm object (e.g., lat, lon, date, vmax) can be accessed in two ways. The first is directly from the Storm object:
-    
+
+    All the variables associated with a Storm object (e.g., lat, lon, time, vmax) can be accessed in two ways. The first is directly from the Storm object:
+
     >>> storm.lat
     array([17.8, 18.1, 18.4, 18.8, 19.1, 19.7, 20.2, 20.9, 21.7, 22.7, 23.7,
            24.6, 25.6, 26.6, 27.7, 29. , 30. , 30.2, 31.5, 32.8, 34.1, 35.6,
            36.5, 37.3, 39.1, 41.1, 43.1, 44.8, 46.4, 47.6, 48.4, 48.8, 48.6,
            47.5, 45.9, 44.4, 42.8, 41.2])
-    
+
     The second is via ``storm.vars``, which returns a dictionary of the variables associated with the Storm object. This is also a quick way to access all of the variables associated with a Storm object:
-    
+
     >>> variable_dict = storm.vars
     >>> lat = variable_dict['lat']
     >>> lon = variable_dict['lon']
     >>> print(variable_dict.keys())
-    dict_keys(['date', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin'])
-    
+    dict_keys(['time', 'extra_obs', 'special', 'type', 'lat', 'lon', 'vmax', 'mslp', 'wmo_basin'])
+
     Storm objects also have numerous attributes with information about the storm. ``storm.attrs`` returns a dictionary of the attributes for this Storm object.
-    
+
     >>> print(storm.attrs)
     {'id': 'AL142018',
      'operational_id': 'AL142018',
      'name': 'MICHAEL',
      'year': 2018,
      'season': 2018,
      'basin': 'north_atlantic',
      'source_info': 'NHC Hurricane Database',
      'source': 'hurdat',
      'ace': 12.5,
      'realtime': False,
      'invest': False}
-    
-    
+
+
     """
-    
+
     def __setitem__(self, key, value):
         self.__dict__[key] = value
-        
+
     def __getitem__(self, key):
         return self.__dict__[key]
-    
+
     def __repr__(self):
-         
-        #Label object
+
+        # Label object
         summary = ["<tropycal.tracks.Storm>"]
-        
-        #Format keys for summary
+
+        # Format keys for summary
         type_array = np.array(self.dict['type'])
-        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (type_array == 'TS') | (type_array == 'HU'))[0]
-        if self.invest and len(idx) == 0: idx = np.array([True for i in type_array])
+        idx = np.where((type_array == 'SD') | (type_array == 'SS') | (type_array == 'TD') | (
+            type_array == 'TS') | (type_array == 'HU') | (type_array == 'TY') | (type_array == 'ST'))[0]
+        if self.invest and len(idx) == 0:
+            idx = np.array([True for i in type_array])
         if len(idx) == 0:
-            start_date = 'N/A'
-            end_date = 'N/A'
+            start_time = 'N/A'
+            end_time = 'N/A'
             max_wind = 'N/A'
             min_mslp = 'N/A'
         else:
-            time_tropical = np.array(self.dict['date'])[idx]
-            start_date = time_tropical[0].strftime("%H00 UTC %d %B %Y")
-            end_date = time_tropical[-1].strftime("%H00 UTC %d %B %Y")
-            max_wind = 'N/A' if all_nan(np.array(self.dict['vmax'])[idx]) == True else int(np.nanmax(np.array(self.dict['vmax'])[idx]))
-            min_mslp = 'N/A' if all_nan(np.array(self.dict['mslp'])[idx]) == True else int(np.nanmin(np.array(self.dict['mslp'])[idx]))
-        summary_keys = {'Maximum Wind':f"{max_wind} knots",
-                        'Minimum Pressure':f"{min_mslp} hPa",
-                        'Start Date':start_date,
-                        'End Date':end_date}
-        
-        #Format keys for coordinates
+            time_tropical = np.array(self.dict['time'])[idx]
+            start_time = time_tropical[0].strftime("%H00 UTC %d %B %Y")
+            end_time = time_tropical[-1].strftime("%H00 UTC %d %B %Y")
+            max_wind = 'N/A' if all_nan(np.array(self.dict['vmax'])[idx]) else int(
+                np.nanmax(np.array(self.dict['vmax'])[idx]))
+            min_mslp = 'N/A' if all_nan(np.array(self.dict['mslp'])[idx]) else int(
+                np.nanmin(np.array(self.dict['mslp'])[idx]))
+        summary_keys = {
+            'Maximum Wind': f"{max_wind} knots",
+            'Minimum Pressure': f"{min_mslp} hPa",
+            'Start Time': start_time,
+            'End Time': end_time,
+        }
+
+        # Format keys for coordinates
         variable_keys = {}
         for key in self.vars.keys():
             dtype = type(self.vars[key][0]).__name__
-            dtype = dtype.replace("_","")
+            dtype = dtype.replace("_", "")
             variable_keys[key] = f"({dtype}) [{self.vars[key][0]} .... {self.vars[key][-1]}]"
 
-        #Add storm summary
+        # Add storm summary
         summary.append("Storm Summary:")
-        add_space = np.max([len(key) for key in summary_keys.keys()])+3
+        add_space = np.max([len(key) for key in summary_keys.keys()]) + 3
         for key in summary_keys.keys():
-            key_name = key+":"
-            summary.append(f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
-        
-        #Add coordinates
+            key_name = key + ":"
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{summary_keys[key]}')
+
+        # Add coordinates
         summary.append("\nVariables:")
-        add_space = np.max([len(key) for key in variable_keys.keys()])+3
+        add_space = np.max([len(key) for key in variable_keys.keys()]) + 3
         for key in variable_keys.keys():
             key_name = key
-            summary.append(f'{" "*4}{key_name:<{add_space}}{variable_keys[key]}')
-        
-        #Add additional information
+            summary.append(
+                f'{" "*4}{key_name:<{add_space}}{variable_keys[key]}')
+
+        # Add additional information
         summary.append("\nMore Information:")
-        add_space = np.max([len(key) for key in self.attrs.keys()])+3
+        add_space = np.max([len(key) for key in self.attrs.keys()]) + 3
         for key in self.attrs.keys():
-            key_name = key+":"
-            val = '%0.1f'%(self.attrs[key]) if key == 'ace' else self.attrs[key]
+            key_name = key + ":"
+            val = '%0.1f' % (
+                self.attrs[key]) if key == 'ace' else self.attrs[key]
             summary.append(f'{" "*4}{key_name:<{add_space}}{val}')
 
         return "\n".join(summary)
-    
-    def __init__(self,storm,stormTors=None,read_path=""):
-        
-        if read_path == "" or os.path.isfile(read_path) == False:
 
-            #Save the dict entry of the storm
-            self.dict = storm
+    def __init__(self, storm, stormTors=None):
 
-            #Add other attributes about the storm
-            keys = self.dict.keys()
-            self.attrs = {}
-            self.vars = {}
-            for key in keys:
-                if key == 'realtime': continue
-                if key == 'invest': continue
-                if isinstance(self.dict[key], list) == False and isinstance(self.dict[key], dict) == False:
-                    self[key] = self.dict[key]
-                    self.attrs[key] = self.dict[key]
-                if isinstance(self.dict[key], list) == True and isinstance(self.dict[key], dict) == False:
-                    self.vars[key] = np.array(self.dict[key])
-                    self[key] = np.array(self.dict[key])
-
-            #Assign tornado data
-            if stormTors is not None and isinstance(stormTors,dict):
-                self.stormTors = stormTors['data']
-                self.tornado_dist_thresh = stormTors['dist_thresh']
-                self.attrs['Tornado Count'] = len(stormTors['data'])
+        # Save the dict entry of the storm
+        self.dict = storm
 
-            #Get Archer track data for this storm, if it exists
-            try:
-                self.get_archer()
-            except:
-                pass
-                
-            #Initialize recon dataset instance
-            self.recon = ReconDataset(storm=self)
-
-            #Determine if storm object was retrieved via realtime object
-            if 'realtime' in keys and self.dict['realtime']:
-                self.realtime = True
-                self.attrs['realtime'] = True
-            else:
-                self.realtime = False
-                self.attrs['realtime'] = False
-            
-            #Determine if storm object is an invest
-            if 'invest' in keys and self.dict['invest']:
-                self.invest = True
-                self.attrs['invest'] = True
-            else:
-                self.invest = False
-                self.attrs['invest'] = False
-        
+        # Add other attributes about the storm
+        keys = self.dict.keys()
+        self.attrs = {}
+        self.vars = {}
+        for key in keys:
+            if key in ['realtime', 'invest', 'subset']:
+                continue
+            if not isinstance(self.dict[key], list) and not isinstance(self.dict[key], dict):
+                self[key] = self.dict[key]
+                self.attrs[key] = self.dict[key]
+            if isinstance(self.dict[key], list) and not isinstance(self.dict[key], dict):
+                self.vars[key] = np.array(self.dict[key])
+                self[key] = np.array(self.dict[key])
+
+        # Assign tornado data
+        if stormTors is not None and isinstance(stormTors, dict):
+            self.stormTors = stormTors['data']
+            self.tornado_dist_thresh = stormTors['dist_thresh']
+            self.attrs['Tornado Count'] = len(stormTors['data'])
+
+        # Get Archer track data for this storm, if it exists
+        try:
+            self.get_archer()
+        except:
+            pass
+
+        # Initialize recon dataset instance
+        self.recon = ReconDataset(storm=self)
+
+        # Determine if storm object was retrieved via realtime object
+        if 'realtime' in keys and self.dict['realtime']:
+            self.realtime = True
+            self.attrs['realtime'] = True
         else:
-            
-            #This functionality currently does not exist
-            raise ExceptionError("This functionality has not been implemented yet.")
-    
-    def sel(self,time=None,lat=None,lon=None,vmax=None,mslp=None,\
-            dvmax_dt=None,dmslp_dt=None,stormtype=None,method='exact'):
+            self.realtime = False
+            self.attrs['realtime'] = False
+
+        # Determine if storm object is an invest
+        if 'invest' in keys and self.dict['invest']:
+            self.invest = True
+            self.attrs['invest'] = True
+        else:
+            self.invest = False
+            self.attrs['invest'] = False
         
+        # Determine if storm object is subset
+        if 'subset' in keys and self.dict['subset']:
+            self.subset = True
+            self.attrs['subset'] = True
+        else:
+            self.subset = False
+            self.attrs['subset'] = False
+
+    def sel(self, time=None, lat=None, lon=None, vmax=None, mslp=None,
+            dvmax_dt=None, dmslp_dt=None, stormtype=None, method='exact'):
         r"""
         Subset this storm by any of its parameters and return a new storm object.
-        
+
         Parameters
         ----------
         time : datetime.datetime or list/tuple of datetimes
             Datetime object for single point, or list/tuple of start time and end time.
             Default is None, which returns all points
         lat : float/int or list/tuple of float/int
             Float/int for single point, or list/tuple of latitude bounds (S,N).
             None in either position of a tuple means it is boundless on that side.
         lon : float/int or list/tuple of float/int
             Float/int for single point, or list/tuple of longitude bounds (W,E).
             If either lat or lon is a tuple, the other can be None for no bounds.
             If either is a tuple, the other canNOT be a float/int.
         vmax : list/tuple of float/int
             list/tuple of vmax bounds (min,max).
-            None in either position of a tuple means it is boundless on that side. 
+            None in either position of a tuple means it is boundless on that side.
         mslp : list/tuple of float/int
             list/tuple of mslp bounds (min,max).
-            None in either position of a tuple means it is boundless on that side. 
+            None in either position of a tuple means it is boundless on that side.
         dvmax_dt : list/tuple of float/int
             list/tuple of vmax bounds (min,max). ONLY AVAILABLE AFTER INTERP.
-            None in either position of a tuple means it is boundless on that side. 
+            None in either position of a tuple means it is boundless on that side.
         dmslp_dt : list/tuple of float/int
             list/tuple of mslp bounds (min,max). ONLY AVAILABLE AFTER INTERP.
             None in either position of a tuple means it is boundless on that side.
         stormtype : list/tuple of str
             list/tuple of stormtypes (options: 'LO','EX','TD','SD','TS','SS','HU')
         method : str
             Applies for single point selection in time and lat/lon.
             'exact' requires a point to match exactly with the request. (default)
             'nearest' returns the nearest point to the request
             'floor' ONLY for time, returns the nearest point before the request
             'ceil' ONLY for time, returns the neartest point after the request
-        
+
         Returns
         -------
         storm object
             A new storm object that satisfies the intersection of all subsetting.
         """
-        
-        #create copy of storm object
-        NEW_STORM = Storm(copy.deepcopy(self.dict))
-        idx_final = np.arange(len(self.date))
-        
-        #apply time filter
+
+        # create copy of storm object
+        new_dict = copy.deepcopy(self.dict)
+        new_dict['subset'] = True
+        NEW_STORM = Storm(new_dict)
+        idx_final = np.arange(len(self.time))
+
+        # apply time filter
         if time is None:
             idx = copy.copy(idx_final)
-        
-        elif isinstance(time,dt):
-            time_diff = np.array([(time-i).total_seconds() for i in NEW_STORM.date])
+
+        elif isinstance(time, dt):
+            time_diff = np.array([(time - i).total_seconds()
+                                 for i in NEW_STORM.time])
             idx = np.abs(time_diff).argmin()
-            if time_diff[idx]!=0:
-                if method=='exact':
+            if time_diff[idx] != 0:
+                if method == 'exact':
                     msg = f'no exact match for {time}. Use different time or method.'
                     raise ValueError(msg)
-                elif method=='floor' and time_diff[idx]<0:
+                elif method == 'floor' and time_diff[idx] < 0:
                     idx += -1
-                    if idx<0:
+                    if idx < 0:
                         msg = f'no points before {time}. Use different time or method.'
                         raise ValueError(msg)
-                elif method=='ceil' and time_diff[idx]>0:
+                elif method == 'ceil' and time_diff[idx] > 0:
                     idx += 1
-                    if idx>=len(time_diff):
+                    if idx >= len(time_diff):
                         msg = f'no points after {time}. Use different time or method.'
                         raise ValueError(msg)
-        
-        elif isinstance(time,(tuple,list)) and len(time)==2:
-            time0,time1 = time
+
+        elif isinstance(time, (tuple, list)) and len(time) == 2:
+            time0, time1 = time
             if time0 is None:
-                time0 = min(NEW_STORM.date)
-            elif not isinstance(time0,dt):
+                time0 = min(NEW_STORM.time)
+            elif not isinstance(time0, dt):
                 msg = 'time bounds must be of type datetime.datetime or None.'
                 raise TypeError(msg)
             if time1 is None:
-                time1 = max(NEW_STORM.date)
-            elif not isinstance(time1,dt):
+                time1 = max(NEW_STORM.time)
+            elif not isinstance(time1, dt):
                 msg = 'time bounds must be of type datetime.datetime or None.'
-                raise TypeError(msg)            
-            tmptimes = np.array(NEW_STORM.date)
-            idx = np.where((tmptimes>=time0) & (tmptimes<=time1))[0]
-            if len(idx)==0:
+                raise TypeError(msg)
+            tmptimes = np.array(NEW_STORM.time)
+            idx = np.where((tmptimes >= time0) & (tmptimes <= time1))[0]
+            if len(idx) == 0:
                 msg = f'no points between {time}. Use different time bounds.'
                 raise ValueError(msg)
-                
+
         else:
             msg = 'time must be of type datetime.datetime, tuple/list, or None.'
             raise TypeError(msg)
-        
-        #update idx_final
+
+        # update idx_final
         idx_final = list(set(idx_final) & set(listify(idx)))
 
-        #apply lat/lon filter
+        # apply lat/lon filter
         if lat is None and lon is None:
             idx = copy.copy(idx_final)
-            
-        elif isinstance(lat,(int,np.int,np.integer,float,np.floating)) and isinstance(lon,(int,np.int,np.integer,float,np.floating)):
-            dist = np.array([great_circle((lat,lon),(x,y)).kilometers for x,y in zip(NEW_STORM.lon,NEW_STORM.lat)])
+
+        elif is_number(lat) and is_number(lon):
+            dist = np.array([great_circle((lat, lon), (x, y)).kilometers for x, y in zip(
+                NEW_STORM.lon, NEW_STORM.lat)])
             idx = np.abs(dist).argmin()
-            if dist[idx]!=0:
-                if method=='exact':
+            if dist[idx] != 0:
+                if method == 'exact':
                     msg = f'no exact match for {lat}/{lon}. Use different location or method.'
                     raise ValueError(msg)
-                elif method in ('floor','ceil'):
-                    print('floor and ceil do not apply to lat/lon filtering. Using nearest instead.')
-
-        elif (isinstance(lat,(tuple,list)) and len(lat)==2) \
-            or (isinstance(lon,(tuple,list)) and len(lon)==2):
-            if not isinstance(lat,(tuple,list)):
-                print('Using no lat bounds')
-                lat = (None,None)
-            if not isinstance(lon,(tuple,list)):
-                print('Using no lon bounds')
-                lon = (None,None)
-            lat0,lat1 = lat
-            lon0,lon1 = lon
+                elif method in ('floor', 'ceil'):
+                    warnings.warn(
+                        'floor and ceil do not apply to lat/lon filtering. Using nearest instead.')
+
+        elif (isinstance(lat, (tuple, list)) and len(lat) == 2) or (isinstance(lon, (tuple, list)) and len(lon) == 2):
+            if not isinstance(lat, (tuple, list)):
+                lat = (None, None)
+            if not isinstance(lon, (tuple, list)):
+                lon = (None, None)
+            lat0, lat1 = lat
+            lon0, lon1 = lon
             if lat0 is None:
                 lat0 = min(NEW_STORM.lat)
-            elif not isinstance(lat0,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(lat0):
                 msg = 'lat/lon bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if lat1 is None:
                 lat1 = max(NEW_STORM.lat)
-            elif not isinstance(lat1,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(lat1):
                 msg = 'lat/lon bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if lon0 is None:
                 lon0 = min(NEW_STORM.lon)
-            elif not isinstance(lon0,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(lon0):
                 msg = 'lat/lon bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if lon1 is None:
                 lon1 = max(NEW_STORM.lon)
-            elif not isinstance(lon1,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(lon1):
                 msg = 'lat/lon bounds must be of type float/int or None.'
                 raise TypeError(msg)
-                
-            tmplat,tmplon = np.array(NEW_STORM.lat),np.array(NEW_STORM.lon)%360
-            idx = np.where((tmplat>=lat0) & (tmplat<=lat1) & \
-                           (tmplon>=lon0%360) & (tmplon<=lon1%360))[0]
-            if len(idx)==0:
+
+            tmplat, tmplon = np.array(
+                NEW_STORM.lat), np.array(NEW_STORM.lon) % 360
+            idx = np.where((tmplat >= lat0) & (tmplat <= lat1) &
+                           (tmplon >= lon0 % 360) & (tmplon <= lon1 % 360))[0]
+            if len(idx) == 0:
                 msg = f'no points in {lat}/{lon} box. Use different lat/lon bounds.'
                 raise ValueError(msg)
-                
+
         else:
             msg = 'lat and lon must be of the same type: float/int, tuple/list, or None.'
-            raise TypeError(msg)  
+            raise TypeError(msg)
 
-        #update idx_final
+        # update idx_final
         idx_final = list(set(idx_final) & set(listify(idx)))
 
-        #apply vmax filter
+        # apply vmax filter
         if vmax is None:
             idx = copy.copy(idx_final)
-        
-        elif isinstance(vmax,(tuple,list)) and len(vmax)==2:
-            vmax0,vmax1 = vmax
+
+        elif isinstance(vmax, (tuple, list)) and len(vmax) == 2:
+            vmax0, vmax1 = vmax
             if vmax0 is None:
                 vmax0 = np.nanmin(NEW_STORM.vmax)
-            elif not isinstance(vmax0,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(vmax0):
                 msg = 'vmax bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if vmax1 is None:
                 vmax1 = np.nanmax(NEW_STORM.vmax)
-            elif not isinstance(vmax1,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(vmax1):
                 msg = 'vmax bounds must be of type float/int or None.'
-                raise TypeError(msg)            
+                raise TypeError(msg)
             tmpvmax = np.array(NEW_STORM.vmax)
-            idx = np.where((tmpvmax>=vmax0) & (tmpvmax<=vmax1))[0]
-            if len(idx)==0:
+            idx = np.where((tmpvmax >= vmax0) & (tmpvmax <= vmax1))[0]
+            if len(idx) == 0:
                 msg = f'no points with vmax between {vmax}. Use different vmax bounds.'
                 raise ValueError(msg)
-                
+
         else:
             msg = 'vmax must be of type tuple/list, or None.'
             raise TypeError(msg)
 
-        #update idx_final
+        # update idx_final
         idx_final = list(set(idx_final) & set(listify(idx)))
 
-        #apply mslp filter
+        # apply mslp filter
         if mslp is None:
             idx = copy.copy(idx_final)
-        
-        elif isinstance(mslp,(tuple,list)) and len(mslp)==2:
-            mslp0,mslp1 = mslp
+
+        elif isinstance(mslp, (tuple, list)) and len(mslp) == 2:
+            mslp0, mslp1 = mslp
             if mslp0 is None:
                 mslp0 = np.nanmin(NEW_STORM.mslp)
-            elif not isinstance(mslp0,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(mslp0):
                 msg = 'mslp bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if mslp1 is None:
                 mslp1 = np.nanmax(NEW_STORM.mslp)
-            elif not isinstance(mslp1,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(mslp1):
                 msg = 'mslp bounds must be of type float/int or None.'
-                raise TypeError(msg)            
+                raise TypeError(msg)
             tmpmslp = np.array(NEW_STORM.mslp)
-            idx = np.where((tmpmslp>=mslp0) & (tmpmslp<=mslp1))[0]
-            if len(idx)==0:
+            idx = np.where((tmpmslp >= mslp0) & (tmpmslp <= mslp1))[0]
+            if len(idx) == 0:
                 msg = f'no points with mslp between {mslp}. Use different dmslp_dt bounds.'
                 raise ValueError(msg)
-                
+
         else:
             msg = 'vmax must be of type tuple/list, or None.'
             raise TypeError(msg)
 
-        #update idx_final
+        # update idx_final
         idx_final = list(set(idx_final) & set(listify(idx)))
 
-        #apply dvmax_dt filter
+        # apply dvmax_dt filter
         if dvmax_dt is None:
             idx = copy.copy(idx_final)
-        
+
         elif 'dvmax_dt' not in NEW_STORM.dict.keys():
-            msg = f'dvmax_dt not in storm data. Create new object with interp first.'
-            raise KeyError(msg)            
-        
-        elif isinstance(dvmax_dt,(tuple,list)) and len(dvmax_dt)==2:
-            dvmax_dt0,dvmax_dt1 = dvmax_dt
+            msg = 'dvmax_dt not in storm data. Create new object with interp first.'
+            raise KeyError(msg)
+
+        elif isinstance(dvmax_dt, (tuple, list)) and len(dvmax_dt) == 2:
+            dvmax_dt0, dvmax_dt1 = dvmax_dt
             if dvmax_dt0 is None:
                 dvmax_dt0 = np.nanmin(NEW_STORM.dvmax_dt)
-            elif not isinstance(dvmax_dt0,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(dvmax_dt0):
                 msg = 'dmslp_dt bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if dvmax_dt1 is None:
                 dvmax_dt1 = np.nanmax(NEW_STORM.dvmax_dt)
-            elif not isinstance(dvmax_dt1,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(dvmax_dt1):
                 msg = 'dmslp_dt bounds must be of type float/int or None.'
-                raise TypeError(msg)     
-                        
+                raise TypeError(msg)
+
             tmpvmax = np.array(NEW_STORM.dvmax_dt)
-            idx = np.where((tmpvmax>=dvmax_dt0) & (tmpvmax<=dvmax_dt1))[0]
-            if len(idx)==0:
+            idx = np.where((tmpvmax >= dvmax_dt0) & (tmpvmax <= dvmax_dt1))[0]
+            if len(idx) == 0:
                 msg = f'no points with dvmax_dt between {dvmax_dt}. Use different dvmax_dt bounds.'
                 raise ValueError(msg)
 
-        #update idx_final
+        # update idx_final
         idx_final = list(set(idx_final) & set(listify(idx)))
 
-        #apply dmslp_dt filter
+        # apply dmslp_dt filter
         if dmslp_dt is None:
             idx = copy.copy(idx_final)
-            
+
         elif 'dmslp_dt' not in NEW_STORM.dict.keys():
-            msg = f'dmslp_dt not in storm data. Create new object with interp first.'
-            raise KeyError(msg)   
-            
-        elif isinstance(dmslp_dt,(tuple,list)) and len(dmslp_dt)==2:
-            dmslp_dt0,dmslp_dt1 = dmslp_dt
+            msg = 'dmslp_dt not in storm data. Create new object with interp first.'
+            raise KeyError(msg)
+
+        elif isinstance(dmslp_dt, (tuple, list)) and len(dmslp_dt) == 2:
+            dmslp_dt0, dmslp_dt1 = dmslp_dt
             if dmslp_dt0 is None:
                 dmslp_dt0 = np.nanmin(NEW_STORM.dmslp_dt)
-            elif not isinstance(dmslp_dt0,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(dmslp_dt0):
                 msg = 'dmslp_dt bounds must be of type float/int or None.'
                 raise TypeError(msg)
             if dmslp_dt1 is None:
                 dmslp_dt1 = np.nanmax(NEW_STORM.dmslp_dt)
-            elif not isinstance(dmslp_dt1,(int,np.int,np.integer,float,np.floating)):
+            elif not is_number(dmslp_dt1):
                 msg = 'dmslp_dt bounds must be of type float/int or None.'
-                raise TypeError(msg)            
+                raise TypeError(msg)
             tmpmslp = np.array(NEW_STORM.dmslp_dt)
-            idx = np.where((tmpmslp>=dmslp_dt0) & (tmpmslp<=dmslp_dt1))[0]
-            if len(idx)==0:
+            idx = np.where((tmpmslp >= dmslp_dt0) & (tmpmslp <= dmslp_dt1))[0]
+            if len(idx) == 0:
                 msg = f'no points with dmslp_dt between {dmslp_dt}. Use different dmslp_dt bounds.'
                 raise ValueError(msg)
-                
-        #update idx_final
+
+        # update idx_final
         idx_final = list(set(idx_final) & set(listify(idx)))
 
-        #apply stormtype filter
+        # apply stormtype filter
         if stormtype is None:
             idx = copy.copy(idx_final)
-        
-        elif isinstance(stormtype,(tuple,list,str)):
-            idx = [i for i,j in enumerate(NEW_STORM.type) if j in listify(stormtype)]
-            if len(idx)==0:
+
+        elif isinstance(stormtype, (tuple, list, str)):
+            idx = [i for i, j in enumerate(
+                NEW_STORM.type) if j in listify(stormtype)]
+            if len(idx) == 0:
                 msg = f'no points with type {stormtype}. Use different stormtype.'
                 raise ValueError(msg)
-                
+
         else:
             msg = 'stormtype must be of type tuple/list, str, or None.'
             raise TypeError(msg)
-        
-        #update idx_final
+
+        # update idx_final
         idx_final = sorted(list(set(idx_final) & set(listify(idx))))
 
-        #Construct new storm dict with subset elements
+        # Construct new storm dict with subset elements
         for key in NEW_STORM.dict.keys():
             if isinstance(NEW_STORM.dict[key], list):
-                NEW_STORM.dict[key] = [NEW_STORM.dict[key][i] for i in idx_final]
+                NEW_STORM.dict[key] = [NEW_STORM.dict[key][i]
+                                       for i in idx_final]
             else:
                 NEW_STORM.dict[key] = NEW_STORM.dict[key]
-            
-            #Add other attributes to new storm object
-            if key == 'realtime': continue
-            if isinstance(NEW_STORM.dict[key], list) == False and isinstance(NEW_STORM.dict[key], dict) == False:
+
+            # Add other attributes to new storm object
+            if key == 'realtime':
+                continue
+            if not isinstance(NEW_STORM.dict[key], list) and not isinstance(NEW_STORM.dict[key], dict):
                 NEW_STORM[key] = NEW_STORM.dict[key]
                 NEW_STORM.attrs[key] = NEW_STORM.dict[key]
-            if isinstance(NEW_STORM.dict[key], list) == True and isinstance(NEW_STORM.dict[key], dict) == False:
+            if isinstance(NEW_STORM.dict[key], list) and not isinstance(NEW_STORM.dict[key], dict):
                 NEW_STORM.vars[key] = np.array(NEW_STORM.dict[key])
-                NEW_STORM[key] = np.array(NEW_STORM.dict[key])                
-                
+                NEW_STORM[key] = np.array(NEW_STORM.dict[key])
+
         return NEW_STORM
 
-    def interp(self,hours=1,dt_window=24,dt_align='middle',method='linear'):
-        
+    def interp(self, hours=1, dt_window=24, dt_align='middle', method='linear'):
         r"""
         Interpolate a storm temporally to a specified time resolution.
-        
+
         Parameters
         ----------
         hours : int or float
             Temporal resolution in hours (or fraction of an hour) to interpolate storm data to. Default is 1 hour.
         dt_window : int
             Time window in hours over which to calculate temporal change data. Default is 24 hours.
         dt_align : str
             Whether to align the temporal change window as "start", "middle" (default) or "end" of the dt_window time period.
         method : str
             Interpolation method for lat/lon coordinates passed to scipy. Options are "linear" (default) or "quadratic".
-        
+
         Returns
         -------
         tropycal.tracks.Storm
             New Storm object containing the updated dictionary.
-        
+
         Notes
         -----
         When interpolating data using a non-linear method, all non-standard hour observations (i.e., not within 00, 06, 12 or 18 UTC) are ignored for latitude & longitude interpolation in order to produce a smoother line.
         """
-        
+
         NEW_STORM = copy.deepcopy(self)
-        newdict = interp_storm(NEW_STORM.dict,hours,dt_window,dt_align,method)
-        for key in newdict.keys(): 
+        newdict = interp_storm(NEW_STORM.dict, hours,
+                               dt_window, dt_align, method)
+        for key in newdict.keys():
             NEW_STORM.dict[key] = newdict[key]
 
-        #Add other attributes to new storm object
+        # Add other attributes to new storm object
         for key in NEW_STORM.dict.keys():
-            if key == 'realtime': continue
-            if isinstance(NEW_STORM.dict[key], (np.ndarray,list)) == False and isinstance(NEW_STORM.dict[key], dict) == False:
+            if key == 'realtime':
+                continue
+            if not isinstance(NEW_STORM.dict[key], (np.ndarray, list)) and not isinstance(NEW_STORM.dict[key], dict):
                 NEW_STORM[key] = NEW_STORM.dict[key]
                 NEW_STORM.attrs[key] = NEW_STORM.dict[key]
-            if isinstance(NEW_STORM.dict[key], (np.ndarray,list)) == True and isinstance(NEW_STORM.dict[key], dict) == False:
+            if isinstance(NEW_STORM.dict[key], (np.ndarray, list)) and not isinstance(NEW_STORM.dict[key], dict):
                 NEW_STORM.dict[key] = list(NEW_STORM.dict[key])
                 NEW_STORM.vars[key] = np.array(NEW_STORM.dict[key])
                 NEW_STORM[key] = np.array(NEW_STORM.dict[key])
-                
+
         return NEW_STORM
 
     def to_dict(self):
-        
         r"""
         Returns the dict entry for the storm.
-        
+
         Returns
         -------
         dict
             A dictionary containing information about the storm.
         """
-        
-        #Return dict
+
+        # Return dict
         return self.dict
-        
+
     def to_xarray(self):
-        
         r"""
         Converts the storm dict into an xarray Dataset object.
-        
+
         Returns
         -------
         xarray.Dataset
             An xarray Dataset object containing information about the storm.
         """
-        
-        #Try importing xarray
-        try:
-            import xarray as xr
-        except ImportError as e:
-            raise RuntimeError("Error: xarray is not available. Install xarray in order to use this function.") from e
-            
-        #Set up empty dict for dataset
-        time = self.dict['date']
+
+        # Set up empty dict for dataset
+        time = self.dict['time']
         ds = {}
         attrs = {}
-        
-        #Add every key containing a list into the dict, otherwise add as an attribute
-        keys = [k for k in self.dict.keys() if k != 'date']
+
+        # Add every key containing a list into the dict, otherwise add as an attribute
+        keys = [k for k in self.dict.keys() if k != 'time']
         for key in keys:
             if isinstance(self.dict[key], list):
-                ds[key] = xr.DataArray(self.dict[key],coords=[time],dims=['time'])
+                ds[key] = xr.DataArray(self.dict[key], coords=[
+                                       time], dims=['time'])
             else:
                 attrs[key] = self.dict[key]
-                    
-        #Convert entire dict to a Dataset
-        ds = xr.Dataset(ds,attrs=attrs)
 
-        #Return dataset
+        # Convert entire dict to a Dataset
+        ds = xr.Dataset(ds, attrs=attrs)
+
+        # Return dataset
         return ds
 
     def to_dataframe(self, attrs_as_columns=False):
-        
         r"""
         Converts the storm dict into a pandas DataFrame object.
-        
+
         Parameters
         ----------
         attrs_as_columns : bool
             If True, adds Storm object attributes as columns in the DataFrame returned. Default is False.
-        
+
         Returns
         -------
         pandas.DataFrame
             A pandas DataFrame object containing information about the storm.
         """
-        
-        #Try importing pandas
-        try:
-            import pandas as pd
-        except ImportError as e:
-            raise RuntimeError("Error: pandas is not available. Install pandas in order to use this function.") from e
-            
-        #Set up empty dict for dataframe
-        time = self.dict['date']
+
+        # Set up empty dict for dataframe
         ds = {}
-        
-        #Add every key containing a list into the dict
+
+        # Add every key containing a list into the dict
         keys = [k for k in self.dict.keys()]
         for key in keys:
             if isinstance(self.dict[key], list):
                 ds[key] = self.dict[key]
             else:
                 if attrs_as_columns:
                     ds[key] = self.dict[key]
 
-        #Convert entire dict to a DataFrame
+        # Convert entire dict to a DataFrame
         ds = pd.DataFrame(ds)
 
-        #Return dataset
+        # Return dataset
         return ds
-    
-    def plot(self,domain="dynamic",plot_all_dots=False,ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot(self, domain="dynamic", plot_all_dots=False, ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of the observed track of the storm.
-        
+
         Parameters
         ----------
         domain : str
             Domain for the plot. Default is "dynamic". "dynamic_tropical" is also available. Please refer to :ref:`options-domain` for available domain options.
         plot_all_dots : bool
             Whether to plot dots for all observations along the track. If false, dots will be plotted every 6 hours. Default is false.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of storm track lines. Please refer to :ref:`options-prop` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Create instance of plot object
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         elif max(self.dict['lon']) > 150 or min(self.dict['lon']) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storms([self.dict],domain,plot_all_dots=plot_all_dots,ax=ax,prop=prop,map_prop=map_prop,save_path=save_path)
-        
-        #Return axis
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storms(
+            [self.dict], domain, plot_all_dots=plot_all_dots, ax=ax, prop=prop, map_prop=map_prop, save_path=save_path)
+
+        # Return axis
         return plot_ax
-        
-    def plot_nhc_forecast(self,forecast,track_labels='fhr',cone_days=5,domain="dynamic_forecast",
-                          ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot_nhc_forecast(self, forecast, track_labels='fhr', cone_days=5, domain="dynamic_forecast",
+                          ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of the operational NHC forecast track along with observed track data.
-        
+
         Parameters
         ----------
         forecast : int or datetime.datetime
-            Integer representing the forecast number, or datetime object for the closest issued forecast to this date.
+            Integer representing the forecast number, or datetime object for the closest issued forecast to this time.
         track_labels : str
             Label forecast hours with the following methods:
-            
+
             * **""** = no label
             * **"fhr"** = forecast hour (default)
             * **"valid_utc"** = UTC valid time
             * **"valid_edt"** = EDT valid time
         cone_days : int
             Number of days to plot the forecast cone. Default is 5 days. Can select 2, 3, 4 or 5 days.
         domain : str
             Domain for the plot. Default is "dynamic_forecast". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of NHC forecast plot. Please refer to :ref:`options-prop-nhc` for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Check to ensure the data source is HURDAT
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Check to ensure the data source is HURDAT
         if self.source != "hurdat":
-            raise RuntimeError("Error: NHC data can only be accessed when HURDAT is used as the data source.")
-        
-        #Check to ensure storm is not an invest
+            raise RuntimeError(
+                "Error: NHC data can only be accessed when HURDAT is used as the data source.")
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Create instance of plot object
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
             if max(self.dict['lon']) > 140 or min(self.dict['lon']) < -140:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=180.0)
             else:
-                self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-            
-        #Get forecasts dict saved into storm object, if it hasn't been already
+                self.plot_obj.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
+
+        # Get forecasts dict saved into storm object, if it hasn't been already
         try:
             self.forecast_dict
         except:
             self.get_operational_forecasts()
 
-        #Get all NHC forecast entries
+        # Get all NHC forecast entries
         nhc_forecasts = self.forecast_dict['OFCL']
         carq_forecasts = self.forecast_dict['CARQ']
 
-        #Get list of all NHC forecast initializations
+        # Get list of all NHC forecast initializations
         nhc_forecast_init = [k for k in nhc_forecasts.keys()]
         carq_forecast_init = [k for k in carq_forecasts.keys()]
 
-        #Find closest matching time to the provided forecast date, or time
-        if isinstance(forecast,int):
-            forecast_dict = nhc_forecasts[nhc_forecast_init[forecast-1]]
-            advisory_num = forecast+0
-        elif isinstance(forecast,dt):
-            nhc_forecast_init_dt = [dt.strptime(k,'%Y%m%d%H') for k in nhc_forecast_init]
-            time_diff = np.array([(i-forecast).days + (i-forecast).seconds/86400 for i in nhc_forecast_init_dt])
+        # Find closest matching time to the provided forecast date, or time
+        if isinstance(forecast, int):
+            forecast_dict = nhc_forecasts[nhc_forecast_init[forecast - 1]]
+            advisory_num = forecast + 0
+        elif isinstance(forecast, dt):
+            nhc_forecast_init_dt = [dt.strptime(
+                k, '%Y%m%d%H') for k in nhc_forecast_init]
+            time_diff = np.array(
+                [(i - forecast).days + (i - forecast).seconds / 86400 for i in nhc_forecast_init_dt])
             closest_idx = np.abs(time_diff).argmin()
             forecast_dict = nhc_forecasts[nhc_forecast_init[closest_idx]]
-            advisory_num = closest_idx+1
+            advisory_num = closest_idx + 1
             if np.abs(time_diff[closest_idx]) >= 1.0:
-                warnings.warn(f"The date provided is outside of the duration of the storm. Returning the closest available NHC forecast.")
+                warnings.warn(
+                    "The time provided is outside of the duration of the storm. Returning the closest available NHC forecast.")
         else:
-            raise RuntimeError("Error: Input variable 'forecast' must be of type 'int' or 'datetime.datetime'")
+            raise RuntimeError(
+                "Error: Input variable 'forecast' must be of type 'int' or 'datetime.datetime'")
 
-        #Get observed track as per NHC analyses
-        track_dict = {'lat':[],'lon':[],'vmax':[],'type':[],'mslp':[],'date':[],'extra_obs':[],'special':[],'ace':0.0}
+        # Get observed track as per NHC analyses
+        track_dict = {
+            'lat': [],
+            'lon': [],
+            'vmax': [],
+            'type': [],
+            'mslp': [],
+            'time': [],
+            'extra_obs': [],
+            'special': [],
+            'ace': 0.0,
+        }
         use_carq = True
         for k in nhc_forecast_init:
             hrs = nhc_forecasts[k]['fhr']
-            hrs_carq = carq_forecasts[k]['fhr'] if k in carq_forecast_init else []
-            
-            #Account for old years when hour 0 wasn't included directly
-            #if 0 not in hrs and k in carq_forecast_init and 0 in hrs_carq:
+            hrs_carq = carq_forecasts[k]['fhr'] if k in carq_forecast_init else [
+            ]
+
+            # Account for old years when hour 0 wasn't included directly
+            # if 0 not in hrs and k in carq_forecast_init and 0 in hrs_carq:
             if self.dict['year'] < 2000 and k in carq_forecast_init and 0 in hrs_carq:
-                
+
                 use_carq = True
                 hr_idx = hrs_carq.index(0)
                 track_dict['lat'].append(carq_forecasts[k]['lat'][hr_idx])
                 track_dict['lon'].append(carq_forecasts[k]['lon'][hr_idx])
                 track_dict['vmax'].append(carq_forecasts[k]['vmax'][hr_idx])
                 track_dict['mslp'].append(np.nan)
-                track_dict['date'].append(carq_forecasts[k]['init'])
+                track_dict['time'].append(carq_forecasts[k]['init'])
 
                 itype = carq_forecasts[k]['type'][hr_idx]
-                if itype == "": itype = get_storm_type(carq_forecasts[k]['vmax'][0],False)
+                if itype == "":
+                    itype = get_storm_type(carq_forecasts[k]['vmax'][0], False)
                 track_dict['type'].append(itype)
 
                 hr = carq_forecasts[k]['init'].strftime("%H%M")
-                track_dict['extra_obs'].append(0) if hr in ['0300','0900','1500','2100'] else track_dict['extra_obs'].append(1)
+                track_dict['extra_obs'].append(0) if hr in [
+                    '0300', '0900', '1500', '2100'] else track_dict['extra_obs'].append(1)
                 track_dict['special'].append("")
-                
+
             else:
                 use_carq = False
                 if 3 in hrs:
                     hr_idx = hrs.index(3)
                     hr_add = 3
                 else:
                     hr_idx = 0
                     hr_add = 0
                 track_dict['lat'].append(nhc_forecasts[k]['lat'][hr_idx])
                 track_dict['lon'].append(nhc_forecasts[k]['lon'][hr_idx])
                 track_dict['vmax'].append(nhc_forecasts[k]['vmax'][hr_idx])
                 track_dict['mslp'].append(np.nan)
-                track_dict['date'].append(nhc_forecasts[k]['init']+timedelta(hours=hr_add))
+                track_dict['time'].append(
+                    nhc_forecasts[k]['init'] + timedelta(hours=hr_add))
 
                 itype = nhc_forecasts[k]['type'][hr_idx]
-                if itype == "": itype = get_storm_type(nhc_forecasts[k]['vmax'][0],False)
+                if itype == "":
+                    itype = get_storm_type(nhc_forecasts[k]['vmax'][0], False)
                 track_dict['type'].append(itype)
 
                 hr = nhc_forecasts[k]['init'].strftime("%H%M")
-                track_dict['extra_obs'].append(0) if hr in ['0300','0900','1500','2100'] else track_dict['extra_obs'].append(1)
+                track_dict['extra_obs'].append(0) if hr in [
+                    '0300', '0900', '1500', '2100'] else track_dict['extra_obs'].append(1)
                 track_dict['special'].append("")
-        
-        #Add main elements from storm dict
-        for key in ['id','operational_id','name','year']:
+
+        # Add main elements from storm dict
+        for key in ['id', 'operational_id', 'name', 'year']:
             track_dict[key] = self.dict[key]
 
-        
-        #Add carq to forecast dict as hour 0, if available
-        if use_carq == True and forecast_dict['init'] in track_dict['date']:
-            insert_idx = track_dict['date'].index(forecast_dict['init'])
+        # Add carq to forecast dict as hour 0, if available
+        if use_carq and forecast_dict['init'] in track_dict['time']:
+            insert_idx = track_dict['time'].index(forecast_dict['init'])
             if 0 in forecast_dict['fhr']:
                 forecast_dict['lat'][0] = track_dict['lat'][insert_idx]
                 forecast_dict['lon'][0] = track_dict['lon'][insert_idx]
                 forecast_dict['vmax'][0] = track_dict['vmax'][insert_idx]
                 forecast_dict['mslp'][0] = track_dict['mslp'][insert_idx]
                 forecast_dict['type'][0] = track_dict['type'][insert_idx]
             else:
-                forecast_dict['fhr'].insert(0,0)
-                forecast_dict['lat'].insert(0,track_dict['lat'][insert_idx])
-                forecast_dict['lon'].insert(0,track_dict['lon'][insert_idx])
-                forecast_dict['vmax'].insert(0,track_dict['vmax'][insert_idx])
-                forecast_dict['mslp'].insert(0,track_dict['mslp'][insert_idx])
-                forecast_dict['type'].insert(0,track_dict['type'][insert_idx])
-            
-        #Add other info to forecast dict
+                forecast_dict['fhr'].insert(0, 0)
+                forecast_dict['lat'].insert(0, track_dict['lat'][insert_idx])
+                forecast_dict['lon'].insert(0, track_dict['lon'][insert_idx])
+                forecast_dict['vmax'].insert(0, track_dict['vmax'][insert_idx])
+                forecast_dict['mslp'].insert(0, track_dict['mslp'][insert_idx])
+                forecast_dict['type'].insert(0, track_dict['type'][insert_idx])
+
+        # Add other info to forecast dict
         forecast_dict['advisory_num'] = advisory_num
         forecast_dict['basin'] = self.basin
-        
-        #Plot storm
-        plot_ax = self.plot_obj.plot_storm_nhc(forecast_dict,track_dict,track_labels,cone_days,domain,ax=ax,save_path=save_path,prop=prop,map_prop=map_prop)
-        
-        #Return axis
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_storm_nhc(
+            forecast_dict, track_dict, track_labels, cone_days, domain, ax=ax, save_path=save_path, prop=prop, map_prop=map_prop)
+
+        # Return axis
         return plot_ax
-    
-    
-    def plot_models(self,forecast=None,plot_btk=False,domain="dynamic",ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot_models(self, forecast=None, plot_btk=False, domain="dynamic", ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of operational model forecast tracks.
-        
+
         Parameters
         ----------
         forecast : datetime.datetime, optional
             Datetime object representing the forecast initialization. If None (default), fetches the latest forecast.
         plot_btk : bool, optional
             If True, Best Track will be plotted alongside operational forecast models. Default is False.
         domain : str
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         models : dict
             Dictionary with **key** = model name (case-insensitive) and **value** = model color. Scroll below for available model names.
         prop : dict
             Customization properties of forecast lines. Scroll below for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         .. note::
             1. For years before the HMON model was available, the HMON key instead defaults to the old GFDL model.
-            
+
             2. For storms in the JTWC area of responsibility, the NHC key defaults to JTWC.
-        
+
         The following model names are available as keys in the "model" dict. These names are case-insensitive. To avoid plotting any of these models, set the value to None instead of a color (e.g., ``models = {'gfs':None}`` or ``models = {'GFS':None}``).
-        
+
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Model Acronym
              - Full Model Name
            * - CMC
@@ -951,17 +977,21 @@
              - UK Met Office (UKMET)
            * - ECM
              - European Centre for Medium-range Weather Forecasts (ECMWF)
            * - HMON
              - Hurricanes in a Multi-scale Ocean-coupled Non-hydrostatic Model (HMON)
            * - HWRF
              - Hurricane Weather Research and Forecast (HWRF)
+           * - HAFSA
+             - Hurricane Analysis and Forecast System A (HAFS-A)
+           * - HAFSB
+             - Hurricane Analysis and Forecast System B (HAFS-B)
            * - NHC
              - National Hurricane Center (NHC)
-        
+
         The following properties are available for customizing forecast model tracks, via ``prop``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Property
@@ -969,164 +999,191 @@
            * - linewidth
              - Line width of forecast model track. Default is 2.5.
            * - marker
              - Marker type for forecast hours. Options are 'label' (default), 'dot' or None.
            * - marker_hours
              - List of forecast hours to mark. Default is [24,48,72,96,120,144,168].
         """
-        
-        #Dictionary mapping model names to the interpolated model key
+
+        # Dictionary mapping model names to the interpolated model key
         dict_models = {
-            'cmc':'CMC2',
-            'gfs':'AVNI',
-            'ukm':'UKX2',
-            'ecm':'ECO2',
-            'hmon':'HMNI',
-            'hwrf':'HWFI',
-            'nhc':'OFCI',
+            'cmc': 'CMC2',
+            'gfs': 'AVNI',
+            'ukm': 'UKX2',
+            'ecm': 'ECO2',
+            'hmon': 'HMNI',
+            'hwrf': 'HWFI',
+            'hafsa': 'HFAI',
+            'hafsb': 'HFBI',
+            'nhc': 'OFCI',
         }
         backup_models = {
-            'gfs':['AVNO','AVNX'],
-            'ukm':['UKM2','UKM'],
-            'cmc':['CMC'],
-            'hmon':['GFDI','GFDL'],
-            'nhc':['OFCL','JTWC'],
-            'hwrf':['HWRF'],
+            'gfs': ['AVNO', 'AVNX'],
+            'ukm': ['UKM2', 'UKM'],
+            'cmc': ['CMC'],
+            'hmon': ['GFDI', 'GFDL'],
+            'nhc': ['OFCL', 'JTWC'],
+            'hwrf': ['HWRF'],
+            'hafsa': ['HFSA'],
+            'hafsb': ['HFSB'],
         }
-        
-        #Pop kwargs
-        prop = kwargs.pop('prop',{})
-        models = kwargs.pop('models',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Create instance of plot object
+
+        # Pop kwargs
+        prop = kwargs.pop('prop', {})
+        models = kwargs.pop('models', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #-------------------------------------------------------------------------
-        
-        #Get forecasts dict saved into storm object, if it hasn't been already
+
+        # -------------------------------------------------------------------------
+
+        # Get forecasts dict saved into storm object, if it hasn't been already
         try:
             self.forecast_dict
         except:
             self.get_operational_forecasts()
-        
-        #Fetch latest forecast if None
-        if forecast == None:
-            check_keys = ['AVNI','OFCI','HWFI']
-            if 'HWFI' not in self.forecast_dict.keys(): check_keys[2] = 'HWRF'
-            if 'HWRF' not in self.forecast_dict.keys() and 'HWRF' in check_keys: check_keys.pop(check_keys.index('HWRF'))
-            if 'OFCI' not in self.forecast_dict.keys(): check_keys[1] = 'OFCL'
-            if 'OFCL' not in self.forecast_dict.keys(): check_keys[1] = 'JTWC'
-            if 'JTWC' not in self.forecast_dict.keys() and 'JTWC' in check_keys: check_keys.pop(check_keys.index('JTWC'))
-            if 'AVNI' not in self.forecast_dict.keys(): check_keys[0] = 'AVNO'
-            if 'AVNO' not in self.forecast_dict.keys(): check_keys[0] = 'AVNX'
-            if 'AVNX' not in self.forecast_dict.keys() and 'AVNX' in check_keys: check_keys.pop(check_keys.index('AVNX'))
-            if len(check_keys) == 0: raise ValueError("No models are available for this storm.")
-            inits = [dt.strptime([k for k in self.forecast_dict[key]][-1],'%Y%m%d%H') for key in check_keys]
+
+        # Fetch latest forecast if None
+        if forecast is None:
+            check_keys = ['AVNI', 'OFCI', 'HWFI', 'HFAI']
+            if 'HWFI' not in self.forecast_dict.keys():
+                check_keys[2] = 'HWRF'
+            if 'HWRF' not in self.forecast_dict.keys() and 'HWRF' in check_keys:
+                check_keys.pop(check_keys.index('HWRF'))
+            if 'OFCI' not in self.forecast_dict.keys():
+                check_keys[1] = 'OFCL'
+            if 'OFCL' not in self.forecast_dict.keys():
+                check_keys[1] = 'JTWC'
+            if 'JTWC' not in self.forecast_dict.keys() and 'JTWC' in check_keys:
+                check_keys.pop(check_keys.index('JTWC'))
+            if 'AVNI' not in self.forecast_dict.keys():
+                check_keys[0] = 'AVNO'
+            if 'AVNO' not in self.forecast_dict.keys():
+                check_keys[0] = 'AVNX'
+            if 'AVNX' not in self.forecast_dict.keys() and 'AVNX' in check_keys:
+                check_keys.pop(check_keys.index('AVNX'))
+            if 'HFAI' not in self.forecast_dict.keys():
+                check_keys[3] = 'HFSA'
+            if 'HFSA' not in self.forecast_dict.keys() and 'HWRF' in check_keys:
+                check_keys.pop(check_keys.index('HFSA'))
+            if len(check_keys) == 0:
+                raise ValueError("No models are available for this storm.")
+            inits = [dt.strptime(
+                [k for k in self.forecast_dict[key]][-1], '%Y%m%d%H') for key in check_keys]
             forecast = min(inits)
-        
-        #Error check forecast date
-        if forecast < self.date[0] or forecast > self.date[-1]:
-            raise ValueError("Requested forecast is outside of the storm's duration.")
-        
-        #Construct forecast dict
+
+        # Error check forecast time
+        if forecast < self.time[0] or forecast > self.time[-1]:
+            raise ValueError(
+                "Requested forecast is outside of the storm's duration.")
+
+        # Construct forecast dict
         ds = {}
         proj_lons = []
         forecast_str = forecast.strftime('%Y%m%d%H')
         input_keys = [k for k in models.keys()]
         input_keys_lower = [k.lower() for k in models.keys()]
         for key in dict_models.keys():
-            
-            #Only proceed if model isn't not requested
+
+            # Only proceed if model isn't not requested
             if key in input_keys_lower:
                 idx = input_keys_lower.index(key)
-                if models[input_keys[idx]] == None: continue
-            
-            #Find official key
+                if models[input_keys[idx]] is None:
+                    continue
+
+            # Find official key
             official_key = dict_models[key]
             found = False
             if official_key not in self.forecast_dict.keys():
                 if key in backup_models.keys():
                     for backup_key in backup_models[key]:
                         if backup_key in self.forecast_dict.keys():
                             official_key = backup_key
                             found = True
                             break
             else:
                 found = True
-            
-            #Check for 2 vs. I if needed
-            if found == False or forecast_str not in self.forecast_dict[official_key].keys():
+
+            # Check for 2 vs. I if needed
+            if not found or forecast_str not in self.forecast_dict[official_key].keys():
                 if '2' in official_key:
-                    official_key = dict_models[key].replace('2','I')
+                    official_key = dict_models[key].replace('2', 'I')
                     if official_key not in self.forecast_dict.keys():
                         if key in backup_models.keys():
                             found = False
                             for backup_key_iter in backup_models[key]:
-                                backup_key = backup_key_iter.replace('2','I')
+                                backup_key = backup_key_iter.replace('2', 'I')
                                 if backup_key in self.forecast_dict.keys():
                                     official_key = backup_key
                                     found = True
                                     break
-                            if found == False: continue
+                            if not found:
+                                continue
                         else:
                             continue
                 else:
                     continue
-            
-            #Append forecast data if it exists for this initialization
-            if forecast_str not in self.forecast_dict[official_key].keys(): continue
+
+            # Append forecast data if it exists for this initialization
+            if forecast_str not in self.forecast_dict[official_key].keys():
+                continue
             enter_key = key + ''
-            if key.lower() == 'hmon' and 'gf' in official_key.lower(): enter_key = 'gfdl'
-            if key.lower() == 'nhc' and 'jt' in official_key.lower(): enter_key = 'jtwc'
-            ds[enter_key] = copy.deepcopy(self.forecast_dict[official_key][forecast_str])
-            
-            #Filter out to hour 168
+            if key.lower() == 'hmon' and 'gf' in official_key.lower():
+                enter_key = 'gfdl'
+            if key.lower() == 'nhc' and 'jt' in official_key.lower():
+                enter_key = 'jtwc'
+            ds[enter_key] = copy.deepcopy(
+                self.forecast_dict[official_key][forecast_str])
+
+            # Filter out to hour 168
             if ds[enter_key]['fhr'][-1] > 168:
                 idx = ds[enter_key]['fhr'].index(168)
                 for key in ds[enter_key].keys():
-                    if isinstance(ds[enter_key][key],list):
-                        ds[enter_key][key] = ds[enter_key][key][:idx+1]
-                    
+                    if isinstance(ds[enter_key][key], list):
+                        ds[enter_key][key] = ds[enter_key][key][:idx + 1]
+
             proj_lons += ds[enter_key]['lon']
-        
-        #Proceed if data exists
+
+        # Proceed if data exists
         if len(ds) == 0:
-            raise RuntimeError("No forecasts are available for the given parameters.")
-        
-        #Create cartopy projection
+            raise RuntimeError(
+                "No forecasts are available for the given parameters.")
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         elif np.nanmax(proj_lons) > 150 or np.nanmin(proj_lons) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-        
-        #Account for cases crossing dateline
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Account for cases crossing dateline
         if np.nanmax(proj_lons) > 150 or np.nanmin(proj_lons) < -150:
             for key in ds.keys():
                 new_lons = np.array(ds[key]['lon'])
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 ds[key]['lon'] = new_lons.tolist()
-        
-        #Plot storm
-        plot_ax = self.plot_obj.plot_models(forecast,plot_btk,self.dict,ds,models,domain,ax=ax,prop=prop,map_prop=map_prop,save_path=save_path)
-        
-        #Return axis
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_models(
+            forecast, plot_btk, self.dict, ds, models, domain, ax=ax, prop=prop, map_prop=map_prop, save_path=save_path)
+
+        # Return axis
         return plot_ax
-    
-    
-    def plot_ensembles(self,forecast=None,fhr=None,interpolate=True,domain="dynamic",ax=None,cartopy_proj=None,save_path=None,**kwargs):
-        
+
+    def plot_ensembles(self, forecast=None, fhr=None, interpolate=True, domain="dynamic", ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of individual GEFS ensemble tracks.
-        
+
         Parameters
         ----------
         forecast : datetime.datetime, optional
             Datetime object representing the GEFS run initialization. If None (default), fetches the latest run.
         fhr : int, optional
             Forecast hour to plot. If None (default), a cumulative plot of all forecast hours will be produced. If an integer, a single plot will be produced.
         interpolate : bool, optional
@@ -1135,15 +1192,15 @@
             Domain for the plot. Default is "dynamic". Please refer to :ref:`options-domain` for available domain options.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop_members : dict
             Customization properties of GEFS ensemble member track lines. Scroll down below for available options.
         prop_mean : dict
             Customization properties of GEFS ensemble mean track. Scroll down below for available options.
         prop_gfs : dict
@@ -1152,40 +1209,40 @@
             Customization properties of Best Track line. Scroll down below for available options.
         prop_ellipse : dict
             Customization properties of GEFS ensemble ellipse. Scroll down below for available options.
         prop_density : dict
             Customization properties of GEFS ensemble track density. Scroll down below for available options.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         .. note::
             The total number of GEFS members available for analysis is as follows:
-            
+
             * **2020 - present** - 31 members
             * **2006 - 2019** - 21 members
             * **2005 & back** - 5 members
-            
+
             As the density plot and ensemble ellipse require a minimum of 10 ensemble members, they will not be generated for storms from 2005 and earlier.
-            
+
             Additionally, ellipses are not generated if using the default ``fhr=None``, meaning a cumulative track density plot is generated instead.
-        
+
         The ensemble ellipse used in this function follows the methodology of `Hamill et al. (2011)`_, denoting the spread in ensemble member cyclone positions. The size of the ellipse is calculated to contain 90% of ensemble members at any given time. This ellipse can be used to determine the primary type of ensemble variability:
-        
+
         * **Along-track variability** - if the major axis of the ellipse is parallel to the ensemble mean motion vector.
         * **Across-track variability** - if the major axis of the ellipse is normal to the ensemble mean motion vector.
 
         .. _Hamill et al. (2011): https://doi.org/10.1175/2010MWR3456.1
-        
+
         The following properties are available for customizing ensemble member tracks, via ``prop_members``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Property
@@ -1213,60 +1270,60 @@
              - Description
            * - plot
              - Boolean to determine whether to plot ensemble mean forecast track. Default is True.
            * - linewidth
              - Forecast track linewidth. Default is 3.0.
            * - linecolor
              - Forecast track line color. Default is black.
-        
+
         The following properties are available for customizing GFS forecast track, via ``prop_gfs``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Property
              - Description
            * - plot
              - Boolean to determine whether to plot GFS forecast track. Default is True.
            * - linewidth
              - Forecast track linewidth. Default is 3.0.
            * - linecolor
              - Forecast track line color. Default is red.
-        
+
         The following properties are available for customizing Best Track line, via ``prop_btk``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Property
              - Description
            * - plot
              - Boolean to determine whether to plot Best Track line. Default is True.
            * - linewidth
              - Best Track linewidth. Default is 2.5.
            * - linecolor
              - Best Track line color. Default is blue.
-        
+
         The following properties are available for customizing the ensemble ellipse plot, via ``prop_ellipse``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Property
              - Description
            * - plot
              - Boolean to determine whether to plot ensemble member ellipse. Default is True.
            * - linewidth
              - Ellipse linewidth. Default is 3.0.
            * - linecolor
              - Ellipse line color. Default is blue.
-        
+
         The following properties are available for customizing ensemble member track density, via ``prop_density``.
 
         .. list-table:: 
            :widths: 25 75
            :header-rows: 1
 
            * - Property
@@ -1275,235 +1332,274 @@
              - Boolean to determine whether to plot ensemble member track density. Default is True.
            * - radius
              - Radius (in km) for which to calculate track density. Default is 200 km.
            * - cmap
              - Matplotlib colormap for track density plot. Default is "plasma_r".
            * - levels
              - List of levels for contour filling track density.
-        
+
         """
-        
-        #Pop kwargs
-        prop_members = kwargs.pop('prop_members',{})
-        prop_mean = kwargs.pop('prop_mean',{})
-        prop_gfs = kwargs.pop('prop_gfs',{})
-        prop_btk = kwargs.pop('prop_btk',{})
-        prop_ellipse = kwargs.pop('prop_ellipse',{})
-        prop_density = kwargs.pop('prop_density',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Create instance of plot object
+
+        # Pop kwargs
+        prop_members = kwargs.pop('prop_members', {})
+        prop_mean = kwargs.pop('prop_mean', {})
+        prop_gfs = kwargs.pop('prop_gfs', {})
+        prop_btk = kwargs.pop('prop_btk', {})
+        prop_ellipse = kwargs.pop('prop_ellipse', {})
+        prop_density = kwargs.pop('prop_density', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Create instance of plot object
         try:
             self.plot_obj
         except:
             self.plot_obj = TrackPlot()
-        
-        #-------------------------------------------------------------------------
-        
-        #Get forecasts dict saved into storm object, if it hasn't been already
+
+        # -------------------------------------------------------------------------
+
+        # Get forecasts dict saved into storm object, if it hasn't been already
         try:
             self.forecast_dict
         except:
             self.get_operational_forecasts()
-        
-        #Fetch latest forecast if None
-        if forecast == None:
+
+        # Fetch latest forecast if None
+        if forecast is None:
             inits = []
-            for key in ['AC00','AP01','AP02','AP03','AP04','AP05']:
+            for key in ['AC00', 'AP01', 'AP02', 'AP03', 'AP04', 'AP05']:
                 if key in self.forecast_dict.keys():
-                    inits.append( dt.strptime([k for k in self.forecast_dict[key]][-1],'%Y%m%d%H') )
+                    inits.append(dt.strptime(
+                        [k for k in self.forecast_dict[key]][-1], '%Y%m%d%H'))
             if len(inits) > 0:
                 forecast = min(inits)
             else:
-                raise RuntimeError("Error: Could not determine the latest available GEFS forecast.")
-        
-        #Determine max members by year
+                raise RuntimeError(
+                    "Error: Could not determine the latest available GEFS forecast.")
+
+        # Determine max members by year
         nens = 21
-        if self.year >= 2020 and ('AP21' in self.forecast_dict.keys() or 'AP22' in self.forecast_dict.keys() or 'AP23' in self.forecast_dict.keys()): nens = 31
-        
-        #Enforce fhr type
-        if isinstance(fhr,list): fhr = fhr[0]
-        
-        #If this forecast init was recently used, don't re-calculate
+        if self.year >= 2020 and ('AP21' in self.forecast_dict.keys() or 'AP22' in self.forecast_dict.keys() or 'AP23' in self.forecast_dict.keys()):
+            nens = 31
+
+        # Enforce fhr type
+        if isinstance(fhr, list):
+            fhr = fhr[0]
+
+        # If this forecast init was recently used, don't re-calculate
         init_used = False
         try:
-            if self.gefs_init == forecast: init_used = True
+            if self.gefs_init == forecast:
+                init_used = True
         except:
             pass
-        
-        #Only calculate if needed to
-        if init_used == False:
-            
+
+        # Only calculate if needed to
+        if not init_used:
+
             print("--> Starting to calculate ellipse data")
-            
-            #Create dict to store all data in
-            ds = {'gfs':{'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],'date':[]},
-                  'gefs':{'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],'date':[],
-                          'members':[],'ellipse_lat':[],'ellipse_lon':[]}
+
+            # Create dict to store all data in
+            ds = {'gfs': {'fhr': [], 'lat': [], 'lon': [], 'vmax': [], 'mslp': [], 'time': []},
+                  'gefs': {'fhr': [], 'lat': [], 'lon': [], 'vmax': [], 'mslp': [], 'time': [],
+                           'members': [], 'ellipse_lat': [], 'ellipse_lon': []}
                   }
 
-            #String formatting for ensembles
+            # String formatting for ensembles
             def str2(ens):
-                if ens == 0: return "AC00"
-                if ens < 10: return f"AP0{ens}"
+                if ens == 0:
+                    return "AC00"
+                if ens < 10:
+                    return f"AP0{ens}"
                 return f"AP{ens}"
 
-            #Get GFS forecast entry (AVNX is valid for RAL a-deck source)
+            # Get GFS forecast entry (AVNX is valid for RAL a-deck source)
             gfs_key = 'AVNO' if 'AVNO' in self.forecast_dict.keys() else 'AVNX'
             try:
-                forecast_gfs = self.forecast_dict[gfs_key][forecast.strftime("%Y%m%d%H")]
+                forecast_gfs = self.forecast_dict[gfs_key][forecast.strftime(
+                    "%Y%m%d%H")]
             except:
-                raise RuntimeError("The requested GFS initialization isn't available for this storm.")
+                raise RuntimeError(
+                    "The requested GFS initialization isn't available for this storm.")
 
-            #Enter into dict entry
+            # Enter into dict entry
             ds['gfs']['fhr'] = [int(i) for i in forecast_gfs['fhr']]
-            ds['gfs']['lat'] = [np.round(i,1) for i in forecast_gfs['lat']]
-            ds['gfs']['lon'] = [np.round(i,1) for i in forecast_gfs['lon']]
+            ds['gfs']['lat'] = [np.round(i, 1) for i in forecast_gfs['lat']]
+            ds['gfs']['lon'] = [np.round(i, 1) for i in forecast_gfs['lon']]
             ds['gfs']['vmax'] = [float(i) for i in forecast_gfs['vmax']]
             ds['gfs']['mslp'] = forecast_gfs['mslp']
-            ds['gfs']['date'] = [forecast+timedelta(hours=i) for i in forecast_gfs['fhr']]
+            ds['gfs']['time'] = [forecast +
+                                 timedelta(hours=i) for i in forecast_gfs['fhr']]
 
-            #Retrieve GEFS ensemble data (30 members 2019-present, 20 members prior)
-            for ens in range(0,nens):
+            # Retrieve GEFS ensemble data (30 members 2019-present, 20 members prior)
+            for ens in range(0, nens):
 
-                #Create dict entry
-                ds[f'gefs_{ens}'] = {'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],'date':[]}
+                # Create dict entry
+                ds[f'gefs_{ens}'] = {
+                    'fhr': [],
+                    'lat': [],
+                    'lon': [],
+                    'vmax': [],
+                    'mslp': [],
+                    'time': [],
+                }
 
-                #Retrieve ensemble member data
+                # Retrieve ensemble member data
                 ens_str = str2(ens)
-                if ens_str not in self.forecast_dict.keys(): continue
-                if forecast.strftime("%Y%m%d%H") not in self.forecast_dict[ens_str].keys(): continue
-                forecast_ens = self.forecast_dict[ens_str][forecast.strftime("%Y%m%d%H")]
-
-                #Enter into dict entry
-                ds[f'gefs_{ens}']['fhr'] = [int(i) for i in forecast_ens['fhr']]
-                ds[f'gefs_{ens}']['lat'] = [np.round(i,1) for i in forecast_ens['lat']]
-                ds[f'gefs_{ens}']['lon'] = [np.round(i,1) for i in forecast_ens['lon']]
-                ds[f'gefs_{ens}']['vmax'] = [float(i) for i in forecast_ens['vmax']]
+                if ens_str not in self.forecast_dict.keys():
+                    continue
+                if forecast.strftime("%Y%m%d%H") not in self.forecast_dict[ens_str].keys():
+                    continue
+                forecast_ens = self.forecast_dict[ens_str][forecast.strftime(
+                    "%Y%m%d%H")]
+
+                # Enter into dict entry
+                ds[f'gefs_{ens}']['fhr'] = [int(i)
+                                            for i in forecast_ens['fhr']]
+                ds[f'gefs_{ens}']['lat'] = [
+                    np.round(i, 1) for i in forecast_ens['lat']]
+                ds[f'gefs_{ens}']['lon'] = [
+                    np.round(i, 1) for i in forecast_ens['lon']]
+                ds[f'gefs_{ens}']['vmax'] = [
+                    float(i) for i in forecast_ens['vmax']]
                 ds[f'gefs_{ens}']['mslp'] = forecast_ens['mslp']
-                ds[f'gefs_{ens}']['date'] = [forecast+timedelta(hours=i) for i in forecast_ens['fhr']]
+                ds[f'gefs_{ens}']['time'] = [forecast +
+                                             timedelta(hours=i) for i in forecast_ens['fhr']]
 
-            #Construct ensemble mean data
-            #Iterate through all forecast hours
-            for iter_fhr in range(0,246,6):
+            # Construct ensemble mean data
+            # Iterate through all forecast hours
+            for iter_fhr in range(0, 246, 6):
 
-                #Temporary data arrays
+                # Temporary data arrays
                 temp_data = {}
                 for key in ds['gfs'].keys():
-                    if key not in ['date','fhr']: temp_data[key] = []
+                    if key not in ['time', 'fhr']:
+                        temp_data[key] = []
 
-                #Iterate through ensemble member
+                # Iterate through ensemble member
                 for ens in range(nens):
 
-                    #Determine if member has data valid at this forecast hour
+                    # Determine if member has data valid at this forecast hour
                     if iter_fhr in ds[f'gefs_{ens}']['fhr']:
 
-                        #Retrieve index
+                        # Retrieve index
                         idx = ds[f'gefs_{ens}']['fhr'].index(iter_fhr)
 
-                        #Append data
+                        # Append data
                         for key in ds['gfs'].keys():
-                            if key not in ['date','fhr']: temp_data[key].append(ds[f'gefs_{ens}'][key][idx])
+                            if key not in ['time', 'fhr']:
+                                temp_data[key].append(
+                                    ds[f'gefs_{ens}'][key][idx])
 
-                #Proceed if 20 or more ensemble members
+                # Proceed if 20 or more ensemble members
                 if len(temp_data['lat']) >= 10:
 
-                    #Append data
+                    # Append data
                     for key in ds['gfs'].keys():
-                        if key not in ['date','fhr']:
+                        if key not in ['time', 'fhr']:
                             ds['gefs'][key].append(np.nanmean(temp_data[key]))
                     ds['gefs']['fhr'].append(iter_fhr)
-                    ds['gefs']['date'].append(forecast+timedelta(hours=iter_fhr))
+                    ds['gefs']['time'].append(
+                        forecast + timedelta(hours=iter_fhr))
                     ds['gefs']['members'].append(len(temp_data['lat']))
 
-                    #Calculate ellipse data
+                    # Calculate ellipse data
                     if prop_ellipse is not None:
                         try:
-                            ellipse_data = calc_ensemble_ellipse(temp_data['lon'],temp_data['lat'])
-                            ds['gefs']['ellipse_lon'].append(ellipse_data['ellipse_lon'])
-                            ds['gefs']['ellipse_lat'].append(ellipse_data['ellipse_lat'])
+                            ellipse_data = calc_ensemble_ellipse(
+                                temp_data['lon'], temp_data['lat'])
+                            ds['gefs']['ellipse_lon'].append(
+                                ellipse_data['ellipse_lon'])
+                            ds['gefs']['ellipse_lat'].append(
+                                ellipse_data['ellipse_lat'])
                         except:
                             ds['gefs']['ellipse_lon'].append([])
                             ds['gefs']['ellipse_lat'].append([])
                     else:
                         ds['gefs']['ellipse_lon'].append([])
                         ds['gefs']['ellipse_lat'].append([])
-        
-            #Save data for future use if needed
+
+            # Save data for future use if needed
             self.gefs_init = forecast
             self.ds = ds
-            
+
             print("--> Done calculating ellipse data")
-        
-        #Determine lon bounds for cartopy projection
+
+        # Determine lon bounds for cartopy projection
         proj_lons = []
         for key in self.ds.keys():
             proj_lons += self.ds[key]['lon']
-        if fhr != None and fhr in self.ds['gefs']['fhr']:
+        if fhr is not None and fhr in self.ds['gefs']['fhr']:
             fhr_idx = self.ds['gefs']['fhr'].index(fhr)
             proj_lons += self.ds['gefs']['ellipse_lon'][fhr_idx]
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is not None:
             self.plot_obj.proj = cartopy_proj
         elif np.nanmax(proj_lons) > 150 or np.nanmin(proj_lons) < -150:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=180.0)
         else:
-            self.plot_obj.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-        
-        #Account for cases crossing dateline
+            self.plot_obj.create_cartopy(
+                proj='PlateCarree', central_longitude=0.0)
+
+        # Account for cases crossing dateline
         ds = copy.deepcopy(self.ds)
         if np.nanmax(proj_lons) > 150 or np.nanmin(proj_lons) < -150:
             for key in ds.keys():
                 new_lons = np.array(ds[key]['lon'])
-                new_lons[new_lons<0] = new_lons[new_lons<0]+360.0
+                new_lons[new_lons < 0] = new_lons[new_lons < 0] + 360.0
                 ds[key]['lon'] = new_lons.tolist()
-                
-            #Re-calculate GEFS mean
+
+            # Re-calculate GEFS mean
             for iter_hr in ds['gefs']['fhr']:
                 fhr_idx = ds['gefs']['fhr'].index(iter_hr)
-                ds['gefs']['lon'][fhr_idx] = np.nanmean([ds[f'gefs_{ens}']['lon'][ds[f'gefs_{ens}']['fhr'].index(iter_hr)] for ens in range(nens) if iter_hr in ds[f'gefs_{ens}']['fhr']])
-        
-        #Plot storm
-        plot_ax = self.plot_obj.plot_ensembles(forecast,self.dict,fhr,interpolate,prop_members,prop_mean,prop_gfs,prop_btk,prop_ellipse,prop_density,nens,domain,ds,ax=ax,map_prop=map_prop,save_path=save_path)
-        
-        #Return axis
+                ds['gefs']['lon'][fhr_idx] = np.nanmean([ds[f'gefs_{ens}']['lon'][ds[f'gefs_{ens}']['fhr'].index(
+                    iter_hr)] for ens in range(nens) if iter_hr in ds[f'gefs_{ens}']['fhr']])
+
+        # Plot storm
+        plot_ax = self.plot_obj.plot_ensembles(forecast, self.dict, fhr, interpolate, prop_members, prop_mean,
+                                               prop_gfs, prop_btk, prop_ellipse, prop_density, nens, domain,
+                                               ds, ax=ax, map_prop=map_prop, save_path=save_path)
+
+        # Return axis
         return plot_ax
-    
+
     def list_nhc_discussions(self):
-        
         r"""
         Retrieves a list of NHC forecast discussions for this storm, archived on https://ftp.nhc.noaa.gov/atcf/archive/.
-        
+
         Returns
         -------
         dict
             Dictionary containing entries for each forecast discussion for this storm.
         """
-        
-        #Check to ensure the data source is HURDAT
+
+        # Check to ensure the data source is HURDAT
         if self.source != "hurdat":
-            raise RuntimeError("Error: NHC data can only be accessed when HURDAT is used as the data source.")
-        
-        #Check to ensure storm is not an invest
+            raise RuntimeError(
+                "Error: NHC data can only be accessed when HURDAT is used as the data source.")
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Get storm ID & corresponding data URL
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Get storm ID & corresponding data URL
         storm_id = self.dict['operational_id']
         storm_year = self.dict['year']
-        
-        #Error check
+
+        # Error check
         if storm_id == '':
-            raise RuntimeError("Error: This storm was identified post-operationally. No NHC operational data is available.")
-        
-        #Get list of available NHC advisories & map discussions
+            raise RuntimeError(
+                "Error: This storm was identified post-operationally. No NHC operational data is available.")
+
+        # Get list of available NHC advisories & map discussions
         if storm_year == (dt.now()).year:
-            
-            #Get list of all discussions for all storms this year
+
+            # Get list of all discussions for all storms this year
             try:
                 url_disco = 'https://ftp.nhc.noaa.gov/atcf/dis/'
                 page = requests.get(url_disco).text
                 content = page.split("\n")
                 files = []
                 for line in content:
                     if ".discus." in line and self.id.lower() in line:
@@ -1512,67 +1608,77 @@
                         files.append(filename)
                 del content
             except:
                 ftp = FTP('ftp.nhc.noaa.gov')
                 ftp.login()
                 ftp.cwd('atcf/dis')
                 files = ftp.nlst()
-                files = [i for i in files if ".discus." in i and self.id.lower() in i]
-                out = ftp.quit()
-            
-            #Read in all NHC forecast discussions
-            discos = {'id':[],'utc_date':[],'url':[],'mode':0}
+                files = [
+                    i for i in files if ".discus." in i and self.id.lower() in i]
+                ftp.quit()
+
+            # Read in all NHC forecast discussions
+            discos = {
+                'id': [],
+                'utc_time': [],
+                'url': [],
+                'mode': 0,
+            }
             for file in files:
-                
-                #Get info about forecast
+
+                # Get info about forecast
                 file_info = file.split(".")
                 disco_number = int(file_info[2])
-                
-                #Open file to get info about time issued
+
+                # Open file to get info about time issued
                 f = urllib.request.urlopen(url_disco + file)
                 content = f.read()
                 content = content.decode("utf-8")
                 content = content.split("\n")
                 f.close()
-                
-                #Figure out time issued
+
+                # Figure out time issued
                 hr = content[5].split(" ")[0]
                 zone = content[5].split(" ")[2]
-                disco_time = num_to_str2(int(hr)) + ' '.join(content[5].split(" ")[1:])
-                
+                disco_time = num_to_str2(int(hr)) + \
+                    ' '.join(content[5].split(" ")[1:])
+
                 format_time = content[5].split(" ")[0]
-                if len(format_time) == 3: format_time = "0" + format_time
-                format_time = format_time + " " +  ' '.join(content[5].split(" ")[1:])
-                disco_date = dt.strptime(format_time,f'%I00 %p {zone} %a %b %d %Y')
-                
+                if len(format_time) == 3:
+                    format_time = "0" + format_time
+                format_time = format_time + " " + \
+                    ' '.join(content[5].split(" ")[1:])
+                disco_time = dt.strptime(
+                    format_time, f'%I00 %p {zone} %a %b %d %Y')
+
                 time_zones = {
-                'ADT':-3,
-                'AST':-4,
-                'EDT':-4,
-                'EST':-5,
-                'CDT':-5,
-                'CST':-6,
-                'MDT':-6,
-                'MST':-7,
-                'PDT':-7,
-                'PST':-8,
-                'HDT':-9,
-                'HST':-10}
-                offset = time_zones.get(zone,0)
-                disco_date = disco_date + timedelta(hours=offset*-1)
-                
-                #Add times issued
+                    'ADT': -3,
+                    'AST': -4,
+                    'EDT': -4,
+                    'EST': -5,
+                    'CDT': -5,
+                    'CST': -6,
+                    'MDT': -6,
+                    'MST': -7,
+                    'PDT': -7,
+                    'PST': -8,
+                    'HDT': -9,
+                    'HST': -10}
+                offset = time_zones.get(zone, 0)
+                disco_time = disco_time + timedelta(hours=offset * -1)
+
+                # Add times issued
                 discos['id'].append(disco_number)
-                discos['utc_date'].append(disco_date)
+                discos['utc_time'].append(disco_time)
                 discos['url'].append(url_disco + file)
-            
+
         elif storm_year < 1992:
             raise RuntimeError("NHC discussion data is unavailable.")
         elif storm_year < 2000:
-            #Get directory path of storm and read it in
+            # Get directory path of storm and read it in
             url_disco = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
             url = url_disco + f'{storm_id.lower()}_msg.zip'
             try:
                 request = urllib.request.Request(url)
                 response = urllib.request.urlopen(request)
                 file_like_object = BytesIO(response.read())
                 tar = zipfile.ZipFile(file_like_object)
@@ -1582,81 +1688,90 @@
                     url = url_disco + f'{storm_id.lower()}_msg.zip'
                     request = urllib.request.Request(url)
                     response = urllib.request.urlopen(request)
                     file_like_object = BytesIO(response.read())
                     tar = zipfile.ZipFile(file_like_object)
                 except:
                     raise RuntimeError("NHC discussion data is unavailable.")
-            
-            #Get file list
+
+            # Get file list
             members = '\n'.join([i for i in tar.namelist()])
             nums = "[0123456789]"
             search_pattern = f'n{storm_id[0:4].lower()}{str(storm_year)[2:]}.[01]{nums}{nums}'
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(members)
             files = []
             for file in filelist:
-                if file not in files: files.append(file) #remove duplicates
-            
-            #Read in all NHC forecast discussions
-            discos = {'id':[],'utc_date':[],'url':[],'mode':4}
+                if file not in files:
+                    files.append(file)  # remove duplicates
+
+            # Read in all NHC forecast discussions
+            discos = {
+                'id': [],
+                'utc_time': [],
+                'url': [],
+                'mode': 4,
+            }
             for file in files:
-                
-                #Get info about forecast
+
+                # Get info about forecast
                 file_info = file.split(".")
                 disco_number = int(file_info[1])
-                
-                #Open file to get info about time issued
+
+                # Open file to get info about time issued
                 members = tar.namelist()
                 members_names = [i for i in members]
                 idx = members_names.index(file)
                 content = (tar.read(members[idx])).decode()
                 content = content.split("\n")
-                
-                #Figure out time issued
+
+                # Figure out time issued
                 slice_idx = 5 if storm_year < 1998 else 4
-                for temp_idx in [slice_idx,slice_idx-1,slice_idx+1,slice_idx-2,slice_idx+2]:
+                for temp_idx in [slice_idx, slice_idx - 1, slice_idx + 1, slice_idx - 2, slice_idx + 2]:
                     try:
                         hr = content[temp_idx].split(" ")[0]
                         if 'NOON' in content[temp_idx]:
-                            temp_line = content[temp_idx].replace("NOON","12 PM")
+                            temp_line = content[temp_idx].replace(
+                                "NOON", "12 PM")
                             zone = temp_line.split(" ")[1]
-                            disco_date = dt.strptime(temp_line.rstrip(),f'%I %p {zone} %a %b %d %Y')
+                            disco_time = dt.strptime(
+                                temp_line.rstrip(), f'%I %p {zone} %a %b %d %Y')
                         else:
                             zone = content[temp_idx].split(" ")[2]
-                            disco_date = dt.strptime(content[temp_idx].rstrip(),f'%I %p {zone} %a %b %d %Y')
+                            disco_time = dt.strptime(
+                                content[temp_idx].rstrip(), f'%I %p {zone} %a %b %d %Y')
                     except:
                         pass
-                
+
                 time_zones = {
-                'ADT':-3,
-                'AST':-4,
-                'EDT':-4,
-                'EST':-5,
-                'CDT':-5,
-                'CST':-6,
-                'MDT':-6,
-                'MST':-7,
-                'PDT':-7,
-                'PST':-8,
-                'HDT':-9,
-                'HST':-10}
-                offset = time_zones.get(zone,0)
-                disco_date = disco_date + timedelta(hours=offset*-1)
-                
-                #Add times issued
+                    'ADT': -3,
+                    'AST': -4,
+                    'EDT': -4,
+                    'EST': -5,
+                    'CDT': -5,
+                    'CST': -6,
+                    'MDT': -6,
+                    'MST': -7,
+                    'PDT': -7,
+                    'PST': -8,
+                    'HDT': -9,
+                    'HST': -10}
+                offset = time_zones.get(zone, 0)
+                disco_time = disco_time + timedelta(hours=offset * -1)
+
+                # Add times issued
                 discos['id'].append(disco_number)
-                discos['utc_date'].append(disco_date)
+                discos['utc_time'].append(disco_time)
                 discos['url'].append(file)
-                
+
             response.close()
             tar.close()
-            
+
         elif storm_year == 2000:
-            #Get directory path of storm and read it in
+            # Get directory path of storm and read it in
             url_disco = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
             url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
             try:
                 request = urllib.request.Request(url)
                 response = urllib.request.urlopen(request)
                 file_like_object = BytesIO(response.read())
                 tar = tarfile.open(fileobj=file_like_object)
@@ -1666,633 +1781,679 @@
                     url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
                     request = urllib.request.Request(url)
                     response = urllib.request.urlopen(request)
                     file_like_object = BytesIO(response.read())
                     tar = tarfile.open(fileobj=file_like_object)
                 except:
                     raise RuntimeError("NHC discussion data is unavailable.")
-            
-            #Get file list
+
+            # Get file list
             members = '\n'.join([i.name for i in tar.getmembers()])
             nums = "[0123456789]"
             search_pattern = f'N{storm_id[0:4]}{str(storm_year)[2:]}.[01]{nums}{nums}'
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(members)
             files = []
             for file in filelist:
-                if file not in files: files.append(file) #remove duplicates
-            
-            #Read in all NHC forecast discussions
-            discos = {'id':[],'utc_date':[],'url':[],'mode':3}
+                if file not in files:
+                    files.append(file)  # remove duplicates
+
+            # Read in all NHC forecast discussions
+            discos = {
+                'id': [],
+                'utc_time': [],
+                'url': [],
+                'mode': 3,
+            }
             for file in files:
 
-                #Get info about forecast
+                # Get info about forecast
                 file_info = file.split(".")
                 disco_number = int(file_info[1])
-                
-                #Open file to get info about time issued
+
+                # Open file to get info about time issued
                 members = tar.getmembers()
                 members_names = [i.name for i in members]
                 idx = members_names.index(file)
                 f = tar.extractfile(members[idx])
                 content = (f.read()).decode()
                 f.close()
                 content = content.split("\n")
-                
-                #Figure out time issued
+
+                # Figure out time issued
                 hr = content[4].split(" ")[0]
                 zone = content[4].split(" ")[2]
-                disco_time = num_to_str2(int(hr)) + ' '.join(content[4].split(" ")[1:])
-                disco_date = dt.strptime(content[4],f'%I %p {zone} %a %b %d %Y')
-                
+                disco_time = num_to_str2(int(hr)) + \
+                    ' '.join(content[4].split(" ")[1:])
+                disco_time = dt.strptime(
+                    content[4], f'%I %p {zone} %a %b %d %Y')
+
                 time_zones = {
-                'ADT':-3,
-                'AST':-4,
-                'EDT':-4,
-                'EST':-5,
-                'CDT':-5,
-                'CST':-6,
-                'MDT':-6,
-                'MST':-7,
-                'PDT':-7,
-                'PST':-8,
-                'HDT':-9,
-                'HST':-10}
-                offset = time_zones.get(zone,0)
-                disco_date = disco_date + timedelta(hours=offset*-1)
-                
-                #Add times issued
+                    'ADT': -3,
+                    'AST': -4,
+                    'EDT': -4,
+                    'EST': -5,
+                    'CDT': -5,
+                    'CST': -6,
+                    'MDT': -6,
+                    'MST': -7,
+                    'PDT': -7,
+                    'PST': -8,
+                    'HDT': -9,
+                    'HST': -10}
+                offset = time_zones.get(zone, 0)
+                disco_time = disco_time + timedelta(hours=offset * -1)
+
+                # Add times issued
                 discos['id'].append(disco_number)
-                discos['utc_date'].append(disco_date)
+                discos['utc_time'].append(disco_time)
                 discos['url'].append(file)
-                
+
             response.close()
             tar.close()
-            
-        elif storm_year in range(2001,2006):
-            #Get directory path of storm and read it in
+
+        elif storm_year in range(2001, 2006):
+            # Get directory path of storm and read it in
             try:
                 url_disco = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
                 url = url_disco + f'{storm_id.lower()}_msgs.tar.gz'
-                if storm_year < 2003: url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
+                if storm_year < 2003:
+                    url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
                 request = urllib.request.Request(url)
                 response = urllib.request.urlopen(request)
                 file_like_object = BytesIO(response.read())
                 tar = tarfile.open(fileobj=file_like_object)
             except:
                 try:
                     url_disco = f"ftp://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
                     url = url_disco + f'{storm_id.lower()}_msgs.tar.gz'
-                    if storm_year < 2003: url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
+                    if storm_year < 2003:
+                        url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
                     request = urllib.request.Request(url)
                     response = urllib.request.urlopen(request)
                     file_like_object = BytesIO(response.read())
                     tar = tarfile.open(fileobj=file_like_object)
                 except:
                     raise RuntimeError("NHC discussion data is unavailable.")
 
-            #Get file list
+            # Get file list
             members = '\n'.join([i.name for i in tar.getmembers()])
             nums = "[0123456789]"
             search_pattern = f'{storm_id.lower()}.discus.[01]{nums}{nums}.{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}'
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(members)
             files = []
             for file in filelist:
-                if file not in files: files.append(file) #remove duplicates
+                if file not in files:
+                    files.append(file)  # remove duplicates
             response.close()
             tar.close()
-            
-            #Read in all NHC forecast discussions
-            discos = {'id':[],'utc_date':[],'url':[],'mode':1}
+
+            # Read in all NHC forecast discussions
+            discos = {
+                'id': [],
+                'utc_time': [],
+                'url': [],
+                'mode': 1,
+            }
             for file in files:
 
-                #Get info about forecast
+                # Get info about forecast
                 file_info = file.split(".")
                 disco_number = int(file_info[2])
                 disco_time = file_info[3]
                 disco_year = storm_year
                 if disco_time[0:2] == "01" and int(storm_id[2:4]) > 3:
                     disco_year = storm_year + 1
-                disco_date = dt.strptime(str(disco_year)+disco_time,'%Y%m%d%H%M')
+                disco_time = dt.strptime(
+                    str(disco_year) + disco_time, '%Y%m%d%H%M')
 
                 discos['id'].append(disco_number)
-                discos['utc_date'].append(disco_date)
+                discos['utc_time'].append(disco_time)
                 discos['url'].append(file)
-                
-            if storm_year < 2003: discos['mode'] = 2
-            
+
+            if storm_year < 2003:
+                discos['mode'] = 2
+
         else:
-            #Retrieve list of NHC discussions for this storm
+            # Retrieve list of NHC discussions for this storm
             try:
                 url_disco = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
                 path_disco = urllib.request.urlopen(url_disco)
                 string = path_disco.read().decode('utf-8')
                 nums = "[0123456789]"
                 search_pattern = f'{storm_id.lower()}.discus.[01]{nums}{nums}.{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}'
                 pattern = re.compile(search_pattern)
                 filelist = pattern.findall(string)
                 files = []
                 for file in filelist:
-                    if file not in files: files.append(file) #remove duplicates
+                    if file not in files:
+                        files.append(file)  # remove duplicates
                 path_disco.close()
             except:
                 try:
                     url_disco = f"ftp://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
                     path_disco = urllib.request.urlopen(url_disco)
                     string = path_disco.read().decode('utf-8')
                     nums = "[0123456789]"
                     search_pattern = f'{storm_id.lower()}.discus.[01]{nums}{nums}.{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}'
                     pattern = re.compile(search_pattern)
                     filelist = pattern.findall(string)
                     files = []
                     for file in filelist:
-                        if file not in files: files.append(file) #remove duplicates
+                        if file not in files:
+                            files.append(file)  # remove duplicates
                     path_disco.close()
                 except:
                     raise RuntimeError("NHC discussion data is unavailable.")
 
-            #Read in all NHC forecast discussions
-            discos = {'id':[],'utc_date':[],'url':[],'mode':0}
+            # Read in all NHC forecast discussions
+            discos = {
+                'id': [],
+                'utc_time': [],
+                'url': [],
+                'mode': 0,
+            }
             for file in files:
 
-                #Get info about forecast
+                # Get info about forecast
                 file_info = file.split(".")
                 disco_number = int(file_info[2])
                 disco_time = file_info[3]
                 disco_year = storm_year
                 if disco_time[0:2] == "01" and int(storm_id[2:4]) > 3:
                     disco_year = storm_year + 1
-                disco_date = dt.strptime(str(disco_year)+disco_time,'%Y%m%d%H%M')
+                disco_time = dt.strptime(
+                    str(disco_year) + disco_time, '%Y%m%d%H%M')
 
                 discos['id'].append(disco_number)
-                discos['utc_date'].append(disco_date)
+                discos['utc_time'].append(disco_time)
                 discos['url'].append(url_disco + file)
-                
-        #Return dict entry
+
+        # Return dict entry
         try:
             discos
         except:
             raise RuntimeError("NHC discussion data is unavailable.")
-            
+
         if len(discos['id']) == 0:
             raise RuntimeError("NHC discussion data is unavailable.")
         return discos
-        
-    def get_nhc_discussion(self,forecast,save_path=None):
-        
+
+    def get_nhc_discussion(self, forecast, save_path=None):
         r"""
         Retrieves a single NHC forecast discussion.
-        
+
         Parameters
         ----------
         forecast : datetime.datetime or int
             Datetime object representing the desired forecast discussion time (in UTC), or integer representing the forecast discussion ID. If -1 is passed, the latest forecast discussion is returned.
         save_path : str, optional
             Directory path to save the forecast discussion text to. If None (default), forecast won't be saved.
-        
+
         Returns
         -------
         dict
             Dictionary containing the forecast discussion text and accompanying information about this discussion.
         """
-        
-        #Check to ensure the data source is HURDAT
+
+        # Check to ensure the data source is HURDAT
         if self.source != "hurdat":
             msg = "Error: NHC data can only be accessed when HURDAT is used as the data source."
             raise RuntimeError(msg)
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Get storm ID & corresponding data URL
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Get storm ID & corresponding data URL
         storm_id = self.dict['operational_id']
         storm_year = self.dict['year']
-        
-        #Error check
+
+        # Error check
         if storm_id == '':
             msg = "No NHC operational data is available for this storm."
             raise RuntimeError(msg)
-        
-        #Error check
-        if isinstance(forecast,int) == False and isinstance(forecast,dt) == False:
+
+        # Error check
+        if not isinstance(forecast, int) and not isinstance(forecast, dt):
             msg = "forecast must be of type int or datetime.datetime"
             raise TypeError(msg)
-        
-        #Get list of storm discussions
+
+        # Get list of storm discussions
         disco_dict = self.list_nhc_discussions()
-        
-        if isinstance(forecast,dt):
-            #Find closest discussion to the time provided
-            disco_times = disco_dict['utc_date']
+
+        if isinstance(forecast, dt):
+            # Find all discussion times
+            disco_times = disco_dict['utc_time']
             disco_ids = [int(i) for i in disco_dict['id']]
-            disco_diff = np.array([(i-forecast).days + (i-forecast).seconds/86400 for i in disco_times])
-            closest_idx = np.abs(disco_diff).argmin()
+            disco_diff = np.array(
+                [(i - forecast).days + (i - forecast).seconds / 86400 for i in disco_times])
+            # Find most recent discussion
+            indices = np.argwhere(disco_diff <= 0)
+            if len(indices) > 0:
+                closest_idx = indices[-1][0]
+            else:
+                closest_idx = 0
             closest_diff = disco_diff[closest_idx]
             closest_id = disco_ids[closest_idx]
             closest_time = disco_times[closest_idx]
-        
-            #Raise warning if difference is >=1 day
+
+            # Raise warning if difference is >=1 day
             if np.abs(closest_diff) >= 1.0:
-                warnings.warn(f"The date provided is unavailable or outside of the duration of the storm. Use the \"list_nhc_discussions()\" function to retrieve a list of available NHC discussions for this storm. Returning the closest available NHC discussion.")
-                
-        if isinstance(forecast,int):
-            #Find closest discussion ID to the one provided
-            disco_times = disco_dict['utc_date']
+                warnings.warn("The time provided is unavailable or outside of the duration of the storm. Use the \"list_nhc_discussions()\" function to retrieve a list of available NHC discussions for this storm. Returning the closest available NHC discussion.")
+
+        if isinstance(forecast, int):
+            # Find closest discussion ID to the one provided
+            disco_times = disco_dict['utc_time']
             disco_ids = [int(i) for i in disco_dict['id']]
             if forecast == -1:
                 closest_idx = -1
             else:
-                disco_diff = np.array([i-forecast for i in disco_ids])
+                disco_diff = np.array([i - forecast for i in disco_ids])
                 closest_idx = np.abs(disco_diff).argmin()
                 closest_diff = disco_diff[closest_idx]
-                
-                #Raise warning if difference is >=1 ids
+
+                # Raise warning if difference is >=1 ids
                 if np.abs(closest_diff) >= 2.0:
-                    msg = f"The ID provided is unavailable or outside of the duration of the storm. Use the \"list_nhc_discussions()\" function to retrieve a list of available NHC discussions for this storm. Returning the closest available NHC discussion."
+                    msg = "The ID provided is unavailable or outside of the duration of the storm. Use the \"list_nhc_discussions()\" function to retrieve a list of available NHC discussions for this storm. Returning the closest available NHC discussion."
                     warnings.warn(msg)
 
             closest_id = disco_ids[closest_idx]
             closest_time = disco_times[closest_idx]
-        
-            
-        #Read content of NHC forecast discussion
+
+        # Read content of NHC forecast discussion
         if disco_dict['mode'] == 0:
             url_disco = disco_dict['url'][closest_idx]
-            if requests.get(url_disco).status_code != 200: raise RuntimeError("NHC discussion data is unavailable.")
+            if requests.get(url_disco).status_code != 200:
+                raise RuntimeError("NHC discussion data is unavailable.")
             f = urllib.request.urlopen(url_disco)
             content = f.read()
             content = content.decode("utf-8")
             f.close()
-            
-        elif disco_dict['mode'] in [1,2,3]:
-            #Get directory path of storm and read it in
+
+        elif disco_dict['mode'] in [1, 2, 3]:
+            # Get directory path of storm and read it in
             url_disco = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
             url = url_disco + f'{storm_id.lower()}_msgs.tar.gz'
-            if disco_dict['mode'] in [2,3]: url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
-            if requests.get(url).status_code != 200: raise RuntimeError("NHC discussion data is unavailable.")
+            if disco_dict['mode'] in [2, 3]:
+                url = url_disco + f'{storm_id.lower()}.msgs.tar.gz'
+            if requests.get(url).status_code != 200:
+                raise RuntimeError("NHC discussion data is unavailable.")
             request = urllib.request.Request(url)
             response = urllib.request.urlopen(request)
             file_like_object = BytesIO(response.read())
             tar = tarfile.open(fileobj=file_like_object)
-            
+
             members = tar.getmembers()
             members_names = [i.name for i in members]
             idx = members_names.index(disco_dict['url'][closest_idx])
             f = tar.extractfile(members[idx])
             content = (f.read()).decode()
             f.close()
             tar.close()
             response.close()
-            
+
         elif disco_dict['mode'] in [4]:
-            #Get directory path of storm and read it in
+            # Get directory path of storm and read it in
             url_disco = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/messages/"
             url = url_disco + f'{storm_id.lower()}_msg.zip'
-            if requests.get(url).status_code != 200: raise RuntimeError("NHC discussion data is unavailable.")
+            if requests.get(url).status_code != 200:
+                raise RuntimeError("NHC discussion data is unavailable.")
             request = urllib.request.Request(url)
             response = urllib.request.urlopen(request)
             file_like_object = BytesIO(response.read())
             tar = zipfile.ZipFile(file_like_object)
-            
+
             members = tar.namelist()
             members_names = [i for i in members]
             idx = members_names.index(disco_dict['url'][closest_idx])
             content = (tar.read(members[idx])).decode()
             tar.close()
             response.close()
-        
-        #Save file, if specified
+
+        # Save file, if specified
         if save_path is not None:
             closest_time = disco_times[closest_idx].strftime("%Y%m%d_%H%M")
             fname = f"nhc_disco_{self.name.lower()}_{self.year}_{closest_time}.txt"
-            o = open(save_path+fname,"w")
+            o = open(save_path + fname, "w")
             o.write(content)
             o.close()
-        
-        #Return text of NHC forecast discussion
-        return {'id':closest_id,'time_issued':closest_time,'text':content}
-    
-    
-    def query_nhc_discussions(self,query):
-        
+
+        # Return text of NHC forecast discussion
+        return {'id': closest_id, 'time_issued': closest_time, 'text': content}
+
+    def query_nhc_discussions(self, query):
         r"""
         Searches for the given word or phrase through all NHC forecast discussions for this storm.
-        
+
         Parameters
         ----------
         query : str or list
             String or list representing a word(s) or phrase(s) to search for within the NHC forecast discussions (e.g., "rapid intensification"). Query is case insensitive.
-        
+
         Returns
         -------
         list
             List of dictionaries containing all relevant forecast discussions.
         """
-        
-        #Check to ensure the data source is HURDAT
+
+        # Check to ensure the data source is HURDAT
         if self.source != "hurdat":
             msg = "Error: NHC data can only be accessed when HURDAT is used as the data source."
             raise RuntimeError(msg)
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Get storm ID & corresponding data URL
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Get storm ID & corresponding data URL
         storm_id = self.dict['operational_id']
-        storm_year = self.dict['year']
-        
-        #Error check
+
+        # Error check
         if storm_id == '':
             msg = "No NHC operational data is available for this storm."
             raise RuntimeError(msg)
-        if isinstance(query,str) == False and isinstance(query,list) == False:
+        if not isinstance(query, str) and not isinstance(query, list):
             msg = "'query' must be of type str or list."
             raise TypeError(msg)
-        if isinstance(query,list):
+        if isinstance(query, list):
             for i in query:
-                if isinstance(i,str) == False:
+                if not isinstance(i, str):
                     msg = "Entries of list 'query' must be of type str."
                     raise TypeError(msg)
-        
-        #Get list of storm discussions
+
+        # Get list of storm discussions
         disco_dict = self.list_nhc_discussions()
-        
-        #Iterate over every entry to retrieve its discussion text
+
+        # Iterate over every entry to retrieve its discussion text
         output = []
-        for idx,forecast_date in enumerate(disco_dict['utc_date']):
-            
-            #Get forecast discussion
-            forecast = self.get_nhc_discussion(forecast=forecast_date)
-            
-            #Get forecast text and query for word
+        for idx, forecast_time in enumerate(disco_dict['utc_time']):
+
+            # Get forecast discussion
+            forecast = self.get_nhc_discussion(forecast=forecast_time)
+
+            # Get forecast text and query for word
             text = forecast['text'].lower()
-            
-            #If found, add into list
-            if isinstance(query,str):
-                if text.find(query.lower()) >= 0: output.append(forecast)
+
+            # If found, add into list
+            if isinstance(query, str):
+                if text.find(query.lower()) >= 0:
+                    output.append(forecast)
             else:
                 found = False
                 for i_query in query:
-                    if text.find(i_query.lower()) >= 0: found = True
-                if found: output.append(forecast)
-            
-        #Return list
+                    if text.find(i_query.lower()) >= 0:
+                        found = True
+                if found:
+                    output.append(forecast)
+
+        # Return list
         return output
-    
 
     def get_operational_forecasts(self):
-
         r"""
         Retrieves operational model and NHC forecasts throughout the entire life duration of the storm.
 
         Returns
         -------
         dict
             Dictionary containing all forecast entries.
-        
+
         Notes
         -----
         This function fetches all available forecasts, whether operational (e.g., NHC or JTWC), deterministic or ensemble model forecasts, from the Best Track a-deck as far back as data allows (1954 for NHC's area of responsibility, 2019 for JTWC).
-        
+
         For example, this code retrieves all forecasts for Hurricane Michael (2018):
-        
+
         .. code-block:: python
-    
+
             #Get Storm object
             from tropycal import tracks
             basin = tracks.TrackDataset()
             storm = basin.get_storm(('michael',2018))
-            
+
             #Retrieve all forecasts
             forecasts = storm.get_operational_forecasts()
-        
+
         The resulting dict is structured as follows:
-        
+
         >>> print(forecasts.keys())
         dict_keys(['CARQ', 'NAM', 'AC00', 'AEMN', 'AP01', 'AP02', 'AP03', 'AP04', 'AP05', 'AP06', 'AP07', 'AP08', 'AP09',
         'AP10', 'AP11', 'AP12', 'AP13', 'AP14', 'AP15', 'AP16', 'AP17', 'AP18', 'AP19', 'AP20', 'AVNO', 'AVNX', 'CLP5',
         'CTCX', 'DSHP', 'GFSO', 'HCCA', 'IVCN', 'IVDR', 'LGEM', 'OCD5', 'PRFV', 'SHF5', 'SHIP', 'TABD', 'TABM', 'TABS',
         'TCLP', 'XTRP', 'CMC', 'NGX', 'UKX', 'AEMI', 'AHNI', 'AVNI', 'CEMN', 'CHCI', 'CTCI', 'DSPE', 'EGRR', 'LGME',
         'NAMI', 'NEMN', 'RVCN', 'RVCX', 'SHPE', 'TBDE', 'TBME', 'TBSE', 'TVCA', 'TVCE', 'TVCN', 'TVCX', 'TVDG', 'UE00',
         'UE01', 'UE02', 'UE03', 'UE04', 'UE05', 'UE06', 'UE07', 'UE08', 'UE09', 'UE10', 'UE11', 'UE12', 'UE13', 'UE14',
         'UE15', 'UE16', 'UE17', 'UE18', 'UE19', 'UE20', 'UE21', 'UE22', 'UE23', 'UE24', 'UE25', 'UE26', 'UE27', 'UE28',
         'UE29', 'UE30', 'UE31', 'UE32', 'UE33', 'UE34', 'UE35', 'UEMN', 'CEMI', 'CMCI', 'COTC', 'EGRI', 'HMON', 'HWRF',
         'NGXI', 'NVGM', 'PRV2', 'PRVI', 'UEMI', 'UKXI', 'CEM2', 'CMC2', 'COTI', 'EGR2', 'HHFI', 'HHNI', 'HMNI', 'HWFI',
         'ICON', 'IVRI', 'NGX2', 'NVGI', 'OFCP', 'TCOA', 'TCOE', 'TCON', 'UEM2', 'UKX2', 'CHC2', 'CTC2', 'NAM2', 'OFCL',
         'OFPI', 'AEM2', 'AHN2', 'AVN2', 'DRCL', 'HHF2', 'HHN2', 'HMN2', 'HWF2', 'NVG2', 'OFCI', 'OFP2', 'FSSE', 'RI25', 'RI30'])
-        
+
         Each of these keys represents a forecast model/ensemble member/center. If we select the GFS (AVNO), we now get a dictionary containing all forecast initializations for this model:
-        
+
         >>> print(forecasts['AVNO'].keys())
         dict_keys(['2018100518', '2018100600', '2018100606', '2018100612', '2018100700', '2018100706', '2018100712',
         '2018100718', '2018100800', '2018100806', '2018100812', '2018100818', '2018100900', '2018100906', '2018100912',
         '2018100918', '2018101000', '2018101006', '2018101012', '2018101018', '2018101100', '2018101106', '2018101112',
         '2018101118', '2018101200', '2018101206', '2018101212'])
-        
+
         Providing a forecast initialization, for example 1200 UTC 8 October 2018 (2018100812), we now get a forecast dict containing the GFS initialized at this time:
-        
+
         >>> print(forecasts['AVNO']['2018100812'])
         {'fhr': [0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114, 120, 126, 132, 138, 144],
          'lat': [20.8, 21.7, 22.7, 24.0, 25.1, 25.9, 26.9, 27.9, 29.0, 30.0, 31.1, 32.1, 33.1, 34.1, 35.4, 37.2, 38.9, 40.2,
                  42.4, 44.9, 46.4, 48.2, 49.3, 50.5, 51.3],
          'lon': [-85.1, -85.1, -85.2, -85.6, -86.4, -86.8, -86.9, -86.9, -86.6, -86.2, -85.3, -84.4, -83.0, -81.3, -78.9,
                  -75.9, -72.2, -67.8, -62.5, -56.5, -50.5, -44.4, -38.8, -32.8, -27.9],
          'vmax': [57, 62, 61, 67, 74, 69, 75, 78, 80, 76, 47, 38, 34, 36, 38, 41, 52, 58, 55, 51, 48, 44, 37, 36, 37],
          'mslp': [984, 982, 979, 974, 972, 970, 966, 964, 957, 959, 969, 982, 988, 993, 994, 990, 984, 978, 978, 975, 979,
                   984, 987, 989, 991],
          'type': ['XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX',
                   'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX', 'XX'],
          'init': datetime.datetime(2018, 10, 8, 12, 0)}
         """
-        
-        #Real time ensemble data:
-        #https://www.ftp.ncep.noaa.gov/data/nccf/com/ens_tracker/prod/
-        
-        #If forecasts dict already exist, simply return the dict
+
+        # Real time ensemble data:
+        # https://www.ftp.ncep.noaa.gov/data/nccf/com/ens_tracker/prod/
+
+        # If forecasts dict already exist, simply return the dict
         try:
             self.forecast_dict
             return self.forecast_dict
         except:
             pass
-        
-        #Follow HURDAT procedure
+
+        # Follow HURDAT procedure
         if self.source == "hurdat":
-            
-            #Get storm ID & corresponding data URL
+
+            # Get storm ID & corresponding data URL
             storm_id = self.dict['operational_id']
             storm_year = self.dict['year']
-            if storm_year <= 2006: storm_id = self.dict['id']
+            if storm_year <= 2006:
+                storm_id = self.dict['id']
             if storm_year < 1954:
                 msg = "Forecast data is unavailable for storms prior to 1954."
                 raise RuntimeError(msg)
 
-            #Error check
+            # Error check
             if storm_id == '':
                 msg = "No NHC operational data is available for this storm."
                 raise RuntimeError(msg)
 
-            #Check if archive directory exists for requested year, if not redirect to realtime directory
+            # Check if archive directory exists for requested year, if not redirect to realtime directory
             url_models = f"https://ftp.nhc.noaa.gov/atcf/archive/{storm_year}/a{storm_id.lower()}.dat.gz"
             if requests.get(url_models).status_code != 200:
                 url_models = f"https://ftp.nhc.noaa.gov/atcf/aid_public/a{storm_id.lower()}.dat.gz"
-        
-            #Retrieve model data text
+
+            # Retrieve model data text
             if requests.get(url_models).status_code == 200:
                 request = urllib.request.Request(url_models)
                 response = urllib.request.urlopen(request)
                 sio_buffer = BytesIO(response.read())
-                gzf = gzip.GzipFile(fileobj = sio_buffer)
+                gzf = gzip.GzipFile(fileobj=sio_buffer)
                 data = gzf.read()
                 content = data.splitlines()
                 content = [(i.decode()).split(",") for i in content]
                 content = [i for i in content if len(i) > 10]
                 response.close()
             else:
-                raise RuntimeError("No operational model data is available for this storm.")
-        
-        #Follow JTWC procedure
+                raise RuntimeError(
+                    "No operational model data is available for this storm.")
+
+        # Follow JTWC procedure
         else:
-            
+
             url_models_noaa = f"https://www.ssd.noaa.gov/PS/TROP/DATA/ATCF/JTWC/a{self.id.lower()}.dat"
             url_models_ucar = f"http://hurricanes.ral.ucar.edu/repository/data/adecks_open/{self.year}/a{self.id.lower()}.dat"
-            
-            #Retrieve model data text
+
+            # Retrieve model data text
             try:
-                content = read_url(url_models_noaa,split=True,subsplit=False)
+                content = read_url(url_models_noaa, split=True, subsplit=False)
             except:
                 try:
-                    content = read_url(url_models_ucar,split=True,subsplit=False)
+                    content = read_url(
+                        url_models_ucar, split=True, subsplit=False)
                 except:
-                    raise RuntimeError("No operational model data is available for this storm.")
+                    raise RuntimeError(
+                        "No operational model data is available for this storm.")
             content = [i.split(",") for i in content]
             content = [i for i in content if len(i) > 10]
 
-        #Iterate through every line in content:
+        # Iterate through every line in content:
         forecasts = {}
         for line in content:
 
-            #Get basic components
-            lineArray = [i.replace(" ","") for i in line]
+            # Get basic components
+            lineArray = [i.replace(" ", "") for i in line]
             try:
-                basin,number,run_init,n_a,model,fhr,lat,lon,vmax,mslp,stype,rad,windcode,neq,seq,swq,nwq = lineArray[:17]
+                basin, number, run_init, n_a, model, fhr, lat, lon, vmax, mslp, stype, rad, windcode, neq, seq, swq, nwq = lineArray[
+                    :17]
                 use_wind = True
             except:
-                basin,number,run_init,n_a,model,fhr,lat,lon,vmax,mslp,stype = lineArray[:11]
+                basin, number, run_init, n_a, model, fhr, lat, lon, vmax, mslp, stype = lineArray[
+                    :11]
                 use_wind = False
-            
-            #Check init date is within storm date range
-            run_init_dt = dt.strptime(run_init,'%Y%m%d%H')
-            if run_init_dt < self.dict['date'][0]-timedelta(hours=6) or run_init_dt > self.dict['date'][-1]+timedelta(hours=6): continue
-            
-            #Enter into forecast dict
-            if model not in forecasts.keys(): forecasts[model] = {}
-            if run_init not in forecasts[model].keys(): forecasts[model][run_init] = {
-                'init':run_init_dt,'fhr':[],'lat':[],'lon':[],'vmax':[],'mslp':[],'type':[],'windrad':[]
-            }
 
-            #Format lat & lon
+            # Check init time is within storm time range
+            run_init_dt = dt.strptime(run_init, '%Y%m%d%H')
+            if run_init_dt < self.dict['time'][0] - timedelta(hours=6) or run_init_dt > self.dict['time'][-1] + timedelta(hours=6):
+                continue
+
+            # Enter into forecast dict
+            if model not in forecasts.keys():
+                forecasts[model] = {}
+            if run_init not in forecasts[model].keys():
+                forecasts[model][run_init] = {
+                    'init': run_init_dt, 'fhr': [], 'lat': [], 'lon': [], 'vmax': [], 'mslp': [], 'type': [], 'windrad': []
+                }
+
+            # Format lat & lon
             fhr = int(fhr)
             if "N" in lat:
                 lat_temp = lat.split("N")[0]
-                lat = round(float(lat_temp) * 0.1,1)
+                lat = round(float(lat_temp) * 0.1, 1)
             elif "S" in lat:
                 lat_temp = lat.split("S")[0]
-                lat = round(float(lat_temp) * -0.1,1)
+                lat = round(float(lat_temp) * -0.1, 1)
             if "W" in lon:
                 lon_temp = lon.split("W")[0]
-                lon = round(float(lon_temp) * -0.1,1)
+                lon = round(float(lon_temp) * -0.1, 1)
             elif "E" in lon:
                 lon_temp = lon.split("E")[0]
-                lon = round(float(lon_temp) * 0.1,1)
-            
-            #Format vmax & MSLP
+                lon = round(float(lon_temp) * 0.1, 1)
+
+            # Format vmax & MSLP
             if vmax == '':
                 vmax = np.nan
             else:
                 vmax = int(vmax)
-                if vmax < 10 or vmax > 300: vmax = np.nan
+                if vmax < 10 or vmax > 300:
+                    vmax = np.nan
             if mslp == '':
                 mslp = np.nan
             else:
                 mslp = int(mslp)
-                if mslp < 1: mslp = np.nan
+                if mslp < 1:
+                    mslp = np.nan
 
-            #Format wind radii
+            # Format wind radii
             if use_wind:
                 try:
                     rad = int(rad)
-                    if rad in [0,35]: rad = 34
-                    neq = np.nan if windcode=='' else int(neq)
-                    seq = np.nan if windcode in ['','AAA'] else int(seq)
-                    swq = np.nan if windcode in ['','AAA'] else int(swq)
-                    nwq = np.nan if windcode in ['','AAA'] else int(nwq)
+                    if rad in [0, 35]:
+                        rad = 34
+                    neq = np.nan if windcode == '' else int(neq)
+                    seq = np.nan if windcode in ['', 'AAA'] else int(seq)
+                    swq = np.nan if windcode in ['', 'AAA'] else int(swq)
+                    nwq = np.nan if windcode in ['', 'AAA'] else int(nwq)
                 except:
                     rad = 34
                     neq = np.nan
                     seq = np.nan
                     swq = np.nan
                     nwq = np.nan
             else:
                 rad = 34
                 neq = np.nan
                 seq = np.nan
                 swq = np.nan
                 nwq = np.nan
-            
-            #Add forecast data to dict if forecast hour isn't already there
+
+            # Add forecast data to dict if forecast hour isn't already there
             if fhr not in forecasts[model][run_init]['fhr']:
-                if model in ['OFCL','OFCI'] and fhr > 120:
+                if model in ['OFCL', 'OFCI'] and fhr > 120:
                     pass
                 else:
                     if lat == 0.0 and lon == 0.0:
                         continue
                     forecasts[model][run_init]['fhr'].append(fhr)
                     forecasts[model][run_init]['lat'].append(lat)
                     forecasts[model][run_init]['lon'].append(lon)
                     forecasts[model][run_init]['vmax'].append(vmax)
                     forecasts[model][run_init]['mslp'].append(mslp)
-                    forecasts[model][run_init]['windrad'].append({rad:[neq,seq,swq,nwq]})
-                    
-                    #Get storm type, if it can be determined
-                    if stype in ['','DB'] and vmax != 0 and np.isnan(vmax) == False:
-                        stype = get_storm_type(vmax,False)
+                    forecasts[model][run_init]['windrad'].append(
+                        {rad: [neq, seq, swq, nwq]})
+
+                    # Get storm type, if it can be determined
+                    if stype in ['', 'DB'] and vmax != 0 and not np.isnan(vmax):
+                        stype = get_storm_type(vmax, False)
                     forecasts[model][run_init]['type'].append(stype)
             else:
                 ifhr = forecasts[model][run_init]['fhr'].index(fhr)
-                forecasts[model][run_init]['windrad'][ifhr][rad]=[neq,seq,swq,nwq]
-        
-        #Save dict locally
+                forecasts[model][run_init]['windrad'][ifhr][rad] = [
+                    neq, seq, swq, nwq]
+
+        # Save dict locally
         self.forecast_dict = forecasts
-        
-        #Return dict
+
+        # Return dict
         return forecasts
 
-    
-    def get_nhc_forecast_dict(self,time):
-        
+    def get_nhc_forecast_dict(self, time):
         r"""
         Retreive a dictionary of official NHC forecasts for a valid time.
-        
+
         Parameters
         ----------
         time : datetime.datetime
             Time of requested forecast.
-        
+
         Returns
         -------
         dict
             Dictionary containing forecast data.
-        
+
         Notes
         -----
         This dict can be provided to ``utils.generate_nhc_cone()`` to generate the cone of uncertainty. Below is an example forecast dict for Hurricane Michael (2018):
-        
+
         >>> storm.get_nhc_forecast_dict(dt.datetime(2018,10,8,0))
         {'fhr': [0, 3, 12, 24, 36, 48, 72, 96, 120],
          'lat': [19.8, 20.0, 21.1, 22.7, 24.4, 26.3, 30.4, 34.9, 40.7],
          'lon': [-85.4, -85.4, -85.3, -85.6, -86.0, -86.1, -84.5, -78.4, -64.4],
          'vmax': [50, 50, 60, 65, 75, 85, 75, 55, 55],
          'mslp': [nan, 997, nan, nan, nan, nan, nan, nan, nan],
          'type': ['TS', 'TS', 'TS', 'HU', 'HU', 'HU', 'HU', 'TS', 'TS'],
@@ -2302,99 +2463,101 @@
           {34: [130, 140, 90, 90], 50: [50, 50, 0, 0], 64: [20, 20, 0, 0]},
           {34: [130, 130, 80, 90], 50: [50, 50, 0, 0], 64: [20, 20, 0, 0]},
           {34: [130, 130, 70, 90], 50: [60, 60, 30, 40], 64: [25, 25, 15, 25]},
           {34: [130, 130, 70, 80], 50: [60, 60, 30, 40]},
           {34: [0, 0, 0, 0]},
           {34: [0, 0, 0, 0]}],
          'init': datetime.datetime(2018, 10, 8, 0, 0)}
-        
+
         As of Tropycal v0.5, ``windrad`` represents the forecast sustained wind radii (34, 50 and 64 knots) organized by [NE quadrant,SE quadrant,SW quadrant,NW quadrant] in nautical miles.
         """
-        
-        #Check to ensure the data source is HURDAT
+
+        # Check to ensure the data source is HURDAT
         if self.source != "hurdat":
-            raise RuntimeError("Error: NHC data can only be accessed when HURDAT is used as the data source.")
-        
-        #Check to ensure storm is not an invest
+            raise RuntimeError(
+                "Error: NHC data can only be accessed when HURDAT is used as the data source.")
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Get forecasts dict saved into storm object, if it hasn't been already
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Get forecasts dict saved into storm object, if it hasn't been already
         try:
             self.forecast_dict
         except:
             self.get_operational_forecasts()
 
-        #Get all NHC forecast entries
+        # Get all NHC forecast entries
         nhc_forecasts = self.forecast_dict['OFCL']
 
-        #Get list of all NHC forecast initializations
+        # Get list of all NHC forecast initializations
         nhc_forecast_init = [k for k in nhc_forecasts.keys()]
 
-        #Find closest matching time to the provided forecast time
-        nhc_forecast_init_dt = [dt.strptime(k,'%Y%m%d%H') for k in nhc_forecast_init]
-        time_diff = np.array([(i-time).days + (i-time).seconds/86400 for i in nhc_forecast_init_dt])
+        # Find closest matching time to the provided forecast time
+        nhc_forecast_init_dt = [dt.strptime(
+            k, '%Y%m%d%H') for k in nhc_forecast_init]
+        time_diff = np.array(
+            [(i - time).days + (i - time).seconds / 86400 for i in nhc_forecast_init_dt])
         closest_idx = np.abs(time_diff).argmin()
         forecast_dict = nhc_forecasts[nhc_forecast_init[closest_idx]]
         if np.abs(time_diff[closest_idx]) >= 1.0:
-            warnings.warn(f"The date provided is outside of the duration of the storm. Returning the closest available NHC forecast.")
-        
+            warnings.warn(
+                f"The time provided is outside of the duration of the storm. Returning the closest available NHC forecast.")
+
         return forecast_dict
-    
-    
-    def download_tcr(self,save_path=""):
-        
+
+    def download_tcr(self, save_path=""):
         r"""
         Downloads the NHC offical Tropical Cyclone Report (TCR) for the requested storm to the requested directory. Available only for storms with advisories issued by the National Hurricane Center.
-        
+
         Parameters
         ----------
         save_path : str
             Path of directory to download the TCR into. Default is current working directory.
         """
-        
-        #Check to ensure storm is not an invest
+
+        # Check to ensure storm is not an invest
         if self.invest:
-            raise RuntimeError("Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
-        
-        #Error check
+            raise RuntimeError(
+                "Error: NHC does not issue advisories for invests that have not been designated as Potential Tropical Cyclones.")
+
+        # Error check
         if self.source != "hurdat":
             msg = "NHC data can only be accessed when HURDAT is used as the data source."
             raise RuntimeError(msg)
         if self.year < 1995:
             msg = "Tropical Cyclone Reports are unavailable prior to 1995."
             raise RuntimeError(msg)
-        if isinstance(save_path,str) == False:
+        if not isinstance(save_path, str):
             msg = "'save_path' must be of type str."
             raise TypeError(msg)
-        
-        #Format URL
+
+        # Format URL
         storm_id = self.dict['id'].upper()
         storm_name = self.dict['name'].title()
         url = f"https://www.nhc.noaa.gov/data/tcr/{storm_id}_{storm_name}.pdf"
-        
-        #Check to make sure PDF is available
+
+        # Check to make sure PDF is available
         request = requests.get(url)
         if request.status_code != 200:
             msg = "This tropical cyclone does not have a Tropical Cyclone Report (TCR) available."
             raise RuntimeError(msg)
-        
-        #Retrieve PDF
+
+        # Retrieve PDF
         response = requests.get(url)
-        full_path = os.path.join(save_path,f"TCR_{storm_id}_{storm_name}.pdf")
+        full_path = os.path.join(save_path, f"TCR_{storm_id}_{storm_name}.pdf")
         with open(full_path, 'wb') as f:
             f.write(response.content)
 
-            
-    def plot_tors(self,dist_thresh=1000,Tors=None,domain="dynamic",plotPPH=False,plot_all=False,\
-                  ax=None,cartopy_proj=None,save_path=None,**kwargs):
-                
+    def plot_tors(self, dist_thresh=1000, Tors=None, domain="dynamic", plotPPH=False, plot_all=False,
+                  ax=None, cartopy_proj=None, save_path=None, **kwargs):
         r"""
         Creates a plot of the storm and associated tornado tracks.
-        
+
         Parameters
         ----------
         dist_thresh : int
             Distance threshold (in kilometers) from the tropical cyclone track over which to attribute tornadoes to the TC. Default is 1000 km.
         Tors : pandas.DataFrame
             DataFrame containing tornado data associated with the storm. If None, data is automatically retrieved from TornadoDatabase. A dataframe of tornadoes associated with the TC will then be saved to this instance of storm for future use.
         domain : str
@@ -2410,272 +2573,374 @@
             Whether to plot dots for all observations along the track. If false, dots will be plotted every 6 hours. Default is false.
         ax : axes
             Instance of axes to plot on. If none, one will be generated. Default is none.
         cartopy_proj : ccrs
             Instance of a cartopy projection to use. If none, one will be generated. Default is none.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Other Parameters
         ----------------
         prop : dict
             Customization properties of plot.
         map_prop : dict
             Customization properties of Cartopy map. Please refer to :ref:`options-map-prop` for available options.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
         """
-        
-        #Retrieve kwargs
-        prop = kwargs.pop('prop',{})
-        map_prop = kwargs.pop('map_prop',{})
-        
-        #Set default colormap for TC plots to Wistia
+
+        # Retrieve kwargs
+        prop = kwargs.pop('prop', {})
+        map_prop = kwargs.pop('map_prop', {})
+
+        # Set default colormap for TC plots to Wistia
         try:
             prop['PPHcolors']
         except:
-            prop['PPHcolors']='Wistia'
-        
+            prop['PPHcolors'] = 'Wistia'
+
         if Tors is None:
             try:
                 self.stormTors
             except:
                 warn_message = "Reading in tornado data for this storm. If you seek to analyze tornado data for multiple storms, run \"TrackDataset.assign_storm_tornadoes()\" to avoid this warning in the future."
                 warnings.warn(warn_message)
                 Tors = TornadoDataset()
-                self.stormTors = Tors.get_storm_tornadoes(self,dist_thresh)
-    
-        #Error check if no tornadoes are found
+                self.stormTors = Tors.get_storm_tornadoes(self, dist_thresh)
+
+        # Error check if no tornadoes are found
         if len(self.stormTors) == 0:
             raise RuntimeError("No tornadoes were found with this storm.")
-    
-        #Warning if few tornadoes were found
+
+        # Warning if few tornadoes were found
         if len(self.stormTors) < 5:
             warn_message = f"{len(self.stormTors)} tornadoes were found with this storm. Default domain to east_conus."
             warnings.warn(warn_message)
             domain = 'east_conus'
-    
-        #Create instance of plot object
+
+        # Create instance of plot object
         self.plot_obj_tc = TrackPlot()
         try:
             self.plot_obj_tor = TornadoPlot()
         except:
             from ..tornado.plot import TornadoPlot
             self.plot_obj_tor = TornadoPlot()
-        
-        #Create cartopy projection
+
+        # Create cartopy projection
         if cartopy_proj is None:
             if max(self.dict['lon']) > 150 or min(self.dict['lon']) < -150:
-                self.plot_obj_tor.create_cartopy(proj='PlateCarree',central_longitude=180.0)
-                self.plot_obj_tc.create_cartopy(proj='PlateCarree',central_longitude=180.0)
+                self.plot_obj_tor.create_cartopy(
+                    proj='PlateCarree', central_longitude=180.0)
+                self.plot_obj_tc.create_cartopy(
+                    proj='PlateCarree', central_longitude=180.0)
             else:
-                self.plot_obj_tor.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-                self.plot_obj_tc.create_cartopy(proj='PlateCarree',central_longitude=0.0)
-                
-        #Plot tornadoes
-        plot_ax,leg_tor,domain = self.plot_obj_tor.plot_tornadoes(self.stormTors,domain,ax=ax,return_ax=True,return_domain=True,\
-                                             plotPPH=plotPPH,prop=prop,map_prop=map_prop)
+                self.plot_obj_tor.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
+                self.plot_obj_tc.create_cartopy(
+                    proj='PlateCarree', central_longitude=0.0)
+
+        # Plot tornadoes
+        plot_ax, leg_tor, domain = self.plot_obj_tor.plot_tornadoes(self.stormTors, domain, ax=ax, return_ax=True, return_domain=True,
+                                                                    plotPPH=plotPPH, prop=prop, map_prop=map_prop)
         tor_title = plot_ax.get_title('left')
 
-        #Plot storm
-        plot_ax = self.plot_obj_tc.plot_storms([self.dict],domain=domain,ax=plot_ax,prop=prop,map_prop=map_prop)
-        
+        # Plot storm
+        plot_ax = self.plot_obj_tc.plot_storms(
+            [self.dict], domain=domain, ax=plot_ax, prop=prop, map_prop=map_prop)
+
         plot_ax.add_artist(leg_tor)
-        
+
         storm_title = plot_ax.get_title('left')
-        plot_ax.set_title(f'{storm_title}\n{tor_title}',loc='left',fontsize=17,fontweight='bold')
-        
-        #Save plot
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return axis
+        plot_ax.set_title(f'{storm_title}\n{tor_title}',
+                          loc='left', fontsize=17, fontweight='bold')
+
+        # Save plot
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return axis
         return plot_ax
 
-    def plot_TCtors_rotated(self,dist_thresh=1000,save_path=None):
-        
+    def plot_TCtors_rotated(self, dist_thresh=1000, save_path=None):
         r"""
         Plot tracks of tornadoes relative to the storm motion vector of the tropical cyclone.
-        
+
         Parameters
         ----------
         dist_thresh : int
             Distance threshold (in kilometers) from the tropical cyclone track over which to attribute tornadoes to the TC. Default is 1000 km. Ignored if tornado data was passed into Storm from TrackDataset.
         save_path : str
             Relative or full path of directory to save the image in. If none, image will not be saved.
-        
+
         Returns
         -------
         ax
             Instance of axes containing the plot is returned.
-        
+
         Notes
         -----
         The motion vector is oriented upwards (in the +y direction).
         """
-        
-        #Checks to see if stormTors exists
+
+        # Checks to see if stormTors exists
         try:
             self.stormTors
             dist_thresh = self.tornado_dist_thresh
         except:
             warn_message = "Reading in tornado data for this storm. If you seek to analyze tornado data for multiple storms, run \"TrackDataset.assign_storm_tornadoes()\" to avoid this warning in the future."
             warnings.warn(warn_message)
             Tors = TornadoDataset()
-            stormTors = Tors.get_storm_tornadoes(self,dist_thresh)
-            self.stormTors = Tors.rotateToHeading(self,stormTors)
-        
-        #Create figure for plotting
-        plt.figure(figsize=(9,9),dpi=150)
+            stormTors = Tors.get_storm_tornadoes(self, dist_thresh)
+            self.stormTors = Tors.rotateToHeading(self, stormTors)
+
+        # Create figure for plotting
+        plt.figure(figsize=(9, 9), dpi=150)
         ax = plt.subplot()
-        
-        #Default EF color scale
+
+        # Default EF color scale
         EFcolors = get_colors_ef('default')
-        
-        #Plot all tornado tracks in motion relative coords
-        for _,row in self.stormTors.iterrows():
-            plt.plot([row['rot_xdist_s'],row['rot_xdist_e']+.01],[row['rot_ydist_s'],row['rot_ydist_e']+.01],\
-                     lw=2,c=EFcolors[row['mag']])
-            
-        #Plot dist_thresh radius
+
+        # Plot all tornado tracks in motion relative coords
+        for _, row in self.stormTors.iterrows():
+            plt.plot([row['rot_xdist_s'], row['rot_xdist_e'] + .01], [row['rot_ydist_s'], row['rot_ydist_e'] + .01],
+                     lw=2, c=EFcolors[row['mag']])
+
+        # Plot dist_thresh radius
         ax.set_facecolor('#F6F6F6')
-        circle = plt.Circle((0,0), dist_thresh, color='w')
+        circle = plt.Circle((0, 0), dist_thresh, color='w')
         ax.add_artist(circle)
         an = np.linspace(0, 2 * np.pi, 100)
-        ax.plot(dist_thresh * np.cos(an), dist_thresh * np.sin(an),'k')
-        ax.plot([-dist_thresh,dist_thresh],[0,0],'k--',lw=.5)
-        ax.plot([0,0],[-dist_thresh,dist_thresh],'k--',lw=.5)
-        
-        #Plot motion vector
-        plt.arrow(0, -dist_thresh*.1, 0, dist_thresh*.2, length_includes_head=True,
-          head_width=45, head_length=45,fc='k',lw=2,zorder=100)
-        
-        #Labels
+        ax.plot(dist_thresh * np.cos(an), dist_thresh * np.sin(an), 'k')
+        ax.plot([-dist_thresh, dist_thresh], [0, 0], 'k--', lw=.5)
+        ax.plot([0, 0], [-dist_thresh, dist_thresh], 'k--', lw=.5)
+
+        # Plot motion vector
+        plt.arrow(0, -dist_thresh * .1, 0, dist_thresh * .2, length_includes_head=True,
+                  head_width=45, head_length=45, fc='k', lw=2, zorder=100)
+
+        # Labels
         ax.set_aspect('equal', 'box')
-        ax.set_xlabel('Left/Right of Storm Heading (km)',fontsize=13)
-        ax.set_ylabel('Behind/Ahead of Storm Heading (km)',fontsize=13)
-        ax.set_title(f'{self.name} {self.year} tornadoes relative to heading',fontsize=17)
+        ax.set_xlabel('Left/Right of Storm Heading (km)', fontsize=13)
+        ax.set_ylabel('Behind/Ahead of Storm Heading (km)', fontsize=13)
+        ax.set_title(
+            f'{self.name} {self.year} tornadoes relative to heading', fontsize=17)
         ax.tick_params(axis='both', which='major', labelsize=11.5)
-        
-        #Add legend
-        handles=[]
-        for ef,color in enumerate(EFcolors):
-            count = len(self.stormTors[self.stormTors['mag']==ef])
-            handles.append(mlines.Line2D([], [], linestyle='-',color=color,label=f'EF-{ef} ({count})'))
-        ax.legend(handles=handles,loc='lower left',fontsize=11.5)
-        
-        #Add attribution
-        ax.text(0.99,0.01,plot_credit(),fontsize=8,color='k',alpha=0.7,
-                transform=ax.transAxes,ha='right',va='bottom',zorder=10)
-        
-        #Save plot
-        if save_path is not None and isinstance(save_path,str):
-            plt.savefig(save_path,bbox_inches='tight')
-        
-        #Return axis or show figure
+
+        # Add legend
+        handles = []
+        for ef, color in enumerate(EFcolors):
+            count = len(self.stormTors[self.stormTors['mag'] == ef])
+            handles.append(mlines.Line2D([], [], linestyle='-',
+                           color=color, label=f'EF-{ef} ({count})'))
+        ax.legend(handles=handles, loc='lower left', fontsize=11.5)
+
+        # Add attribution
+        ax.text(0.99, 0.01, plot_credit(), fontsize=8, color='k', alpha=0.7,
+                transform=ax.transAxes, ha='right', va='bottom', zorder=10)
+
+        # Save plot
+        if save_path is not None and isinstance(save_path, str):
+            plt.savefig(save_path, bbox_inches='tight')
+
+        # Return axis or show figure
         return ax
 
-    def get_recon(self,path_vdm=None,path_hdobs=None,path_dropsondes=None):
-        
+    def get_recon(self, path_vdm=None, path_hdobs=None, path_dropsondes=None):
         r"""
         Retrieves all aircraft reconnaissance data for this storm.
-        
+
         Parameters
         ----------
         path_vdm : str, optional
             Filepath of pickle file containing VDM data retrieved from ``vdms.to_pickle()``. If provided, data will be retrieved from the local pickle file instead of the NHC server.
         path_hdobs : str, optional
             Filepath of pickle file containing HDOBs data retrieved from ``hdobs.to_pickle()``. If provided, data will be retrieved from the local pickle file instead of the NHC server.
         path_dropsondes : str, optional
             Filepath of pickle file containing dropsonde data retrieved from ``dropsondes.to_pickle()``. If provided, data will be retrieved from the local pickle file instead of the NHC server.
-        
+
         Returns
         -------
         ReconDataset
             Instance of ReconDataset is returned.
-        
+
         Notes
         -----
         In addition to returning an instance of ``ReconDataset``, this function additionally stores it as an attribute of this Storm object, such that all attributes and methods associated with the ``vdms``, ``hdobs`` and ``dropsondes`` classes can be directly accessed from this Storm object.
-        
+
         One method of accessing the ``hdobs.plot_points()`` method is as follows:
-        
+
         .. code-block:: python
-    
+
             #Get data for Hurricane Michael (2018)
             from tropycal import tracks
             basin = tracks.TrackDataset()
             storm = basin.get_storm(('michael',2018))
-            
+
             #Get all recon data for this storm
             storm.get_recon()
-            
+
             #Plot HDOBs points
             storm.recon.hdobs.plot_points()
-        
+
         The other method is using the returned ReconDataset instance from this function:
-        
+
         .. code-block:: python
-    
+
             #Get data for Hurricane Michael (2018)
             from tropycal import tracks
             basin = tracks.TrackDataset()
             storm = basin.get_storm(('michael',2018))
-            
+
             #Get all recon data for this storm
             recon = storm.get_recon()
-            
+
             #Plot HDOBs points
             recon.hdobs.plot_points()
         """
-        
+
         self.recon.get_vdms(data=path_vdm)
         self.recon.get_hdobs(data=path_hdobs)
         self.recon.get_dropsondes(data=path_dropsondes)
         return self.recon
     
-    def get_archer(self):
+    def search_ships(self):
+        r"""
+        Searches for available SHIPS files for this storm, if available.
+
+        Returns
+        -------
+        list
+            List of available SHIPS times.
+
+        Notes
+        -----
+        SHIPS data is available courtesy of the UCAR Research Applications Laboratory (RAL).
         
+        These available times can be plugged into ``Storm.get_ships()`` to get an object containing SHIPS data initialized at this time.
+        """
+
+        # Error check
+        if self.year <= 2010:
+            raise ValueError('SHIPS data is unavailable prior to 2011.')
+
+        # Format basin name and ID
+        basin_dict = {
+            'north_atlantic':'northatlantic',
+            'east_pacific':'northeastpacific',
+            'west_pacific':'northwestpacific',
+            'north_indian':'northindian'
+        }
+        basin_name = basin_dict.get(self.basin,'southernhemisphere')
+        reformatted_id = f'{self.id[:-4]}{self.id[-2:]}'
+
+        # Format URL from RAL and retrieve file list
+        url = f'http://hurricanes.ral.ucar.edu/realtime/plots/{basin_name}/{self.year}/{self.id.lower()}/stext/'
+        try:
+            page = requests.get(url).text
+        except:
+            raise ValueError('SHIPS data is unavailable for the requested storm.')
+        content = page.split("\n")
+        files = []
+        for line in content:
+            if '<a href="' in line and '_ships.txt' in line:
+                filename = (line.split('<a href="')[1]).split('">')[0]
+
+                # Remove entries outside of duration of storm
+                time = dt.strptime(filename[:8],'%y%m%d%H')
+                if time < self.time[0] or time > self.time[-1]: continue
+
+                # Add entries definitely associated with this storm
+                if reformatted_id == filename.split('_ships')[0][-6:]:
+                    files.append(filename)
+
+        # Organize by date and format for printing
+        return sorted([dt.strptime(i[:8],'%y%m%d%H') for i in files])
+    
+    def get_ships(self, time):
+        r"""
+        Retrieves a Ships object containing SHIPS data for a requested time.
+
+        Parameters
+        ----------
+        time : datetime.datetime
+            Requested time of SHIPS forecast.
+
+        Returns
+        -------
+        tropycal.ships.Ships
+            Instance of a Ships object containing SHIPS data for the requested time.
+
+        Notes
+        -----
+        SHIPS data is available courtesy of the UCAR Research Applications Laboratory (RAL).
+
+        1. A list of available times for SHIPS data can be retrieved using ``Storm.search_ships()``.
+
+        2. On rare occasions, SHIPS data files from UCAR have empty data associated with them. In these cases, a value of None is returned.
+        """
+
+        # Format URL
+        basin_dict = {
+            'north_atlantic':'northatlantic',
+            'east_pacific':'northeastpacific',
+            'west_pacific':'northwestpacific',
+            'north_indian':'northindian'
+        }
+        basin_name = basin_dict.get(self.basin,'southernhemisphere')
+        url = f'http://hurricanes.ral.ucar.edu/realtime/plots/{basin_name}/{self.year}/{self.id.lower()}/stext/'
+        url += f'{time.strftime("%y%m%d%H")}{self.id[:-4]}{self.id[-2:]}_ships.txt'
+
+        # Fetch SHIPS content
+        try:
+            content = read_url(url, split=False, subsplit=False)
+            if len(content) < 10:
+                warnings.warn('Improper SHIPS entry for this time. Returning a value of None.')
+                return None
+        except:
+            raise ValueError('SHIPS data is unavailable for the requested storm or time.')
+
+        return Ships(content, storm_name=self.name, forecast_init=time)
+
+    def get_archer(self):
         r"""
         Retrieves satellite-derived ARCHER track data for this storm, if available.
-        
+
         Returns
         -------
         dict
             Dictionary containing ARCHER data for this storm.
-        
+
         Notes
         -----
         The ARCHER (Automated Rotational Center Hurricane Eye Retrieval) data is provided courtesy of the `University of Wisconsin`_. This data is at a much higher temporal resolution than the Best Track data.
 
         This function additionally saves the ARCHER data as an attribute of this object (storm.archer).
-        
+
         .. _University of Wisconsin: http://tropic.ssec.wisc.edu/real-time/archerOnline/web/index.shtml
         """
-        
-        #Format URL
+
+        # Format URL
         url = f'http://tropic.ssec.wisc.edu/real-time/adt/archive{self.year}/{self.id[2:4]}{self.id[1]}-list.txt'
-        
-        #Read in data
-        a = requests.get(url).content.decode("utf-8") 
+
+        # Read in data
+        a = requests.get(url).content.decode("utf-8")
         content = [[c.strip() for c in b.split()] for b in a.split('\n')]
-        #data = [[dt.strptime(line[0]+'/'+line[1][:4],'%Y%b%d/%H%M'),-1*float(line[-4]),float(line[-5])] for line in content[-100:-3]]
+        # data = [[dt.strptime(line[0]+'/'+line[1][:4],'%Y%b%d/%H%M'),-1*float(line[-4]),float(line[-5])] for line in content[-100:-3]]
         archer = {}
-        for name in ['time','lat','lon','mnCldTmp']:
-            archer[name]=[]
-        for i,line in enumerate(content):
+        for name in ['time', 'lat', 'lon', 'mnCldTmp']:
+            archer[name] = []
+        for i, line in enumerate(content):
             try:
                 ndx = ('MWinit' in line[-1])
-                archer['time'].append(dt.strptime(line[0]+'/'+line[1][:4],'%Y%b%d/%H%M'))
-                archer['lat'].append(float(line[-5-ndx]))
-                archer['lon'].append(-1*float(line[-4-ndx]))
-                archer['mnCldTmp'].append(float(line[-9-ndx]))
+                archer['time'].append(dt.strptime(
+                    line[0] + '/' + line[1][:4], '%Y%b%d/%H%M'))
+                archer['lat'].append(float(line[-5 - ndx]))
+                archer['lon'].append(-1 * float(line[-4 - ndx]))
+                archer['mnCldTmp'].append(float(line[-9 - ndx]))
             except:
                 continue
         self.archer = archer
-        
+
         return archer
-    
-
```

### Comparing `tropycal-0.6.1/src/tropycal/tracks/tools.py` & `tropycal-1.0/src/tropycal/tracks/tools.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,747 +1,824 @@
-import os, sys
-import math
+import os
 import numpy as np
-import pandas as pd
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt, timedelta
 import requests
+import re
 import urllib
 import matplotlib.dates as mdates
-import matplotlib.colors as mcolors
-import matplotlib as mlib
-import warnings
 import scipy.interpolate as interp
 from mpl_toolkits.axes_grid1 import make_axes_locatable
 
 from .. import constants
+from ..utils import create_storm_dict
 
-def find_var(request,thresh):
-    
+def find_latest_hurdat_files():
+    r"""
+    Identifies latest available HURDATv2 files from the NHC web server for the Atlantic and Pacific basins.
+
+    Returns
+    -------
+    list
+        List containing URL strings for the latest available Best Track file for the Atlantic & Pacific basins.
+    """
+
+    # Check if NHC website is up. If not, return HRD HURDATv2 URL
+    url_found = True
+    try:
+        check = urllib.request.urlopen(
+            "https://www.nhc.noaa.gov/data/hurdat/").getcode()
+        if check != 200:
+            url_found = False
+    except:
+        url_found = False
+    if not url_found:
+        atlantic_url = 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2.html'
+        pacific_url = 'https://www.aoml.noaa.gov/hrd/hurdat/hurdat2-nepac.html'
+        return atlantic_url, pacific_url
+
+    # Store data for iteration
+    atlantic_url = ''
+    pacific_url = ''
+    url_data = {
+        'url': {
+            'atl': [],
+            'pac': [],
+        },
+        'date': {
+            'atl': [],
+            'pac': [],
+        }
+    }
+
+    # Iterate over all HURDATv2 files available on NHC's website
+    nhc_directory = 'https://www.nhc.noaa.gov/data/hurdat/'
+    page = requests.get(nhc_directory).text
+    content = page.split("\n")
+    for line in content:
+        if ".txt" in line:
+            fname = (line.split('href="')[1]).split('">')[0]
+
+            # Identify Atlantic vs. Pacific
+            if 'nepac' in fname:
+                file_basin = 'pac'
+                fname_split = '-'.join(fname.split('-')[4:])
+            else:
+                file_basin = 'atl'
+                fname_split = '-'.join(fname.split('-')[3:])
+
+            # Try to identify date file was added
+            try:
+                strdate = fname_split.split(".txt")[0]
+                strdate = re.sub("[^0-9]", "", strdate)
+                if len(strdate) == 6:
+                    strdate = f'{strdate[0:4]}20{strdate[4:]}'
+                date_obj = dt.strptime(strdate, '%m%d%Y')
+
+                if file_basin == 'pac':
+                    url_data['url']['pac'].append(f'{nhc_directory}{fname}')
+                    url_data['date']['pac'].append(date_obj)
+                else:
+                    url_data['url']['atl'].append(f'{nhc_directory}{fname}')
+                    url_data['date']['atl'].append(date_obj)
+            except:
+                continue
+
+    min_date_atl = max(url_data['date']['atl'])
+    atlantic_url = url_data['url']['atl'][url_data['date']
+                                          ['atl'].index(min_date_atl)]
+    min_date_pac = max(url_data['date']['pac'])
+    pacific_url = url_data['url']['pac'][url_data['date']
+                                         ['pac'].index(min_date_pac)]
+
+    return atlantic_url, pacific_url
+
+
+def find_var(request, thresh):
     r"""
     Given a request and threshold, returns the variable for plotting. Referenced from ``TrackDataset.gridded_stats()`` and ``TrackPlot.plot_gridded()``. Internal function.
-    
+
     Parameters
     ----------
     request : str
         Descriptor of the requested plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
     thresh : dict
         Dictionary containing thresholds for the plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
-    
+
     Returns
     -------
     thresh : dict
         Returns the thresh dictionary, modified depending on the request.
     varname : str
         String denoting the variable for plotting.
     """
-    
-    #Convert command to lowercase
+
+    # Convert command to lowercase
     request = request.lower()
-    
-    #Count of number of storms
+
+    # Count of number of storms
     if request.find('count') >= 0 or request.find('num') >= 0:
         return thresh, 'type'
 
     if request.find('date') >= 0 or request.find('day') >= 0:
         return thresh, 'date'
-    
-    #Sustained wind, or change in wind speed
+
+    # Sustained wind, or change in wind speed
     if request.find('wind') >= 0 or request.find('vmax') >= 0:
-        #If change in wind, determine time interval
+        # If change in wind, determine time interval
         if request.find('change') >= 0:
             try:
-                thresh['dt_window'] = int(''.join([c for i,c in enumerate(request) \
-                      if c.isdigit() and i > request.find('hour')-4]))
+                thresh['dt_window'] = int(''.join([c for i, c in enumerate(request)
+                                                   if c.isdigit() and i > request.find('hour') - 4]))
             except:
                 raise RuntimeError("Error: specify time interval (hours)")
-            return thresh,'dvmax_dt'
-        #Otherwise, sustained wind
+            return thresh, 'dvmax_dt'
+        # Otherwise, sustained wind
         else:
-            return thresh,'vmax'
+            return thresh, 'vmax'
 
-    elif request.find('ace')>=0:
-        return thresh,'ace'
-    elif request.find('acie')>=0:
-        return thresh,'acie'
-    
-    #Minimum MSLP, or change in MSLP
+    elif request.find('ace') >= 0:
+        return thresh, 'ace'
+    elif request.find('acie') >= 0:
+        return thresh, 'acie'
+
+    # Minimum MSLP, or change in MSLP
     elif request.find('pressure') >= 0 or request.find('slp') >= 0:
-        #If change in MSLP, determine time interval
+        # If change in MSLP, determine time interval
         if request.find('change') >= 0:
             try:
-                thresh['dt_window'] = int(''.join([c for i,c in enumerate(request) \
-                      if c.isdigit() and i > request.find('hour')-4]))
+                thresh['dt_window'] = int(''.join([c for i, c in enumerate(request)
+                                                   if c.isdigit() and i > request.find('hour') - 4]))
             except:
                 raise RuntimeError("Error: specify time interval (hours)")
-            return thresh,'dmslp_dt'
-        #Otherwise, minimum MSLP
+            return thresh, 'dmslp_dt'
+        # Otherwise, minimum MSLP
         else:
-            return thresh,'mslp'
-    
-    #Storm motion or heading (vector)
+            return thresh, 'mslp'
+
+    # Storm motion or heading (vector)
     elif request.find('heading') >= 0 or request.find('motion') >= 0:
-        return thresh,('dx_dt','dy_dt')
-    
-    elif request.find('movement')>=0 or request.find('speed') >= 0:
-        return thresh,'speed'
-    
-    
-    #Otherwise, error
+        return thresh, ('dx_dt', 'dy_dt')
+
+    elif request.find('movement') >= 0 or request.find('speed') >= 0:
+        return thresh, 'speed'
+
+    # Otherwise, error
     else:
         msg = "Error: Could not decipher variable. Please refer to documentation for examples on how to phrase the \"request\" string."
         raise RuntimeError(msg)
-        
-def find_func(request,thresh):
-    
+
+
+def find_func(request, thresh):
     r"""
     Given a request and threshold, returns the requested function. Referenced from ``TrackDataset.gridded_stats()``. Internal function.
-    
+
     Parameters
     ----------
     request : str
         Descriptor of the requested plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
     thresh : dict
         Dictionary containing thresholds for the plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
-    
+
     Returns
     -------
     thresh : dict
         Returns the thresh dictionary, modified depending on the request.
     func : lambda
         Returns a function to apply to the data.
     """
-    
+
     print(request)
-    
-    #Convert command to lowercase
+
+    # Convert command to lowercase
     request = request.lower()
-    
-    #Numpy maximum function
+
+    # Numpy maximum function
     if request.find('max') == 0 or request.find('latest') == 0:
         return thresh, lambda x: np.nanmax(x)
-    
-    #Numpy minimum function
+
+    # Numpy minimum function
     if request.find('min') == 0 or request.find('earliest') == 0:
         return thresh, lambda x: np.nanmin(x)
-    
-    #Numpy average function
+
+    # Numpy average function
     elif request.find('mean') >= 0 or request.find('average') >= 0 or request.find('avg') >= 0:
-        thresh['sample_min'] = max([5,thresh['sample_min']]) #Ensure sample minimum is at least 5 per gridpoint
+        # Ensure sample minimum is at least 5 per gridpoint
+        thresh['sample_min'] = max([5, thresh['sample_min']])
         return thresh, lambda x: np.nanmean(x)
-    
-    #Numpy percentile function
+
+    # Numpy percentile function
     elif request.find('percentile') >= 0:
-        ptile = int(''.join([c for i,c in enumerate(request) if c.isdigit() and i < request.find('percentile')]))
-        thresh['sample_min'] = max([5,thresh['sample_min']]) #Ensure sample minimum is at least 5 per gridpoint
-        return thresh, lambda x: np.nanpercentile(x,ptile)
-    
-    #Count function
+        ptile = int(''.join([c for i, c in enumerate(
+            request) if c.isdigit() and i < request.find('percentile')]))
+        # Ensure sample minimum is at least 5 per gridpoint
+        thresh['sample_min'] = max([5, thresh['sample_min']])
+        return thresh, lambda x: np.nanpercentile(x, ptile)
+
+    # Count function
     elif request.find('count') >= 0 or request.find('num') >= 0:
         return thresh, lambda x: len(x)
-    
-    #ACE - cumulative function
-    elif request.find('ace') >=0:
+
+    # ACE - cumulative function
+    elif request.find('ace') >= 0:
         return thresh, lambda x: np.nansum(x)
-    elif request.find('acie') >=0:
+    elif request.find('acie') >= 0:
         return thresh, lambda x: np.nansum(x)
-    
-    #Otherwise, function cannot be identified
+
+    # Otherwise, function cannot be identified
     else:
         msg = "Cannot decipher the function. Please refer to documentation for examples on how to phrase the \"request\" string."
         raise RuntimeError(msg)
 
+
 def construct_title(thresh):
-    
     r"""
     Construct a plot title for ``TrackDataset.gridded_stats()``. Internal function.
-    
+
     Parameters
     ----------
     thresh : dict
         Dictionary containing thresholds for the plot. Detailed more in the ``TrackDataset.gridded_dataset()`` function.
-    
+
     Returns
     -------
     thresh : dict
         Returns the thresh dictionary, modified depending on the threshold(s) specified.
     plot_subtitle : str
         String denoting the title for the plot.
     """
-    
-    #List containing entry for plot title, later merged into a string
+
+    # List containing entry for plot title, later merged into a string
     plot_subtitle = []
-    
-    #Symbols for greater/less than or equal to signs
+
+    # Symbols for greater/less than or equal to signs
     gteq = u"\u2265"
     lteq = u"\u2264"
-    
-    #Add sample minimum
+
+    # Add sample minimum
     if not np.isnan(thresh['sample_min']):
         plot_subtitle.append(f"{gteq} {thresh['sample_min']} storms/bin")
     else:
         thresh['sample_min'] = 0
-    
-    #Add minimum wind speed
+
+    # Add minimum wind speed
     if not np.isnan(thresh['v_min']):
         plot_subtitle.append(f"{gteq} {thresh['v_min']}kt")
     else:
         thresh['v_min'] = 0
-    
-    #Add maximum MSLP
+
+    # Add maximum MSLP
     if not np.isnan(thresh['p_max']):
-        plot_subtitle.append(f"{lteq} {thresh['p_max']}hPa")            
+        plot_subtitle.append(f"{lteq} {thresh['p_max']}hPa")
     else:
         thresh['p_max'] = 9999
-    
-    #Add minimum change in wind speed
+
+    # Add minimum change in wind speed
     if not np.isnan(thresh['dv_min']):
-        plot_subtitle.append(f"{gteq} {thresh['dv_min']}kt / {thresh['dt_window']}hr")            
+        plot_subtitle.append(
+            f"{gteq} {thresh['dv_min']}kt / {thresh['dt_window']}hr")
     else:
         thresh['dv_min'] = -9999
-    
-    #Add maximum change in MSLP
+
+    # Add maximum change in MSLP
     if not np.isnan(thresh['dp_max']):
-        plot_subtitle.append(f"{lteq} {thresh['dp_max']}hPa / {thresh['dt_window']}hr")            
+        plot_subtitle.append(
+            f"{lteq} {thresh['dp_max']}hPa / {thresh['dt_window']}hr")
     else:
         thresh['dp_max'] = 9999
-    
-    #Add maximum change in wind speed
+
+    # Add maximum change in wind speed
     if not np.isnan(thresh['dv_max']):
-        plot_subtitle.append(f"{lteq} {thresh['dv_max']}kt / {thresh['dt_window']}hr")            
+        plot_subtitle.append(
+            f"{lteq} {thresh['dv_max']}kt / {thresh['dt_window']}hr")
     else:
         thresh['dv_max'] = 9999
-    
-    #Add minimum change in MSLP
+
+    # Add minimum change in MSLP
     if not np.isnan(thresh['dp_min']):
-        plot_subtitle.append(f"{gteq} {thresh['dp_min']}hPa / {thresh['dt_window']}hr")            
+        plot_subtitle.append(
+            f"{gteq} {thresh['dp_min']}hPa / {thresh['dt_window']}hr")
     else:
         thresh['dp_min'] = -9999
-    
-    #Combine plot_subtitle into string
-    if len(plot_subtitle)>0:
-        plot_subtitle = '\n'+', '.join(plot_subtitle)
+
+    # Combine plot_subtitle into string
+    if len(plot_subtitle) > 0:
+        plot_subtitle = '\n' + ', '.join(plot_subtitle)
     else:
         plot_subtitle = ''
-    
-    #Return modified thresh and plot title
+
+    # Return modified thresh and plot title
     return thresh, plot_subtitle
 
 
-def interp_storm(storm_dict,hours=1,dt_window=24,dt_align='middle',method='linear'):
-    
+def interp_storm(storm_dict, hours=1, dt_window=24, dt_align='middle', method='linear'):
     r"""
     Interpolate a storm dictionary temporally to a specified time resolution. Referenced from ``TrackDataset.filter_storms()``. Internal function.
-    
+
     Parameters
     ----------
     storm_dict : dict
         Dictionary containing a storm entry.
     hours : int
         Temporal resolution in hours to interpolate storm data to. Default is 1 hour.
     dt_window : int
         Time window in hours over which to calculate temporal change data. Default is 24 hours.
     dt_align : str
         Whether to align the temporal change window as "start", "middle" or "end" of the dt_window time period.
     method : str
         Method by which to interpolate lat & lon coordinates. Options are "linear" (default) or "quadratic".
-    
+
     Returns
     -------
     dict
         Dictionary containing the updated storm entry.
     """
-    
-    #Create an empty dict for the new storm entry
+
+    # Create an empty dict for the new storm entry
     new_storm = {}
-    
-    #Copy over non-list attributes
+
+    # Copy over non-list attributes
     for key in storm_dict.keys():
-        if isinstance(storm_dict[key],list) == False:
+        if not isinstance(storm_dict[key], list):
             new_storm[key] = storm_dict[key]
-    
-    #Create an empty list for entries
-    for name in ['date','vmax','mslp','lat','lon','type']:
+
+    # Create an empty list for entries
+    for name in ['time', 'vmax', 'mslp', 'lat', 'lon', 'type']:
         new_storm[name] = []
-    
-    #Convert dates to numbers for ease of calculation
-    times = mdates.date2num(storm_dict['date'])
-    
-    #Convert lat & lons to arrays, and ensure lons are out of 360 degrees
+
+    # Convert times to numbers for ease of calculation
+    times = mdates.date2num(storm_dict['time'])
+
+    # Convert lat & lons to arrays, and ensure lons are out of 360 degrees
     storm_dict['type'] = np.asarray(storm_dict['type'])
     storm_dict['lon'] = np.array(storm_dict['lon']) % 360
-    
-    def round_datetime(tm,nearest_minute=10):
+
+    def round_datetime(tm, nearest_minute=10):
         discard = timedelta(minutes=tm.minute % nearest_minute,
-                             seconds=tm.second,
-                             microseconds=tm.microsecond)
+                            seconds=tm.second,
+                            microseconds=tm.microsecond)
         tm -= discard
-        if discard >= timedelta(minutes=int(nearest_minute/2)):
+        if discard >= timedelta(minutes=int(nearest_minute / 2)):
             tm += timedelta(minutes=nearest_minute)
         return tm
-    
-    #Attempt temporal interpolation
+
+    # Attempt temporal interpolation
     try:
-        
-        #Create a list of target times given the requested temporal resolution
-        targettimes = np.arange(times[0],times[-1]+hours/24.0,hours/24.0)
-        targettimes = targettimes[targettimes<=times[-1]+0.001]
-        
-        #Update dates
-        use_minutes = 10 if hours > (1.0/6.0) else hours * 60.0
-        new_storm['date'] = [round_datetime(t.replace(tzinfo=None),use_minutes) for t in mdates.num2date(targettimes)]
-        targettimes = mdates.date2num(np.array(new_storm['date']))
-        
-        #Create same-length lists for other things
-        new_storm['special'] = ['']*len(new_storm['date'])
-        new_storm['extra_obs'] = [0]*len(new_storm['date'])
-        
-        #WMO basin. Simple linear interpolation.
-        basinnum = np.cumsum([0]+[1 if storm_dict['wmo_basin'][i+1]!=j else 0\
-                    for i,j in enumerate(storm_dict['wmo_basin'][:-1])])
-        basindict = {k:v for k,v in zip(basinnum,storm_dict['wmo_basin'])}
-        basininterp = np.round(np.interp(targettimes,times,basinnum)).astype(int)
+
+        # Create a list of target times given the requested temporal resolution
+        targettimes = np.arange(
+            times[0], times[-1] + hours / 24.0, hours / 24.0)
+        targettimes = targettimes[targettimes <= times[-1] + 0.001]
+
+        # Update times
+        use_minutes = 10 if hours > (1.0 / 6.0) else hours * 60.0
+        new_storm['time'] = [round_datetime(
+            t.replace(tzinfo=None), use_minutes) for t in mdates.num2date(targettimes)]
+        targettimes = mdates.date2num(np.array(new_storm['time']))
+
+        # Create same-length lists for other things
+        new_storm['special'] = [''] * len(new_storm['time'])
+        new_storm['extra_obs'] = [0] * len(new_storm['time'])
+
+        # WMO basin. Simple linear interpolation.
+        basinnum = np.cumsum([0] + [1 if storm_dict['wmo_basin'][i + 1] != j else 0
+                                    for i, j in enumerate(storm_dict['wmo_basin'][:-1])])
+        basindict = {k: v for k, v in zip(basinnum, storm_dict['wmo_basin'])}
+        basininterp = np.round(
+            np.interp(targettimes, times, basinnum)).astype(int)
         new_storm['wmo_basin'] = [basindict[k] for k in basininterp]
-        
-        #Interpolate and fill in storm type
-        stormtype = [1 if i in constants.TROPICAL_STORM_TYPES else -1 for i in storm_dict['type']]
-        isTROP = np.interp(targettimes,times,stormtype)
-        stormtype = [1 if i in constants.SUBTROPICAL_ONLY_STORM_TYPES else -1 for i in storm_dict['type']]
-        isSUB = np.interp(targettimes,times,stormtype)
-        stormtype = [1 if i=='LO' else -1 for i in storm_dict['type']]
-        isLO = np.interp(targettimes,times,stormtype)
-        stormtype = [1 if i=='DB' else -1 for i in storm_dict['type']]
-        isDB = np.interp(targettimes,times,stormtype)
-        newtype = np.where(isTROP>0,'TROP','EX')
-        newtype[newtype=='TROP'] = np.where(isSUB[newtype=='TROP']>0,'SUB','TROP')
-        newtype[newtype=='EX'] = np.where(isLO[newtype=='EX']>0,'LO','EX')
-        newtype[newtype=='EX'] = np.where(isDB[newtype=='EX']>0,'DB','EX')
-        
-        #Interpolate and fill in other variables
-        for name in ['vmax','mslp']:
-            new_storm[name] = np.interp(targettimes,times,storm_dict[name])
-            new_storm[name] = np.array([int(round(i)) if np.isnan(i) == False else np.nan for i in new_storm[name]])
-        for name in ['lat','lon']:
+
+        # Interpolate and fill in storm type
+        stormtype = [1 if i in constants.TROPICAL_STORM_TYPES else -
+                     1 for i in storm_dict['type']]
+        isTROP = np.interp(targettimes, times, stormtype)
+        stormtype = [
+            1 if i in constants.SUBTROPICAL_ONLY_STORM_TYPES else -1 for i in storm_dict['type']]
+        isSUB = np.interp(targettimes, times, stormtype)
+        stormtype = [1 if i == 'LO' else -1 for i in storm_dict['type']]
+        isLO = np.interp(targettimes, times, stormtype)
+        stormtype = [1 if i == 'DB' else -1 for i in storm_dict['type']]
+        isDB = np.interp(targettimes, times, stormtype)
+        newtype = np.where(isTROP > 0, 'TROP', 'EX')
+        newtype[newtype == 'TROP'] = np.where(
+            isSUB[newtype == 'TROP'] > 0, 'SUB', 'TROP')
+        newtype[newtype == 'EX'] = np.where(
+            isLO[newtype == 'EX'] > 0, 'LO', 'EX')
+        newtype[newtype == 'EX'] = np.where(
+            isDB[newtype == 'EX'] > 0, 'DB', 'EX')
+
+        # Interpolate and fill in other variables
+        for name in ['vmax', 'mslp']:
+            new_storm[name] = np.interp(targettimes, times, storm_dict[name])
+            new_storm[name] = np.array([int(round(i)) if not np.isnan(i) else np.nan for i in new_storm[name]])
+        for name in ['lat', 'lon']:
             filtered_array = np.array(storm_dict[name])
-            new_times = np.array(storm_dict['date'])
+            new_times = np.array(storm_dict['time'])
             if 'linear' not in method:
-                converted_hours = np.array([1 if i.strftime('%H%M') in constants.STANDARD_HOURS else 0 for i in storm_dict['date']])
+                converted_hours = np.array([1 if i.strftime(
+                    '%H%M') in constants.STANDARD_HOURS else 0 for i in storm_dict['time']])
                 filtered_array = filtered_array[converted_hours == 1]
                 new_times = new_times[converted_hours == 1]
             new_times = mdates.date2num(new_times)
             if len(filtered_array) >= 3:
-                func = interp.interp1d(new_times,filtered_array,kind=method)
+                func = interp.interp1d(new_times, filtered_array, kind=method)
                 new_storm[name] = func(targettimes)
-                new_storm[name] = np.array([round(i,2) if np.isnan(i) == False else np.nan for i in new_storm[name]])
+                new_storm[name] = np.array([round(i, 2) if not np.isnan(i) else np.nan for i in new_storm[name]])
             else:
-                new_storm[name] = np.interp(targettimes,times,storm_dict[name])
-                new_storm[name] = np.array([int(round(i)) if np.isnan(i) == False else np.nan for i in new_storm[name]])
-        
-        #Correct storm type by intensity
-        newtype[newtype=='TROP'] = [['TD','TS','HU'][int(i>34)+int(i>63)] for i in new_storm['vmax'][newtype=='TROP']]
-        newtype[newtype=='SUB'] = [['SD','SS'][int(i>34)] for i in new_storm['vmax'][newtype=='SUB']]
+                new_storm[name] = np.interp(
+                    targettimes, times, storm_dict[name])
+                new_storm[name] = np.array([int(round(i)) if not np.isnan(i) else np.nan for i in new_storm[name]])
+
+        # Correct storm type by intensity
+        newtype[newtype == 'TROP'] = [['TD', 'TS', 'HU', 'TY', 'ST'][int(
+            i > 34) + int(i > 63)] for i in new_storm['vmax'][newtype == 'TROP']]
+        newtype[newtype == 'SUB'] = [['SD', 'SS']
+                                     [int(i > 34)] for i in new_storm['vmax'][newtype == 'SUB']]
         new_storm['type'] = newtype
-        
-        #Calculate change in wind & MSLP over temporal resolution
-        new_storm['dvmax_dt'] = [np.nan] + list((new_storm['vmax'][1:]-new_storm['vmax'][:-1]) / hours)
-        new_storm['dmslp_dt'] = [np.nan] + list((new_storm['mslp'][1:]-new_storm['mslp'][:-1]) / hours)
-        
-        #Calculate x and y position change over temporal window
-        rE = 6.371e3 * 0.539957 #nautical miles
-        d2r = np.pi/180.
-        new_storm['dx_dt'] = [np.nan]+list(d2r*(new_storm['lon'][1:]-new_storm['lon'][:-1])* \
-                 rE*np.cos(d2r*np.mean([new_storm['lat'][1:],new_storm['lat'][:-1]],axis=0))/hours)
-        new_storm['dy_dt'] = [np.nan]+list(d2r*(new_storm['lat'][1:]-new_storm['lat'][:-1])* \
-                 rE/hours)
-        new_storm['speed'] = [(x**2+y**2)**0.5 for x,y in zip(new_storm['dx_dt'],new_storm['dy_dt'])]
-        
-        #Convert change in wind & MSLP to change over specified window
-        for name in ['dvmax_dt','dmslp_dt']:
-            tmp = np.round(np.convolve(new_storm[name],[1]*int(dt_window/hours),mode='valid'),1)         
-            if dt_align=='end':
-                new_storm[name] = [np.nan]*(len(new_storm[name])-len(tmp))+list(tmp)
-            if dt_align=='middle':
-                tmp2 = [np.nan]*int((len(new_storm[name])-len(tmp))//2)+list(tmp)
-                new_storm[name] = tmp2+[np.nan]*(len(new_storm[name])-len(tmp2))
-            if dt_align=='start':
-                new_storm[name] = list(tmp)+[np.nan]*(len(new_storm[name])-len(tmp))
-            new_storm[name] = list(np.array(new_storm[name])*(hours))
-        
-        #Convert change in position to change over specified window
-        for name in ['dx_dt','dy_dt','speed']:
-            tmp = np.convolve(new_storm[name],[hours/dt_window]*int(dt_window/hours),mode='valid')
-            if dt_align=='end':
-                new_storm[name] = [np.nan]*(len(new_storm[name])-len(tmp))+list(tmp)
-            if dt_align=='middle':
-                tmp2 = [np.nan]*int((len(new_storm[name])-len(tmp))//2)+list(tmp)
-                new_storm[name] = tmp2+[np.nan]*(len(new_storm[name])-len(tmp2))
-            if dt_align=='start':
-                new_storm[name] = list(tmp)+[np.nan]*(len(new_storm[name])-len(tmp))
-        
+
+        # Calculate change in wind & MSLP over temporal resolution
+        new_storm['dvmax_dt'] = [np.nan] + \
+            list((new_storm['vmax'][1:] - new_storm['vmax'][:-1]) / hours)
+        new_storm['dmslp_dt'] = [np.nan] + \
+            list((new_storm['mslp'][1:] - new_storm['mslp'][:-1]) / hours)
+
+        # Calculate x and y position change over temporal window
+        rE = 6.371e3 * 0.539957  # nautical miles
+        d2r = np.pi / 180.
+        new_storm['dx_dt'] = [np.nan] + list(d2r * (new_storm['lon'][1:] - new_storm['lon'][:-1]) *
+                                             rE * np.cos(d2r * np.mean([new_storm['lat'][1:], new_storm['lat'][:-1]], axis=0)) / hours)
+        new_storm['dy_dt'] = [np.nan] + list(d2r * (new_storm['lat'][1:] - new_storm['lat'][:-1]) *
+                                             rE / hours)
+        new_storm['speed'] = [(x**2 + y**2)**0.5 for x,
+                              y in zip(new_storm['dx_dt'], new_storm['dy_dt'])]
+
+        # Convert change in wind & MSLP to change over specified window
+        for name in ['dvmax_dt', 'dmslp_dt']:
+            tmp = np.round(np.convolve(new_storm[name], [
+                           1] * int(dt_window / hours), mode='valid'), 1)
+            if dt_align == 'end':
+                new_storm[name] = [np.nan] * \
+                    (len(new_storm[name]) - len(tmp)) + list(tmp)
+            if dt_align == 'middle':
+                tmp2 = [np.nan] * \
+                    int((len(new_storm[name]) - len(tmp)) // 2) + list(tmp)
+                new_storm[name] = tmp2 + [np.nan] * \
+                    (len(new_storm[name]) - len(tmp2))
+            if dt_align == 'start':
+                new_storm[name] = list(tmp) + [np.nan] * \
+                    (len(new_storm[name]) - len(tmp))
+            new_storm[name] = list(np.array(new_storm[name]) * (hours))
+
+        # Convert change in position to change over specified window
+        for name in ['dx_dt', 'dy_dt', 'speed']:
+            tmp = np.convolve(new_storm[name], [
+                              hours / dt_window] * int(dt_window / hours), mode='valid')
+            if dt_align == 'end':
+                new_storm[name] = [np.nan] * \
+                    (len(new_storm[name]) - len(tmp)) + list(tmp)
+            if dt_align == 'middle':
+                tmp2 = [np.nan] * \
+                    int((len(new_storm[name]) - len(tmp)) // 2) + list(tmp)
+                new_storm[name] = tmp2 + [np.nan] * \
+                    (len(new_storm[name]) - len(tmp2))
+            if dt_align == 'start':
+                new_storm[name] = list(tmp) + [np.nan] * \
+                    (len(new_storm[name]) - len(tmp))
+
         new_storm['dt_window'] = dt_window
         new_storm['dt_align'] = dt_align
-        
-        #Return new dict
+
+        # Return new dict
         return new_storm
-    
-    #Otherwise, simply return NaNs
+
+    # Otherwise, simply return NaNs
     except:
         for name in new_storm.keys():
             try:
                 storm_dict[name]
             except:
-                storm_dict[name]=np.ones(len(new_storm[name]))*np.nan
+                storm_dict[name] = np.ones(len(new_storm[name])) * np.nan
         return storm_dict
 
 
-def filter_storms_vp(trackdata,year_min=0,year_max=9999,subset_domain=None):
-    
+def filter_storms_vp(trackdata, year_min=0, year_max=9999, subset_domain=None):
     r"""
     Calculate a wind-pressure relationship. Referenced from ``TrackDataset.wind_pres_relationship()``. Internal function.
-    
+
     Parameters
     ----------
     trackdata : tropycal.tracks.TrackDataset
         TrackDataset object.
     year_min : int
         Starting year of analysis.
     year_max : int
         Ending year of analysis.
     subset_domain : str
         String representing either a bounded region 'latW/latE/latS/latN', or a basin name.
-    
+
     Returns
     -------
     list
         List representing pressure-wind relationship.
     """
-    
-    #If no subset domain is passed, use global data
+
+    # If no subset domain is passed, use global data
     if subset_domain is None:
-        lon_min,lon_max,lat_min,lat_max = [0,360,-90,90]
+        lon_min, lon_max, lat_min, lat_max = [0, 360, -90, 90]
     else:
-        lon_min,lon_max,lat_min,lat_max = [float(i) for i in subset_domain.split("/")]
-    
-    #Empty list for v-p relationship data
+        lon_min, lon_max, lat_min, lat_max = [
+            float(i) for i in subset_domain.split("/")]
+
+    # Empty list for v-p relationship data
     vp = []
-    
-    #Iterate over every storm in dataset
+
+    # Iterate over every storm in dataset
     for key in trackdata.keys:
-        
-        #Retrieve storm dictionary
+
+        # Retrieve storm dictionary
         istorm = trackdata.data[key]
-        
-        #Iterate over every storm time step
-        for i,(iwind,imslp,itype,ilat,ilon,itime) in \
-        enumerate(zip(istorm['vmax'],istorm['mslp'],istorm['type'],istorm['lat'],istorm['lon'],istorm['date'])):
-            
-            #Ensure both have data and are while the cyclone is tropical
-            if np.nan not in [iwind,imslp] and itype in ['TD','TS','SS','HU','TY'] \
-                   and lat_min<=ilat<=lat_max and lon_min<=ilon%360<=lon_max \
-                   and year_min<=itime.year<=year_max:
-                vp.append([imslp,iwind])
-    
-    #Return v-p relationship list
+
+        # Iterate over every storm time step
+        for i, (iwind, imslp, itype, ilat, ilon, itime) in \
+                enumerate(zip(istorm['vmax'], istorm['mslp'], istorm['type'], istorm['lat'], istorm['lon'], istorm['time'])):
+
+            # Ensure both have data and are while the cyclone is tropical
+            if np.nan not in [iwind, imslp] and itype in constants.TROPICAL_STORM_TYPES \
+                and lat_min <= ilat <= lat_max and lon_min <= ilon % 360 <= lon_max \
+                    and year_min <= itime.year <= year_max:
+                vp.append([imslp, iwind])
+
+    # Return v-p relationship list
     return vp
 
-def testfit(data,x,order):
-    
+
+def testfit(data, x, order):
     r"""
     Calculate a line of best fit for wind-pressure relationship. Referenced from ``TrackDataset.wind_pres_relationship()``. Internal function.
-    
+
     Parameters
     ----------
     data : list
         List of tuples representing wind-pressure relationship. Obtained from ``filter_storms_vp()``.
     x : float
         x value corresponding to the maximum sustained wind.
     order : int
         Function order to pass to ``np.polyfit()``.
-    
+
     Returns
     -------
     float
         y value corresponding to the polyfit function for the given x value.
     """
-    
-    #Make sure there are enough samples
+
+    # Make sure there are enough samples
     if len(data) > 50:
-        f = np.polyfit([i[1] for i in data],[i[0] for i in data],order)
-        y = sum([f[i]*x**(order-i) for i in range(order+1)])
+        f = np.polyfit([i[1] for i in data], [i[0] for i in data], order)
+        y = sum([f[i] * x**(order - i) for i in range(order + 1)])
         return y
     else:
         return np.nan
-    
+
+
 def rolling_window(a, window):
-    
     r"""
     Calculate a rolling window of an array given a window. Referenced from ``TrackDataset.ace_climo()`` and ``TrackDataset.hurricane_days_climo()``. Internal function.
-    
+
     Parameters
     ----------
     a : numpy.ndarray
         1D array containing data.
     window : int
         Window over which to compute a rolling window.
-    
+
     Returns
     -------
     numpy.ndarray
         Array containing the rolling window.
     """
-    
+
     shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
     strides = a.strides + (a.strides[-1],)
     return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)
 
-def convert_to_julian(date):
-    
+
+def convert_to_julian(time):
     r"""
-    Convert a date to Julian days. Referenced from ``TrackDataset.ace_climo()`` and ``TrackDataset.hurricane_days_climo()``. Internal function.
-    
+    Convert a datetime object to Julian days. Referenced from ``TrackDataset.ace_climo()`` and ``TrackDataset.hurricane_days_climo()``. Internal function.
+
     Parameters
     ----------
-    date : datetime.datetime
-        Datetime object of the date to be converted to Julian days.
-    
+    time : datetime.datetime
+        Datetime object of the time to be converted to Julian days.
+
     Returns
     -------
     int
-        Integer representing the Julian day of the requested date.
+        Integer representing the Julian day of the requested time.
     """
-    
-    year = date.year
-    return ((date - dt(year,1,1,0)).days + (date - dt(year,1,1,0)).seconds/86400.0) + 1
+
+    year = time.year
+    return ((time - dt(year, 1, 1, 0)).days + (time - dt(year, 1, 1, 0)).seconds / 86400.0) + 1
+
 
 def months_in_julian(year):
-    
     r"""
     Format months in Julian days for plotting time series. Referenced from ``TrackDataset.ace_climo()`` and ``TrackDataset.hurricane_days_climo()``. Internal function.
-    
+
     Parameters
     ----------
     year : int
         Year for which to determine Julian month days.
-    
+
     Returns
     -------
     dict
         Dictionary containing data for constructing time series.
     """
-    
-    #Get number of days in year
-    length_of_year = convert_to_julian(dt(year,12,31,0))+1.0
-    
-    #Construct a list of months and names
-    months = range(1,13,1)
-    months_names = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
-    months_dates = [dt(year,i,1,0) for i in months]
-    
-    #Get midpoint x-axis location of month
+
+    # Get number of days in year
+    length_of_year = convert_to_julian(dt(year, 12, 31, 0)) + 1.0
+
+    # Construct a list of months and names
+    months = range(1, 13, 1)
+    months_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May',
+                    'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
+    months_dates = [dt(year, i, 1, 0) for i in months]
+
+    # Get midpoint x-axis location of month
     months_julian = [int(convert_to_julian(i)) for i in months_dates]
-    midpoint_julian = (np.array(months_julian) + np.array(months_julian[1:]+[length_of_year]))/2.0
-    return {'start':months_julian,'midpoint':midpoint_julian.tolist(),'name':months_names}
+    midpoint_julian = (np.array(months_julian) +
+                       np.array(months_julian[1:] + [length_of_year])) / 2.0
+    return {'start': months_julian, 'midpoint': midpoint_julian.tolist(), 'name': months_names}
+
 
 def num_to_str2(number):
-    
     r"""
     Convert an integer to a 2-character string. Internal function.
-    
+
     Parameters
     ----------
     number : int
         Integer to be converted to a string.
-    
+
     Returns
     -------
     str
         Two character string.
     """
-    
-    #If number is less than 10, add a leading zero out front
+
+    # If number is less than 10, add a leading zero out front
     if number < 10:
         return f'0{number}'
-    
-    #Otherwise, simply convert to a string
+
+    # Otherwise, simply convert to a string
     return str(number)
 
+
 def plot_credit():
     return "Plot generated using troPYcal"
 
-def add_credit(ax,text):
-    import matplotlib.patheffects as path_effects    
-    a = ax.text(0.99,0.01,text,fontsize=9,color='k',alpha=0.7,fontweight='bold',
-            transform=ax.transAxes,ha='right',va='bottom',zorder=10)
+
+def add_credit(ax, text):
+    import matplotlib.patheffects as path_effects
+    a = ax.text(0.99, 0.01, text, fontsize=9, color='k', alpha=0.7, fontweight='bold',
+                transform=ax.transAxes, ha='right', va='bottom', zorder=10)
     a.set_path_effects([path_effects.Stroke(linewidth=5, foreground='white'),
-                   path_effects.Normal()])
+                        path_effects.Normal()])
+
 
 def pac_2006_cyclone():
-    
     """
     Data for 2006 Central Pacific cyclone obtained from a simple MSLP minimum based tracker applied to the ERA-5 reanalysis dataset. Sustained wind values from the duration of the storm's subtropical and tropical stages were obtained from an estimate from Dr. Karl Hoarau of the Cergy-Pontoise University in Paris:
-    
+
     https://australiasevereweather.com/cyclones/2007/trak0611.htm
     """
     
-    #add empty entry into dict
-    storm_id = 'CP052006'
-    storm_dict = {}
-    
-    storm_dict = {'id':'CP052006','operational_id':'','name':'UNNAMED','season':2006,'year':2006,'basin':'east_pacific'}
+    #Find storm path
+    data_dir = os.path.join(os.path.dirname(__file__), 'data')
+    filepath = os.path.join(data_dir, 'pacific_2006.csv')
+
+    #Create storm dict
+    storm_dict = create_storm_dict(
+        filepath,
+        storm_name = 'UNNAMED',
+        storm_id = 'CP052006',
+    )
+    storm_dict['operational_id'] = ''
     storm_dict['source'] = 'hurdat'
-    storm_dict['source_info'] = 'ERA5 Reanalysis'
+    storm_dict['source_info'] = 'ERA5 Reanalysis & Dr. Karl Hoarau reanalysis'
 
-    #add empty lists
-    for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
-        storm_dict[val] = []
-    storm_dict['ace'] = 0.0
-    
-    #Add obs from reference
-    storm_dict['date'] = ['2006102812', '2006102815', '2006102818', '2006102821', '2006102900', '2006102903', '2006102906', '2006102909', '2006102912', '2006102915', '2006102918', '2006102921', '2006103000', '2006103003', '2006103006', '2006103009', '2006103012', '2006103015', '2006103018', '2006103021', '2006103100', '2006103103', '2006103106', '2006103109', '2006103112', '2006103115', '2006103118', '2006103121', '2006110100', '2006110103', '2006110106', '2006110109', '2006110112', '2006110115', '2006110118', '2006110121', '2006110200', '2006110203', '2006110206', '2006110209', '2006110212', '2006110215', '2006110218', '2006110221', '2006110300', '2006110303', '2006110306', '2006110309', '2006110312', '2006110315', '2006110318']
-    storm_dict['lat'] = [36.0, 37.75, 38.25, 38.5, 39.5, 39.75, 40.0, 40.0, 39.25, 38.5, 37.5, 37.0, 36.75, 36.75, 36.25, 36.0, 36.0, 36.25, 36.75, 37.25, 37.75, 38.5, 38.75, 39.25, 39.75, 40.25, 40.75, 41.25, 42.0, 42.5, 42.75, 42.75, 42.75, 42.75, 42.5, 42.25, 42.25, 42.0, 42.0, 42.25, 42.5, 42.75, 43.0, 43.5, 44.0, 44.5, 45.5, 46.25, 46.75, 47.75, 48.5]
-    storm_dict['lon'] = [-148.25, -147.75, -148.25, -148.25, -148.5, -148.75, -149.75, -150.5, -151.5, -151.75, -151.75, -151.0, -150.25, -150.0, -149.5, -148.5, -147.5, -146.5, -145.5, -144.75, -144.0, -143.5, -143.25, -143.0, -142.75, -142.5, -142.5, -143.0, -143.5, -144.0, -144.75, -145.5, -146.0, -146.25, -146.0, -145.75, -145.25, -144.25, -143.25, -142.25, -140.75, -139.5, -138.0, -136.5, -135.0, -133.5, -132.0, -130.5, -128.5, -126.75, -126.0]
-    storm_dict['mslp'] = [1007, 1003, 999, 995, 992, 989, 990, 990, 991, 991, 992, 993, 993, 992, 994, 994, 994, 994, 995, 995, 993, 993, 994, 993, 993, 993, 993, 993, 990, 989, 989, 989, 988, 988, 989, 989, 988, 989, 990, 991, 991, 991, 993, 994, 993, 994, 995, 996, 996, 996, 997]
-    storm_dict['vmax'] = [30, 40, 50, 45, 45, 45, 40, 40, 40, 40, 35, 35, 35, 35, 35, 35, 30, 30, 35, 35, 35, 40, 45, 45, 40, 40, 45, 45, 45, 45, 50, 50, 55, 55, 50, 50, 50, 50, 50, 50, 45, 40, 35, 35, 30, 30, 25, 25, 30, 30, 25]
-    storm_dict['vmax_era5'] = [31, 38, 47, 46, 47, 45, 38, 42, 43, 40, 37, 36, 35, 33, 35, 35, 33, 31, 33, 32, 31, 29, 28, 30, 28, 28, 29, 29, 30, 32, 31, 29, 28, 26, 25, 26, 28, 27, 28, 29, 28, 27, 27, 27, 28, 26, 27, 27, 31, 30, 26]
-    storm_dict['type'] = ['EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'SS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'TS', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX', 'EX']
-    storm_dict['date'] = [dt.strptime(i,'%Y%m%d%H') for i in storm_dict['date']]
-    
-    #Add other variables
-    storm_dict['extra_obs'] = [0 if i.hour in [0,6,12,18] else 1 for i in storm_dict['date']]
-    storm_dict['special'] = ['' for i in storm_dict['date']]
-    storm_dict['wmo_basin'] = ['east_pacific' for i in storm_dict['date']]
-    
-    #Calculate ACE
-    for i,(vmax,storm_type,idate) in enumerate(zip(storm_dict['vmax'],storm_dict['type'],storm_dict['date'])):
-        ace = (10**-4) * (vmax**2)
-        hhmm = idate.strftime('%H%M')
-        if hhmm in ['0000','0600','1200','1800'] and storm_type in ['SS','TS','HU']:
-            storm_dict['ace'] += ace
-    
-    #Replace original entry with this
+    # Replace original entry with this
     return storm_dict
 
+
 def cyclone_catarina():
-    
     """
     https://journals.ametsoc.org/doi/pdf/10.1175/MWR3330.1
     """
     
-    #add empty entry into dict
-    storm_id = 'AL502004'
-    storm_dict = {}
-    
-    storm_dict = {'id':'AL502004','operational_id':'','name':'CATARINA','season':2004,'year':2004,'basin':'south_atlantic'}
+    #Find storm path
+    data_dir = os.path.join(os.path.dirname(__file__), 'data')
+    filepath = os.path.join(data_dir, 'catarina.csv')
+    
+    #Create storm dict
+    storm_dict = create_storm_dict(
+        filepath,
+        storm_name = 'CATARINA',
+        storm_id = 'AL502004',
+    )
+    storm_dict['operational_id'] = ''
     storm_dict['source'] = 'ibtracs'
     storm_dict['source_info'] = 'McTaggart-Cowan et al. (2006): https://doi.org/10.1175/MWR3330.1'
 
-    #add empty lists
-    for val in ['date','extra_obs','special','type','lat','lon','vmax','mslp','wmo_basin']:
-        storm_dict[val] = []
-    storm_dict['ace'] = 0.0
-    
-    #Add obs from reference
-    storm_dict['date'] = ['200403191800','200403200000','200403200600','200403201200','200403201800','200403210000','200403210600','200403211200','200403211800','200403220000','200403220600','200403221200','200403221800','200403230000','200403230600','200403231200','200403231800','200403240000','200403240600','200403241200','200403241800','200403250000','200403250600','200403251200','200403251800','200403260000','200403260600','200403261200','200403261800','200403270000','200403270600','200403271200','200403271800','200403280000','200403280600','200403281200','200403281800']
-    storm_dict['lat'] = [-27.0,-26.5,-25.3,-25.5,-26.5,-26.8,-27.5,-28.7,-29.5,-30.9,-31.9,-32.3,-31.5,-30.7,-29.8,-29.5,-29.4,-29.3,-29.2,-29.1,-29.1,-29.0,-28.9,-28.7,-28.7,-28.7,-28.7,-28.8,-28.9,-29.1,-29.2,-29.5,-29.5,-29.3,-29.0,-28.5,-28.5]
-    storm_dict['lon'] = [-49.0,-48.5,-48.0,-46.0,-44.5,-43.0,-42.0,-40.5,-39.5,-38.5,-37.0,-36.7,-36.5,-36.7,-37.0,-37.5,-38.1,-38.5,-38.8,-39.0,-39.4,-39.9,-40.4,-41.2,-41.9,-42.6,-43.1,-43.7,-44.2,-44.9,-45.6,-46.4,-47.5,-48.3,-49.7,-50.1,-51.0]
-    storm_dict['mslp'] = [np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,1002,990,991,993,992,990,990,993,993,994,994,989,989,982,975,974,974,972,972,972,np.nan,np.nan,np.nan]
-    storm_dict['vmax'] = [25.0,25.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,30.0,35.0,35.0,35.0,35.0,40.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,70.0,70.0,75.0,75.0,80.0,80.0,85.0,60.0,45.0]
-    storm_dict['type'] = ['EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','EX','SS','SS','SS','SS','SS','TS','TS','TS','TS','HU','HU','HU','HU','HU','HU','HU','HU','HU','TS','TS']
-    storm_dict['date'] = [dt.strptime(i,'%Y%m%d%H%M') for i in storm_dict['date']]
-    
-    #Add other variables
-    storm_dict['extra_obs'] = [0 for i in storm_dict['date']]
-    storm_dict['special'] = ['' for i in storm_dict['date']]
-    storm_dict['wmo_basin'] = ['south_atlantic' for i in storm_dict['date']]
-    
-    #Calculate ACE
-    for i,(vmax,storm_type,idate) in enumerate(zip(storm_dict['vmax'],storm_dict['type'],storm_dict['date'])):
-        ace = (10**-4) * (vmax**2)
-        hhmm = idate.strftime('%H%M')
-        if hhmm in ['0000','0600','1200','1800'] and storm_type in ['SS','TS','HU']:
-            storm_dict['ace'] += ace
-    
-    #Replace original entry with this
+    # Replace original entry with this
     return storm_dict
 
+
 def num_to_text(number):
-    
     r"""
     Retrieve a text representation of a number less than 100. Internal function.
-    
+
     Parameters
     ----------
     number : int
         Integer to be converted to a string.
-    
+
     Returns
     -------
     str
         Text representing the number.
     """
-    
-    #Dictionary mapping numbers to string representations
-    d = { 0 : 'zero', 1 : 'one', 2 : 'two', 3 : 'three', 4 : 'four', 5 : 'five',
-          6 : 'six', 7 : 'seven', 8 : 'eight', 9 : 'nine', 10 : 'ten',
-          11 : 'eleven', 12 : 'twelve', 13 : 'thirteen', 14 : 'fourteen',
-          15 : 'fifteen', 16 : 'sixteen', 17 : 'seventeen', 18 : 'eighteen',
-          19 : 'nineteen', 20 : 'twenty',
-          30 : 'thirty', 40 : 'forty', 50 : 'fifty', 60 : 'sixty',
-          70 : 'seventy', 80 : 'eighty', 90 : 'ninety' }
 
-    #If number is less than 20, return string
+    # Dictionary mapping numbers to string representations
+    d = {0: 'zero', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five',
+         6: 'six', 7: 'seven', 8: 'eight', 9: 'nine', 10: 'ten',
+         11: 'eleven', 12: 'twelve', 13: 'thirteen', 14: 'fourteen',
+         15: 'fifteen', 16: 'sixteen', 17: 'seventeen', 18: 'eighteen',
+         19: 'nineteen', 20: 'twenty',
+         30: 'thirty', 40: 'forty', 50: 'fifty', 60: 'sixty',
+         70: 'seventy', 80: 'eighty', 90: 'ninety'}
+
+    # If number is less than 20, return string
     if number < 20:
         return d[number]
-    
-    #Otherwise, form number from combination of strings
+
+    # Otherwise, form number from combination of strings
     elif number < 100:
         if number % 10 == 0:
             return d[number]
         else:
             return d[number // 10 * 10] + '-' + d[number % 10]
-    
-    #If larger than 100, raise error
+
+    # If larger than 100, raise error
     else:
         msg = "Please choose a number less than 100."
         raise ValueError(msg)
 
+
 def listify(x):
-    if isinstance(x,(tuple,list,np.ndarray)):
+    if isinstance(x, (tuple, list, np.ndarray)):
         return [i for i in x]
     else:
         return [x]
-    
-def make_var_label(x,storm_dict):
+
+
+def make_var_label(x, storm_dict):
     delta = u"\u0394"
     x = list(x)
-    if x[0]=='d' and x[-3:-1]==['_','d']:
-        x[0] = delta; del x[-3:]
+    if x[0] == 'd' and x[-3:-1] == ['_', 'd']:
+        x[0] = delta
+        del x[-3:]
         x.append(f' / {storm_dict["dt_window"]}hr')
     x = ''.join(x)
     if 'mslp' in x:
-        x=x.replace('mslp','mslp (hPa)')
+        x = x.replace('mslp', 'mslp (hPa)')
     if 'vmax' in x:
-        x=x.replace('vmax','vmax (kt)')
+        x = x.replace('vmax', 'vmax (kt)')
     if 'speed' in x:
-        x=x.replace('speed','speed (kt)')    
+        x = x.replace('speed', 'speed (kt)')
     return ''.join(x)
 
-def date_diff(a,b):
-    if isinstance(a,np.datetime64):
-        a=dt.utcfromtimestamp((a - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's'))
-    if isinstance(b,np.datetime64):
-        b=dt.utcfromtimestamp((b - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's'))
-    c = a.replace(year=2000)-b.replace(year=2000)
-    if c<timedelta(0):
+
+def date_diff(a, b):
+    if isinstance(a, np.datetime64):
+        a = dt.utcfromtimestamp(
+            (a - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's'))
+    if isinstance(b, np.datetime64):
+        b = dt.utcfromtimestamp(
+            (b - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's'))
+    c = a.replace(year=2000) - b.replace(year=2000)
+    if c < timedelta(0):
         try:
-            c = a.replace(year=2001)-b.replace(year=2000)
+            c = a.replace(year=2001) - b.replace(year=2000)
         except:
-            c = a.replace(year=2000)-b.replace(year=1999)
+            c = a.replace(year=2000) - b.replace(year=1999)
     return c
 
-def add_colorbar(mappable=None,location='right',size="2.5%",pad='1%',fig=None,ax=None,**kwargs):
+
+def add_colorbar(mappable=None, location='right', size="2.5%", pad='1%', fig=None, ax=None, **kwargs):
     """
     Uses the axes_grid toolkit to add a colorbar to the parent axis and rescale its size to match
     that of the parent axis. This is adapted from Basemap's original ``colorbar()`` method.
 
     Parameters
     ----------
     mappable
@@ -754,41 +831,45 @@
     pad
         Pad of colorbar from axis. Default is 1%.
     ax
         Axes instance to associated the colorbar with. If none provided, or if no
         axis is associated with the instance of Map, then plt.gca() is used.
     """
 
-    #Get current mappable if none is specified
+    # Get current mappable if none is specified
     if fig is None or mappable is None:
         import matplotlib.pyplot as plt
     if fig is None:
         fig = plt.gcf()
 
     if mappable is None:
         mappable = plt.gci()
 
-    #Create axis to insert colorbar in
+    # Create axis to insert colorbar in
     divider = make_axes_locatable(ax)
 
     if location == "left":
         orientation = 'vertical'
-        ax_cb = divider.new_horizontal(size, pad, pack_start=True, axes_class=plt.Axes)
+        ax_cb = divider.new_horizontal(
+            size, pad, pack_start=True, axes_class=plt.Axes)
     elif location == "right":
         orientation = 'vertical'
-        ax_cb = divider.new_horizontal(size, pad, pack_start=False, axes_class=plt.Axes)
+        ax_cb = divider.new_horizontal(
+            size, pad, pack_start=False, axes_class=plt.Axes)
     elif location == "bottom":
         orientation = 'horizontal'
-        ax_cb = divider.new_vertical(size, pad, pack_start=True, axes_class=plt.Axes)
+        ax_cb = divider.new_vertical(
+            size, pad, pack_start=True, axes_class=plt.Axes)
     elif location == "top":
         orientation = 'horizontal'
-        ax_cb = divider.new_vertical(size, pad, pack_start=False, axes_class=plt.Axes)
+        ax_cb = divider.new_vertical(
+            size, pad, pack_start=False, axes_class=plt.Axes)
     else:
         raise ValueError('Improper location entered')
 
-    #Create colorbar
+    # Create colorbar
     fig.add_axes(ax_cb)
     cb = plt.colorbar(mappable, orientation=orientation, cax=ax_cb, **kwargs)
 
-    #Reset parent axis as the current axis
+    # Reset parent axis as the current axis
     fig.sca(ax)
     return cb
```

### Comparing `tropycal-0.6.1/src/tropycal/utils/colors.py` & `tropycal-1.0/src/tropycal/utils/colors.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,92 +1,86 @@
 r"""Utility functions that are used across modules for colors.
 
 Public utility functions should be added to documentation in the '/docs/_templates/overrides/tropycal.utils.rst' file."""
 
-import os, sys
 import numpy as np
-import pandas as pd
-from datetime import datetime as dt,timedelta
-import requests
-import urllib
-import matplotlib.dates as mdates
 import matplotlib.colors as mcolors
 import matplotlib as mlib
 import warnings
 
 from .generic_utils import *
 
-#===========================================================================================================
+# ===========================================================================================================
 # Public utilities
 # These are used internally and have use externally. Add these to documentation.
-#===========================================================================================================
+# ===========================================================================================================
+
 
 def get_colors_sshws(wind_speed):
-    
     r"""
     Retrieve the default colors for the Saffir-Simpson Hurricane Wind Scale (SSHWS).
-    
+
     Parameters
     ----------
     wind_speed : int or list
         Sustained wind speed in knots.
-    
+
     Returns
     -------
     str
         Hex string for the corresponding color.
     """
-    
-    #If category string passed, convert to wind
-    if isinstance(wind_speed,str) == True:
+
+    # If category string passed, convert to wind
+    if isinstance(wind_speed, str):
         wind_speed = category_label_to_wind(wind_speed)
-    
-    #Return default SSHWS category color scale
+
+    # Return default SSHWS category color scale
     if wind_speed < 5:
         return '#FFFFFF'
     elif wind_speed < 34:
-        return '#8FC2F2' #'#7DB7ED'
+        return '#8FC2F2'  # '#7DB7ED'
     elif wind_speed < 64:
         return '#3185D3'
     elif wind_speed < 83:
         return '#FFFF00'
     elif wind_speed < 96:
         return '#FF9E00'
     elif wind_speed < 113:
         return '#DD0000'
     elif wind_speed < 137:
         return '#FF00FC'
     else:
         return '#8B0088'
-    
+
+
 def get_colors_sshws_recon(wind_speed):
-    
     r"""
     Retrieve modified colors for the Saffir-Simpson Hurricane Wind Scale (SSHWS).
-    
+
     Parameters
     ----------
     wind_speed : int or list
         Sustained wind speed in knots.
-    
+
     Returns
     -------
     str
         Hex string for the corresponding color.
-    
+
     Notes
     -----
     These are the same colors as the default SSHWS colors, but include a special 50-64 knot category.
     """
-    
-    #If category string passed, convert to wind
-    if isinstance(wind_speed,str) == True:
+
+    # If category string passed, convert to wind
+    if isinstance(wind_speed, str):
         wind_speed = category_label_to_wind(wind_speed)
-    
-    #Return default SSHWS category color scale
+
+    # Return default SSHWS category color scale
     if wind_speed < 5:
         return '#FFFFFF'
     elif wind_speed < 34:
         return '#8FC2F2'
     elif wind_speed < 50:
         return '#3185D3'
     elif wind_speed < 64:
@@ -98,247 +92,278 @@
     elif wind_speed < 113:
         return '#DD0000'
     elif wind_speed < 137:
         return '#FF00FC'
     else:
         return '#8B0088'
 
-def make_colormap(colors,whiten=0):
-    
-    z  = np.array(sorted(colors.keys()))
-    n  = len(z)
+
+def make_colormap(colors, whiten=0):
+
+    z = np.array(sorted(colors.keys()))
+    n = len(z)
     z1 = min(z)
     zn = max(z)
     x0 = (z - z1) / (zn - z1)
-    
+
     CC = mcolors.ColorConverter()
     R = []
     G = []
     B = []
     for i in range(n):
         Ci = colors[z[i]]
         if type(Ci) == str:
             RGB = CC.to_rgb(Ci)
         else:
             RGB = Ci
         R.append(RGB[0] + (1-RGB[0])*whiten)
         G.append(RGB[1] + (1-RGB[1])*whiten)
         B.append(RGB[2] + (1-RGB[2])*whiten)
-    
+
     cmap_dict = {}
-    cmap_dict['red']   = [(x0[i],R[i],R[i]) for i in range(len(R))]
-    cmap_dict['green'] = [(x0[i],G[i],G[i]) for i in range(len(G))]
-    cmap_dict['blue']  = [(x0[i],B[i],B[i]) for i in range(len(B))]
-    mymap = mcolors.LinearSegmentedColormap('mymap',cmap_dict)
-    
+    cmap_dict['red'] = [(x0[i], R[i], R[i]) for i in range(len(R))]
+    cmap_dict['green'] = [(x0[i], G[i], G[i]) for i in range(len(G))]
+    cmap_dict['blue'] = [(x0[i], B[i], B[i]) for i in range(len(B))]
+    mymap = mcolors.LinearSegmentedColormap('mymap', cmap_dict)
+
     return mymap
 
+
 def get_colors_ef(colormap='default'):
-    
     r"""
     Retrieve a list of colors for the Enhanced Fujita (EF) tornado scale.
-    
+
     Parameters
     ----------
     colormap : str or list
         Matplotlib colormap to use. Default is 'default', which uses Tropycal's default colors for the EF scale. If a list, this list must have 6 colors in order from EF0 to EF5.
-    
+
     Returns
     -------
     list or cmap
         If used a matplotlib colormap, a cmap is returned, otherwise a list of colors is returned.
     """
-    
-    #Matplotlib colormap
-    if isinstance(colormap,str) and colormap != 'default':
+
+    # Matplotlib colormap
+    if isinstance(colormap, str) and colormap != 'default':
         try:
             cmap = mlib.cm.get_cmap(colormap)
             norm = mlib.colors.Normalize(vmin=0, vmax=5)
-            colors = cmap(norm([0,1,2,3,4,5]))
+            colors = cmap(norm([0, 1, 2, 3, 4, 5]))
         except:
-            #colors = [colormap]*6
+            # colors = [colormap]*6
             raise ValueError('Colormap not found.')
-    
-    #User-passed list of colors
-    elif isinstance(colormap,list):
+
+    # User-passed list of colors
+    elif isinstance(colormap, list):
         if len(colormap) == 6:
             colors = colormap
         else:
-            raise ValueError('Must pass a list of 6 colors to correspond from EF0 to EF5.')
-    
-    #Otherwise, return default colors
+            raise ValueError(
+                'Must pass a list of 6 colors to correspond from EF0 to EF5.')
+
+    # Otherwise, return default colors
     else:
-        colors = ['lightsalmon','tomato','red','firebrick','darkred','purple']
-    
-    #Return list of colors, or colormap
+        colors = ['lightsalmon', 'tomato', 'red',
+                  'firebrick', 'darkred', 'purple']
+
+    # Return list of colors, or colormap
     return colors
 
-def get_colors_pph(plot_type,colormap,levels=None):
-    
+
+def get_colors_pph(plot_type, colormap, levels=None):
     r"""
     Retrieve a list of colors for Practically Perfect Hindcast (PPH) for tornadoes.
-    
+
     Parameters
     ----------
     plot_type : str
         Plot type for PPH. Can be "daily" for single-day PPH, or "total" for multi-day PPH.
     colormap : str or list
         Matplotlib colormap to use. Default is 'spc', which uses Tropycal's default colors for the EF scale. If a list, this list must have 6 colors in order from EF0 to EF5.
     levels : list
         List of contour levels. Default is SPC intervals.
-    
+
     Returns
     -------
     colors : list or cmap
         If used a matplotlib colormap, a cmap is returned, otherwise a list of colors is returned.
     levels : list
         List of contour levels.
     """
-    
-    #Insert default levels if none specified
-    if levels is None: levels = [2,5,10,15,30,45,60,100]
-    
-    #Default SPC colormap
+
+    # Insert default levels if none specified
+    if levels is None:
+        levels = [2, 5, 10, 15, 30, 45, 60, 100]
+
+    # Default SPC colormap
     if colormap == 'spc':
-        
-        #Daily plot type
+
+        # Daily plot type
         if plot_type == 'daily':
-            levels = [2,5,10,15,30,45,60,100]
-            colors = ['#008B00','#8B4726','#FFC800','#FF0000','#FF00FF','#912CEE','#104E8B']
-        
-        #Multi-day plot type
+            levels = [2, 5, 10, 15, 30, 45, 60, 100]
+            colors = ['#008B00', '#8B4726', '#FFC800',
+                      '#FF0000', '#FF00FF', '#912CEE', '#104E8B']
+
+        # Multi-day plot type
         else:
-            warnings.warn('SPC colors only allowed for daily PPH.\n'+\
+            warnings.warn('SPC colors only allowed for daily PPH.\n' +
                           'Defaulting to plasma colormap.')
             colormap = 'plasma'
-    
-    #User-defined colormap
+
+    # User-defined colormap
     if colormap != 'spc':
-        
-        #Matplotlib colormap
-        if isinstance(colormap,str):
+
+        # Matplotlib colormap
+        if isinstance(colormap, str):
             cmap = mlib.cm.get_cmap(colormap)
             norm = mlib.colors.Normalize(vmin=0, vmax=len(levels)-2)
             colors = cmap(norm(np.arange(len(levels))))
-        
-        #User defined list of colors
-        elif isinstance(colormap,list):
+
+        # User defined list of colors
+        elif isinstance(colormap, list):
             colors = colormap
-        
-        #If a cmap is passed
+
+        # If a cmap is passed
         else:
             norm = mlib.colors.Normalize(vmin=0, vmax=len(levels)-2)
             colors = colormap(norm(np.arange(len(levels))))
-    
-    #Return colormap and values
+
+    # Return colormap and values
     return colors, levels
 
-#===========================================================================================================
+# ===========================================================================================================
 # Private utilities
 # These are primarily intended to be used internally. Do not add these to documentation.
-#===========================================================================================================
+# ===========================================================================================================
+
 
-def get_cmap_levels(varname,colormap,levels,linear=False):
-    
+def rgb_tuple_to_str(rgb_tuple):
+    r"""
+    Convert an RGB tuple to hex string.
+
+    Parameters
+    ----------
+    rgb_tuple : tuple
+        Tuple ordered in (r,g,b) containing integers from 0 to 255.
+
+    Returns
+    -------
+    str
+        Hex string for RGB value.
+    """
+
+    r, g, b = rgb_tuple
+    r = int(r)
+    g = int(g)
+    b = int(b)
+    return '#%02x%02x%02x' % (r, g, b)
+
+
+def get_cmap_levels(varname, colormap, levels, linear=False):
     r"""
     Retrieve a list of colors for Practically Perfect Hindcast (PPH) for tornadoes.
-    
+
     Parameters
     ----------
     varname : str
         if 'vmax', 'sfmr', or 'fl_to_sfc', then SSHWS category colors can be used.
     colormap : str or list
         Matplotlib colormap to use. Default is 'category', which uses Tropycal's default colors for the SSHWS scale. If a list, a colormap is generated from this list.
     levels : list
         List of contour levels. Default is SPC intervals.
     linear : bool
         If colormap is 'category', determine whether to generate a colorbar using linear category increments (True) or wind increments (False).
-    
+
     Returns
     -------
     colors : cmap
         Matplotlib colormap object.
     levels : list
         List of contour levels.
     """
-    
-    #Default SSHWS colormap
-    if colormap in ['category','category_recon']:
-        
-        #Ensure variable contains some element of surface wind
-        if varname in ['vmax','sfmr','wspd','fl_to_sfc']:
-            
-            #Generate contour levels
-            levels = [category_to_wind(c) for c in range(-1,6)]+[200]
-            
-            #Linear category increments
-            if linear == True:
-                colors = [mcolors.to_rgba(get_colors_sshws(lev)) \
-                               for c,lev in enumerate(levels[:-1]) for _ in range(levels[c+1]-levels[c])]
-            
-            #Linear wind increments
+
+    # Default SSHWS colormap
+    if colormap in ['category', 'category_recon']:
+
+        # Ensure variable contains some element of surface wind
+        if varname in ['vmax', 'sfmr', 'wspd', 'fl_to_sfc']:
+
+            # Generate contour levels
+            levels = [category_to_wind(c) for c in range(-1, 6)]+[200]
+
+            # Linear category increments
+            if linear:
+                colors = [mcolors.to_rgba(get_colors_sshws(lev))
+                          for c, lev in enumerate(levels[:-1]) for _ in range(levels[c+1]-levels[c])]
+
+            # Linear wind increments
             else:
                 if colormap == 'category':
-                    levels = [category_to_wind(c) for c in range(-1,6)]+[200]
+                    levels = [category_to_wind(c) for c in range(-1, 6)]+[200]
                     colors = [get_colors_sshws(lev) for lev in levels[:-1]]
                 else:
-                    levels = [category_to_wind(c) for c in range(-1,1)]+[50]+[category_to_wind(c) for c in range(1,6)]+[200]
-                    colors = [get_colors_sshws_recon(lev) for lev in levels[:-1]]
+                    levels = [category_to_wind(
+                        c) for c in range(-1, 1)]+[50]+[category_to_wind(c) for c in range(1, 6)]+[200]
+                    colors = [get_colors_sshws_recon(
+                        lev) for lev in levels[:-1]]
                 cmap = mcolors.ListedColormap(colors)
-        
-        #Otherwise, default to plasma colormap
+
+        # Otherwise, default to plasma colormap
         else:
-            warnings.warn('Saffir Simpson category colors are not allowed for this variable.')
+            warnings.warn(
+                'Saffir Simpson category colors are not allowed for this variable.')
             colormap = 'plasma'
-    
-    #Other colormap options
-    if colormap not in ['category','category_recon']:
-        
-        #Matplotlib colormap name
-        if isinstance(colormap,str):
+
+    # Other colormap options
+    if colormap not in ['category', 'category_recon']:
+
+        # Matplotlib colormap name
+        if isinstance(colormap, str):
             cmap = mlib.cm.get_cmap(colormap)
-        
-        #User defined list of colors
-        elif isinstance(colormap,list):
+
+        # User defined list of colors
+        elif isinstance(colormap, list):
             cmap = mcolors.ListedColormap(colormap)
-        
-        #Dictionary
-        elif isinstance(colormap,dict):
+
+        # Dictionary
+        elif isinstance(colormap, dict):
             cmap = make_colormap(colormap)
-        
-        #ListedColormap
-        elif isinstance(colormap,mlib.colors.ListedColormap):
+
+        # ListedColormap
+        elif isinstance(colormap, mlib.colors.ListedColormap):
             cmap = colormap
-        
-        #Default to plasma
+
+        # Default to plasma
         else:
             cmap = mlib.cm.get_cmap('plasma')
-        
-        #Normalize colors relative to levels
+
+        # Normalize colors relative to levels
         norm = mlib.colors.Normalize(vmin=0, vmax=len(levels)-1)
-        
-        #If more than 2 levels were passed, use those for the contour levels
+
+        # If more than 2 levels were passed, use those for the contour levels
         if len(levels) > 2:
             colors = cmap(norm(np.arange(len(levels)-1)))
             cmap = mcolors.ListedColormap(colors)
-        
-        #Otherwise, create a list of colors based on levels
+
+        # Otherwise, create a list of colors based on levels
         else:
-            colors = cmap(norm(np.linspace(0,1,256)))
-            cmap = mcolors.LinearSegmentedColormap.from_list('my_colormap',colors)
-            
+            colors = cmap(norm(np.linspace(0, 1, 256)))
+            cmap = mcolors.LinearSegmentedColormap.from_list(
+                'my_colormap', colors)
+
             y0 = min(levels)
             y1 = max(levels)
             dy = (y1-y0)/8
             scalemag = int(np.log(dy)/np.log(10))
             dy_scaled = dy*10**-scalemag
-            dc = min([1,2,5,10], key=lambda x:abs(x-dy_scaled))
+            dc = min([1, 2, 5, 10], key=lambda x: abs(x-dy_scaled))
             dc = dc*10**scalemag
             c0 = np.ceil(y0/dc)*dc
             c1 = np.floor(y1/dc)*dc
-            levels = np.arange(c0,c1+dc,dc)
-    
+            levels = np.arange(c0, c1+dc, dc)
+
             if scalemag > 0:
                 levels = levels.astype(int)
-    
-    #Return colormap and levels
+
+    # Return colormap and levels
     return cmap, levels
```

### Comparing `tropycal-0.6.1/src/tropycal/utils/generic_utils.py` & `tropycal-1.0/src/tropycal/utils/generic_utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,222 +1,217 @@
 r"""Utility functions that are used across modules.
 
 Ensure these do not reference any function from colors.py. If a function does need to reference another function from colors.py,
 add it in that file.
 
 Public utility functions should be added to documentation in the '/docs/_templates/overrides/tropycal.utils.rst' file."""
 
-import os, sys
+import shapely.geometry as sgeom
 import math
 import numpy as np
-import pandas as pd
-from datetime import datetime as dt,timedelta
+from datetime import datetime as dt
 import requests
 import urllib
-import matplotlib.dates as mdates
-import matplotlib.colors as mcolors
-import matplotlib as mlib
-from matplotlib import path
 import warnings
 import scipy.interpolate as interp
 import re
 import shapefile
 import zipfile
-from io import StringIO, BytesIO
+from io import BytesIO
 
 from .. import constants
 
-#===========================================================================================================
+# ===========================================================================================================
 # Public utilities
 # These are used internally and have use externally. Add these to documentation.
-#===========================================================================================================
+# ===========================================================================================================
+
 
 def wind_to_category(wind_speed):
-    
     r"""
     Convert sustained wind speed in knots to Saffir-Simpson Hurricane Wind Scale category.
-    
+
     Parameters
     ----------
     wind_speed : int
         Sustained wind speed in knots.
-    
+
     Returns
     -------
     int
         Category corresponding to the sustained wind. 0 is tropical storm, -1 is tropical depression.
     """
-    
-    #Category 5 hurricane
+
+    # Category 5 hurricane
     if wind_speed >= 137:
         return 5
-    
-    #Category 4 hurricane
+
+    # Category 4 hurricane
     elif wind_speed >= 113:
         return 4
-    
-    #Category 3 hurricane
+
+    # Category 3 hurricane
     elif wind_speed >= 96:
         return 3
-    
-    #Category 2 hurricane
+
+    # Category 2 hurricane
     elif wind_speed >= 83:
         return 2
-    
-    #Category 1 hurricane
+
+    # Category 1 hurricane
     elif wind_speed >= 64:
         return 1
-    
-    #Tropical storm
+
+    # Tropical storm
     elif wind_speed >= 34:
         return 0
-    
-    #Tropical depression
+
+    # Tropical depression
     else:
         return -1
 
+
 def category_to_wind(category):
-    
     r"""
     Convert Saffir-Simpson Hurricane Wind Scale category to minimum threshold sustained wind speed in knots.
-    
+
     Parameters
     ----------
     category : int
         Saffir-Simpson Hurricane Wind Scale category. Use 0 for tropical storm, -1 for tropical depression.
-    
+
     Returns
     -------
     int
         Sustained wind speed in knots corresponding to the minimum threshold of the requested category.
     """
-    
-    #Construct dictionary of thresholds
-    conversion = {-1:5,
-                  0:34,
-                  1:64,
-                  2:83,
-                  3:96,
-                  4:113,
-                  5:137}
-    
-    #Return category
-    return conversion.get(category,np.nan)
+
+    # Construct dictionary of thresholds
+    conversion = {-1: 5,
+                  0: 34,
+                  1: 64,
+                  2: 83,
+                  3: 96,
+                  4: 113,
+                  5: 137}
+
+    # Return category
+    return conversion.get(category, np.nan)
+
 
 def classify_subtropical(storm_type):
-    
     r"""
-    Determine whether a tropical cyclone was purely subtropical.
-    
+    Check whether a tropical cyclone was purely subtropical.
+
     Parameters
     ----------
     storm_type : list or numpy.ndarray
         List or array containing storm types.
-    
+
     Returns
     -------
     bool
         Boolean identifying whether the tropical cyclone was purely subtropical.
     """
-    
-    #Ensure storm_type is a numpy array
+
+    # Ensure storm_type is a numpy array
     storm_type_check = np.array(storm_type)
-    
-    #Ensure all storm types are uppercase
+
+    # Ensure all storm types are uppercase
     storm_track_check = [i.upper() for i in storm_type_check]
-    
-    #Check for subtropical depression status
+
+    # Check for subtropical depression status
     if 'SD' in storm_type_check:
-        if 'SD' in storm_type_check and True not in np.isin(storm_type_check,['TD','TS','HU']):
+        if 'SD' in storm_type_check and True not in np.isin(storm_track_check, ['TD', 'TS', 'HU']):
             return True
-    
-    #Check for subtropical storm status
-    if 'SS' in storm_type_check and True not in np.isin(storm_type_check,['TD','TS','HU']):
+
+    # Check for subtropical storm status
+    if 'SS' in storm_type_check and True not in np.isin(storm_track_check, ['TD', 'TS', 'HU']):
         return True
-    
-    #Otherwise, it was a tropical cyclone at some point in its life cycle
+
+    # Otherwise, it was a tropical cyclone at some point in its life cycle
     else:
         return False
 
-def get_storm_classification(wind_speed,subtropical_flag,basin):
-    
+
+def get_storm_classification(wind_speed, subtropical_flag, basin):
     r"""
     Retrieve the tropical cyclone classification given its subtropical status and current basin.
-    
+
     These strings take the format of "Tropical Storm", "Hurricane", "Typhoon", etc.
-    
+
     Parameters
     ----------
     wind_speed : int
         Integer denoting sustained wind speed in knots.
     subtropical_flag : bool
         Boolean denoting whether the cyclone is subtropical or not.
     basin : str
         String denoting basin in which the tropical cyclone is located.
-    
+
     Returns
     -------
     str
         String denoting the classification of the tropical cyclone.
-    
+
     Notes
     -----
-    
+
     .. warning::
 
         This function currently does not differentiate between 1-minute, 3-minute and 10-minute sustained wind speeds.
-    
+
     """
-    
-    #North Atlantic and East Pacific basins
-    if basin in ['north_atlantic','east_pacific']:
+
+    # North Atlantic and East Pacific basins
+    if basin in constants.NHC_BASINS:
         if wind_speed == 0:
             return "Unknown"
         elif wind_speed < 34:
-            if subtropical_flag == True:
+            if subtropical_flag:
                 return "Subtropical Depression"
             else:
                 return "Tropical Depression"
         elif wind_speed < 63:
-            if subtropical_flag == True:
+            if subtropical_flag:
                 return "Subtropical Storm"
             else:
                 return "Tropical Storm"
         else:
             return "Hurricane"
-    
-    #West Pacific basin
+
+    # West Pacific basin
     elif basin == 'west_pacific':
         if wind_speed == 0:
             return "Unknown"
         elif wind_speed < 34:
-            if subtropical_flag == True:
+            if subtropical_flag:
                 return "Subtropical Depression"
             else:
                 return "Tropical Depression"
         elif wind_speed < 63:
-            if subtropical_flag == True:
+            if subtropical_flag:
                 return "Subtropical Storm"
             else:
                 return "Tropical Storm"
         elif wind_speed < 130:
             return "Typhoon"
         else:
             return "Super Typhoon"
-    
-    #Australia and South Pacific basins
+
+    # Australia and South Pacific basins
     elif basin == 'australia' or basin == 'south_pacific':
         if wind_speed == 0:
             return "Unknown"
         elif wind_speed < 63:
             return "Tropical Cyclone"
         else:
             return "Severe Tropical Cyclone"
-    
-    #North Indian Ocean
+
+    # North Indian Ocean
     elif basin == 'north_indian':
         if wind_speed == 0:
             return "Unknown"
         elif wind_speed < 28:
             return "Depression"
         elif wind_speed < 34:
             return "Deep Depression"
@@ -226,16 +221,16 @@
             return "Severe Cyclonic Storm"
         elif wind_speed < 90:
             return "Very Severe Cyclonic Storm"
         elif wind_speed < 120:
             return "Extremely Severe Cyclonic Storm"
         else:
             return "Super Cyclonic Storm"
-    
-    #South Indian Ocean
+
+    # South Indian Ocean
     elif basin == 'south_indian':
         if wind_speed == 0:
             return "Unknown"
         elif wind_speed < 28:
             return "Tropical Disturbance"
         elif wind_speed < 34:
             return "Tropical Depression"
@@ -245,475 +240,518 @@
             return "Severe Tropical Storm"
         elif wind_speed < 90:
             return "Tropical Cyclone"
         elif wind_speed < 115:
             return "Intense Tropical Cyclone"
         else:
             return "Very Intense Tropical Cyclone"
-    
-    #Otherwise, return a generic "Cyclone" classification
+
+    # Otherwise, return a generic "Cyclone" classification
     else:
         return "Cyclone"
 
-def get_storm_type(wind_speed,subtropical_flag):
-    
+
+def get_storm_type(wind_speed, subtropical_flag, typhoon=False):
     r"""
     Retrieve the 2-character tropical cyclone type (e.g., "TD", "TS", "HU") given its subtropical status.
-    
+
     Parameters
     ----------
     wind_speed : int
         Integer denoting sustained wind speed in knots.
     subtropical_flag : bool
         Boolean denoting whether the cyclone is subtropical or not.
-    
+    typhoon : bool, optional
+        Boolean denoting whether typhoon (True) or hurricane (False) classification should be used for wind speeds at or above 64 kt. Default is False.
+
     Returns
     -------
     str
         String denoting the tropical cyclone type.
-    
+
     Notes
     -----
     The available types and their descriptions are as follows:
-    
+
     .. list-table:: 
        :widths: 25 75
        :header-rows: 1
 
        * - Property
          - Description
+       * - SD
+         - Subtropical Depression
+       * - SS
+         - Subtropical Storm
        * - TD
          - Tropical Depression
        * - TS
          - Tropical Storm
        * - HU
          - Hurricane
-       * - SD
-         - Subtropical Depression
-       * - SS
-         - Subtropical Storm
+       * - TY
+         - Typhoon
+       * - ST
+         - Super Typhoon
     """
-    
-    #Tropical depression
+
+    # Tropical depression
     if wind_speed < 34:
-        if subtropical_flag == True:
+        if subtropical_flag:
             return "SD"
         else:
             return "TD"
-    
-    #Tropical storm
+
+    # Tropical storm
     elif wind_speed < 63:
-        if subtropical_flag == True:
+        if subtropical_flag:
             return "SS"
         else:
             return "TS"
-    
-    #Hurricane
-    else:
+
+    # Hurricane
+    elif not typhoon:
         return "HU"
 
-def get_basin(lat,lon,source_basin=""):
-    
+    # Typhoon
+    elif wind_speed < 130:
+        return "TY"
+
+    # Super Typhoon
+    else:
+        return "ST"
+
+
+def get_basin(lat, lon, source_basin=""):
     r"""
     Returns the current basin of the tropical cyclone.
-    
+
     Parameters
     ----------
     lat : int or float
         Latitude of the storm.
     lon : int or float
         Longitude of the storm.
-    
+
     Other Parameters
     ----------------
     source_basin : str, optional
         String representing the origin storm basin (e.g., "north_atlantic", "east_pacific").
-    
+
     Returns
     -------
     str
         String representing the current basin (e.g., "north_atlantic", "east_pacific").
-    
+
     Notes
     -----
     For storms in the North Atlantic or East Pacific basin, ``source_basin`` must be provided. This is because storms located over Mexico or Central America could be in either basin depending on where they originated (e.g., storms originated in the Atlantic basin are considered to be within the Atlantic basin while over Mexico or Central America until emerging in the Pacific Ocean).
     """
-    
-    #Error check
-    if isinstance(lat,(int,np.int,np.integer,float,np.floating)) == False:
+
+    # Error check
+    if not is_number(lat):
         msg = "\"lat\" must be of type int or float."
         raise TypeError(msg)
-    if isinstance(lon,(int,np.int,np.integer,float,np.floating)) == False:
+    if not is_number(lon):
         msg = "\"lon\" must be of type int or float."
         raise TypeError(msg)
-    
-    #Fix longitude
-    if lon < 0.0: lon = lon + 360.0
-    
-    #Northern hemisphere check
+
+    # Fix longitude
+    if lon < 0.0:
+        lon = lon + 360.0
+
+    # Northern hemisphere check
     if lat >= 0.0:
-        
+
         if lon < 100.0:
             if lat < 40.0:
                 return "north_indian"
             else:
                 if lon < 70.0:
                     return "north_atlantic"
                 else:
                     return "west_pacific"
         elif lon <= 180.0:
             return "west_pacific"
         else:
             if source_basin == "north_atlantic":
-                if constants.PATH_PACIFIC.contains_point((lat,lon)):
+                if constants.PATH_PACIFIC.contains_point((lat, lon)):
                     return "east_pacific"
                 else:
                     return "north_atlantic"
             elif source_basin == "east_pacific":
-                if constants.PATH_ATLANTIC.contains_point((lat,lon)):
+                if constants.PATH_ATLANTIC.contains_point((lat, lon)):
                     return "north_atlantic"
                 else:
                     return "east_pacific"
             else:
                 msg = "Cannot determine whether storm is in North Atlantic or East Pacific basins."
                 raise RuntimeError(msg)
-    
-    #Southern hemisphere check
+
+    # Southern hemisphere check
     else:
-        
+
         if lon < 20.0:
             return "south_atlantic"
         elif lon < 90.0:
             return "south_indian"
         elif lon < 160.0:
             return "australia"
         elif lon < 280.0:
             return "south_pacific"
         else:
             return "south_atlantic"
 
+
 def knots_to_mph(wind_speed):
-    
     r"""
     Convert wind from knots to miles per hour, in increments of 5 as used by NHC.
-    
+
     Parameters
     ----------
     wind_speed : int
         Sustained wind in knots.
-    
+
     Returns
     -------
     int
         Sustained wind in miles per hour.
     """
-    
-    #Ensure input is rounded down to nearest multiple of 5
+
+    # Ensure input is rounded down to nearest multiple of 5
     wind_speed = wind_speed - (wind_speed % 5)
 
-    #Define knots and mph conversions
-    kts = [0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,125,130,135,140,145,150,155,160,165,170,175,180,185]
-    mphs = [0,10,15,20,25,30,35,40,45,50,60,65,70,75,80,85,90,100,105,110,115,120,125,130,140,145,150,155,160,165,175,180,185,190,195,200,205,210]
-    
-    #If value is in list, return converted value
+    # Define knots and mph conversions
+    kts = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100,
+           105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185]
+    mphs = [0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 65, 70, 75, 80, 85, 90, 100, 105, 110,
+            115, 120, 125, 130, 140, 145, 150, 155, 160, 165, 175, 180, 185, 190, 195, 200, 205, 210]
+
+    # If value is in list, return converted value
     if wind_speed in kts:
         return mphs[kts.index(wind_speed)]
-    
-    #Otherwise, return original value
+
+    # Otherwise, return original value
     return wind_speed
 
-def accumulated_cyclone_energy(wind_speed,hours=6):
-    
+
+def accumulated_cyclone_energy(wind_speed, hours=6):
     r"""
     Calculate Accumulated Cyclone Energy (ACE) based on sustained wind speed in knots.
-    
+
     Parameters
     ----------
     wind_speed : int or list, numpy.ndarray
         Sustained wind in knots.
     hours : int, optional
         Duration in hours over which the sustained wind was observed. Default is 6 hours.
-    
+
     Returns
     -------
     float
         Accumulated cyclone energy.
-    
+
     Notes
     -----
-    
+
     As defined in `Bell et al. (2000)`_, Accumulated Cyclone Energy (ACE) is calculated as follows:
-    
+
     .. math:: ACE = 10^{-4} \sum v^{2}_{max}
-    
+
     As shown above, ACE is the sum of the squares of the estimated maximum sustained wind speed (in knots). By default, this assumes data is provided every 6 hours, as is the standard in HURDATv2 and NHC's Best Track, though this function provides an option to use a different hour duration.
-    
+
     .. _Bell et al. (2000): https://journals.ametsoc.org/view/journals/bams/81/6/1520-0477_2000_81_s1_caf_2_0_co_2.xml
     """
-    
-    #Determine types
-    if isinstance(wind_speed,(np.ndarray,list)):
+
+    # Determine types
+    if isinstance(wind_speed, (np.ndarray, list)):
         wind_speed = np.copy(wind_speed)
-    
-    #Calculate ACE
+
+    # Calculate ACE
     ace = ((10**-4) * (wind_speed**2)) * (hours/6.0)
-    
-    #Coerce to zero if wind speed less than TS force
-    if isinstance(ace,(np.ndarray,list)):
+
+    # Coerce to zero if wind speed less than TS force
+    if isinstance(ace, (np.ndarray, list)):
         ace[wind_speed < 34] = 0.0
-        return np.round(ace,4)
+        return np.round(ace, 4)
     else:
-        if wind_speed < 34: ace = 0.0
-        return round(ace,4)
+        if wind_speed < 34:
+            ace = 0.0
+        return round(ace, 4)
 
-def dropsonde_mslp_estimate(mslp,surface_wind):
-    
+
+def dropsonde_mslp_estimate(mslp, surface_wind):
     r"""
     Apply a NHC rule of thumb for estimating a TC's minimum central MSLP. This is intended for a dropsonde released in the eye, accounting for drifting by factoring in the surface wind in knots.
-    
+
     Parameters
     ----------
     mslp : int or float
         Dropsonde surface MSLP, in hPa.
     surface_wind : int or float
         Surface wind as measured by the dropsonde. This **must** be surface wind; this cannot be substituted by a level just above the surface.
-    
+
     Returns
     -------
     float
         Estimated TC minimum central MSLP using the NHC estimation method.
-    
+
     Notes
     -----
     Source is from NHC presentation:
     https://www.nhc.noaa.gov/outreach/presentations/nhc2013_aircraftData.pdf
     """
-    
+
     return mslp - (surface_wind / 10.0)
 
+
 def get_two_current():
-    
     r"""
     Retrieve the latest NHC Tropical Weather Outlook (TWO).
-    
+
     Returns
     -------
     dict
         A dictionary of shapefiles for points, areas and lines.
-    
+
     Notes
     -----
     The shapefiles returned are modified versions of Cartopy's BasicReader, allowing to read in shapefiles directly from URL without having to download the shapefile locally first.
     """
-    
-    #Retrieve NHC shapefiles for development areas
+
+    # Retrieve NHC shapefiles for development areas
     shapefiles = {}
-    for name in ['areas','lines','points']:
+    for name in ['areas', 'lines', 'points']:
 
         try:
-            #Read in shapefile zip from NHC
+            # Read in shapefile zip from NHC
             url = 'https://www.nhc.noaa.gov/xgtwo/gtwo_shapefiles.zip'
             request = urllib.request.Request(url)
             response = urllib.request.urlopen(request)
             file_like_object = BytesIO(response.read())
             tar = zipfile.ZipFile(file_like_object)
 
-            #Get file list (points, areas)
+            # Get file list (points, areas)
             members = '\n'.join([i for i in tar.namelist()])
             nums = "[0123456789]"
             search_pattern = f'gtwo_{name}_20{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}.shp'
             pattern = re.compile(search_pattern)
             filelist = pattern.findall(members)
             files = []
             for file in filelist:
-                if file not in files: files.append(file.split(".shp")[0]) #remove duplicates
+                if file not in files:
+                    files.append(file.split(".shp")[0])  # remove duplicates
 
-            #Retrieve necessary components for shapefile
+            # Retrieve necessary components for shapefile
             members = tar.namelist()
             members_names = [i for i in members]
-            data = {'shp':0,'dbf':0,'prj':0,'shx':0}
+            data = {'shp': 0, 'dbf': 0, 'prj': 0, 'shx': 0}
             for key in data.keys():
                 idx = members_names.index(files[0]+"."+key)
                 data[key] = BytesIO(tar.read(members[idx]))
 
-            #Read in shapefile
-            orig_reader = shapefile.Reader(shp=data['shp'], dbf=data['dbf'], prj=data['prj'], shx=data['shx'])
+            # Read in shapefile
+            orig_reader = shapefile.Reader(
+                shp=data['shp'], dbf=data['dbf'], prj=data['prj'], shx=data['shx'])
             shapefiles[name] = BasicReader(orig_reader)
         except:
             shapefiles[name] = None
-    
+
     return shapefiles
-    
+
+
 def get_two_archive(time):
-    
     r"""
-    Retrieve an archived NHC Tropical Weather Outlook (TWO). If none available within 30 hours of the specified date, an empty dict is returned.
-    
+    Retrieve an archived NHC Tropical Weather Outlook (TWO). If none available within 30 hours of the specified time, an empty dict is returned.
+
     Parameters
     ----------
     time : datetime
         Valid time for archived shapefile.
-    
+
     Returns
     -------
     dict
         A dictionary of shapefiles for points, areas and lines.
-    
+
     Notes
     -----
     The shapefiles returned are modified versions of Cartopy's BasicReader, allowing to read in shapefiles directly from URL without having to download the shapefile locally first.
-    
+
     TWO shapefiles are available courtesy of the National Hurricane Center beginning 28 July 2010.
     """
-    
-    #Find closest NHC shapefile if within 24 hours
-    url = 'https://www.nhc.noaa.gov/gis/archive_gtwo.php'
-    page = requests.get(url).text
+
+    # Determine TWO URL and info based on requested time
+    if time >= dt(2023, 5, 1):
+        directory_url = 'https://www.nhc.noaa.gov/gis/gtwo/archive/'
+    elif time >= dt(2014, 6, 1):
+        directory_url = 'https://www.nhc.noaa.gov/gis/gtwo_5day/archive/'
+    elif time >= dt(2010, 7, 28):
+        directory_url = 'https://www.nhc.noaa.gov/gis/gtwo_2day/archive/'
+    else:
+        return {'areas': None, 'lines': None, 'points': None}
+
+    # Fetch list of TWOs based on requested time
+    page = requests.get(directory_url).text
     content = page.split("\n")
     files = []
     for line in content:
-        if '<a href="gtwo/archive/2' in line:
+        if '<a href="' in line and 'zip">' in line:
             filename = line.split('zip">')[1]
             filename = filename.split("</a>")[0]
-            files.append(filename)
+            if '_' not in filename:
+                continue
+            if 'gtwo' not in filename.split('_')[0]:
+                files.append(filename)
     del content
-    dates = [dt.strptime(i.split("_")[0],'%Y%m%d%H%M') for i in files]
+
+    # Find closest NHC shapefile if within 24 hours
+    dates = [dt.strptime(i.split("_")[0], '%Y%m%d%H%M') for i in files]
     diff = [(time-i).total_seconds()/3600 for i in dates]
     diff = [i for i in diff if i >= 0]
 
-    #Continue if less than 24 hours difference
+    # Continue if less than 24 hours difference
     if len(diff) > 0 and np.nanmin(diff) <= 30:
         two_date = dates[diff.index(np.nanmin(diff))].strftime('%Y%m%d%H%M')
 
-        #Retrieve NHC shapefiles for development areas
+        # Retrieve NHC shapefiles for development areas
         shapefiles = {}
-        for name in ['areas','lines','points']:
+        for name in ['areas', 'lines', 'points']:
 
             try:
-                #Read in shapefile zip from NHC
-                url = f'https://www.nhc.noaa.gov/gis/gtwo/archive/{two_date}_gtwo.zip'
-                request = urllib.request.Request(url)
+                # Read in shapefile zip from NHC
+                file_url = f'{directory_url}{two_date}_gtwo.zip'
+                request = urllib.request.Request(file_url)
                 response = urllib.request.urlopen(request)
                 file_like_object = BytesIO(response.read())
                 tar = zipfile.ZipFile(file_like_object)
 
-                #Get file list (points, areas)
+                # Get file list (points, areas)
                 members = '\n'.join([i for i in tar.namelist()])
                 nums = "[0123456789]"
                 search_pattern = f'gtwo_{name}_20{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}.shp'
                 pattern = re.compile(search_pattern)
                 filelist = pattern.findall(members)
                 files = []
                 for file in filelist:
-                    if file not in files: files.append(file.split(".shp")[0]) #remove duplicates
-                
-                #Alternatively, check files for older format (generally 2014 and earlier)
+                    if file not in files:
+                        # remove duplicates
+                        files.append(file.split(".shp")[0])
+
+                # Alternatively, check files for older format (generally 2014 and earlier)
                 if len(files) == 0:
-                    if name in ['lines','points']:
+                    if name in ['lines', 'points']:
                         shapefiles[name] = None
                         continue
                     search_pattern = f'20{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}{nums}_gtwo.shp'
                     pattern = re.compile(search_pattern)
                     filelist = pattern.findall(members)
                     for file in filelist:
-                        if file not in files: files.append(file.split(".shp")[0]) #remove duplicates
+                        if file not in files:
+                            # remove duplicates
+                            files.append(file.split(".shp")[0])
 
-                #Retrieve necessary components for shapefile
+                # Retrieve necessary components for shapefile
                 members = tar.namelist()
                 members_names = [i for i in members]
-                data = {'shp':0,'dbf':0,'prj':0,'shx':0}
+                data = {'shp': 0, 'dbf': 0, 'prj': 0, 'shx': 0}
                 for key in data.keys():
                     idx = members_names.index(files[0]+"."+key)
                     data[key] = BytesIO(tar.read(members[idx]))
 
-                #Read in shapefile
-                orig_reader = shapefile.Reader(shp=data['shp'], dbf=data['dbf'], prj=data['prj'], shx=data['shx'])
+                # Read in shapefile
+                orig_reader = shapefile.Reader(
+                    shp=data['shp'], dbf=data['dbf'], prj=data['prj'], shx=data['shx'])
                 shapefiles[name] = BasicReader(orig_reader)
             except:
                 shapefiles[name] = None
     else:
-        shapefiles = {'areas':None,'lines':None,'points':None}
-    
+        shapefiles = {'areas': None, 'lines': None, 'points': None}
+
     return shapefiles
 
-def nhc_cone_radii(year,basin,forecast_hour=None):
-    
+
+def nhc_cone_radii(year, basin, forecast_hour=None):
     r"""
     Retrieve the official NHC Cone of Uncertainty radii by basin, year and forecast hour(s). Units are in nautical miles.
-    
+
     Parameters
     ----------
     year : int
         Valid year for cone of uncertainty radii.
     basin : str
         Basin for cone of uncertainty radii. If basin is invalid, return value will be an empty dict. Please refer to :ref:`options-domain` for available basin options.
     forecast_hour : int or list, optional
         Forecast hour(s) to retrieve the cone of uncertainty for. If empty, all available forecast hours will be retrieved.
-    
+
     Returns
     -------
     dict
         Dictionary with forecast hour(s) as the keys, and the cone radius in nautical miles for each respective forecast hour as the values.
-    
+
     Notes
     -----
     1. NHC cone radii are available beginning 2008 onward. Radii for years before 2008 will be defaulted to 2008, and if the current year's radii are not available yet, the radii for the most recent year will be returned.
-    
+
     2. NHC began producing cone radii for forecast hour 60 in 2020. Years before 2020 do not have a forecast hour 60.
     """
-    
-    #Source: https://www.nhc.noaa.gov/verification/verify3.shtml
-    #Source 2: https://www.nhc.noaa.gov/aboutcone.shtml
-    #Radii are in nautical miles
-    cone_climo_hr = [3,12,24,36,48,72,96,120]
-    
-    #Basin check
-    if basin not in ['north_atlantic','east_pacific']:
+
+    # Source: https://www.nhc.noaa.gov/verification/verify3.shtml
+    # Source 2: https://www.nhc.noaa.gov/aboutcone.shtml
+    # Radii are in nautical miles
+    cone_climo_hr = [3, 12, 24, 36, 48, 72, 96, 120]
+
+    # Basin check
+    if basin not in ['north_atlantic', 'east_pacific']:
         return {}
-    
-    #Fix for 2020 and later that incorporates 60 hour forecasts
+
+    # Fix for 2020 and later that incorporates 60 hour forecasts
     if year >= 2020:
-        cone_climo_hr = [3,12,24,36,48,60,72,96,120]
-    
-    #Forecast hour check
+        cone_climo_hr = [3, 12, 24, 36, 48, 60, 72, 96, 120]
+
+    # Forecast hour check
     if forecast_hour is None:
         forecast_hour = cone_climo_hr
-    elif isinstance(forecast_hour,int) == True:
+    elif isinstance(forecast_hour, int):
         if forecast_hour not in cone_climo_hr:
-            raise ValueError(f"Forecast hour {forecast_hour} is invalid. Available forecast hours for {year} are: {cone_climo_hr}")
+            raise ValueError(
+                f"Forecast hour {forecast_hour} is invalid. Available forecast hours for {year} are: {cone_climo_hr}")
         else:
             forecast_hour = [forecast_hour]
-    elif isinstance(forecast_hour,list) == True:
+    elif isinstance(forecast_hour, list):
         forecast_hour = [i for i in forecast_hour if i in cone_climo_hr]
         if len(forecast_hour) == 0:
-            raise ValueError(f"Requested forecast hours are invalid. Available forecast hours for {year} are: {cone_climo_hr}")
+            raise ValueError(
+                f"Requested forecast hours are invalid. Available forecast hours for {year} are: {cone_climo_hr}")
     else:
         raise TypeError("forecast_hour must be of type int or list")
-    
-    #Year check
+
+    # Year check
     if year > np.max([k for k in constants.CONE_SIZE_ATL.keys()]):
         year = [k for k in constants.CONE_SIZE_ATL.keys()][0]
-        warnings.warn(f"No cone information is available for the requested year. Defaulting to {year} cone.")
+        warnings.warn(
+            f"No cone information is available for the requested year. Defaulting to {year} cone.")
     elif year not in constants.CONE_SIZE_ATL.keys():
         year = 2008
-        warnings.warn(f"No cone information is available for the requested year. Defaulting to 2008 cone.")
-    
-    #Retrieve data
+        warnings.warn(
+            "No cone information is available for the requested year. Defaulting to 2008 cone.")
+
+    # Retrieve data
     cone_radii = {}
     for hour in list(np.sort(forecast_hour)):
         hour_index = cone_climo_hr.index(hour)
         if basin == 'north_atlantic':
             cone_radii[hour] = constants.CONE_SIZE_ATL[year][hour_index]
         elif basin == 'east_pacific':
             cone_radii[hour] = constants.CONE_SIZE_PAC[year][hour_index]
-    
+
     return cone_radii
 
-def generate_nhc_cone(forecast,basin,shift_lons=False,cone_days=5,cone_year=None,return_xarray=False):
 
+def generate_nhc_cone(forecast, basin, shift_lons=False, cone_days=5, cone_year=None, return_xarray=False):
     r"""
     Generates a gridded cone of uncertainty using forecast data from NHC.
 
     Parameters
     ----------
     forecast : dict
         Dictionary containing forecast data
@@ -723,296 +761,322 @@
         If true, grid will be shifted to +0 to +360 degrees longitude. Default is False (-180 to +180 degrees).
     cone_days : int, optional
         Number of forecast days to generate the cone through. Default is 5 days.
     cone_year : int, optional
         Year valid for cone radii. If None, this fuction will attempt to retrieve the year from the forecast dict.
     return_xarray : bool, optional
         If True, returns output as an xarray Dataset. Default is False, returning output as a dictionary.
-    
+
     Returns
     -------
     dict or xarray.Dataset
         Depending on `return_xarray`, returns either a dictionary or an xarray Dataset containing the gridded cone of uncertainty and its accompanying attributes.
-    
+
     Notes
     -----
     Forecast dicts can be retrieved for realtime storm objects using ``RealtimeStorm.get_forecast_realtime()``, and for archived storms using ``Storm.get_nhc_forecast_dict()``.
     """
-    
-    #Check forecast dict has all required keys
-    check_dict = [True if i in forecast.keys() else False for i in ['fhr','lat','lon','init']]
+
+    # Check forecast dict has all required keys
+    check_dict = [True if i in forecast.keys() else False for i in [
+        'fhr', 'lat', 'lon', 'init']]
     if False in check_dict:
-        raise ValueError("forecast dict must contain keys 'fhr', 'lat', 'lon' and 'init'. You may retrieve a forecast dict for a Storm object through 'storm.get_operational_forecasts()'.")
-    
-    #Check forecast basin
+        raise ValueError(
+            "forecast dict must contain keys 'fhr', 'lat', 'lon' and 'init'. You may retrieve a forecast dict for a Storm object through 'storm.get_operational_forecasts()'.")
+
+    # Check forecast basin
     if basin not in constants.ALL_BASINS:
         raise ValueError("basin cannot be identified.")
 
-    #Retrieve cone of uncertainty year
+    # Retrieve cone of uncertainty year
     if cone_year is None:
         cone_year = forecast['init'].year
     if cone_year > np.max([k for k in constants.CONE_SIZE_ATL.keys()]):
         cone_year = [k for k in constants.CONE_SIZE_ATL.keys()][0]
-        warnings.warn(f"No cone information is available for the requested year. Defaulting to {cone_year} cone.")
+        warnings.warn(
+            f"No cone information is available for the requested year. Defaulting to {cone_year} cone.")
     elif cone_year not in constants.CONE_SIZE_ATL.keys():
         cone_year = 2008
-        warnings.warn(f"No cone information is available for the requested year. Defaulting to 2008 cone.")
+        warnings.warn(
+            "No cone information is available for the requested year. Defaulting to 2008 cone.")
 
-    #Retrieve cone size and hours for given year
-    if basin in ['north_atlantic','east_pacific']:
-        output = nhc_cone_radii(cone_year,basin)
+    # Retrieve cone size and hours for given year
+    if basin in ['north_atlantic', 'east_pacific']:
+        output = nhc_cone_radii(cone_year, basin)
         cone_climo_hr = [k for k in output.keys()]
         cone_size = [output[k] for k in output.keys()]
     else:
-        cone_climo_hr = [3,12,24,36,48,72,96,120]
+        cone_climo_hr = [3, 12, 24, 36, 48, 72, 96, 120]
         cone_size = 0
 
-    #Function for interpolating between 2 times
+    # Function for interpolating between 2 times
     def temporal_interpolation(value, orig_times, target_times):
-        f = interp.interp1d(orig_times,value)
+        f = interp.interp1d(orig_times, value)
         ynew = f(target_times)
         return ynew
 
-    #Function for finding nearest value in an array
-    def findNearest(array,val):
+    # Function for finding nearest value in an array
+    def find_nearest(array, val):
         return array[np.abs(array - val).argmin()]
 
-    #Function for adding a radius surrounding a point
-    def add_radius(lats,lons,vlat,vlon,rad):
+    # Function for adding a radius surrounding a point
+    def add_radius(lats, lons, vlat, vlon, rad):
 
-        #construct new array expanding slightly over rad from lat/lon center
-        grid_res = 0.05 #1 degree is approx 111 km
+        # construct new array expanding slightly over rad from lat/lon center
+        grid_res = 0.05  # 1 degree is approx 111 km
         grid_fac = (rad*4)/111.0
 
-        #Make grid surrounding position coordinate & radius of circle
-        nlon = np.arange(findNearest(lons,vlon-grid_fac),findNearest(lons,vlon+grid_fac+grid_res),grid_res)
-        nlat = np.arange(findNearest(lats,vlat-grid_fac),findNearest(lats,vlat+grid_fac+grid_res),grid_res)
-        lons,lats = np.meshgrid(nlon,nlat)
+        # Make grid surrounding position coordinate & radius of circle
+        nlon = np.arange(find_nearest(lons, vlon-grid_fac),
+                         find_nearest(lons, vlon+grid_fac+grid_res), grid_res)
+        nlat = np.arange(find_nearest(lats, vlat-grid_fac),
+                         find_nearest(lats, vlat+grid_fac+grid_res), grid_res)
+        lons, lats = np.meshgrid(nlon, nlat)
         return_arr = np.zeros((lons.shape))
 
-        #Calculate distance from vlat/vlon at each gridpoint
+        # Calculate distance from vlat/vlon at each gridpoint
         r_earth = 6.371 * 10**6
-        dlat = np.subtract(np.radians(lats),np.radians(vlat))
-        dlon = np.subtract(np.radians(lons),np.radians(vlon))
+        dlat = np.subtract(np.radians(lats), np.radians(vlat))
+        dlon = np.subtract(np.radians(lons), np.radians(vlon))
 
-        a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats)) * np.cos(np.radians(vlat)) * np.sin(dlon/2) * np.sin(dlon/2)
-        c = 2 * np.arctan(np.sqrt(a), np.sqrt(1-a));
+        a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats)) * \
+            np.cos(np.radians(vlat)) * np.sin(dlon/2) * np.sin(dlon/2)
+        c = 2 * np.arctan(np.sqrt(a), np.sqrt(1-a))
         dist = (r_earth * c)/1000.0
-        dist = dist * 0.621371 #to miles
-        dist = dist * 0.868976 #to nautical miles
+        dist = dist * 0.621371  # to miles
+        dist = dist * 0.868976  # to nautical miles
 
-        #Mask out values less than radius
+        # Mask out values less than radius
         return_arr[dist <= rad] = 1
 
-        #Attach small array into larger subset array
-        small_coords = {'lat':nlat,'lon':nlon}
+        # Attach small array into larger subset array
+        small_coords = {'lat': nlat, 'lon': nlon}
 
         return return_arr, small_coords
 
-    #--------------------------------------------------------------------
+    # --------------------------------------------------------------------
 
-    #Check if fhr3 is available, then get forecast data
+    # Check if fhr3 is available, then get forecast data
     flag_12 = 0
     if forecast['fhr'][0] == 12:
         flag_12 = 1
         cone_climo_hr = cone_climo_hr[1:]
         fcst_lon = forecast['lon']
         fcst_lat = forecast['lat']
         fhr = forecast['fhr']
         t = np.array(forecast['fhr'])/6.0
         subtract_by = t[0]
         t = t - t[0]
-        interp_fhr_idx = np.arange(t[0],t[-1]+0.1,0.1) - t[0]
+        interp_fhr_idx = np.arange(t[0], t[-1]+0.1, 0.1) - t[0]
     elif 3 in forecast['fhr'] and 1 in forecast['fhr'] and 0 in forecast['fhr']:
         fcst_lon = forecast['lon'][2:]
         fcst_lat = forecast['lat'][2:]
         fhr = forecast['fhr'][2:]
         t = np.array(fhr)/6.0
-        interp_fhr_idx = np.arange(t[0],t[-1]+0.01,0.1)
+        interp_fhr_idx = np.arange(t[0], t[-1]+0.01, 0.1)
     elif 3 in forecast['fhr'] and 0 in forecast['fhr']:
-        idx = np.array([i for i,j in enumerate(forecast['fhr']) if j in cone_climo_hr])
+        idx = np.array([i for i, j in enumerate(
+            forecast['fhr']) if j in cone_climo_hr])
         fcst_lon = np.array(forecast['lon'])[idx]
         fcst_lat = np.array(forecast['lat'])[idx]
         fhr = np.array(forecast['fhr'])[idx]
         t = np.array(fhr)/6.0
-        interp_fhr_idx = np.arange(t[0],t[-1]+0.01,0.1)
+        interp_fhr_idx = np.arange(t[0], t[-1]+0.01, 0.1)
     elif forecast['fhr'][1] < 12:
         cone_climo_hr[0] = 0
         fcst_lon = [forecast['lon'][0]]+forecast['lon'][2:]
         fcst_lat = [forecast['lat'][0]]+forecast['lat'][2:]
         fhr = [forecast['fhr'][0]]+forecast['fhr'][2:]
         t = np.array(fhr)/6.0
-        interp_fhr_idx = np.arange(t[0]/6.0,t[-1]+0.1,0.1)
+        interp_fhr_idx = np.arange(t[0]/6.0, t[-1]+0.1, 0.1)
     else:
         cone_climo_hr[0] = 0
         fcst_lon = forecast['lon']
         fcst_lat = forecast['lat']
         fhr = forecast['fhr']
         t = np.array(fhr)/6.0
-        interp_fhr_idx = np.arange(t[0],t[-1]+0.1,0.1)
+        interp_fhr_idx = np.arange(t[0], t[-1]+0.1, 0.1)
 
-    #Determine index of forecast day cap
+    # Determine index of forecast day cap
     if (cone_days*24) in fhr:
         cone_day_cap = list(fhr).index(cone_days*24)+1
         fcst_lon = fcst_lon[:cone_day_cap]
         fcst_lat = fcst_lat[:cone_day_cap]
         fhr = fhr[:cone_day_cap]
         t = np.array(fhr)/6.0
-        interp_fhr_idx = np.arange(interp_fhr_idx[0],t[-1]+0.1,0.1)
+        interp_fhr_idx = np.arange(interp_fhr_idx[0], t[-1]+0.1, 0.1)
     else:
         cone_day_cap = len(fhr)
 
-    #Account for dateline
-    if shift_lons == True:
+    # Account for dateline
+    if shift_lons:
         temp_lon = np.array(fcst_lon)
-        temp_lon[temp_lon<0] = temp_lon[temp_lon<0]+360.0
+        temp_lon[temp_lon < 0] = temp_lon[temp_lon < 0]+360.0
         fcst_lon = temp_lon.tolist()
 
-    #Interpolate forecast data temporally and spatially
+    # Interpolate forecast data temporally and spatially
     interp_kind = 'quadratic'
-    if len(t) == 2: interp_kind = 'linear' #Interpolate linearly if only 2 forecast points
-    x1 = interp.interp1d(t,fcst_lon,kind=interp_kind)
-    y1 = interp.interp1d(t,fcst_lat,kind=interp_kind)
+    if len(t) == 2:
+        interp_kind = 'linear'  # Interpolate linearly if only 2 forecast points
+    x1 = interp.interp1d(t, fcst_lon, kind=interp_kind)
+    y1 = interp.interp1d(t, fcst_lat, kind=interp_kind)
     interp_fhr = interp_fhr_idx * 6
     interp_lon = x1(interp_fhr_idx)
     interp_lat = y1(interp_fhr_idx)
 
-    #Return if no cone specified
+    # Return if no cone specified
     if cone_size == 0:
-        return_dict = {'center_lon':interp_lon,'center_lat':interp_lat}
+        return_dict = {'center_lon': interp_lon, 'center_lat': interp_lat}
         if return_xarray:
             import xarray as xr
             return xr.Dataset(return_dict)
         else:
             return return_dict
 
-    #Interpolate cone radius temporally
+    # Interpolate cone radius temporally
     cone_climo_hr = cone_climo_hr[:cone_day_cap]
     cone_size = cone_size[:cone_day_cap]
 
     cone_climo_fhrs = np.array(cone_climo_hr)
     if flag_12 == 1:
         interp_fhr += (subtract_by*6.0)
         cone_climo_fhrs = cone_climo_fhrs[1:]
-    idxs = np.nonzero(np.in1d(np.array(fhr),np.array(cone_climo_hr)))
+    idxs = np.nonzero(np.in1d(np.array(fhr), np.array(cone_climo_hr)))
     temp_arr = np.array(cone_size)[idxs]
-    interp_rad = np.apply_along_axis(lambda n: temporal_interpolation(n,fhr,interp_fhr),axis=0,arr=temp_arr)
+    interp_rad = np.apply_along_axis(lambda n: temporal_interpolation(
+        n, fhr, interp_fhr), axis=0, arr=temp_arr)
 
-    #Initialize 0.05 degree grid
-    gridlats = np.arange(min(interp_lat)-7,max(interp_lat)+7,0.05)
-    gridlons = np.arange(min(interp_lon)-7,max(interp_lon)+7,0.05)
-    gridlons2d,gridlats2d = np.meshgrid(gridlons,gridlats)
+    # Initialize 0.05 degree grid
+    gridlats = np.arange(min(interp_lat)-7, max(interp_lat)+7, 0.05)
+    gridlons = np.arange(min(interp_lon)-7, max(interp_lon)+7, 0.05)
+    gridlons2d, gridlats2d = np.meshgrid(gridlons, gridlats)
 
-    #Iterate through fhr, calculate cone & add into grid
-    large_coords = {'lat':gridlats,'lon':gridlons}
+    # Iterate through fhr, calculate cone & add into grid
+    large_coords = {'lat': gridlats, 'lon': gridlons}
     griddata = np.zeros((gridlats2d.shape))
-    for i,(ilat,ilon,irad) in enumerate(zip(interp_lat,interp_lon,interp_rad)):
-        temp_grid, small_coords = add_radius(gridlats,gridlons,ilat,ilon,irad)
+    for i, (ilat, ilon, irad) in enumerate(zip(interp_lat, interp_lon, interp_rad)):
+        temp_grid, small_coords = add_radius(
+            gridlats, gridlons, ilat, ilon, irad)
         plug_grid = np.zeros((griddata.shape))
-        plug_grid = plug_array(temp_grid,plug_grid,small_coords,large_coords)
-        griddata = np.maximum(griddata,plug_grid)
-    
+        plug_grid = plug_array(temp_grid, plug_grid,
+                               small_coords, large_coords)
+        griddata = np.maximum(griddata, plug_grid)
+
     if return_xarray:
         import xarray as xr
-        cone = xr.DataArray(griddata,coords=[gridlats,gridlons],dims=['grid_lat','grid_lon'])
+        cone = xr.DataArray(griddata, coords=[gridlats, gridlons], dims=[
+                            'grid_lat', 'grid_lon'])
         return_ds = {
-            'cone':cone,
-            'center_lon':interp_lon,
-            'center_lat':interp_lat
+            'cone': cone,
+            'center_lon': interp_lon,
+            'center_lat': interp_lat
         }
         return_ds = xr.Dataset(return_ds)
         return_ds.attrs['year'] = cone_year
         return return_ds
-    
+
     else:
-        return_dict = {'lat':gridlats,'lon':gridlons,'lat2d':gridlats2d,'lon2d':gridlons2d,'cone':griddata,
-                   'center_lon':interp_lon,'center_lat':interp_lat,'year':cone_year}
+        return_dict = {'lat': gridlats, 'lon': gridlons, 'lat2d': gridlats2d, 'lon2d': gridlons2d, 'cone': griddata,
+                       'center_lon': interp_lon, 'center_lat': interp_lat, 'year': cone_year}
         return return_dict
 
-def calc_ensemble_ellipse(member_lons,member_lats):
-    
+
+def calc_ensemble_ellipse(member_lons, member_lats):
     r"""
     Calculate an ellipse representing ensemble member location spread. This function follows the methodology of Hamill et al. (2011).
-    
+
     Parameters
     ----------
     member_lons : list
         List containing longitudes of ensemble members valid at a single time.
     member_lats : list
         List containing latitudes of ensemble members valid at a single time.
-    
+
     Returns
     -------
     dict
         Dictionary containing the longitude and latitude of the ellipse.
-    
+
     Notes
     -----
     The ensemble ellipse used in this function follows the methodology of `Hamill et al. (2011)`_, denoting the spread in ensemble member cyclone positions. The size of the ellipse is calculated to contain 90% of ensemble members at any given time. This ellipse can be used to determine the primary type of ensemble variability:
 
     * **Along-track variability** - if the major axis of the ellipse is parallel to the ensemble mean motion vector.
     * **Across-track variability** - if the major axis of the ellipse is normal to the ensemble mean motion vector.
 
     .. _Hamill et al. (2011): https://doi.org/10.1175/2010MWR3456.1
-    
+
     This code is adapted from NCAR Command Language (NCL) code courtesy of Ryan Torn and Thomas Hamill.
-    """
     
-    #Compute ensemble mean lon & lat
+    Examples
+    --------
+    
+    >>> lons = [-60, -59, -59, -61, -67, -58, -60, -57]
+    >>> lats = [40, 40, 41, 39, 37, 42, 40, 41]
+    >>> ellipse = utils.calc_ensemble_ellipse(lons, lats)
+    >>> print(ellipse.keys())
+    dict_keys(['ellipse_lon', 'ellipse_lat'])
+    
+    """
+
+    # Compute ensemble mean lon & lat
     mean_lon = np.average(member_lons)
     mean_lat = np.average(member_lats)
 
-    Pb = [[0,0],[0,0]]
+    Pb = [[0, 0], [0, 0]]
     for i in range(len(member_lats)):
-        Pb[0][0] = Pb[0][0] + (member_lons[i]-mean_lon) * (member_lons[i]-mean_lon)
-        Pb[1][1] = Pb[1][1] + (member_lats[i]-mean_lat) * (member_lats[i]-mean_lat)
-        Pb[1][0] = Pb[1][0] + (member_lats[i]-mean_lat) * (member_lons[i]-mean_lon)
+        Pb[0][0] = Pb[0][0] + (member_lons[i]-mean_lon) * \
+            (member_lons[i]-mean_lon)
+        Pb[1][1] = Pb[1][1] + (member_lats[i]-mean_lat) * \
+            (member_lats[i]-mean_lat)
+        Pb[1][0] = Pb[1][0] + (member_lats[i]-mean_lat) * \
+            (member_lons[i]-mean_lon)
     Pb[0][1] = Pb[1][0]
     Pb = np.array(Pb) / float(len(member_lats)-1)
 
     rho = Pb[1][0] / (np.sqrt(Pb[0][0]) * np.sqrt(Pb[1][1]))
     sigmax = np.sqrt(Pb[0][0])
     sigmay = np.sqrt(Pb[1][1])
     fac = 1.0 / (2.0 * (1 - rho**2))
 
-    #Calculate lon & lat coordinates of ellipse
+    # Calculate lon & lat coordinates of ellipse
     ellipse_lon = []
     ellipse_lat = []
     increment = np.pi/180.0
-    for radians in np.arange(0,(2.0*np.pi)+increment,increment):
+    for radians in np.arange(0, (2.0*np.pi)+increment, increment):
         xstart = np.cos(radians)
         ystart = np.sin(radians)
 
-        for rdistance in np.arange(0.,2400.):
+        for rdistance in np.arange(0., 2400.):
             xloc = xstart * rdistance/80.0
             yloc = ystart * rdistance/80.0
-            dist = np.sqrt(xloc * xloc + yloc * yloc)
-            prob = np.exp(-1.0 * fac * ((xloc/sigmax)**2 + (yloc/sigmay)**2 - 2.0 * rho * (xloc/sigmax)*(yloc/sigmay)))
+            prob = np.exp(-1.0 * fac * ((xloc/sigmax)**2 + (yloc/sigmay)
+                          ** 2 - 2.0 * rho * (xloc/sigmax)*(yloc/sigmay)))
             if prob < 0.256:
                 ellipse_lon.append(xloc + mean_lon)
                 ellipse_lat.append(yloc + mean_lat)
                 break
 
-    #Return ellipse data
-    return {'ellipse_lon':ellipse_lon, 'ellipse_lat':ellipse_lat}
+    # Return ellipse data
+    return {'ellipse_lon': ellipse_lon, 'ellipse_lat': ellipse_lat}
 
-def create_storm_dict(filepath,storm_name,storm_id,delimiter=',',time_format='%Y%m%d%H',**kwargs):
-    
+
+def create_storm_dict(filepath, storm_name, storm_id, delimiter=',', time_format='%Y%m%d%H', **kwargs):
     r"""
     Creates a storm dict from custom user-provided data.
-    
+
     Parameters
     ----------
     filepath : str
         Relative or absolute file path containing custom storm data.
     storm_name : str
         Storm name for custom storm entry.
     storm_id : str
         Storm ID for custom storm entry.
-    
+
     Other Parameters
     ----------------
     delimiter : str
         Delimiter separating columns for text files. Default is ",".
     time_format : str
         Time format for which to convert times to Datetime objects. Default is ``"%Y%m%d%H"`` (e.g., ``"2015070112"`` for 1200 UTC 1 July 2015).
     time_header : str
@@ -1023,34 +1087,34 @@
         Name of longitude dimension. If not provided, function will attempt to locate it internally.
     vmax_header : str
         Name of maximum sustained wind (knots) dimension. If not provided, function will attempt to locate it internally.
     mslp_header : str
         Name of minimum MSLP (hPa) dimension. If not provided, function will attempt to locate it internally.
     type_header : str
         Name of storm type dimension. If not provided, function will attempt to locate it internally. If not part of entry data, storm type will be derived based on provided wind speed values.
-    
+
     Returns
     -------
     dict
         Dictionary containing formatted custom storm data.
-    
+
     Notes
     -----
     This function creates a formatted storm data dictionary using custom user-provided data. The constraints for the parser are as follows:
-    
+
     1. Rows that begin with a ``#`` or ``\`` are automatically ignored.
     2. The first non-commented row must be a header row.
     3. The header row must contain entries for time, latitude, longitude, maximum sustained wind (knots), and minimum MSLP (hPa). The order of these columns does not matter.
-    4. Preferred header names are "date", "lat", "lon", "vmax" and "mslp" for the main 5 categories, but custom header names can be provided. Refer to the "Other Parameters" section above.
+    4. Preferred header names are "time", "lat", "lon", "vmax" and "mslp" for the main 5 categories, but custom header names can be provided. Refer to the "Other Parameters" section above.
     5. Providing a "type" column (e.g., "TS", "HU", "EX") is not required, but is recommended especially if dealing with subtropical or non-tropical types.
-    
+
     Below is an example file which we'll call ``data.txt``::
 
         # The row below is a header row. The order of the columns doesn't matter.
-        date,lat,lon,vmax,mslp,type
+        time,lat,lon,vmax,mslp,type
         2021080518,19.4,-59.9,25,1014,DB
         2021080600,19.7,-60.2,25,1014,DB
         2021080606,20.1,-60.5,30,1012,DB
         2021080612,20.5,-60.8,30,1011,TD
         2021080618,20.7,-61.3,30,1011,TD
         2021080700,20.8,-61.8,35,1008,TS
         2021080706,20.8,-62.6,35,1007,TS
@@ -1064,26 +1128,26 @@
         2021080906,25.0,-67.5,55,994,TS
         2021080912,26.5,-67.1,55,993,TS
         2021080918,28.0,-66.4,50,992,TS
         2021081000,29.5,-65.6,50,994,TS
         2021081006,31.4,-64.0,45,996,TS
         2021081012,33.4,-62.0,45,997,EX
         2021081018,35.4,-59.5,50,997,EX
-    
+
     Reading it into the parser returns the following dict:
-    
+
     >>> from tropycal import utils
     >>> storm_dict = utils.create_storm_dict(filename='data.txt', storm_name='Test', storm_id='AL502021')
     >>> print(storm_dict)
     {'id': 'AL502021',
      'operational_id': 'AL502021',
      'name': 'Test',
      'source_info': 'Custom User Data',
      'source': 'custom',
-     'date': [datetime.datetime(2021, 8, 5, 18, 0),
+     'time': [datetime.datetime(2021, 8, 5, 18, 0),
               datetime.datetime(2021, 8, 6, 0, 0),
               ....
               datetime.datetime(2021, 8, 10, 12, 0),
               datetime.datetime(2021, 8, 10, 18, 0)],
      'extra_obs': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      'special': ['','',....,'',''],
      'type': ['DB','DB','DB','TD','TD','TS','TS','TS','TS','TS','TS','HU','HU','TS','TS','TS','TS','TS','TS','EX','EX'],
@@ -1092,29 +1156,29 @@
      'vmax': [25.0,25.0,30.0,30.0,30.0,35.0,35.0,40.0,45.0,55.0,60.0,65.0,65.0,60.0,55.0,55.0,50.0,50.0,45.0,45.0,50.0],
      'mslp': [1014,1014,1012,1011,1011,1008,1007,1004,1002,998,994,989,988,992,994,993,992,994,996,997,997],
      'wmo_basin': ['north_atlantic','north_atlantic',....,'north_atlantic','north_atlantic'],
      'ace': 3.7825,
      'basin': 'north_atlantic',
      'year': 2021,
      'season': 2021}
-    
+
     This custom data can then be plugged into a Storm object:
-    
+
     >>> from tropycal import tracks
     >>> storm = tracks.Storm(storm_dict)
     >>> print(storm)
     <tropycal.tracks.Storm>
     Storm Summary:
         Maximum Wind:      65 knots
         Minimum Pressure:  988 hPa
-        Start Date:        1200 UTC 06 August 2021
-        End Date:          0600 UTC 10 August 2021
+        Start Time:        1200 UTC 06 August 2021
+        End Time:          0600 UTC 10 August 2021
     .
     Variables:
-        date        (datetime) [2021-08-05 18:00:00 .... 2021-08-10 18:00:00]
+        time        (datetime) [2021-08-05 18:00:00 .... 2021-08-10 18:00:00]
         extra_obs   (int32) [0 .... 0]
         special     (str) [ .... ]
         type        (str) [DB .... EX]
         lat         (float64) [19.4 .... 35.4]
         lon         (float64) [-59.9 .... -59.5]
         vmax        (float64) [25.0 .... 50.0]
         mslp        (float64) [1014.0 .... 997.0]
@@ -1130,353 +1194,514 @@
         basin:           north_atlantic
         year:            2021
         season:          2021
         realtime:        False
         invest:          False
     """
 
-    #Pop kwargs
-    time_header = kwargs.pop('time_header',None)
-    lat_header = kwargs.pop('lat_header',None)
-    lon_header = kwargs.pop('lon_header',None)
-    vmax_header = kwargs.pop('vmax_header',None)
-    mslp_header = kwargs.pop('mslp_header',None)
-    type_header = kwargs.pop('type_header',None)
-    
-    #Check for file extension
+    # Pop kwargs
+    time_header = kwargs.pop('time_header', None)
+    lat_header = kwargs.pop('lat_header', None)
+    lon_header = kwargs.pop('lon_header', None)
+    vmax_header = kwargs.pop('vmax_header', None)
+    mslp_header = kwargs.pop('mslp_header', None)
+    type_header = kwargs.pop('type_header', None)
+
+    # Check for file extension
     netcdf = True if filepath[-3:] == '.nc' else False
-    
-    #Create empty dict
-    data = {'id':storm_id,
-            'operational_id':storm_id,
-            'name':storm_name,
-            'source_info':'Custom User Data',
-            'source':'custom',
-            'date':[],
-            'extra_obs':[],
-            'special':[],
-            'type':[],
-            'lat':[],
-            'lon':[],
-            'vmax':[],
-            'mslp':[],
-            'wmo_basin':[],
-            'ace':0.0}
-
-    #Parse text file
-    if netcdf == False:
-        
-        #Store header data
+
+    # Create empty dict
+    data = {'id': storm_id,
+            'operational_id': storm_id,
+            'name': storm_name,
+            'source_info': 'Custom User Data',
+            'source': 'custom',
+            'time': [],
+            'extra_obs': [],
+            'special': [],
+            'type': [],
+            'lat': [],
+            'lon': [],
+            'vmax': [],
+            'mslp': [],
+            'wmo_basin': [],
+            'ace': 0.0}
+
+    # Parse text file
+    if not netcdf:
+
+        # Store header data
         header = {}
-        
-        #Read file content
-        f = open(filepath,"r")
+
+        # Read file content
+        f = open(filepath, "r")
         content = f.readlines()
         f.close()
         for line in content:
-            if len(line) < 5: continue
-            if line[0] in ['#','/']: continue
+            if len(line) < 5:
+                continue
+            if line[0] in ['#', '/']:
+                continue
             line = line.split("\n")[0]
             line = line.split("\r")[0]
-            
-            #Split line
-            if delimiter in ['',' ']: delimiter = None
+
+            # Split line
+            if delimiter in ['', ' ']:
+                delimiter = None
             lineArray = line.split(delimiter)
-            lineArray = [i.replace(" ","") for i in lineArray]
-            
-            #Determine header
+            lineArray = [i.replace(" ", "") for i in lineArray]
+
+            # Determine header
             if len(header) == 0:
                 for value in lineArray:
-                    if value == time_header or value.lower() in ['time','date','valid_time','valid_date']:
-                        header['date'] = [value,lineArray.index(value)]
-                    elif value == lat_header or value.lower() in ['lat','latitude','lat_0']:
-                        header['lat'] = [value,lineArray.index(value)]
-                    elif value == lon_header or value.lower() in ['lon','longitude','lon_0']:
-                        header['lon'] = [value,lineArray.index(value)]
-                    elif value == vmax_header or value.lower() in ['vmax','wind','wspd','max_wind','wind_speed']:
-                        header['vmax'] = [value,lineArray.index(value)]
-                    elif value == mslp_header or value.lower() in ['mslp','slp','min_mslp','pressure','pres','minimum_mslp']:
-                        header['mslp'] = [value,lineArray.index(value)]
-                    elif value == type_header or value.lower() in ['type','storm_type']:
-                        header['type'] = [value,lineArray.index(value)]
-                found = [i in header.keys() for i in ['date','lat','lon','vmax','mslp']]
-                if False in found: raise ValueError("Data must have columns for 'date', 'lat', 'lon', 'vmax' and 'mslp'.")
+                    if value == time_header or value.lower() in ['time', 'date', 'valid_time', 'valid_date']:
+                        header['time'] = [value, lineArray.index(value)]
+                    elif value == lat_header or value.lower() in ['lat', 'latitude', 'lat_0']:
+                        header['lat'] = [value, lineArray.index(value)]
+                    elif value == lon_header or value.lower() in ['lon', 'longitude', 'lon_0']:
+                        header['lon'] = [value, lineArray.index(value)]
+                    elif value == vmax_header or value.lower() in ['vmax', 'wind', 'wspd', 'max_wind', 'wind_speed']:
+                        header['vmax'] = [value, lineArray.index(value)]
+                    elif value == mslp_header or value.lower() in ['mslp', 'slp', 'min_mslp', 'pressure', 'pres', 'minimum_mslp']:
+                        header['mslp'] = [value, lineArray.index(value)]
+                    elif value == type_header or value.lower() in ['type', 'storm_type']:
+                        header['type'] = [value, lineArray.index(value)]
+                found = [i in header.keys()
+                         for i in ['time', 'lat', 'lon', 'vmax', 'mslp']]
+                if False in found:
+                    raise ValueError(
+                        "Data must have columns for 'time', 'lat', 'lon', 'vmax' and 'mslp'.")
                 continue
-                
-            #Enter standard data into dict
-            enter_date = dt.strptime(lineArray[header.get('date')[1]],time_format)
-            if enter_date in data['date']: raise ValueError("Error: Multiple entries detected for the same valid time.")
-            data['date'].append(enter_date)
+
+            # Enter standard data into dict
+            enter_date = dt.strptime(
+                lineArray[header.get('time')[1]], time_format)
+            if enter_date in data['time']:
+                raise ValueError(
+                    "Error: Multiple entries detected for the same valid time.")
+            data['time'].append(enter_date)
             data['lat'].append(float(lineArray[header.get('lat')[1]]))
             data['lon'].append(float(lineArray[header.get('lon')[1]]))
             data['vmax'].append(float(lineArray[header.get('vmax')[1]]))
             data['mslp'].append(float(lineArray[header.get('mslp')[1]]))
-            
-            #Derive storm type if needed
+
+            # Derive storm type if needed
             if 'type' in header.keys():
                 temp_type = lineArray[header.get('type')[1]]
-                if temp_type in ['TY','ST']: temp_type = 'HU'
                 data['type'].append(temp_type)
             else:
-                data['type'].append(get_storm_type(data['vmax'][-1],False))
-            
-            #Derive ACE
-            if data['date'][-1].strftime('%H%M') in constants.STANDARD_HOURS and data['type'][-1] in constants.NAMED_TROPICAL_STORM_TYPES:
+                data['type'].append(get_storm_type(data['vmax'][-1], False))
+
+            # Derive ACE
+            if data['time'][-1].strftime('%H%M') in constants.STANDARD_HOURS and data['type'][-1] in constants.NAMED_TROPICAL_STORM_TYPES:
                 data['ace'] += accumulated_cyclone_energy(data['vmax'][-1])
-            
-            #Derive basin
+
+            # Derive basin
             if len(data['wmo_basin']) == 0:
-                data['wmo_basin'].append(get_basin(data['lat'][-1],data['lon'][-1],'north_atlantic'))
+                data['wmo_basin'].append(
+                    get_basin(data['lat'][-1], data['lon'][-1], 'north_atlantic'))
                 data['basin'] = data['wmo_basin'][-1]
             else:
-                data['wmo_basin'].append(get_basin(data['lat'][-1],data['lon'][-1],data['basin']))
-            
-            #Other entries
-            extra_obs = 0 if data['date'][-1].strftime('%H%M') in constants.STANDARD_HOURS else 1
+                data['wmo_basin'].append(
+                    get_basin(data['lat'][-1], data['lon'][-1], data['basin']))
+
+            # Other entries
+            extra_obs = 0 if data['time'][-1].strftime(
+                '%H%M') in constants.STANDARD_HOURS else 1
             data['extra_obs'].append(extra_obs)
             data['special'].append('')
-        
-        #Add other info
-        data['year'] = data['date'][0].year
-        data['season'] = data['date'][0].year
-        
-        #Return dict
+
+        # Add other info
+        data['year'] = data['time'][0].year
+        data['season'] = data['time'][0].year
+
+        # Return dict
         return data
+
+def ships_parser(content):
+    r"""
+    Parses SHIPS text data into multiple sorted dictionaries.
     
+    Parameters
+    ----------
+    content : str
+        SHIPS file content.
+    
+    Returns
+    -------
+    dict
+        Dictionary containing parsed SHIPS data.
+    
+    Notes
+    -----
+    This function is referenced internally when creating SHIPS objects, but can also be used as a standalone function.
+    """
+
+    data = {}
+    data_ri = {}
+    data_attrs = {}
+
+    def split_first_group(line):
+        subset_line = line[15:]
+        chunk_size = 6
+        return [(subset_line[i:i+chunk_size]).replace(' ','') for i in range(0, len(subset_line), chunk_size)]
+
+    def split_prob(line):
+        line_subset_1 = int((line.split('threshold=')[1]).split('%')[0])
+        line_subset_2 = float((line.split('% is')[1]).split('times')[0])
+        if 'mean (' in line:
+            line_subset_3 = float((line.split('mean (')[1]).split('%')[0])
+        else:
+            line_subset_3 = float((line.split('mean(')[1]).split('%')[0])
+        return line_subset_1, line_subset_2, line_subset_3
+
+    content = content.split('\n')
+    for line in content:
+
+        # Parse first group into dict
+        first_group = {
+            'TIME (HR)': ['fhr',int],
+            'LAT (DEG N)': ['lat',float],
+            'LONG(DEG W)': ['lon',float],
+            'V (KT) NO LAND': ['vmax_noland_kt',int],
+            'V (KT) LAND': ['vmax_land_kt',int],
+            'V (KT) LGEM': ['vmax_lgem_kt',int],
+            'Storm Type': ['storm_type',str],
+            'SHEAR (KT)': ['shear_kt',int],
+            'SHEAR ADJ (KT)': ['shear_adj_kt',int],
+            'SHEAR DIR': ['shear_dir',int],
+            'SST (C)': ['sst_c',float],
+            'POT. INT. (KT)': ['vmax_pot_kt',int],
+            '200 MB T (C)': ['200mb_temp_c',float],
+            'TH_E DEV (C)': ['thetae_dev_c',int],
+            '700-500 MB RH': ['700_500_rh',int],
+            'MODEL VTX (KT)': ['model_vortex_kt',int],
+            '850 MB ENV VOR': ['850mb_env_vort',int],
+            '200 MB DIV': ['200mb_div',int],
+            '700-850 TADV': ['700_850_tadv',int],
+            'LAND (KM)': ['dist_land_km',int],
+            'STM SPEED (KT)': ['storm_speed_kt',int],
+            'HEAT CONTENT': ['heat_content',int]
+        }
+        checks = ['N/A','LOST','XX.X','XXX.X','DIS']
+        for key in first_group.keys():
+            if line.startswith(key):
+                data[first_group[key][0]] = [first_group[key][1](i) if i.upper() not in checks
+                                             else np.nan for i in split_first_group(line)]
+                if key == 'LONG(DEG W)':
+                    data[first_group[key][0]] = [i*-1 if i < 180 else (i - 360) * -1
+                                                 for i in data[first_group[key][0]]]
+
+        # Parse attributes
+        if 'INITIAL HEADING/SPEED (DEG/KT)' in line:
+            line_subset = line.split('INITIAL HEADING/SPEED (DEG/KT):')[1][:10]
+            data_attrs['storm_bearing_deg'] = int(line_subset.split('/')[0])
+            data_attrs['storm_motion_kt'] = int(line_subset.split('/')[1])
+        if 'T-12 MAX WIND' in line:
+            data_attrs['max_wind_t-12_kt'] = int(line.split('T-12 MAX WIND:')[1][:10])
+        if 'PRESSURE OF STEERING LEVEL (MB)' in line:
+            line_subset = line.split('PRESSURE OF STEERING LEVEL (MB):')[1]
+            line_subset_mean = (line_subset.split('MEAN=')[1]).split(')')[0]
+            data_attrs['steering_level_pres_hpa'] = int(line_subset.split('(')[0])
+            data_attrs['steering_level_pres_mean_hpa'] = int(line_subset_mean)
+        if 'GOES IR BRIGHTNESS TEMP. STD DEV.' in line:
+            line_subset = line.split('50-200 KM RAD:')[1]
+            line_subset_mean = (line_subset.split('MEAN=')[1]).split(')')[0]
+            data_attrs['brightness_temp_stdev'] = float(line_subset.split('(')[0])
+            data_attrs['brightness_temp_stdev_mean'] = float(line_subset_mean)
+        if 'GOES IR PIXELS WITH T' in line:
+            line_subset = line.split('50-200 KM RAD:')[1]
+            line_subset_mean = (line_subset.split('MEAN=')[1]).split(')')[0]
+            data_attrs['pixels_below_-20c'] = float(line_subset.split('(')[0])
+            data_attrs['pixels_below_-20c_mean'] = float(line_subset_mean)
+
+        # Rapid intensification probabilities
+        ri_group = {
+            'Prob RI for 20kt/ 12hr RI': '20kt/12hr',
+            'Prob RI for 25kt/ 24hr RI': '25kt/24hr',
+            'Prob RI for 30kt/ 24hr RI': '30kt/24hr',
+            'Prob RI for 35kt/ 24hr RI': '35kt/24hr',
+            'Prob RI for 40kt/ 24hr RI': '40kt/24hr',
+            'Prob RI for 45kt/ 36hr RI': '45kt/36hr',
+            'Prob RI for 55kt/ 48hr RI': '55kt/48hr',
+            'Prob RI for 65kt/ 72hr RI': '65kt/72hr',
+            'RI for 25 kt RI': '25kt/24hr',
+            'RI for 30 kt RI': '30kt/24hr',
+            'RI for 35 kt RI': '35kt/24hr',
+            'RI for 40 kt RI': '40kt/24hr',
+        }
+        for key in ri_group.keys():
+            if key in line:
+                prob, times, climo = split_prob(line)
+                data_ri[ri_group[key]] = {
+                    'probability': prob if prob != 999 else np.nan,
+                    'climo_mean': climo,
+                    'prob / climo': times if times != 999 else np.nan,
+                }
+
+    # Add current location to attributes
+    data_attrs['lat'] = data['lat'][0]
+    data_attrs['lon'] = data['lon'][0]
+
+    return {
+        'data': data,
+        'data_ri': data_ri,
+        'data_attrs': data_attrs,
+    }
 
-#===========================================================================================================
+# ===========================================================================================================
 # Private utilities
 # These are primarily intended to be used internally. Do not add these to documentation.
-#===========================================================================================================
+# ===========================================================================================================
 
-#Function for plugging small array into larger array
-def plug_array(small,large,small_coords,large_coords):
-    
+# Function for plugging small array into larger array
+def plug_array(small, large, small_coords, large_coords):
     r"""
     Plug small array into large array with matching coords.
-    
+
     Parameters
     ----------
     small : numpy.ndarray
         Small array to be plugged into the larger array.
     large : numpy.ndarray
         Large array for the small array to be plugged into.
     small_coords : dict
         Dictionary containing 'lat' and 'lon' keys, whose values are numpy.ndarrays of lat & lon for the small array.
     large_coords : dict
         Dictionary containing 'lat' and 'lon' keys, whose values are numpy.ndarrays of lat & lon for the large array.
-    
+
     Returns
     -------
     numpy.ndarray
         An array of the same dimensions as "large", with the small array plugged inside the large array.
     """
 
-    small_lat = np.round(small_coords['lat'],2)
-    small_lon = np.round(small_coords['lon'],2)
-    large_lat = np.round(large_coords['lat'],2)
-    large_lon = np.round(large_coords['lon'],2)
+    small_lat = np.round(small_coords['lat'], 2)
+    small_lon = np.round(small_coords['lon'], 2)
+    large_lat = np.round(large_coords['lat'], 2)
+    large_lon = np.round(large_coords['lon'], 2)
 
     small_minlat = np.nanmin(small_lat)
     small_maxlat = np.nanmax(small_lat)
     small_minlon = np.nanmin(small_lon)
     small_maxlon = np.nanmax(small_lon)
 
     if small_minlat in large_lat:
-        minlat = np.where(large_lat==small_minlat)[0][0]
+        minlat = np.where(large_lat == small_minlat)[0][0]
     else:
         minlat = min(large_lat)
     if small_maxlat in large_lat:
-        maxlat = np.where(large_lat==small_maxlat)[0][0]
+        maxlat = np.where(large_lat == small_maxlat)[0][0]
     else:
         maxlat = max(large_lat)
     if small_minlon in large_lon:
-        minlon = np.where(large_lon==small_minlon)[0][0]
+        minlon = np.where(large_lon == small_minlon)[0][0]
     else:
         minlon = min(large_lon)
     if small_maxlon in large_lon:
-        maxlon = np.where(large_lon==small_maxlon)[0][0]
+        maxlon = np.where(large_lon == small_maxlon)[0][0]
     else:
         maxlon = max(large_lon)
 
-    large[minlat:maxlat+1,minlon:maxlon+1] = small
+    large[minlat:maxlat+1, minlon:maxlon+1] = small
 
     return large
 
-def calc_distance(lats2d,lons2d,lat,lon):
-    
+
+def calc_distance(lats2d, lons2d, lat, lon):
     r"""
     Calculates distance (km) for each gridpoint in a 2D array from a provided coordinate.
-    
+
     Parameters
     ----------
     lats2d : numpy.ndarray
         2D array containing latitude in degrees
     lons2d : numpy.ndarray
         2D array containing longitude in degrees
     lat : float or int
         Latitude of requested coordinate
     lon : float or int
         Longitude of requested coordinate
-    
+
     Returns
     -------
     list
         First element returned is an empty 2D array of dimension (lons,lats). Second element returned is an array of the same shape containing the distance from the requested coordinate in km.
     """
-    
-    #Define empty array
+
+    # Define empty array
     return_arr = np.zeros((lats2d.shape))
-    
-    #Calculate distance from lat/lon at each gridpoint
+
+    # Calculate distance from lat/lon at each gridpoint
     r_earth = 6.371 * 10**6
-    dlat = np.subtract(np.radians(lats2d),np.radians(lat))
-    dlon = np.subtract(np.radians(lons2d),np.radians(lon))
-    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats2d)) * np.cos(np.radians(lat)) * np.sin(dlon/2) * np.sin(dlon/2)
-    c = 2 * np.arctan(np.sqrt(a), np.sqrt(1-a));
+    dlat = np.subtract(np.radians(lats2d), np.radians(lat))
+    dlon = np.subtract(np.radians(lons2d), np.radians(lon))
+    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats2d)) * \
+        np.cos(np.radians(lat)) * np.sin(dlon/2) * np.sin(dlon/2)
+    c = 2 * np.arctan(np.sqrt(a), np.sqrt(1-a))
     dist = (r_earth * c)/1000.0
-    
+
     return return_arr, dist
 
-def add_radius(lats2d,lons2d,lat,lon,rad):
-    
+
+def add_radius(lats2d, lons2d, lat, lon, rad):
     r"""
     Determines whether a requested coordinate is within a specified radius in a 2D array.
-    
+
     Parameters
     ----------
     lats : numpy.ndarray
         2D array containing latitude in degrees
     lons : numpy.ndarray
         2D array containing longitude in degrees
     lat : float or int
         Latitude of requested coordinate
     lon : float or int
         Longitude of requested coordinate
     rad : float or int
         Requested radius in kilometers
     res : float or int, optional
         Resolution of grid to create. Default is 0.25 degrees.
-    
+
     Returns
     -------
     numpy.ndarray
         2D array containing 1 where the requested coordinate is within the requested radius, and 0 otherwise.
     """
-    
-    #Define empty array
+
+    # Define empty array
     return_arr = np.zeros((lats2d.shape))
-    
-    #Calculate distance from lat/lon at each gridpoint
+
+    # Calculate distance from lat/lon at each gridpoint
     r_earth = 6.371 * 10**6
-    dlat = np.subtract(np.radians(lats2d),np.radians(lat))
-    dlon = np.subtract(np.radians(lons2d),np.radians(lon))
+    dlat = np.subtract(np.radians(lats2d), np.radians(lat))
+    dlon = np.subtract(np.radians(lons2d), np.radians(lon))
 
-    a = np.sin(dlat*0.5) * np.sin(dlat*0.5) + np.cos(np.radians(lats2d)) * np.cos(np.radians(lat)) * np.sin(dlon*0.5) * np.sin(dlon*0.5)
-    c = 2 * np.arctan(np.sqrt(a), np.sqrt(1-a));
+    a = np.sin(dlat*0.5) * np.sin(dlat*0.5) + np.cos(np.radians(lats2d)) * \
+        np.cos(np.radians(lat)) * np.sin(dlon*0.5) * np.sin(dlon*0.5)
+    c = 2 * np.arctan(np.sqrt(a), np.sqrt(1-a))
     dist = (r_earth * c) * 0.001
-    
-    #Mask out values less than radius
+
+    # Mask out values less than radius
     return_arr[dist > rad] = 0
     return_arr[dist <= rad] = 1
     return return_arr
 
-def add_radius_quick(lats,lons,lat,lon,rad,res=0.25):
-    
+
+def add_radius_quick(lats, lons, lat, lon, rad, res=0.25):
     r"""
     Determines whether a requested coordinate is within a specified radius in a 2D array. Performs a faster calculation than the default "add_radius" function that is a good approximation outside of the poles.
-    
+
     Parameters
     ----------
     lats : numpy.ndarray
         1D array containing latitude in degrees
     lons : numpy.ndarray
         1D array containing longitude in degrees
     lat : float or int
         Latitude of requested coordinate
     lon : float or int
         Longitude of requested coordinate
     rad : float or int
         Requested radius in kilometers
     res : float or int, optional
         Resolution of grid to create. Default is 0.25 degrees.
-    
+
     Returns
     -------
     numpy.ndarray
         2D array containing 1 where the requested coordinate is within the requested radius, and 0 otherwise.
     """
 
     lons2d, lats2d = np.meshgrid(lons, lats)
     return_arr = np.zeros((lats2d.shape))
     dist = np.zeros((lats2d.shape)) + 9999
-    if lon == None or lat == None: return return_arr
+    if lon is None or lat is None:
+        return return_arr
 
-    new_lats = np.arange(np.round(lat-5),np.round(lat+5+res),res)
+    new_lats = np.arange(np.round(lat-5), np.round(lat+5+res), res)
     if np.nanmin(lats) >= 0:
-        if np.nanmin(new_lats) < 0: new_lats = np.arange(0,np.round(lat+5+res),res)
+        if np.nanmin(new_lats) < 0:
+            new_lats = np.arange(0, np.round(lat+5+res), res)
     else:
-        if np.nanmax(new_lats) > 0: new_lats = np.arange(np.round(lat-5),0+res,res)
-    new_lons = np.arange(np.round(lon-10),np.round(lon+10+res),res)
+        if np.nanmax(new_lats) > 0:
+            new_lats = np.arange(np.round(lat-5), 0+res, res)
+    new_lons = np.arange(np.round(lon-10), np.round(lon+10+res), res)
     new_lons_2d, new_lats_2d = np.meshgrid(new_lons, new_lats)
-    new_arr, new_dist = calc_distance(new_lats_2d,new_lons_2d,lat,lon)
+    new_arr, new_dist = calc_distance(new_lats_2d, new_lons_2d, lat, lon)
 
-    return_arr = plug_array(new_arr,return_arr,{'lat':new_lats,'lon':new_lons},{'lat':lats,'lon':lons})
-    return_dist = plug_array(new_dist,dist,{'lat':new_lats,'lon':new_lons},{'lat':lats,'lon':lons})	
+    return_arr = plug_array(new_arr, return_arr, {
+                            'lat': new_lats, 'lon': new_lons}, {'lat': lats, 'lon': lons})
 
-    #Mask out values less than radius
+    # Mask out values less than radius
     return_arr[dist > rad] = 0
     return_arr[dist <= rad] = 1
     return return_arr
 
+
 def all_nan(arr):
-    
     r"""
     Determine whether the entire array is filled with NaNs.
-    
+
     Parameters
     ----------
     arr : list or numpy.ndarray
         List or array to be checked.
-    
+
     Returns
     -------
     bool
         Returns whether the array is filled with all NaNs.
     """
-    
-    #Convert array to numpy array
+
+    # Convert array to numpy array
     arr_copy = np.array(arr)
-    
-    #Check if there are non-NaN values in the array
+
+    # Check if there are non-NaN values in the array
     if len(arr_copy[~np.isnan(arr_copy)]) == 0:
         return True
     else:
         return False
 
-def category_label_to_wind(category):
+def is_number(value):
+    r"""
+    Determine whether the provided value is a number.
     
+    Parameters
+    ----------
+    value
+        A value to check the type of.
+    
+    Returns
+    -------
+    bool
+        Returns True if the value is a number, otherwise False.
+    """
+    
+    return isinstance(value, (int, np.integer, float, np.floating))
+
+def category_label_to_wind(category):
     r"""
     Convert Saffir-Simpson Hurricane Wind Scale category label to minimum threshold sustained wind speed in knots. Internal function.
-    
+
     Parameters
     ----------
     category : int
         Saffir-Simpson Hurricane Wind Scale category. Use 0 for tropical storm, -1 for tropical depression.
-    
+
     Returns
     -------
     int
         Sustained wind speed in knots corresponding to the minimum threshold of the requested category.
     """
-    
-    #Convert category to lowercase
+
+    # Convert category to lowercase
     category_lowercase = category.lower()
-    
-    #Return thresholds based on category label
+
+    # Return thresholds based on category label
     if category_lowercase == 'td' or category_lowercase == 'sd':
         return category_to_wind(0) - 1
     elif category_lowercase == 'ts' or category_lowercase == 'ss':
         return category_to_wind(0)
     elif category_lowercase == 'c1':
         return category_to_wind(1)
     elif category_lowercase == 'c2':
@@ -1484,16 +1709,16 @@
     elif category_lowercase == 'c3':
         return category_to_wind(3)
     elif category_lowercase == 'c4':
         return category_to_wind(4)
     else:
         return category_to_wind(5)
 
-def dynamic_map_extent(min_lon,max_lon,min_lat,max_lat,recon=False):
 
+def dynamic_map_extent(min_lon, max_lon, min_lat, max_lat, recon=False):
     r"""
     Sets up a dynamic map extent with an aspect ratio of 3:2 given latitude and longitude bounds.
 
     Parameters:
     -----------
     min_lon : float
         Minimum longitude bound.
@@ -1508,193 +1733,192 @@
 
     Returns:
     --------
     list
         List containing new west, east, north, south map bounds, respectively.
     """
 
-    #Get lat/lon bounds
+    # Get lat/lon bounds
     bound_w = min_lon+0.0
     bound_e = max_lon+0.0
     bound_s = min_lat+0.0
     bound_n = max_lat+0.0
 
-    #If only one coordinate point, artificially induce a spread
+    # If only one coordinate point, artificially induce a spread
     if bound_w == bound_e:
         bound_w = bound_w - 0.6
         bound_e = bound_e + 0.6
     if bound_s == bound_n:
         bound_n = bound_n + 0.6
         bound_s = bound_s - 0.6
 
-    #Function for fixing map ratio
-    def fix_map_ratio(bound_w,bound_e,bound_n,bound_s,nthres=1.45):
+    # Function for fixing map ratio
+    def fix_map_ratio(bound_w, bound_e, bound_n, bound_s, nthres=1.45):
         xrng = abs(bound_w-bound_e)
         yrng = abs(bound_n-bound_s)
         diff = float(xrng) / float(yrng)
-        if diff < nthres: #plot too tall, need to make it wider
+        if diff < nthres:  # plot too tall, need to make it wider
             goal_diff = nthres * (yrng)
             factor = abs(xrng - goal_diff) / 2.0
             bound_w = bound_w - factor
             bound_e = bound_e + factor
-        elif diff > nthres: #plot too wide, need to make it taller
+        elif diff > nthres:  # plot too wide, need to make it taller
             goal_diff = xrng / nthres
             factor = abs(yrng - goal_diff) / 2.0
             bound_s = bound_s - factor
             bound_n = bound_n + factor
-        return bound_w,bound_e,bound_n,bound_s
+        return bound_w, bound_e, bound_n, bound_s
 
-    #First round of fixing ratio
-    bound_w,bound_e,bound_n,bound_s = fix_map_ratio(bound_w,bound_e,bound_n,bound_s,1.45)
+    # First round of fixing ratio
+    bound_w, bound_e, bound_n, bound_s = fix_map_ratio(
+        bound_w, bound_e, bound_n, bound_s, 1.45)
 
-    #Adjust map width depending on extent of storm
+    # Adjust map width depending on extent of storm
     xrng = abs(bound_e-bound_w)
     yrng = abs(bound_n-bound_s)
     factor = 0.1
-    if min(xrng,yrng) < 15.0:
+    if min(xrng, yrng) < 15.0:
         factor = 0.2
-    if min(xrng,yrng) < 12.0:
+    if min(xrng, yrng) < 12.0:
         factor = 0.4
-    if min(xrng,yrng) < 10.0:
+    if min(xrng, yrng) < 10.0:
         factor = 0.6
-    if min(xrng,yrng) < 8.0:
+    if min(xrng, yrng) < 8.0:
         factor = 0.75
-    if min(xrng,yrng) < 6.0:
+    if min(xrng, yrng) < 6.0:
         factor = 0.9
-    if recon: factor = factor * 0.3
+    if recon:
+        factor = factor * 0.3
     bound_w = bound_w-(xrng*factor)
     bound_e = bound_e+(xrng*factor)
     bound_s = bound_s-(yrng*factor)
     bound_n = bound_n+(yrng*factor)
 
-    #Second round of fixing ratio
-    bound_w,bound_e,bound_n,bound_s = fix_map_ratio(bound_w,bound_e,bound_n,bound_s,1.45)
+    # Second round of fixing ratio
+    bound_w, bound_e, bound_n, bound_s = fix_map_ratio(
+        bound_w, bound_e, bound_n, bound_s, 1.45)
 
-    #Return map bounds
-    return bound_w,bound_e,bound_s,bound_n
+    # Return map bounds
+    return bound_w, bound_e, bound_s, bound_n
+
+
+def read_url(url, split=True, subsplit=True):
 
-def read_url(url,split=True,subsplit=True):
-    
     f = urllib.request.urlopen(url)
     content = f.read()
     content = content.decode("utf-8")
-    if split: content = content.split("\n")
-    if subsplit: content = [(i.replace(" ","")).split(",") for i in content]
+    if split:
+        content = content.split("\n")
+    if subsplit:
+        content = [(i.replace(" ", "")).split(",") for i in content]
     f.close()
-    
+
     return content
 
+
 class Distance:
-    
-    def __init__(self,dist,units='kilometers'):
-        
+
+    def __init__(self, dist, units='kilometers'):
+
         # Conversion fractions (numerator_denominator)
         mi_km = 0.621371
         nmi_km = 0.539957
         m_km = 1000.
         ft_km = 3280.84
-        
-        if units in ['kilometers','km']:
+
+        if units in ['kilometers', 'km']:
             self.kilometers = dist
-        elif units in ['miles','mi']:
+        elif units in ['miles', 'mi']:
             self.kilometers = dist / mi_km
-        elif units in ['nauticalmiles','nautical','nmi']:
+        elif units in ['nauticalmiles', 'nautical', 'nmi']:
             self.kilometers = dist / nmi_km
-        elif units in ['feet','ft']:
+        elif units in ['feet', 'ft']:
             self.kilometers = dist / ft_km
-        elif units in ['meters','m']:
+        elif units in ['meters', 'm']:
             self.kilometers = dist / m_km
         self.miles = self.kilometers * mi_km
         self.nautical = self.kilometers * nmi_km
         self.meters = self.kilometers * m_km
         self.feet = self.kilometers * ft_km
 
 
 class great_circle(Distance):
 
     def __init__(self, start_point, end_point, **kwargs):
         r"""
-        
+
         Parameters
         ----------
         start_point : tuple or int
             Starting pair of coordinates, in order of (latitde, longitude)
         end_point : tuple or int
             Starting pair of coordinates, in order of (latitde, longitude)
         radius : float, optional
             Radius of Earth. Default is 6371.009 km.
-        
+
         Returns
         -------
         great_circle
             Instance of a great_circle object. To retrieve the distance, add the requested unit at the end (e.g., "great_circle(start,end).kilometers").
-        
+
         Notes
         -----
         Use spherical geometry to calculate the surface distance between two points. Uses the mean earth radius as defined by the International Union of Geodesy and Geophysics, approx 6371.009 km (for WGS-84), resulting in an error of up to about 0.5%. Otherwise set which radius of the earth to use by specifying a 'radius' keyword argument. It must be in kilometers.
         """
-        
-        #Set Earth's radius
+
+        # Set Earth's radius
         self.RADIUS = kwargs.pop('radius', 6371.009)
-        
-        #Compute 
-        dist = self.measure(start_point,end_point)
-        Distance.__init__(self,dist,units='kilometers')
+
+        # Compute
+        dist = self.measure(start_point, end_point)
+        Distance.__init__(self, dist, units='kilometers')
 
     def measure(self, start_point, end_point):
-        
-        #Retrieve latitude and longitude coordinates from input pairs
-        lat1, lon1 = math.radians(start_point[0]), math.radians(start_point[1]%360)
-        lat2, lon2 = math.radians(end_point[0]), math.radians(end_point[1]%360)
 
-        #Compute sin and cos of coordinates
+        # Retrieve latitude and longitude coordinates from input pairs
+        lat1, lon1 = math.radians(
+            start_point[0]), math.radians(start_point[1] % 360)
+        lat2, lon2 = math.radians(
+            end_point[0]), math.radians(end_point[1] % 360)
+
+        # Compute sin and cos of coordinates
         sin_lat1, cos_lat1 = math.sin(lat1), math.cos(lat1)
         sin_lat2, cos_lat2 = math.sin(lat2), math.cos(lat2)
 
-        #Compute sin and cos of delta longitude
+        # Compute sin and cos of delta longitude
         delta_lon = lon2 - lon1
         cos_delta_lon, sin_delta_lon = math.cos(delta_lon), math.sin(delta_lon)
 
-        #Compute great circle distance
+        # Compute great circle distance
         d = math.atan2(math.sqrt((cos_lat2 * sin_delta_lon) ** 2 +
                        (cos_lat1 * sin_lat2 -
                         sin_lat1 * cos_lat2 * cos_delta_lon) ** 2),
-                sin_lat1 * sin_lat2 + cos_lat1 * cos_lat2 * cos_delta_lon)
+                       sin_lat1 * sin_lat2 + cos_lat1 * cos_lat2 * cos_delta_lon)
 
-        #Return great circle distance
+        # Return great circle distance
         return self.RADIUS * d
 
+
 r"""
-This is a modified version of Cartopy's shapereader functionality, specified to directly read in an already-existing
-Shapely object as opposed to expecting a local shapefile to be read in.
+The two classes below are a modified version of Cartopy's shapereader functionality, specified to directly read in an already-existing Shapely object as opposed to expecting a local shapefile to be read in.
 """
-import shapely.geometry as sgeom
-import shapefile
 
 class Record:
     """
-    A single logical entry from a shapefile, combining the attributes with
-    their associated geometry.
-
+    A single logical entry from a shapefile, combining the attributes with their associated geometry. Adapted from Cartopy's Record class.
     """
+
     def __init__(self, shape, attributes, fields):
         self._shape = shape
-
         self._bounds = None
-        # if the record defines a bbox, then use that for the shape's bounds,
-        # rather than using the full geometry in the bounds property
         if hasattr(shape, 'bbox'):
             self._bounds = tuple(shape.bbox)
 
         self._geometry = None
-        """The cached geometry instance for this Record."""
-
         self.attributes = attributes
-        """A dictionary mapping attribute names to attribute values."""
-
         self._fields = fields
 
     def __repr__(self):
         return f'<Record: {self.geometry!r}, {self.attributes!r}, <fields>>'
 
     def __str__(self):
         return f'Record({self.geometry}, {self.attributes}, <fields>)'
@@ -1718,61 +1942,48 @@
         shapefile.
 
         """
         if not self._geometry and self._shape.shapeType != shapefile.NULL:
             self._geometry = sgeom.shape(self._shape)
         return self._geometry
 
+
 class BasicReader:
     """
-    Provide an interface for accessing the contents of a shapefile.
-
-    The primary methods used on a Reader instance are
-    :meth:`~Reader.records` and :meth:`~Reader.geometries`.
-
+    This is a modified version of Cartopy's BasicReader class. This allows to read in a shapefile fetched online without it being stored as a file locally.
     """
+
     def __init__(self, reader):
         # Validate the filename/shapefile
         self._reader = reader
         if reader.shp is None or reader.shx is None or reader.dbf is None:
-            raise ValueError("Incomplete shapefile definition "
-                             "in '%s'." % filename)
+            raise ValueError("Unable to open shapefile")
 
         self._fields = self._reader.fields
 
-
     def close(self):
         return self._reader.close()
 
     def __len__(self):
         return len(self._reader)
 
     def geometries(self):
         """
         Return an iterator of shapely geometries from the shapefile.
-
-        This interface is useful for accessing the geometries of the
-        shapefile where knowledge of the associated metadata is not necessary.
-        In the case where further metadata is needed use the
-        :meth:`~Reader.records`
-        interface instead, extracting the geometry from the record with the
-        :meth:`~Record.geometry` method.
-
         """
         to_return = []
         for shape in self._reader.iterShapes():
             # Skip the shape that can not be represented as geometry.
             if shape.shapeType != shapefile.NULL:
                 to_return.append(sgeom.shape(shape))
         return to_return
 
     def records(self):
         """
         Return an iterator of :class:`~Record` instances.
-
         """
         # Ignore the "DeletionFlag" field which always comes first
         to_return = []
         fields = self._reader.fields[1:]
         for shape_record in self._reader.iterShapeRecords():
             attributes = shape_record.record.as_dict()
             to_return.append(Record(shape_record.shape, attributes, fields))
```

### Comparing `tropycal-0.6.1/src/tropycal.egg-info/PKG-INFO` & `tropycal-1.0/src/tropycal.egg-info/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tropycal
-Version: 0.6.1
+Version: 1.0
 Summary: Package for retrieving and analyzing tropical cyclone data
 Home-page: https://github.com/tropycal/tropycal
 Author: Tomer Burg, Sam Lillo
 Project-URL: Documentation, https://tropycal.github.io/tropycal/
 Project-URL: Source Code, https://github.com/tropycal/tropycal
 Keywords: meteorology,weather
 Platform: any
@@ -18,15 +18,15 @@
 License-File: LICENSE
 
 # Tropycal
 Tropycal is a Python package intended to simplify the process of retrieving and analyzing tropical cyclone data, both for past storms and in real time, and is geared towards the research and operational meteorology sectors.
 
 Tropycal can read in HURDAT2 and IBTrACS reanalysis data and operational National Hurricane Center (NHC) Best Track data and conform them to the same format, which can be used to perform climatological, seasonal and individual storm analyses. For each individual storm, operational NHC and model forecasts, aircraft reconnaissance data, rainfall data, and any associated tornado activity can be retrieved and plotted.
 
-The latest version of Tropycal is v0.6.1.
+The latest version of Tropycal is v1.0.
 
 ## Installation
 
 
 ### Conda
 
 The currently recommended method of installation is via conda:
```

### Comparing `tropycal-0.6.1/src/tropycal.egg-info/SOURCES.txt` & `tropycal-1.0/src/tropycal.egg-info/SOURCES.txt`

 * *Files 10% similar despite different names*

```diff
@@ -68,17 +68,19 @@
 docs/options/gridded_stats.rst
 docs/options/map_prop.rst
 docs/samples/tracks.TrackDataset.rst
 docs/samples/tracks.storm.rst
 docs/samples/tracks.tornado.rst
 examples/README.txt
 examples/analogs.py
+examples/cartopy.py
 examples/customize_storm.py
 examples/realtime.py
 examples/recon.py
+examples/ships.py
 examples/tc_rainfall.py
 examples/tracks.TrackDataset.py
 examples/tracks.season.py
 examples/tracks.storm.py
 examples/tracks.tornado.py
 src/tropycal/__init__.py
 src/tropycal/_version.py
@@ -97,22 +99,32 @@
 src/tropycal/realtime/realtime.py
 src/tropycal/realtime/storm.py
 src/tropycal/recon/__init__.py
 src/tropycal/recon/dataset.py
 src/tropycal/recon/plot.py
 src/tropycal/recon/realtime.py
 src/tropycal/recon/tools.py
+src/tropycal/ships/__init__.py
+src/tropycal/ships/ships.py
 src/tropycal/tornado/__init__.py
 src/tropycal/tornado/dataset.py
 src/tropycal/tornado/plot.py
 src/tropycal/tornado/tools.py
 src/tropycal/tracks/__init__.py
 src/tropycal/tracks/dataset.py
 src/tropycal/tracks/plot.py
 src/tropycal/tracks/season.py
 src/tropycal/tracks/storm.py
 src/tropycal/tracks/tools.py
+src/tropycal/tracks/data/catarina.csv
+src/tropycal/tracks/data/pacific_2006.csv
 src/tropycal/utils/__init__.py
 src/tropycal/utils/cartopy_utils.py
 src/tropycal/utils/colors.py
 src/tropycal/utils/generic_utils.py
-tests/tracks.py
+tests/data/sample_hurdat.txt
+tests/data/sample_storm.txt
+tests/ships/test_ships.py
+tests/tracks/test_dataset.py
+tests/tracks/test_season.py
+tests/tracks/test_storm.py
+tests/utils/test_utils.py
```

