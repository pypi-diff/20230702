# Comparing `tmp/tfrecords-0.2.8-cp39-cp39-win_amd64.whl.zip` & `tmp/tfrecords-0.2.9-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,36 +1,37 @@
-Zip file size: 6584733 bytes, number of entries: 34
--rw-rw-rw-  2.0 fat     8155 b- defN 23-Jul-02 03:06 tfrecords/README.md
--rw-rw-rw-  2.0 fat       69 b- defN 23-Jul-02 03:06 tfrecords/__init__.py
--rw-rw-rw-  2.0 fat       36 b- defN 23-Jul-02 03:06 tfrecords/version_config.py
--rw-rw-rw-  2.0 fat      104 b- defN 23-Jul-02 03:06 tfrecords/lib/__init__.py
--rw-rw-rw-  2.0 fat 18535424 b- defN 23-Jul-02 03:07 tfrecords/lib/arrow_cc.pyd
--rw-rw-rw-  2.0 fat  2313216 b- defN 23-Jul-02 03:07 tfrecords/lib/tfrecords_cc.pyd
--rw-rw-rw-  2.0 fat      255 b- defN 23-Jul-02 03:06 tfrecords/python/__init__.py
--rw-rw-rw-  2.0 fat       99 b- defN 23-Jul-02 03:06 tfrecords/python/framework/__init__.py
--rw-rw-rw-  2.0 fat     1003 b- defN 23-Jul-02 03:06 tfrecords/python/framework/errors.py
--rw-rw-rw-  2.0 fat    16075 b- defN 23-Jul-02 03:06 tfrecords/python/framework/errors_impl.py
--rw-rw-rw-  2.0 fat      123 b- defN 23-Jul-02 03:06 tfrecords/python/io/__init__.py
--rw-rw-rw-  2.0 fat    21638 b- defN 23-Jul-02 03:06 tfrecords/python/io/arrow.py
--rw-rw-rw-  2.0 fat    28567 b- defN 23-Jul-02 03:06 tfrecords/python/io/file_io.py
--rw-rw-rw-  2.0 fat    27406 b- defN 23-Jul-02 03:06 tfrecords/python/io/file_io_test.py
--rw-rw-rw-  2.0 fat      701 b- defN 23-Jul-02 03:06 tfrecords/python/io/gfile.py
--rw-rw-rw-  2.0 fat     7682 b- defN 23-Jul-02 03:06 tfrecords/python/io/tf_leveldb.py
--rw-rw-rw-  2.0 fat     6101 b- defN 23-Jul-02 03:06 tfrecords/python/io/tf_lmdb.py
--rw-rw-rw-  2.0 fat    11427 b- defN 23-Jul-02 03:06 tfrecords/python/io/tf_record.py
--rw-rw-rw-  2.0 fat    24321 b- defN 23-Jul-02 03:06 tfrecords/python/io/tf_record_test.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-02 03:06 tfrecords/python/util/__init__.py
--rw-rw-rw-  2.0 fat     6636 b- defN 23-Jul-02 03:06 tfrecords/python/util/compat.py
--rw-rw-rw-  2.0 fat     9664 b- defN 23-Jul-02 03:06 tfrecords/python/util/tf_decorator.py
--rw-rw-rw-  2.0 fat    14016 b- defN 23-Jul-02 03:06 tfrecords/python/util/tf_export.py
--rw-rw-rw-  2.0 fat    15675 b- defN 23-Jul-02 03:06 tfrecords/python/util/tf_inspect.py
--rw-rw-rw-  2.0 fat       77 b- defN 23-Jul-02 03:06 tfrecords/testing/__init__.py
--rw-rw-rw-  2.0 fat     1022 b- defN 23-Jul-02 03:06 tfrecords/testing/demo_arrow.py
--rw-rw-rw-  2.0 fat     1390 b- defN 23-Jul-02 03:06 tfrecords/testing/demo_leveldb.py
--rw-rw-rw-  2.0 fat     1650 b- defN 23-Jul-02 03:06 tfrecords/testing/demo_lmdb.py
--rw-rw-rw-  2.0 fat     1104 b- defN 23-Jul-02 03:06 tfrecords/testing/demo_parquet.py
--rw-rw-rw-  2.0 fat     2257 b- defN 23-Jul-02 03:06 tfrecords/testing/demo_record.py
--rw-rw-rw-  2.0 fat     9513 b- defN 23-Jul-02 03:07 tfrecords-0.2.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Jul-02 03:07 tfrecords-0.2.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 23-Jul-02 03:07 tfrecords-0.2.8.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2950 b- defN 23-Jul-02 03:07 tfrecords-0.2.8.dist-info/RECORD
-34 files, 21068466 bytes uncompressed, 6579989 bytes compressed:  68.8%
+Zip file size: 6587000 bytes, number of entries: 35
+-rw-rw-rw-  2.0 fat     8979 b- defN 23-Jul-02 11:26 tfrecords/README.md
+-rw-rw-rw-  2.0 fat       69 b- defN 23-Jul-02 11:26 tfrecords/__init__.py
+-rw-rw-rw-  2.0 fat       36 b- defN 23-Jul-02 11:26 tfrecords/version_config.py
+-rw-rw-rw-  2.0 fat      104 b- defN 23-Jul-02 11:26 tfrecords/lib/__init__.py
+-rw-rw-rw-  2.0 fat 18538496 b- defN 23-Jul-02 11:26 tfrecords/lib/arrow_cc.pyd
+-rw-rw-rw-  2.0 fat  2313216 b- defN 23-Jul-02 11:26 tfrecords/lib/tfrecords_cc.pyd
+-rw-rw-rw-  2.0 fat      255 b- defN 23-Jul-02 11:26 tfrecords/python/__init__.py
+-rw-rw-rw-  2.0 fat       99 b- defN 23-Jul-02 11:26 tfrecords/python/framework/__init__.py
+-rw-rw-rw-  2.0 fat     1003 b- defN 23-Jul-02 11:26 tfrecords/python/framework/errors.py
+-rw-rw-rw-  2.0 fat    16075 b- defN 23-Jul-02 11:26 tfrecords/python/framework/errors_impl.py
+-rw-rw-rw-  2.0 fat      123 b- defN 23-Jul-02 11:26 tfrecords/python/io/__init__.py
+-rw-rw-rw-  2.0 fat    25785 b- defN 23-Jul-02 11:26 tfrecords/python/io/arrow.py
+-rw-rw-rw-  2.0 fat    28567 b- defN 23-Jul-02 11:26 tfrecords/python/io/file_io.py
+-rw-rw-rw-  2.0 fat    27406 b- defN 23-Jul-02 11:26 tfrecords/python/io/file_io_test.py
+-rw-rw-rw-  2.0 fat      701 b- defN 23-Jul-02 11:26 tfrecords/python/io/gfile.py
+-rw-rw-rw-  2.0 fat     7682 b- defN 23-Jul-02 11:26 tfrecords/python/io/tf_leveldb.py
+-rw-rw-rw-  2.0 fat     6101 b- defN 23-Jul-02 11:26 tfrecords/python/io/tf_lmdb.py
+-rw-rw-rw-  2.0 fat    11427 b- defN 23-Jul-02 11:26 tfrecords/python/io/tf_record.py
+-rw-rw-rw-  2.0 fat    24321 b- defN 23-Jul-02 11:26 tfrecords/python/io/tf_record_test.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jul-02 11:26 tfrecords/python/util/__init__.py
+-rw-rw-rw-  2.0 fat     6636 b- defN 23-Jul-02 11:26 tfrecords/python/util/compat.py
+-rw-rw-rw-  2.0 fat     9664 b- defN 23-Jul-02 11:26 tfrecords/python/util/tf_decorator.py
+-rw-rw-rw-  2.0 fat    14016 b- defN 23-Jul-02 11:26 tfrecords/python/util/tf_export.py
+-rw-rw-rw-  2.0 fat    15675 b- defN 23-Jul-02 11:26 tfrecords/python/util/tf_inspect.py
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Jul-02 11:26 tfrecords/testing/__init__.py
+-rw-rw-rw-  2.0 fat     1057 b- defN 23-Jul-02 11:26 tfrecords/testing/demo_arrow.py
+-rw-rw-rw-  2.0 fat     1338 b- defN 23-Jul-02 11:26 tfrecords/testing/demo_arrow_file.py
+-rw-rw-rw-  2.0 fat     1390 b- defN 23-Jul-02 11:26 tfrecords/testing/demo_leveldb.py
+-rw-rw-rw-  2.0 fat     1650 b- defN 23-Jul-02 11:26 tfrecords/testing/demo_lmdb.py
+-rw-rw-rw-  2.0 fat     1104 b- defN 23-Jul-02 11:26 tfrecords/testing/demo_parquet.py
+-rw-rw-rw-  2.0 fat     2257 b- defN 23-Jul-02 11:26 tfrecords/testing/demo_record.py
+-rw-rw-rw-  2.0 fat    10337 b- defN 23-Jul-02 11:26 tfrecords-0.2.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jul-02 11:26 tfrecords-0.2.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       10 b- defN 23-Jul-02 11:26 tfrecords-0.2.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3044 b- defN 23-Jul-02 11:26 tfrecords-0.2.9.dist-info/RECORD
+35 files, 21078800 bytes uncompressed, 6582108 bytes compressed:  68.8%
```

## zipnote {}

```diff
@@ -72,32 +72,35 @@
 
 Filename: tfrecords/testing/__init__.py
 Comment: 
 
 Filename: tfrecords/testing/demo_arrow.py
 Comment: 
 
+Filename: tfrecords/testing/demo_arrow_file.py
+Comment: 
+
 Filename: tfrecords/testing/demo_leveldb.py
 Comment: 
 
 Filename: tfrecords/testing/demo_lmdb.py
 Comment: 
 
 Filename: tfrecords/testing/demo_parquet.py
 Comment: 
 
 Filename: tfrecords/testing/demo_record.py
 Comment: 
 
-Filename: tfrecords-0.2.8.dist-info/METADATA
+Filename: tfrecords-0.2.9.dist-info/METADATA
 Comment: 
 
-Filename: tfrecords-0.2.8.dist-info/WHEEL
+Filename: tfrecords-0.2.9.dist-info/WHEEL
 Comment: 
 
-Filename: tfrecords-0.2.8.dist-info/top_level.txt
+Filename: tfrecords-0.2.9.dist-info/top_level.txt
 Comment: 
 
-Filename: tfrecords-0.2.8.dist-info/RECORD
+Filename: tfrecords-0.2.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tfrecords/README.md

```diff
@@ -210,21 +210,17 @@
 test_read(db_path)
 
 ```
 
 
 ### 4. arrow demo
 
+### Stream
 ```python
 
-# -*- coding: utf-8 -*-
-# @Time:  15:32
-# @Author: tk
-# @File：demo_arrow
-
 from tfrecords.python.io.arrow import IPC_Writer,IPC_StreamReader,arrow
 
 path_file = "d:/tmp/data.arrow"
 
 def test_write():
     schema = arrow.schema([
         arrow.field('id', arrow.int32()),
@@ -240,16 +236,14 @@
     b = b.Finish().Value()
 
     table = arrow.Table.Make(schema = schema,arrays=[a,b])
     fs = IPC_Writer(path_file,schema,with_stream = True)
     fs.write_table(table)
     fs.close()
 
-
-
 def test_read():
     fs = IPC_StreamReader(path_file)
     table = fs.read_all()
     fs.close()
     print(table)
 
     col = table.GetColumnByName('text')
@@ -259,14 +253,52 @@
         print(type(x), x)
 
 
 test_write()
 test_read()
 ```
 
+### file
+```python
+from tfrecords.python.io.arrow import IPC_Writer,IPC_StreamReader,IPC_MemoryMappedFileReader,arrow
+
+path_file = "d:/tmp/data.arrow"
+
+def test_write():
+    schema = arrow.schema([
+        arrow.field('id', arrow.int32()),
+        arrow.field('text', arrow.utf8())
+    ])
+
+    a = arrow.Int32Builder()
+    a.AppendValues([0,1,4])
+    a = a.Finish().Value()
+
+    b = arrow.StringBuilder()
+    b.AppendValues(["aaaa","你是谁","张三"])
+    b = b.Finish().Value()
+
+    table = arrow.Table.Make(schema = schema,arrays=[a,b])
+    fs = IPC_Writer(path_file,schema,with_stream = False)
+    fs.write_table(table)
+    fs.close()
+
+
+def test_read():
+
+    fs = IPC_MemoryMappedFileReader(path_file)
+    for i in range(fs.num_record_batches()):
+        batch = fs.read_batch(i)
+        print(batch)
+    fs.close()
+
+
+test_write()
+test_read()
+```
 
 
 ### 4. parquet demo
 
 
 ```python
 from tfrecords.python.io.arrow import ParquetWriter,IPC_StreamReader,ParquetReader,arrow
@@ -305,8 +337,10 @@
     for i in range(text_list.length()):
         x = text_list.Value(i)
         print(type(x),x)
 
 
 test_write()
 test_read()
-```
+```
+
+
```

## tfrecords/python/io/arrow.py

```diff
@@ -7,15 +7,15 @@
 
 from ...lib.arrow_cc import arrow,parquet
 
 
 __all__ = [
     'arrow',
     'parquet',
-    'IPC_FileReader',
+    'IPC_MemoryMappedFileReader',
     'IPC_StreamReader',
     'IPC_Writer',
     'ParquetReader',
     'ParquetWriter'
 ]
 
 
@@ -75,39 +75,135 @@
 
 parquet.ReaderProperties.__repr__ = __ReaderProperties_repr
 parquet.WriterProperties.__repr__ = __WriterProperties_repr
 parquet.ArrowWriterProperties.__repr__ = __ArrowWriterProperties_repr
 
 
 
+
+def __IpcReadOptions_repr(self: arrow.ipc.IpcReadOptions):
+    rstr =  f'parquet.IpcReadOptions: \n' + \
+            f'max_recursion_depth => {self.max_recursion_depth} \n' + \
+            f'memory_pool => {self.memory_pool} \n' + \
+            f'included_fields => {self.included_fields} \n' + \
+            f'use_threads => {self.use_threads} \n' + \
+            f'pre_buffer_cache_options => {self.pre_buffer_cache_options} \n'
+
+    return rstr
+
+
+def __IpcWriteOptions_repr(self: arrow.ipc.IpcWriteOptions):
+    rstr =  f'parquet.IpcWriteOptions: \n' + \
+            f'allow_64bit => {self.allow_64bit} \n' + \
+            f'max_recursion_depth => {self.max_recursion_depth} \n' + \
+            f'alignment => {self.alignment} \n' + \
+            f'write_legacy_ipc_format => {self.write_legacy_ipc_format} \n' + \
+            f'codec => {self.codec} \n' + \
+            f'min_space_savings => {self.min_space_savings} \n' + \
+            f'use_threads => {self.use_threads} \n' + \
+            f'emit_dictionary_deltas => {self.emit_dictionary_deltas} \n' + \
+            f'unify_dictionaries => {self.unify_dictionaries} \n' + \
+            f'metadata_version => {self.metadata_version} \n'
+    return rstr
+
+def __IpcCacheOptions_repr(self: arrow.ipc.CacheOptions):
+    rstr =  f'arrow.ipc.CacheOptions: \n' + \
+            f'hole_size_limit => {self.hole_size_limit} \n' + \
+            f'range_size_limit => {self.range_size_limit} \n' + \
+            f'lazy => {self.lazy} \n'
+    return rstr
+
+
+arrow.ipc.IpcReadOptions.__repr__ = __IpcReadOptions_repr
+arrow.ipc.IpcWriteOptions.__repr__ = __IpcWriteOptions_repr
+arrow.ipc.CacheOptions.__repr__ = __IpcCacheOptions_repr
 # UNCOMPRESSED,
 # SNAPPY,
 # GZIP,
 # BROTLI,
 # ZSTD,
 # LZ4,
 # LZ4_FRAME,
 # LZO,
 # BZ2,
 # LZ4_HADOOP
 
 
+def get_ipc_read_options(max_recursion_depth=64,
+                            included_fields=None,
+                            use_threads=None,
+                            ensure_native_endian=None):
+
+    options = arrow.ipc.IpcReadOptions.Defaults()
+    if max_recursion_depth is not None:
+        options.max_recursion_depth = max_recursion_depth
+
+    if included_fields is not None:
+        options.included_fields = included_fields
+
+    if use_threads is not None:
+        options.use_threads = use_threads
+
+    if ensure_native_endian is not None:
+        options.ensure_native_endian = ensure_native_endian
+
+    return options
+
+def get_ipc_write_options(default_options):
+    options = arrow.ipc.IpcWriteOptions()
+    if default_options.get('allow_64bit',None) is not None:
+        options.allow_64bit = default_options['allow_64bit']
+    if default_options.get('max_recursion_depth', None) is not None:
+        options.max_recursion_depth = default_options['max_recursion_depth']
+    if default_options.get('alignment', None) is not None:
+        options.alignment = default_options['alignment']
+    if default_options.get('write_legacy_ipc_format', None) is not None:
+        options.write_legacy_ipc_format = default_options['write_legacy_ipc_format']
+    if default_options.get('memory_pool', None) is not None:
+        options.memory_pool = default_options['memory_pool']
+    if default_options.get('min_space_savings', None) is not None:
+        options.min_space_savings = default_options['min_space_savings']
+    if default_options.get('use_threads',None) is not None:
+        options.use_threads = default_options['use_threads']
+    if default_options.get('emit_dictionary_deltas', None) is not None:
+        options.emit_dictionary_deltas = default_options['emit_dictionary_deltas']
+    if default_options.get('unify_dictionaries', None) is not None:
+        options.unify_dictionaries = default_options['unify_dictionaries']
+    if default_options.get('max_recursion_depth', None) is not None:
+        options.max_recursion_depth = default_options['max_recursion_depth']
+    if default_options.get('metadata_version', None) is not None:
+        options.metadata_version = default_options['metadata_version']
+    return options
 
-
-class IPC_FileReader:
-    def __init__(self,filename,offset=None,length=None,file_mode=arrow.io.FileMode.READ):
+class IPC_MemoryMappedFileReader:
+    def __init__(self,filename,offset=None,length=None,file_mode=arrow.io.FileMode.READ,options: typing.Optional[typing.Dict] = None):
         if offset is not None and length is not None:
             self._sink = arrow.io.MemoryMappedFile.Open(path=filename, mode=file_mode,offset=offset,length=length).Value()
         else:
             self._sink = arrow.io.MemoryMappedFile.Open(path=filename,mode=file_mode).Value()
-        self._file_reader: arrow.RecordBatchReader = arrow.ipc.RecordBatchFileReader.Open(self._sink).Value()
+
+        default_options = dict(
+            max_recursion_depth=64,
+            included_fields=None,
+            use_threads=None,
+            ensure_native_endian=None
+        )
+        if options is not None:
+            default_options.update(options)
+
+        self._ipc_readoptions: arrow.ipc.IpcReadOptions =  get_ipc_read_options(**default_options)
+
+        self._file_reader: arrow.RecordBatchReader = arrow.ipc.RecordBatchFileReader.Open(self._sink,options=self._ipc_readoptions).Value()
 
     def __del__(self):
         self.close()
 
+    def schema(self):
+        return self._file_reader.schema()
+
     def num_record_batches(self):
         return self._file_reader.num_record_batches()
 
     def version(self):
         return self._file_reader.version()
 
     def metadata(self):
@@ -118,46 +214,83 @@
 
     def read_batch_with_metadata(self,i):
         return self._file_reader.ReadRecordBatchWithCustomMetadata(i).Value()
 
     def stats(self):
         return self._file_reader.stats()
 
+    def count_rows(self):
+        return self._file_reader.CountRows().Value()
+
+    def pre_buffer_metadata(self,indices):
+        self._file_reader.PreBufferMetadata(indices)
     def close(self):
         if self._sink is not None:
             self._sink.Close()
             self._sink = None
 
+    def get_options(self):
+        return self._ipc_readoptions
+
 class IPC_StreamReader:
-    def __init__(self,filename):
+    def __init__(self,filename,options: typing.Optional[typing.Dict] = None):
         self._sink = arrow.io.ReadableFile.Open(filename).Value()
-        self._file_reader: arrow.RecordBatchReader = arrow.ipc.RecordBatchStreamReader.Open(self._sink).Value()
+
+        default_options = dict(
+            max_recursion_depth=64,
+            included_fields=None,
+            use_threads=None,
+            ensure_native_endian=None
+        )
+        if options is not None:
+            default_options.update(options)
+
+        self._ipc_readoptions: arrow.ipc.IpcReadOptions = get_ipc_read_options(**default_options)
+
+        self._file_reader: arrow.ipc.RecordBatchStreamReader = arrow.ipc.RecordBatchStreamReader.Open(self._sink,options=self._ipc_readoptions).Value()
+
 
     def __del__(self):
         self.close()
 
     def schema(self):
-        return self._file_reader.schema().Value()
+        return self._file_reader.schema()
 
     def next(self):
         batch = self._file_reader.Next().Value()
         return batch
 
     def read_all(self):
         table = self._file_reader.ReadAll().Value()
         return table
+
+    def to_table(self):
+        table = self._file_reader.ToTable().Value()
+        return table
+
+    def read_next(self):
+        table = self._file_reader.ReadNext().Value()
+        return table
+
     def read_all_batch(self):
         table = self._file_reader.ReadAllBatch().Value()
         return table
 
     def close(self):
+        if self._file_reader is not None:
+            self._file_reader.Close()
+            self._file_reader = None
+
         if self._sink is not None:
             self._sink.Close()
             self._sink = None
 
+    def get_options(self):
+        return self._ipc_readoptions
+
 class IPC_Writer:
     def __init__(self,filename,schema,with_stream = True,metadata = None,options = None):
         if metadata is not None and with_stream:
             raise ValueError('stream is not support metadata')
 
         metata_obj = None
         if metadata is not None and len(metadata) > 0:
@@ -180,59 +313,39 @@
             emit_dictionary_deltas=False,
             unify_dictionaries=False,
             metadata_version=arrow.ipc.MetadataVersion.V5
         )
         if options is not None:
             default_options.update(options)
 
-        self.options = self._parse_options(default_options)
+        self._options = get_ipc_write_options(default_options)
+        self._sink = arrow.io.FileOutputStream.Open(filename).Value()
         if with_stream:
-            self._sink = arrow.io.FileOutputStream.Open(filename).Value()
-            self._file_writer: arrow.ipc.RecordBatchWriter = arrow.ipc.MakeStreamWriter(sink=self._sink, schema=schema,
-                                                                            options=self.options).Value()
+            self._file_writer: arrow.ipc.RecordBatchWriter = arrow.ipc.MakeStreamWriter(sink=self._sink,
+                                                                                        schema=schema,
+                                                                                        options=self._options).Value()
         else:
-            self._sink = None
-            self._file_writer: arrow.ipc.RecordBatchWriter = arrow.ipc.MakeFileWriter(sink=self._sink, schema=schema,
-                                                                          options=self.options,
-                                                                          metadata=metata_obj).Value()
+            self._file_writer: arrow.ipc.RecordBatchWriter = arrow.ipc.MakeFileWriter(sink=self._sink,
+                                                                                      schema=schema,
+                                                                                      options=self._options,
+                                                                                      metadata=metata_obj).Value()
 
     def __del__(self):
         self.close()
 
-    def _parse_options(self,default_options):
+    def get_options(self):
+        return self._options
+
 
-        options = arrow.ipc.IpcWriteOptions()
-        if default_options.get('allow_64bit',None) is not None:
-            options.allow_64bit = default_options['allow_64bit']
-        if default_options.get('max_recursion_depth', None) is not None:
-            options.max_recursion_depth = default_options['max_recursion_depth']
-        if default_options.get('alignment', None) is not None:
-            options.alignment = default_options['alignment']
-        if default_options.get('write_legacy_ipc_format', None) is not None:
-            options.write_legacy_ipc_format = default_options['write_legacy_ipc_format']
-        if default_options.get('memory_pool', None) is not None:
-            options.memory_pool = default_options['memory_pool']
-        if default_options.get('min_space_savings', None) is not None:
-            options.min_space_savings = default_options['min_space_savings']
-        if default_options.get('use_threads',None) is not None:
-            options.use_threads = default_options['use_threads']
-        if default_options.get('emit_dictionary_deltas', None) is not None:
-            options.emit_dictionary_deltas = default_options['emit_dictionary_deltas']
-        if default_options.get('unify_dictionaries', None) is not None:
-            options.unify_dictionaries = default_options['unify_dictionaries']
-        if default_options.get('max_recursion_depth', None) is not None:
-            options.max_recursion_depth = default_options['max_recursion_depth']
-        if default_options.get('metadata_version', None) is not None:
-            options.metadata_version = default_options['metadata_version']
-        return options
 
     def close(self):
         if self._file_writer is not None:
             self._file_writer.Close()
             self._file_writer = None
+
         if self._sink is not None:
             self._sink.Close()
             self._sink = None
 
     def write_table(self,table):
         status = self._file_writer.WriteTable(table)
         return status.ok()
```

## tfrecords/testing/demo_arrow.py

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 # @Time:  15:32
 # @Author: tk
 # @File：demo_arrow
 
-from tfrecords.python.io.arrow import IPC_Writer,IPC_StreamReader,arrow
+from tfrecords.python.io.arrow import IPC_Writer,IPC_StreamReader,IPC_MemoryMappedFileReader,arrow
 
 path_file = "d:/tmp/data.arrow"
 
 def test_write():
     schema = arrow.schema([
         arrow.field('id', arrow.int32()),
         arrow.field('text', arrow.utf8())
@@ -37,9 +37,12 @@
     col = table.GetColumnByName('text')
     text_list = col.chunk(0)
     for i in range(text_list.length()):
         x = text_list.Value(i)
         print(type(x), x)
 
 
+
+
 test_write()
-test_read()
+test_read()
+
```

## Comparing `tfrecords-0.2.8.dist-info/METADATA` & `tfrecords-0.2.9.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tfrecords
-Version: 0.2.8
+Version: 0.2.9
 Summary: tfrecords: fast and simple reader and writer
 Home-page: https://github.com/ssbuild/tfrecords
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache 2.0
 Keywords: tfrecords,tfrecords,tfrecord,records,datasets
 Platform: win32_AMD64
@@ -242,21 +242,17 @@
 test_read(db_path)
 
 ```
 
 
 ### 4. arrow demo
 
+### Stream
 ```python
 
-# -*- coding: utf-8 -*-
-# @Time:  15:32
-# @Author: tk
-# @File：demo_arrow
-
 from tfrecords.python.io.arrow import IPC_Writer,IPC_StreamReader,arrow
 
 path_file = "d:/tmp/data.arrow"
 
 def test_write():
     schema = arrow.schema([
         arrow.field('id', arrow.int32()),
@@ -272,16 +268,14 @@
     b = b.Finish().Value()
 
     table = arrow.Table.Make(schema = schema,arrays=[a,b])
     fs = IPC_Writer(path_file,schema,with_stream = True)
     fs.write_table(table)
     fs.close()
 
-
-
 def test_read():
     fs = IPC_StreamReader(path_file)
     table = fs.read_all()
     fs.close()
     print(table)
 
     col = table.GetColumnByName('text')
@@ -291,14 +285,52 @@
         print(type(x), x)
 
 
 test_write()
 test_read()
 ```
 
+### file
+```python
+from tfrecords.python.io.arrow import IPC_Writer,IPC_StreamReader,IPC_MemoryMappedFileReader,arrow
+
+path_file = "d:/tmp/data.arrow"
+
+def test_write():
+    schema = arrow.schema([
+        arrow.field('id', arrow.int32()),
+        arrow.field('text', arrow.utf8())
+    ])
+
+    a = arrow.Int32Builder()
+    a.AppendValues([0,1,4])
+    a = a.Finish().Value()
+
+    b = arrow.StringBuilder()
+    b.AppendValues(["aaaa","你是谁","张三"])
+    b = b.Finish().Value()
+
+    table = arrow.Table.Make(schema = schema,arrays=[a,b])
+    fs = IPC_Writer(path_file,schema,with_stream = False)
+    fs.write_table(table)
+    fs.close()
+
+
+def test_read():
+
+    fs = IPC_MemoryMappedFileReader(path_file)
+    for i in range(fs.num_record_batches()):
+        batch = fs.read_batch(i)
+        print(batch)
+    fs.close()
+
+
+test_write()
+test_read()
+```
 
 
 ### 4. parquet demo
 
 
 ```python
 from tfrecords.python.io.arrow import ParquetWriter,IPC_StreamReader,ParquetReader,arrow
@@ -339,7 +371,10 @@
         print(type(x),x)
 
 
 test_write()
 test_read()
 ```
 
+
+
+
```

## Comparing `tfrecords-0.2.8.dist-info/RECORD` & `tfrecords-0.2.9.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,34 +1,35 @@
-tfrecords/README.md,sha256=vIfFXBBtpg5-j_s4qOTtvGICqmZcRR7epYupAef1Yus,8155
+tfrecords/README.md,sha256=xjZbPWXYT0tJy-1QXRPQU9bLY-i6m4mVAQDzZNZChE0,8979
 tfrecords/__init__.py,sha256=fDDq-a0ha5So6pFBTVkT9uZFTNNKZtdM9uY4JvyHM9k,69
 tfrecords/version_config.py,sha256=5mvBjbfRGGadH3fQih-qpfGPjL1Enc5tW2sf1qHroW4,36
 tfrecords/lib/__init__.py,sha256=flMtCQdSl76L4s9qfuWTgSkY9N7MYAnXgGxyROW1bzM,104
-tfrecords/lib/arrow_cc.pyd,sha256=F0vFz7BtbR3GKaNC24uag03h6giBqbiIiKgWfjMTFRA,18535424
+tfrecords/lib/arrow_cc.pyd,sha256=m6wdIpoHnV0Q9Y0wSlOa9ngu7sWsPX1brXQ2URyAvxI,18538496
 tfrecords/lib/tfrecords_cc.pyd,sha256=16GyQAUjRWRerA5kKp2ZbfGEkCWLwI4eUePP0OJHSzo,2313216
 tfrecords/python/__init__.py,sha256=dXyiJJWGgTpIa91FuHoXqYLV0pukHDL6IMgmuniu5Sc,255
 tfrecords/python/framework/__init__.py,sha256=_MVHfS1sIzl7wG47YxwykJfG4uoLX1scclSaaWosCTc,99
 tfrecords/python/framework/errors.py,sha256=kQQDvEgy2aryQf7QwEx7fPRSkea5YTSpF5KVNB1CVFg,1003
 tfrecords/python/framework/errors_impl.py,sha256=Kv6t_OqKqPvW28W44tcbzoq5W4aQ8K7nl4BhtvuWECQ,16075
 tfrecords/python/io/__init__.py,sha256=_1HH7zU199XVY5GrX6I0Ttl7-PzMZApeewPo4SK-hxU,123
-tfrecords/python/io/arrow.py,sha256=E1OMfEKzoaktlo8_E4-0Gka_PqbrRaHDap_GKGwajNA,21638
+tfrecords/python/io/arrow.py,sha256=61dGWQINa0jwDXHbCH3fVJ0xeP3Vd0xC4lBwo7GAM5k,25785
 tfrecords/python/io/file_io.py,sha256=FpP_vynumskuWR-kZSSuCjw-23D_iI5Km7fgIs5k4Ok,28567
 tfrecords/python/io/file_io_test.py,sha256=cSLybCcCAD2sOvH8S199_4B4kj-hC1uZ4sraOdcIGMI,27406
 tfrecords/python/io/gfile.py,sha256=eYjfHXEH47ejq7bxvLSi4lh6ZS_mDfEN6fKHdzxF244,701
 tfrecords/python/io/tf_leveldb.py,sha256=--LTkox2hVdTEJru-7yC5uE-_htX0GZsdC_eGn4yKBQ,7682
 tfrecords/python/io/tf_lmdb.py,sha256=FXO5ImH8JcXI4AEElh_DEFvCbUg6akNtWuB7Kl_Uk5M,6101
 tfrecords/python/io/tf_record.py,sha256=T4CVOR_yUykjVsiYmii_amrrJ-k1ipnzLgNU3q1lCsg,11427
 tfrecords/python/io/tf_record_test.py,sha256=udv2EmA5eEayeEyrKJf8GEMlGWO5819WRFF9yqJVM_0,24321
 tfrecords/python/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tfrecords/python/util/compat.py,sha256=qyBkxViFNC6f2BHG0Os5wShuEwkQVWrrgBH_XWSavss,6636
 tfrecords/python/util/tf_decorator.py,sha256=QBUn4Xsw2Q8mwKC-78DbKNPi7qFhhozrT0efxb16v0A,9664
 tfrecords/python/util/tf_export.py,sha256=KEaG3T1IACUL3f-pvYY_Mlhli7yLoQSi1uMQTcs8hrw,14016
 tfrecords/python/util/tf_inspect.py,sha256=Sq8jpuYurHYIzgUq82lfS8Dv8FWMgh2tiXfuMd7n3wg,15675
 tfrecords/testing/__init__.py,sha256=PkC-PFNHeqM4Jzr7SgS3jUKcx5hTGhXk2RAY0fiJy1k,77
-tfrecords/testing/demo_arrow.py,sha256=c6H_mhLZ8qAMYQ5uibYeWoG6DBgCxNVDANKPXULuoIg,1022
+tfrecords/testing/demo_arrow.py,sha256=DKfUR-iM3uHVI_j3Ap4tjcX-zkIDpTm4O2QB01KPI-I,1057
+tfrecords/testing/demo_arrow_file.py,sha256=pcgL1hQGqXielZ07cb3d9NRTUfnArkBS05znoc4Di48,1338
 tfrecords/testing/demo_leveldb.py,sha256=hMGgmZqtjCmw4d61twehQmD1V0oCwfyXgkaD1hjwTQs,1390
 tfrecords/testing/demo_lmdb.py,sha256=VOuRMR0xeKy1ogA7KuW_fp7zeMcxtZIhY0bU8uzHEp4,1650
 tfrecords/testing/demo_parquet.py,sha256=wSWKVVMjt9LTr6Dn4ji4D8oCkhUnyd6XYbTaHs0BjHc,1104
 tfrecords/testing/demo_record.py,sha256=YetIRGADaiLaJDMPQC7Wh3wg3r2WqfM74cQT6g-9XqE,2257
-tfrecords-0.2.8.dist-info/METADATA,sha256=9vueyXxEmgPYCC4ZEWqH1yG35DdKAHqdNujysyL_W4o,9513
-tfrecords-0.2.8.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
-tfrecords-0.2.8.dist-info/top_level.txt,sha256=9krMlLgZcGidjiMhgm9zyjo03vJRdFBDOdxobAEb0ps,10
-tfrecords-0.2.8.dist-info/RECORD,,
+tfrecords-0.2.9.dist-info/METADATA,sha256=27LYiq2SGjnX94dqISftYpM0sEmbASFK62xhbL4RxKU,10337
+tfrecords-0.2.9.dist-info/WHEEL,sha256=eep6QWEFiQfg2wcclssb_WY-D33AnLYLnEKGA9Rn-VU,100
+tfrecords-0.2.9.dist-info/top_level.txt,sha256=9krMlLgZcGidjiMhgm9zyjo03vJRdFBDOdxobAEb0ps,10
+tfrecords-0.2.9.dist-info/RECORD,,
```

